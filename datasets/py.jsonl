{"diff_id": 100, "repo": "ipython/ipython\n", "sha": "d49dfb2290a859f24a0967b6f70c51382ebdef4d\n", "time": "2011-07-28T12:46:57Z\n", "diff": "mmm a / IPython / lib / irunner . py <nl> ppp b / IPython / lib / irunner . py <nl> <nl> import os <nl> import sys <nl> <nl> - # Third - party modules . <nl> - import pexpect <nl> + # Third - party modules : we carry a copy of pexpect to reduce the need for <nl> + # external dependencies , but our import checks for a system version first . <nl> + from IPython . external import pexpect <nl> <nl> # Global usage strings , to avoid indentation issues when typing it below . <nl> USAGE = \" \" \" <nl>\n", "msg": "Use IPython . external for pexpect import .\n"}
{"diff_id": 103, "repo": "tornadoweb/tornado\n", "sha": "a3d255d3b3faeb0e830d2bbd292f3af1870ab710\n", "time": "2015-02-25T20:52:55Z\n", "diff": "mmm a / tornado / locks . py <nl> ppp b / tornado / locks . py <nl> <nl> __all__ = [ ' Condition ' , ' Event ' , ' Semaphore ' ] <nl> <nl> import collections <nl> - import contextlib <nl> <nl> from tornado import gen , ioloop <nl> from tornado . concurrent import Future <nl> def wait ( self , timeout = None ) : <nl> return gen . with_timeout ( timeout , self . _future ) <nl> <nl> <nl> - class _ContextManagerFuture ( Future ) : <nl> - \" \" \" A Future that can be used with the \" with \" statement . <nl> + class _ReleasingContextManager ( object ) : <nl> + \" \" \" Releases a Lock or Semaphore at the end of a \" with \" statement . <nl> <nl> - When a coroutine yields this Future , the return value is a context manager <nl> - that can be used like : <nl> - <nl> - with ( yield future ) : <nl> + with ( yield semaphore . acquire ( ) ) : <nl> pass <nl> <nl> - At the end of the block , the Future ' s exit callback is run . Used for <nl> - Lock . acquire and Semaphore . acquire . <nl> + # Now semaphore . release ( ) has been called . <nl> \" \" \" <nl> - def __init__ ( self , wrapped , exit_callback ) : <nl> - super ( _ContextManagerFuture , self ) . __init__ ( ) <nl> - gen . chain_future ( wrapped , self ) <nl> - self . exit_callback = exit_callback <nl> + def __init__ ( self , obj ) : <nl> + self . _obj = obj <nl> <nl> - def result ( self , timeout = None ) : <nl> - if self . exception ( ) : <nl> - raise self . exception ( ) <nl> + def __enter__ ( self ) : <nl> + pass <nl> <nl> - # Otherwise return a context manager that cleans up after the block . <nl> - @ contextlib . contextmanager <nl> - def f ( ) : <nl> - try : <nl> - yield <nl> - finally : <nl> - self . exit_callback ( ) <nl> - return f ( ) <nl> + def __exit__ ( self , exc_type , exc_val , exc_tb ) : <nl> + self . _obj . release ( ) <nl> <nl> <nl> class Semaphore ( object ) : <nl> def release ( self ) : <nl> for waiter in self . _waiters : <nl> if not waiter . done ( ) : <nl> self . _value - = 1 <nl> - waiter . set_result ( None ) <nl> + <nl> + # If the waiter is a coroutine paused at <nl> + # <nl> + # with ( yield semaphore . acquire ( ) ) : <nl> + # <nl> + # then the context manager ' s __exit__ calls release ( ) at the end <nl> + # of the \" with \" block . <nl> + waiter . set_result ( _ReleasingContextManager ( self ) ) <nl> break <nl> <nl> def acquire ( self , timeout = None ) : <nl> def acquire ( self , timeout = None ) : <nl> \" \" \" <nl> if self . _value > 0 : <nl> self . _value - = 1 <nl> - future = gen . _null_future <nl> + future = Future ( ) <nl> + future . set_result ( _ReleasingContextManager ( self ) ) <nl> else : <nl> waiter = Future ( ) <nl> self . _waiters . append ( waiter ) <nl> def acquire ( self , timeout = None ) : <nl> gen . chain_future ( future , waiter ) <nl> else : <nl> future = waiter <nl> - return _ContextManagerFuture ( future , self . release ) <nl> + return future <nl> <nl> def __enter__ ( self ) : <nl> raise RuntimeError ( <nl>\n", "msg": "Simpler code for Semaphore . acquire ( ) as a context manager .\n"}
{"diff_id": 105, "repo": "python/cpython\n", "sha": "249cbe794e048d7c6c34169d4b59529f1c0baedb\n", "time": "2008-01-28T01:33:23Z\n", "diff": "mmm a / Lib / test / test_resource . py <nl> ppp b / Lib / test / test_resource . py <nl> <nl> limit_set = 0 <nl> f = open ( TESTFN , \" wb \" ) <nl> f . write ( \" X \" * 1024 ) <nl> + f . flush ( ) <nl> try : <nl> f . write ( \" Y \" ) <nl> f . flush ( ) <nl> <nl> for i in range ( 5 ) : <nl> time . sleep ( . 1 ) <nl> f . flush ( ) <nl> + f . close ( ) <nl> except IOError : <nl> if not limit_set : <nl> raise <nl>\n", "msg": "Try harder to provoke the exception since the ia64 buildbot still\n"}
{"diff_id": 125, "repo": "zulip/zulip\n", "sha": "4a8c70593f2daba8f96aaf3adad65526e0cde496\n", "time": "2019-12-28T18:56:03Z\n", "diff": "mmm a / zilencer / management / commands / populate_db . py <nl> ppp b / zilencer / management / commands / populate_db . py <nl> def handle ( self , * * options : Any ) - > None : <nl> ( \" aaron \" , \" AARON @ zulip . com \" ) , <nl> ( \" Polonius \" , \" polonius @ zulip . com \" ) , <nl> ] <nl> - for i in range ( options [ \" extra_users \" ] ) : <nl> - names . append ( ( ' Extra User % d ' % ( i , ) , ' extrauser % d @ zulip . com ' % ( i , ) ) ) <nl> + <nl> + # For testing really large batches : <nl> + # Create extra users with semi realistic names to make search <nl> + # functions somewhat realistic . We ' ll still create 1000 users <nl> + # like Extra222 User for some predicability . <nl> + num_names = options [ ' extra_users ' ] <nl> + num_boring_names = 1000 <nl> + <nl> + for i in range ( min ( num_names , num_boring_names ) ) : <nl> + full_name = ' Extra % 03d User ' % ( i , ) <nl> + names . append ( ( full_name , ' extrauser % d @ zulip . com ' % ( i , ) ) ) <nl> + <nl> + if num_names > num_boring_names : <nl> + fnames = [ ' Amber ' , ' Arpita ' , ' Bob ' , ' Cindy ' , ' Daniela ' , ' Dan ' , ' Dinesh ' , <nl> + ' Faye ' , ' Fran\u00e7ois ' , ' George ' , ' Hank ' , ' Irene ' , <nl> + ' James ' , ' Janice ' , ' Jenny ' , ' Jill ' , ' John ' , <nl> + ' Kate ' , ' Katelyn ' , ' Kobe ' , ' Lexi ' , ' Manish ' , ' Mark ' , ' Matt ' , ' Mayna ' , <nl> + ' Michael ' , ' Pete ' , ' Peter ' , ' Phil ' , ' Phillipa ' , ' Preston ' , <nl> + ' Sally ' , ' Scott ' , ' Sandra ' , ' Steve ' , ' Stephanie ' , <nl> + ' Vera ' ] <nl> + mnames = [ ' de ' , ' van ' , ' von ' , ' Shaw ' , ' T . ' ] <nl> + lnames = [ ' Adams ' , ' Agarwal ' , ' Beal ' , ' Benson ' , ' Bonita ' , ' Davis ' , <nl> + ' George ' , ' Harden ' , ' James ' , ' Jones ' , ' Johnson ' , ' Jordan ' , <nl> + ' Lee ' , ' Leonard ' , ' Singh ' , ' Smith ' , ' Patel ' , ' Towns ' , ' Wall ' ] <nl> + <nl> + for i in range ( num_boring_names , num_names ) : <nl> + fname = random . choice ( fnames ) + str ( i ) <nl> + full_name = fname <nl> + if random . random ( ) < 0 . 7 : <nl> + if random . random ( ) < 0 . 5 : <nl> + full_name + = ' ' + random . choice ( mnames ) <nl> + full_name + = ' ' + random . choice ( lnames ) <nl> + email = fname . lower ( ) + ' @ zulip . com ' <nl> + names . append ( ( full_name , email ) ) <nl> + <nl> create_users ( zulip_realm , names ) <nl> <nl> iago = get_user ( \" iago @ zulip . com \" , zulip_realm ) <nl>\n", "msg": "populate_db : Add random names for large user batches .\n"}
{"diff_id": 219, "repo": "ansible/ansible\n", "sha": "33863eb653f3ed4d6f30ab816743443f473c5eae\n", "time": "2015-12-16T15:47:09Z\n", "diff": "mmm a / lib / ansible / module_utils / urls . py <nl> ppp b / lib / ansible / module_utils / urls . py <nl> class NoSSLError ( SSLValidationError ) : <nl> \" \" \" Needed to connect to an HTTPS url but no ssl library available to verify the certificate \" \" \" <nl> pass <nl> <nl> + # Some environments ( Google Compute Engine ' s CoreOS deploys ) do not compile <nl> + # against openssl and thus do not have any HTTPS support . <nl> + CustomHTTPSConnection = CustomHTTPSHandler = None <nl> + if hasattr ( httplib , ' HTTPSConnection ' ) and hasattr ( urllib2 , ' HTTPSHandler ' ) : <nl> + class CustomHTTPSConnection ( httplib . HTTPSConnection ) : <nl> + def __init__ ( self , * args , * * kwargs ) : <nl> + httplib . HTTPSConnection . __init__ ( self , * args , * * kwargs ) <nl> + if HAS_SSLCONTEXT : <nl> + self . context = create_default_context ( ) <nl> + if self . cert_file : <nl> + self . context . load_cert_chain ( self . cert_file , self . key_file ) <nl> + <nl> + def connect ( self ) : <nl> + \" Connect to a host on a given ( SSL ) port . \" <nl> + <nl> + if hasattr ( self , ' source_address ' ) : <nl> + sock = socket . create_connection ( ( self . host , self . port ) , self . timeout , self . source_address ) <nl> + else : <nl> + sock = socket . create_connection ( ( self . host , self . port ) , self . timeout ) <nl> + <nl> + server_hostname = self . host <nl> + # Note : self . _tunnel_host is not available on py < 2 . 6 but this code <nl> + # isn ' t used on py < 2 . 6 ( lack of create_connection ) <nl> + if self . _tunnel_host : <nl> + self . sock = sock <nl> + self . _tunnel ( ) <nl> + server_hostname = self . _tunnel_host <nl> + <nl> + if HAS_SSLCONTEXT : <nl> + self . sock = self . context . wrap_socket ( sock , server_hostname = server_hostname ) <nl> + else : <nl> + self . sock = ssl . wrap_socket ( sock , keyfile = self . key_file , certfile = self . cert_file , ssl_version = PROTOCOL ) <nl> <nl> - class CustomHTTPSConnection ( httplib . HTTPSConnection ) : <nl> - def __init__ ( self , * args , * * kwargs ) : <nl> - httplib . HTTPSConnection . __init__ ( self , * args , * * kwargs ) <nl> - if HAS_SSLCONTEXT : <nl> - self . context = create_default_context ( ) <nl> - if self . cert_file : <nl> - self . context . load_cert_chain ( self . cert_file , self . key_file ) <nl> - <nl> - def connect ( self ) : <nl> - \" Connect to a host on a given ( SSL ) port . \" <nl> - <nl> - if hasattr ( self , ' source_address ' ) : <nl> - sock = socket . create_connection ( ( self . host , self . port ) , self . timeout , self . source_address ) <nl> - else : <nl> - sock = socket . create_connection ( ( self . host , self . port ) , self . timeout ) <nl> - <nl> - server_hostname = self . host <nl> - # Note : self . _tunnel_host is not available on py < 2 . 6 but this code <nl> - # isn ' t used on py < 2 . 6 ( lack of create_connection ) <nl> - if self . _tunnel_host : <nl> - self . sock = sock <nl> - self . _tunnel ( ) <nl> - server_hostname = self . _tunnel_host <nl> - <nl> - if HAS_SSLCONTEXT : <nl> - self . sock = self . context . wrap_socket ( sock , server_hostname = server_hostname ) <nl> - else : <nl> - self . sock = ssl . wrap_socket ( sock , keyfile = self . key_file , certfile = self . cert_file , ssl_version = PROTOCOL ) <nl> - <nl> - class CustomHTTPSHandler ( urllib2 . HTTPSHandler ) : <nl> + class CustomHTTPSHandler ( urllib2 . HTTPSHandler ) : <nl> <nl> - def https_open ( self , req ) : <nl> - return self . do_open ( CustomHTTPSConnection , req ) <nl> + def https_open ( self , req ) : <nl> + return self . do_open ( CustomHTTPSConnection , req ) <nl> <nl> - https_request = urllib2 . AbstractHTTPHandler . do_request_ <nl> + https_request = urllib2 . AbstractHTTPHandler . do_request_ <nl> <nl> def generic_urlparse ( parts ) : <nl> ' ' ' <nl> def open_url ( url , data = None , headers = None , method = None , use_proxy = True , <nl> handlers . append ( proxyhandler ) <nl> <nl> # pre - 2 . 6 versions of python cannot use the custom https <nl> - # handler , since the socket class is lacking this method <nl> - if hasattr ( socket , ' create_connection ' ) : <nl> + # handler , since the socket class is lacking create_connection . <nl> + # Some python builds lack HTTPS support . <nl> + if hasattr ( socket , ' create_connection ' ) and CustomHTTPSHandler : <nl> handlers . append ( CustomHTTPSHandler ) <nl> <nl> opener = urllib2 . build_opener ( * handlers ) <nl>\n", "msg": "Conditionally create the CustomHTTPSConnection class only if we have the required baseclasses .\n"}
{"diff_id": 268, "repo": "ytdl-org/youtube-dl\n", "sha": "fd28827864f94aee1cb4103179b6c4965f0b6641\n", "time": "2014-01-23T18:04:22Z\n", "diff": "mmm a / youtube_dl / YoutubeDL . py <nl> ppp b / youtube_dl / YoutubeDL . py <nl> def report_file_already_downloaded ( self , file_name ) : <nl> except UnicodeEncodeError : <nl> self . to_screen ( ' [ download ] The file has already been downloaded ' ) <nl> <nl> - def increment_downloads ( self ) : <nl> - \" \" \" Increment the ordinal that assigns a number to each file . \" \" \" <nl> - self . _num_downloads + = 1 <nl> - <nl> def prepare_filename ( self , info_dict ) : <nl> \" \" \" Generate the output filename . \" \" \" <nl> try : <nl> def process_info ( self , info_dict ) : <nl> \" \" \" Process a single resolved IE result . \" \" \" <nl> <nl> assert info_dict . get ( ' _type ' , ' video ' ) = = ' video ' <nl> - # We increment the download the download count here to match the previous behaviour . <nl> - self . increment_downloads ( ) <nl> + <nl> + max_downloads = self . params . get ( ' max_downloads ' ) <nl> + if max_downloads is not None : <nl> + if self . _num_downloads > = int ( max_downloads ) : <nl> + raise MaxDownloadsReached ( ) <nl> <nl> info_dict [ ' fulltitle ' ] = info_dict [ ' title ' ] <nl> if len ( info_dict [ ' title ' ] ) > 200 : <nl> def process_info ( self , info_dict ) : <nl> self . to_screen ( ' [ download ] ' + reason ) <nl> return <nl> <nl> - max_downloads = self . params . get ( ' max_downloads ' ) <nl> - if max_downloads is not None : <nl> - if self . _num_downloads > int ( max_downloads ) : <nl> - raise MaxDownloadsReached ( ) <nl> + self . _num_downloads + = 1 <nl> <nl> filename = self . prepare_filename ( info_dict ) <nl> <nl>\n", "msg": "Do not count unmatched videos for - - max - downloads ( Fixes )\n"}
{"diff_id": 286, "repo": "python/cpython\n", "sha": "65fe8dda1588017bb5f15c8ab2fdbfb3b3c0bac7\n", "time": "2002-11-06T23:15:51Z\n", "diff": "new file mode 100755 <nl> index 0000000000000 . . 93ef14b4831a8 <nl> mmm / dev / null <nl> ppp b / Mac / scripts / buildappbundle . py <nl> <nl> + import sys <nl> + import os <nl> + import shutil <nl> + import getopt <nl> + <nl> + def buildappbundle ( executable , output = None , copyfunc = None , creator = None , <nl> + plist = None , nib = None , resources = None ) : <nl> + if not output : <nl> + output = os . path . split ( executable ) [ 1 ] + ' . app ' <nl> + if not copyfunc : <nl> + copyfunc = shutil . copy2 <nl> + if not creator : <nl> + creator = ' ? ? ? ? ' <nl> + if not resources : <nl> + resources = [ ] <nl> + if nib : <nl> + resources = resources + [ nib ] <nl> + # <nl> + # Create the main directory structure <nl> + # <nl> + if not os . path . isdir ( output ) : <nl> + os . mkdir ( output ) <nl> + contents = os . path . join ( output , ' Contents ' ) <nl> + if not os . path . isdir ( contents ) : <nl> + os . mkdir ( contents ) <nl> + macos = os . path . join ( contents , ' MacOS ' ) <nl> + if not os . path . isdir ( macos ) : <nl> + os . mkdir ( macos ) <nl> + # <nl> + # Create the executable <nl> + # <nl> + shortname = os . path . split ( executable ) [ 1 ] <nl> + execname = os . path . join ( macos , shortname ) <nl> + try : <nl> + os . remove ( execname ) <nl> + except OSError : <nl> + pass <nl> + copyfunc ( executable , execname ) <nl> + # <nl> + # Create the PkgInfo file <nl> + # <nl> + pkginfo = os . path . join ( contents , ' PkgInfo ' ) <nl> + open ( pkginfo , ' wb ' ) . write ( ' APPL ' + creator ) <nl> + if plist : <nl> + # A plist file is specified . Read it . <nl> + plistdata = open ( plist ) . read ( ) <nl> + else : <nl> + # <nl> + # If we have a main NIB we create the extra Cocoa specific info for the plist file <nl> + # <nl> + if not nib : <nl> + nibname = \" \" <nl> + else : <nl> + nibname , ext = os . path . splitext ( os . path . split ( nib ) [ 1 ] ) <nl> + if ext = = ' . lproj ' : <nl> + # Special case : if the main nib is a . lproj we assum a directory <nl> + # and use the first nib from there <nl> + files = os . listdir ( nib ) <nl> + for f in files : <nl> + if f [ - 4 : ] = = ' . nib ' : <nl> + nibname = os . path . split ( f ) [ 1 ] [ : - 4 ] <nl> + break <nl> + else : <nl> + nibname = \" \" <nl> + if nibname : <nl> + cocoainfo = \" \" \" <nl> + < key > NSMainNibFile < / key > <nl> + < string > % s < / string > <nl> + < key > NSPrincipalClass < / key > <nl> + < string > NSApplication < / string > \" \" \" % nibname <nl> + else : <nl> + cocoainfo = \" \" <nl> + plistdata = \\ <nl> + \" \" \" < ? xml version = \" 1 . 0 \" encoding = \" UTF - 8 \" ? > <nl> + < ! DOCTYPE plist SYSTEM \" file : / / localhost / System / Library / DTDs / PropertyList . dtd \" > <nl> + < plist version = \" 0 . 9 \" > <nl> + < dict > <nl> + < key > CFBundleDevelopmentRegion < / key > <nl> + < string > English < / string > <nl> + < key > CFBundleExecutable < / key > <nl> + < string > % s < / string > <nl> + < key > CFBundleInfoDictionaryVersion < / key > <nl> + < string > 6 . 0 < / string > <nl> + < key > CFBundlePackageType < / key > <nl> + < string > APPL < / string > <nl> + < key > CFBundleSignature < / key > <nl> + < string > % s < / string > <nl> + < key > CFBundleVersion < / key > <nl> + < string > 0 . 1 < / string > <nl> + % s <nl> + < / dict > <nl> + < / plist > <nl> + \" \" \" % ( shortname , creator , cocoainfo ) <nl> + # <nl> + # Next , we create the plist file <nl> + # <nl> + infoplist = os . path . join ( contents , ' Info . plist ' ) <nl> + open ( infoplist , ' w ' ) . write ( plistdata ) <nl> + # <nl> + # Finally , if there are nibs or other resources to copy we do so . <nl> + # <nl> + if resources : <nl> + resdir = os . path . join ( contents , ' Resources ' ) <nl> + for src in resources : <nl> + dst = os . path . join ( resdir , os . path . split ( src ) [ 1 ] ) <nl> + if os . path . isdir ( src ) : <nl> + shutil . copytree ( src , dst ) <nl> + else : <nl> + shutil . copy2 ( src , dst ) <nl> + <nl> + def usage ( ) : <nl> + print \" buildappbundle creates an application bundle \" <nl> + print \" Usage : \" <nl> + print \" buildappbundle [ options ] executable \" <nl> + print \" Options : \" <nl> + print \" - - output o Output file ; default executable with . app appended , short - o \" <nl> + print \" - - link Symlink the executable ( default : copy ) , short - l \" <nl> + print \" - - plist file Plist file ( default : generate one ) , short - p \" <nl> + print \" - - nib file Main nib file or lproj folder for Cocoa program , short - n \" <nl> + print \" - - resource r Extra resource file to be copied to Resources , short - r \" <nl> + print \" - - creator c 4 - char creator code ( default : ? ? ? ? ) , short - c \" <nl> + print \" - - help This message , short - ? \" <nl> + sys . exit ( 1 ) <nl> + <nl> + def main ( ) : <nl> + output = None <nl> + copyfunc = None <nl> + creator = None <nl> + plist = None <nl> + nib = None <nl> + resources = [ ] <nl> + SHORTOPTS = \" o : ln : r : p : c : ? \" <nl> + LONGOPTS = ( \" output = \" , \" link \" , \" nib = \" , \" resource = \" , \" plist = \" , \" creator = \" , \" help \" ) <nl> + try : <nl> + options , args = getopt . getopt ( sys . argv [ 1 : ] , SHORTOPTS , LONGOPTS ) <nl> + except getopt . error : <nl> + usage ( ) <nl> + if len ( args ) ! = 1 : <nl> + usage ( ) <nl> + for opt , arg in options : <nl> + if opt in ( ' - o ' , ' - - output ' ) : <nl> + output = arg <nl> + elif opt in ( ' - l ' , ' - - link ' ) : <nl> + copyfunc = os . symlink <nl> + elif opt in ( ' - n ' , ' - - nib ' ) : <nl> + nib = arg <nl> + elif opt in ( ' - r ' , ' - - resource ' ) : <nl> + resources . append ( arg ) <nl> + elif opt in ( ' - c ' , ' - - creator ' ) : <nl> + creator = arg <nl> + elif opt in ( ' - p ' , ' - - plist ' ) : <nl> + plist = arg <nl> + elif opt in ( ' - ? ' , ' - - help ' ) : <nl> + usage ( ) <nl> + buildappbundle ( args [ 0 ] , output = output , copyfunc = copyfunc , creator = creator , <nl> + plist = plist , resources = resources ) <nl> + <nl> + if __name__ = = ' __main__ ' : <nl> + main ( ) <nl> + <nl> \\ No newline at end of file <nl>\n", "msg": "Script to create . app bundles . Largely untested .\n"}
{"diff_id": 372, "repo": "scikit-learn/scikit-learn\n", "sha": "401d2a178c8b4eb6ebdcea493f85ded41d41d4dd\n", "time": "2016-08-28T23:30:49Z\n", "diff": "mmm a / sklearn / model_selection / tests / test_search . py <nl> ppp b / sklearn / model_selection / tests / test_search . py <nl> def test_grid_search_incorrect_param_grid ( ) : <nl> { ' C ' : 1 } ) <nl> <nl> <nl> - def test_grid_search_incorrect_param_grid ( ) : <nl> + def test_grid_search_param_grid_includes_sequence_of_a_zero_length ( ) : <nl> assert_raise_message ( <nl> ValueError , <nl> \" Parameter values for parameter ( C ) need to be a non - empty sequence . \" , <nl>\n", "msg": "Test method test_grid_search_incorrect_param_grid has been renamed to test_grid_search_param_grid_includes_sequence_of_a_zero_length .\n"}
{"diff_id": 633, "repo": "scikit-learn/scikit-learn\n", "sha": "62f1f57ce586fb8c74be6d1e0a736565c9f04b45\n", "time": "2014-07-23T14:31:23Z\n", "diff": "mmm a / doc / sphinxext / gen_rst . py <nl> ppp b / doc / sphinxext / gen_rst . py <nl> def generate_example_rst ( app ) : <nl> examples . <nl> \" \" \" <nl> root_dir = os . path . join ( app . builder . srcdir , ' auto_examples ' ) <nl> - example_dir = os . path . abspath ( app . builder . srcdir + ' / . . / ' + ' examples ' ) <nl> + example_dir = os . path . abspath ( os . path . join ( app . builder . srcdir , ' . . ' , <nl> + ' examples ' ) ) <nl> + generated_dir = os . path . abspath ( os . path . join ( app . builder . srcdir , <nl> + ' modules ' , ' generated ' ) ) <nl> + <nl> try : <nl> plot_gallery = eval ( app . builder . config . plot_gallery ) <nl> except TypeError : <nl> def generate_example_rst ( app ) : <nl> os . makedirs ( example_dir ) <nl> if not os . path . exists ( root_dir ) : <nl> os . makedirs ( root_dir ) <nl> + if not os . path . exists ( generated_dir ) : <nl> + os . makedirs ( generated_dir ) <nl> <nl> # we create an index . rst with all examples <nl> fhindex = open ( os . path . join ( root_dir , ' index . rst ' ) , ' w ' ) <nl> - # Note : The sidebar button has been removed from the examples page for now <nl> + # Note : The sidebar button has been removed from the examples page for now <nl> # due to how it messes up the layout . Will be fixed at a later point <nl> fhindex . write ( \" \" \" \\ <nl> <nl>\n", "msg": "Added directory checking for documentation builds , and corrected for Windows pathing\n"}
{"diff_id": 647, "repo": "home-assistant/core\n", "sha": "caedc14b00044f63c79cc90eb3ef01116c6bc2e7\n", "time": "2019-11-08T17:48:28Z\n", "diff": "mmm a / homeassistant / components / saj / sensor . py <nl> ppp b / homeassistant / components / saj / sensor . py <nl> <nl> from homeassistant . const import ( <nl> CONF_HOST , <nl> CONF_PASSWORD , <nl> + CONF_NAME , <nl> CONF_TYPE , <nl> CONF_USERNAME , <nl> DEVICE_CLASS_POWER , <nl> <nl> PLATFORM_SCHEMA = PLATFORM_SCHEMA . extend ( <nl> { <nl> vol . Required ( CONF_HOST ) : cv . string , <nl> + vol . Optional ( CONF_NAME ) : cv . string , <nl> vol . Optional ( CONF_TYPE , default = INVERTER_TYPES [ 0 ] ) : vol . In ( INVERTER_TYPES ) , <nl> vol . Inclusive ( CONF_USERNAME , \" credentials \" ) : cv . string , <nl> vol . Inclusive ( CONF_PASSWORD , \" credentials \" ) : cv . string , <nl> async def async_setup_platform ( hass , config , async_add_entities , discovery_info = <nl> hass_sensors = [ ] <nl> <nl> for sensor in sensor_def : <nl> - hass_sensors . append ( SAJsensor ( sensor ) ) <nl> + hass_sensors . append ( SAJsensor ( sensor , inverter_name = config . get ( CONF_NAME ) ) ) <nl> <nl> kwargs = { } <nl> - <nl> if wifi : <nl> kwargs [ \" wifi \" ] = True <nl> if config . get ( CONF_USERNAME ) and config . get ( CONF_PASSWORD ) : <nl> def remove_listener ( ) : <nl> class SAJsensor ( Entity ) : <nl> \" \" \" Representation of a SAJ sensor . \" \" \" <nl> <nl> - def __init__ ( self , pysaj_sensor ) : <nl> + def __init__ ( self , pysaj_sensor , inverter_name = None ) : <nl> \" \" \" Initialize the sensor . \" \" \" <nl> self . _sensor = pysaj_sensor <nl> + self . _inverter_name = inverter_name <nl> self . _state = self . _sensor . value <nl> <nl> @ property <nl> def name ( self ) : <nl> \" \" \" Return the name of the sensor . \" \" \" <nl> + if self . _inverter_name : <nl> + return f \" saj_ { self . _inverter_name } _ { self . _sensor . name } \" <nl> + <nl> return f \" saj_ { self . _sensor . name } \" <nl> <nl> @ property <nl> def async_update_values ( self , unknown_state = False ) : <nl> @ property <nl> def unique_id ( self ) : <nl> \" \" \" Return a unique identifier for this sensor . \" \" \" <nl> + if self . _inverter_name : <nl> + return f \" { self . _inverter_name } _ { self . _sensor . name } \" <nl> + <nl> return f \" { self . _sensor . name } \" <nl>\n", "msg": "Added support for multiple SAJ solar inverters ( )\n"}
{"diff_id": 652, "repo": "numpy/numpy\n", "sha": "10a973034d360f559809c97f120f42b7f450a20e\n", "time": "2008-11-16T11:44:02Z\n", "diff": "mmm a / numpy / distutils / mingw32ccompiler . py <nl> ppp b / numpy / distutils / mingw32ccompiler . py <nl> def build_import_library ( ) : <nl> # msg = \" Couldn ' t find import library , and failed to build it . \" <nl> # raise DistutilsPlatformError , msg <nl> return <nl> + <nl> + # Functions to deal with visual studio manifests . Manifest are a mechanism to <nl> + # enforce strong DLL versioning on windows , and has nothing to do with <nl> + # distutils MANIFEST . manifests are XML files with version info , and used by <nl> + # the OS loader ; they are necessary when linking against a DLL no in the system <nl> + # path ; in particular , python 2 . 6 is built against the MS runtime 9 ( the one <nl> + # from VS 2008 ) , which is not available on most windows systems ; python 2 . 6 <nl> + # installer does install it in the Win SxS ( Side by side ) directory , but this <nl> + # requires the manifest too . This is a big mess , thanks MS for a wonderful <nl> + # system . <nl> + <nl> + # XXX : ideally , we should use exactly the same version as used by python , but I <nl> + # have no idea how to obtain the exact version . <nl> + _MSVCRVER_TO_FULLVER = { ' 90 ' : \" 9 . 0 . 21022 . 8 \" } <nl> + <nl> + def msvc_manifest_xml ( maj , min ) : <nl> + \" \" \" Given a major and minor version of the MSVCR , returns the <nl> + corresponding XML file . \" \" \" <nl> + try : <nl> + fullver = _MSVCRVER_TO_FULLVER [ str ( maj * 10 + min ) ] <nl> + except KeyError : <nl> + raise ValueError ( \" Version % d , % d of MSVCRT not supported yet \" \\ <nl> + % ( maj , min ) ) <nl> + # Don ' t be fooled , it looks like an XML , but it is not . In particular , it <nl> + # should not have any space before starting , and its size should be <nl> + # divisible by 4 , most likely for alignement constraints when the xml is <nl> + # embedded in the binary . . . <nl> + # This template was copied directly from the python 2 . 6 binary ( using <nl> + # strings . exe from mingw on python . exe ) . <nl> + template = \" \" \" \\ <nl> + < assembly xmlns = \" urn : schemas - microsoft - com : asm . v1 \" manifestVersion = \" 1 . 0 \" > <nl> + < trustInfo xmlns = \" urn : schemas - microsoft - com : asm . v3 \" > <nl> + < security > <nl> + < requestedPrivileges > <nl> + < requestedExecutionLevel level = \" asInvoker \" uiAccess = \" false \" > < / requestedExecutionLevel > <nl> + < / requestedPrivileges > <nl> + < / security > <nl> + < / trustInfo > <nl> + < dependency > <nl> + < dependentAssembly > <nl> + < assemblyIdentity type = \" win32 \" name = \" Microsoft . VC % ( maj ) d % ( min ) d . CRT \" version = \" % ( fullver ) s \" processorArchitecture = \" * \" publicKeyToken = \" 1fc8b3b9a1e18e3b \" > < / assemblyIdentity > <nl> + < / dependentAssembly > <nl> + < / dependency > <nl> + < / assembly > \" \" \" <nl> + <nl> + return template % { ' fullver ' : fullver , ' maj ' : maj , ' min ' : min } <nl>\n", "msg": "Add a function to get the content of the xml version of manifest to deal with VS .\n"}
{"diff_id": 721, "repo": "zulip/zulip\n", "sha": "e78ef47487c505d210f3db822ca6f4a4469bd9e6\n", "time": "2013-03-15T20:37:40Z\n", "diff": "mmm a / zephyr / lib / cache_helpers . py <nl> ppp b / zephyr / lib / cache_helpers . py <nl> def cache_save_message ( message ) : <nl> <nl> @ cache_with_key ( message_cache_key ) <nl> def cache_get_message ( message_id ) : <nl> - return Message . objects . select_related ( \" sending_client \" , \" sender \" ) . get ( id = message_id ) <nl> + return Message . objects . select_related ( ) . get ( id = message_id ) <nl> <nl> # Called on Tornado startup to ensure our message cache isn ' t empty <nl> def populate_message_cache ( ) : <nl> items_for_memcached = { } <nl> - for m in Message . objects . select_related ( \" sending_client \" , \" sender \" ) . all ( ) . order_by ( <nl> - \" - id \" ) [ 0 : MESSAGE_CACHE_SIZE ] : <nl> + for m in Message . objects . select_related ( ) . all ( ) . order_by ( \" - id \" ) [ 0 : MESSAGE_CACHE_SIZE ] : <nl> items_for_memcached [ message_cache_key ( m . id ) ] = ( m , ) <nl> <nl> djcache . set_many ( items_for_memcached , timeout = 3600 * 24 ) <nl>\n", "msg": "message_cache : Query related models to avoid more queries later .\n"}
{"diff_id": 727, "repo": "scrapy/scrapy\n", "sha": "29d6bcf0d10f929b501aac5bfbb33a1eede7a64d\n", "time": "2012-04-20T12:24:30Z\n", "diff": "mmm a / scrapy / http / request / form . py <nl> ppp b / scrapy / http / request / form . py <nl> def _get_inputs ( form , formdata , dont_click , clickdata , response ) : <nl> def _value ( ele ) : <nl> n = ele . name <nl> v = ele . value <nl> - # Match browser behaviour on simple select tag without options selected <nl> - # Or for select tags wihout options <nl> - if v is None and ele . tag = = ' select ' and not ele . multiple : <nl> + if ele . tag = = ' select ' : <nl> + return _select_value ( ele , n , v ) <nl> + return n , v <nl> + <nl> + def _select_value ( ele , n , v ) : <nl> + multiple = ele . multiple <nl> + if v is None and not multiple : <nl> + # Match browser behaviour on simple select tag without options selected <nl> + # And for select tags wihout options <nl> o = ele . value_options <nl> - if o : <nl> - return n , o [ 0 ] <nl> - else : <nl> - return None , None <nl> + return ( n , o [ 0 ] ) if o else ( None , None ) <nl> + elif v is not None and multiple : <nl> + # This is a workround to bug in lxml fixed 2 . 3 . 1 <nl> + # fix https : / / github . com / lxml / lxml / commit / 57f49eed82068a20da3db8f1b18ae00c1bab8b12 # L1L1139 <nl> + selected_options = ele . xpath ( ' . / / option [ @ selected ] ' ) <nl> + v = [ ( o . get ( ' value ' ) or o . text or u ' ' ) . strip ( ) for o in selected_options ] <nl> return n , v <nl> <nl> + <nl> def _get_clickable ( clickdata , form ) : <nl> \" \" \" <nl> Returns the clickable element specified in clickdata , <nl>\n", "msg": "Workaround bug in lxml for multiple select options\n"}
{"diff_id": 751, "repo": "ipython/ipython\n", "sha": "d7d57eb4b8d5f1506cff191cd1fd6eba665867c7\n", "time": "2011-11-30T07:48:05Z\n", "diff": "mmm a / IPython / core / release . py <nl> ppp b / IPython / core / release . py <nl> <nl> _version_major = 0 <nl> _version_minor = 12 <nl> _version_micro = ' ' # use ' ' for first of series , number for 1 and above <nl> - _version_extra = ' dev ' <nl> + _version_extra = ' beta ' <nl> # _version_extra = ' ' # Uncomment this for full releases <nl> <nl> # Construct full version string from these . <nl>\n", "msg": "Change version number to beta to start the release cycle .\n"}
{"diff_id": 780, "repo": "zulip/zulip\n", "sha": "2f4b62048e93f1eed9ce478ef056b0d3989b2725\n", "time": "2013-02-12T21:25:45Z\n", "diff": "mmm a / zephyr / lib / actions . py <nl> ppp b / zephyr / lib / actions . py <nl> def gather_subscriptions ( user_profile ) : <nl> with_color = StreamColor . objects . filter ( subscription__in = subs ) . select_related ( ) <nl> no_color = subs . exclude ( id__in = with_color . values ( ' subscription_id ' ) ) . select_related ( ) <nl> <nl> + stream_ids = [ sc . subscription . recipient . type_id for sc in with_color ] + \\ <nl> + [ sub . recipient . type_id for sub in no_color ] <nl> + <nl> + stream_hash = { } <nl> + for stream in Stream . objects . filter ( id__in = stream_ids ) : <nl> + stream_hash [ stream . id ] = ( stream . name , stream . invite_only ) <nl> + <nl> result = [ ] <nl> for sc in with_color : <nl> - stream_name = get_display_recipient ( sc . subscription . recipient ) <nl> + ( stream_name , invite_only ) = stream_hash [ sc . subscription . recipient . type_id ] <nl> result . append ( { ' name ' : stream_name , <nl> ' in_home_view ' : sc . subscription . in_home_view , <nl> - ' invite_only ' : get_stream ( stream_name , user_profile . realm ) . invite_only , <nl> + ' invite_only ' : invite_only , <nl> ' color ' : sc . color } ) <nl> for sub in no_color : <nl> - stream_name = get_display_recipient ( sub . recipient ) <nl> + ( stream_name , invite_only ) = stream_hash [ sub . recipient . type_id ] <nl> result . append ( { ' name ' : stream_name , <nl> ' in_home_view ' : sub . in_home_view , <nl> - ' invite_only ' : get_stream ( stream_name , user_profile . realm ) . invite_only , <nl> + ' invite_only ' : invite_only , <nl> ' color ' : StreamColor . DEFAULT_STREAM_COLOR } ) <nl> <nl> return sorted ( result ) <nl>\n", "msg": "gather_subscriptions : Fix making O ( streams ) database queries .\n"}
{"diff_id": 848, "repo": "ipython/ipython\n", "sha": "7eb92cb8b61af9ac4ed13a33e7f5a3dbd4ccc3ce\n", "time": "2011-02-15T16:21:35Z\n", "diff": "mmm a / IPython / external / qt . py <nl> ppp b / IPython / external / qt . py <nl> <nl> from PySide import QtCore , QtGui , QtSvg <nl> <nl> else : <nl> - raise RuntimeError ( ' Invalid Qt API \" % s \" ' % QT_API ) <nl> + raise RuntimeError ( ' Invalid Qt API % r , valid values are : % r or % r ' % <nl> + ( QT_API , QT_API_PYQT , QT_API_PYSIDE ) ) <nl>\n", "msg": "Improved error message for Qt API switcher .\n"}
{"diff_id": 863, "repo": "matplotlib/matplotlib\n", "sha": "ba1f492e796e3baa48f8a51df19e6d8e2e3a7cde\n", "time": "2014-09-27T18:41:27Z\n", "diff": "mmm a / lib / matplotlib / tests / test_colors . py <nl> ppp b / lib / matplotlib / tests / test_colors . py <nl> def test_light_source_topo_surface ( ) : <nl> ax . imshow ( rgb ) <nl> ax . set ( xticks = [ ] , yticks = [ ] ) <nl> <nl> + <nl> def test_light_source_shading_default ( ) : <nl> \" \" \" Array comparison test for the default \" hsv \" blend mode . Ensure the <nl> default result doesn ' t change without warning . \" \" \" <nl> def test_light_source_shading_default ( ) : <nl> ls = mcolors . LightSource ( 315 , 45 ) <nl> rgb = ls . shade ( z , cmap ) <nl> <nl> - r = np . array ( [ [ [ 0 . 87 , 0 . 85 , 0 . 9 , 0 . 9 , 0 . 82 , 0 . 62 , 0 . 34 , 0 . ] , <nl> - [ 0 . 85 , 0 . 94 , 0 . 99 , 1 . , 1 . , 0 . 96 , 0 . 62 , 0 . 17 ] , <nl> - [ 0 . 9 , 0 . 99 , 1 . , 1 . , 1 . , 1 . , 0 . 71 , 0 . 33 ] , <nl> - [ 0 . 9 , 1 . , 1 . , 1 . , 1 . , 0 . 98 , 0 . 51 , 0 . 29 ] , <nl> - [ 0 . 82 , 1 . , 1 . , 1 . , 1 . , 0 . 64 , 0 . 25 , 0 . 13 ] , <nl> - [ 0 . 62 , 0 . 96 , 1 . , 0 . 98 , 0 . 64 , 0 . 22 , 0 . 06 , 0 . 03 ] , <nl> - [ 0 . 34 , 0 . 62 , 0 . 71 , 0 . 51 , 0 . 25 , 0 . 06 , 0 . , 0 . 01 ] , <nl> - [ 0 . , 0 . 17 , 0 . 33 , 0 . 29 , 0 . 13 , 0 . 03 , 0 . 01 , 0 . ] ] , <nl> - <nl> - [ [ 0 . 87 , 0 . 79 , 0 . 83 , 0 . 8 , 0 . 66 , 0 . 44 , 0 . 23 , 0 . ] , <nl> - [ 0 . 79 , 0 . 88 , 0 . 93 , 0 . 92 , 0 . 83 , 0 . 66 , 0 . 38 , 0 . 1 ] , <nl> - [ 0 . 83 , 0 . 93 , 0 . 99 , 1 . , 0 . 92 , 0 . 75 , 0 . 4 , 0 . 18 ] , <nl> - [ 0 . 8 , 0 . 92 , 1 . , 0 . 99 , 0 . 93 , 0 . 75 , 0 . 28 , 0 . 14 ] , <nl> - [ 0 . 66 , 0 . 83 , 0 . 92 , 0 . 93 , 0 . 87 , 0 . 44 , 0 . 12 , 0 . 06 ] , <nl> - [ 0 . 44 , 0 . 66 , 0 . 75 , 0 . 75 , 0 . 44 , 0 . 12 , 0 . 03 , 0 . 01 ] , <nl> - [ 0 . 23 , 0 . 38 , 0 . 4 , 0 . 28 , 0 . 12 , 0 . 03 , 0 . , 0 . ] , <nl> - [ 0 . , 0 . 1 , 0 . 18 , 0 . 14 , 0 . 06 , 0 . 01 , 0 . , 0 . ] ] , <nl> - <nl> - [ [ 0 . 87 , 0 . 75 , 0 . 78 , 0 . 73 , 0 . 55 , 0 . 33 , 0 . 16 , 0 . ] , <nl> - [ 0 . 75 , 0 . 85 , 0 . 9 , 0 . 86 , 0 . 71 , 0 . 48 , 0 . 23 , 0 . 05 ] , <nl> - [ 0 . 78 , 0 . 9 , 0 . 98 , 1 . , 0 . 82 , 0 . 51 , 0 . 21 , 0 . 08 ] , <nl> - [ 0 . 73 , 0 . 86 , 1 . , 0 . 97 , 0 . 84 , 0 . 47 , 0 . 11 , 0 . 05 ] , <nl> - [ 0 . 55 , 0 . 71 , 0 . 82 , 0 . 84 , 0 . 71 , 0 . 2 , 0 . 03 , 0 . 01 ] , <nl> - [ 0 . 33 , 0 . 48 , 0 . 51 , 0 . 47 , 0 . 2 , 0 . 02 , 0 . , 0 . ] , <nl> - [ 0 . 16 , 0 . 23 , 0 . 21 , 0 . 11 , 0 . 03 , 0 . , 0 . , 0 . ] , <nl> - [ 0 . , 0 . 05 , 0 . 08 , 0 . 05 , 0 . 01 , 0 . , 0 . , 0 . ] ] , <nl> - <nl> - [ [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] ] ] ) <nl> # Result stored transposed and rounded for for more compact display . . . <nl> - assert_array_almost_equal ( rgb , r . T , decimal = 2 ) <nl> + expect = np . array ( [ [ [ 0 . 87 , 0 . 85 , 0 . 90 , 0 . 90 , 0 . 82 , 0 . 62 , 0 . 34 , 0 . 00 ] , <nl> + [ 0 . 85 , 0 . 94 , 0 . 99 , 1 . 00 , 1 . 00 , 0 . 96 , 0 . 62 , 0 . 17 ] , <nl> + [ 0 . 90 , 0 . 99 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 0 . 71 , 0 . 33 ] , <nl> + [ 0 . 90 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 0 . 98 , 0 . 51 , 0 . 29 ] , <nl> + [ 0 . 82 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 0 . 64 , 0 . 25 , 0 . 13 ] , <nl> + [ 0 . 62 , 0 . 96 , 1 . 00 , 0 . 98 , 0 . 64 , 0 . 22 , 0 . 06 , 0 . 03 ] , <nl> + [ 0 . 34 , 0 . 62 , 0 . 71 , 0 . 51 , 0 . 25 , 0 . 06 , 0 . 00 , 0 . 01 ] , <nl> + [ 0 . 00 , 0 . 17 , 0 . 33 , 0 . 29 , 0 . 13 , 0 . 03 , 0 . 01 , 0 . 00 ] ] , <nl> + <nl> + [ [ 0 . 87 , 0 . 79 , 0 . 83 , 0 . 80 , 0 . 66 , 0 . 44 , 0 . 23 , 0 . 00 ] , <nl> + [ 0 . 79 , 0 . 88 , 0 . 93 , 0 . 92 , 0 . 83 , 0 . 66 , 0 . 38 , 0 . 10 ] , <nl> + [ 0 . 83 , 0 . 93 , 0 . 99 , 1 . 00 , 0 . 92 , 0 . 75 , 0 . 40 , 0 . 18 ] , <nl> + [ 0 . 80 , 0 . 92 , 1 . 00 , 0 . 99 , 0 . 93 , 0 . 75 , 0 . 28 , 0 . 14 ] , <nl> + [ 0 . 66 , 0 . 83 , 0 . 92 , 0 . 93 , 0 . 87 , 0 . 44 , 0 . 12 , 0 . 06 ] , <nl> + [ 0 . 44 , 0 . 66 , 0 . 75 , 0 . 75 , 0 . 44 , 0 . 12 , 0 . 03 , 0 . 01 ] , <nl> + [ 0 . 23 , 0 . 38 , 0 . 40 , 0 . 28 , 0 . 12 , 0 . 03 , 0 . 00 , 0 . 00 ] , <nl> + [ 0 . 00 , 0 . 10 , 0 . 18 , 0 . 14 , 0 . 06 , 0 . 01 , 0 . 00 , 0 . 00 ] ] , <nl> + <nl> + [ [ 0 . 87 , 0 . 75 , 0 . 78 , 0 . 73 , 0 . 55 , 0 . 33 , 0 . 16 , 0 . 00 ] , <nl> + [ 0 . 75 , 0 . 85 , 0 . 90 , 0 . 86 , 0 . 71 , 0 . 48 , 0 . 23 , 0 . 05 ] , <nl> + [ 0 . 78 , 0 . 90 , 0 . 98 , 1 . 00 , 0 . 82 , 0 . 51 , 0 . 21 , 0 . 08 ] , <nl> + [ 0 . 73 , 0 . 86 , 1 . 00 , 0 . 97 , 0 . 84 , 0 . 47 , 0 . 11 , 0 . 05 ] , <nl> + [ 0 . 55 , 0 . 71 , 0 . 82 , 0 . 84 , 0 . 71 , 0 . 20 , 0 . 03 , 0 . 01 ] , <nl> + [ 0 . 33 , 0 . 48 , 0 . 51 , 0 . 47 , 0 . 20 , 0 . 02 , 0 . 00 , 0 . 00 ] , <nl> + [ 0 . 16 , 0 . 23 , 0 . 21 , 0 . 11 , 0 . 03 , 0 . 00 , 0 . 00 , 0 . 00 ] , <nl> + [ 0 . 00 , 0 . 05 , 0 . 08 , 0 . 05 , 0 . 01 , 0 . 00 , 0 . 00 , 0 . 00 ] ] , <nl> + <nl> + [ [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] ] ] ) . T <nl> + assert_array_almost_equal ( rgb , expect , decimal = 2 ) <nl> <nl> <nl> def test_light_source_masked_shading ( ) : <nl> def test_light_source_masked_shading ( ) : <nl> ls = mcolors . LightSource ( 315 , 45 ) <nl> rgb = ls . shade ( z , cmap ) <nl> <nl> - r = np . array ( [ [ [ 1 . , 0 . 95 , 0 . 96 , 0 . 94 , 0 . 86 , 0 . 67 , 0 . 4 , 0 . 03 ] , <nl> - [ 0 . 95 , 0 . 99 , 1 . , 1 . , 1 . , 0 . 98 , 0 . 67 , 0 . 19 ] , <nl> - [ 0 . 96 , 1 . , 1 . , 1 . , 1 . , 1 . , 0 . 78 , 0 . 36 ] , <nl> - [ 0 . 94 , 1 . , 1 . , 0 . , 0 . , 1 . , 0 . 55 , 0 . 32 ] , <nl> - [ 0 . 86 , 1 . , 1 . , 0 . , 0 . , 1 . , 0 . 27 , 0 . 14 ] , <nl> - [ 0 . 67 , 0 . 98 , 1 . , 1 . , 1 . , 1 . , 0 . 07 , 0 . 03 ] , <nl> - [ 0 . 4 , 0 . 67 , 0 . 78 , 0 . 55 , 0 . 27 , 0 . 07 , 0 . , 0 . 01 ] , <nl> - [ 0 . 03 , 0 . 19 , 0 . 36 , 0 . 32 , 0 . 14 , 0 . 03 , 0 . 01 , 0 . ] ] , <nl> - <nl> - [ [ 1 . , 0 . 93 , 0 . 93 , 0 . 88 , 0 . 72 , 0 . 5 , 0 . 28 , 0 . 03 ] , <nl> - [ 0 . 93 , 0 . 97 , 0 . 99 , 0 . 96 , 0 . 87 , 0 . 7 , 0 . 42 , 0 . 11 ] , <nl> - [ 0 . 93 , 0 . 99 , 0 . 74 , 0 . 78 , 0 . 78 , 0 . 74 , 0 . 45 , 0 . 2 ] , <nl> - [ 0 . 88 , 0 . 96 , 0 . 78 , 0 . , 0 . , 0 . 78 , 0 . 32 , 0 . 16 ] , <nl> - [ 0 . 72 , 0 . 87 , 0 . 78 , 0 . , 0 . , 0 . 78 , 0 . 14 , 0 . 06 ] , <nl> - [ 0 . 5 , 0 . 7 , 0 . 74 , 0 . 78 , 0 . 78 , 0 . 74 , 0 . 03 , 0 . 01 ] , <nl> - [ 0 . 28 , 0 . 42 , 0 . 45 , 0 . 32 , 0 . 14 , 0 . 03 , 0 . , 0 . ] , <nl> - [ 0 . 03 , 0 . 11 , 0 . 2 , 0 . 16 , 0 . 06 , 0 . 01 , 0 . , 0 . ] ] , <nl> - <nl> - [ [ 1 . , 0 . 91 , 0 . 91 , 0 . 84 , 0 . 64 , 0 . 39 , 0 . 21 , 0 . 03 ] , <nl> - [ 0 . 91 , 0 . 96 , 0 . 98 , 0 . 93 , 0 . 77 , 0 . 53 , 0 . 27 , 0 . 06 ] , <nl> - [ 0 . 91 , 0 . 98 , 0 . 47 , 0 . 5 , 0 . 5 , 0 . 47 , 0 . 25 , 0 . 1 ] , <nl> - [ 0 . 84 , 0 . 93 , 0 . 5 , 0 . , 0 . , 0 . 5 , 0 . 13 , 0 . 06 ] , <nl> - [ 0 . 64 , 0 . 77 , 0 . 5 , 0 . , 0 . , 0 . 5 , 0 . 03 , 0 . 01 ] , <nl> - [ 0 . 39 , 0 . 53 , 0 . 47 , 0 . 5 , 0 . 5 , 0 . 47 , 0 . , 0 . ] , <nl> - [ 0 . 21 , 0 . 27 , 0 . 25 , 0 . 13 , 0 . 03 , 0 . , 0 . , 0 . ] , <nl> - [ 0 . 03 , 0 . 06 , 0 . 1 , 0 . 06 , 0 . 01 , 0 . , 0 . , 0 . ] ] , <nl> - <nl> - [ [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 0 . , 0 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 0 . , 0 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] , <nl> - [ 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . , 1 . ] ] ] ) <nl> # Result stored transposed and rounded for for more compact display . . . <nl> - assert_array_almost_equal ( rgb , r . T , decimal = 2 ) <nl> + expect = np . array ( [ [ [ 1 . 00 , 0 . 95 , 0 . 96 , 0 . 94 , 0 . 86 , 0 . 67 , 0 . 40 , 0 . 03 ] , <nl> + [ 0 . 95 , 0 . 99 , 1 . 00 , 1 . 00 , 1 . 00 , 0 . 98 , 0 . 67 , 0 . 19 ] , <nl> + [ 0 . 96 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 0 . 78 , 0 . 36 ] , <nl> + [ 0 . 94 , 1 . 00 , 1 . 00 , 0 . 00 , 0 . 00 , 1 . 00 , 0 . 55 , 0 . 32 ] , <nl> + [ 0 . 86 , 1 . 00 , 1 . 00 , 0 . 00 , 0 . 00 , 1 . 00 , 0 . 27 , 0 . 14 ] , <nl> + [ 0 . 67 , 0 . 98 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 0 . 07 , 0 . 03 ] , <nl> + [ 0 . 40 , 0 . 67 , 0 . 78 , 0 . 55 , 0 . 27 , 0 . 07 , 0 . 00 , 0 . 01 ] , <nl> + [ 0 . 03 , 0 . 19 , 0 . 36 , 0 . 32 , 0 . 14 , 0 . 03 , 0 . 01 , 0 . 00 ] ] , <nl> + <nl> + [ [ 1 . 00 , 0 . 93 , 0 . 93 , 0 . 88 , 0 . 72 , 0 . 50 , 0 . 28 , 0 . 03 ] , <nl> + [ 0 . 93 , 0 . 97 , 0 . 99 , 0 . 96 , 0 . 87 , 0 . 70 , 0 . 42 , 0 . 11 ] , <nl> + [ 0 . 93 , 0 . 99 , 0 . 74 , 0 . 78 , 0 . 78 , 0 . 74 , 0 . 45 , 0 . 20 ] , <nl> + [ 0 . 88 , 0 . 96 , 0 . 78 , 0 . 00 , 0 . 00 , 0 . 78 , 0 . 32 , 0 . 16 ] , <nl> + [ 0 . 72 , 0 . 87 , 0 . 78 , 0 . 00 , 0 . 00 , 0 . 78 , 0 . 14 , 0 . 06 ] , <nl> + [ 0 . 50 , 0 . 70 , 0 . 74 , 0 . 78 , 0 . 78 , 0 . 74 , 0 . 03 , 0 . 01 ] , <nl> + [ 0 . 28 , 0 . 42 , 0 . 45 , 0 . 32 , 0 . 14 , 0 . 03 , 0 . 00 , 0 . 00 ] , <nl> + [ 0 . 03 , 0 . 11 , 0 . 20 , 0 . 16 , 0 . 06 , 0 . 01 , 0 . 00 , 0 . 00 ] ] , <nl> + <nl> + [ [ 1 . 00 , 0 . 91 , 0 . 91 , 0 . 84 , 0 . 64 , 0 . 39 , 0 . 21 , 0 . 03 ] , <nl> + [ 0 . 91 , 0 . 96 , 0 . 98 , 0 . 93 , 0 . 77 , 0 . 53 , 0 . 27 , 0 . 06 ] , <nl> + [ 0 . 91 , 0 . 98 , 0 . 47 , 0 . 50 , 0 . 50 , 0 . 47 , 0 . 25 , 0 . 10 ] , <nl> + [ 0 . 84 , 0 . 93 , 0 . 50 , 0 . 00 , 0 . 00 , 0 . 50 , 0 . 13 , 0 . 06 ] , <nl> + [ 0 . 64 , 0 . 77 , 0 . 50 , 0 . 00 , 0 . 00 , 0 . 50 , 0 . 03 , 0 . 01 ] , <nl> + [ 0 . 39 , 0 . 53 , 0 . 47 , 0 . 50 , 0 . 50 , 0 . 47 , 0 . 00 , 0 . 00 ] , <nl> + [ 0 . 21 , 0 . 27 , 0 . 25 , 0 . 13 , 0 . 03 , 0 . 00 , 0 . 00 , 0 . 00 ] , <nl> + [ 0 . 03 , 0 . 06 , 0 . 10 , 0 . 06 , 0 . 01 , 0 . 00 , 0 . 00 , 0 . 00 ] ] , <nl> + <nl> + [ [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 0 . 00 , 0 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 0 . 00 , 0 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] , <nl> + [ 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 , 1 . 00 ] ] ] ) . T <nl> + assert_array_almost_equal ( rgb , expect , decimal = 2 ) <nl> <nl> <nl> def test_light_source_hillshading ( ) : <nl>\n", "msg": "Strict PEP8 compliance for printed arrays\n"}
{"diff_id": 868, "repo": "ytdl-org/youtube-dl\n", "sha": "efbd1eb51a9e940d01a2c02fd02c3778dd88b14b\n", "time": "2016-06-16T04:27:21Z\n", "diff": "mmm a / youtube_dl / extractor / wimp . py <nl> ppp b / youtube_dl / extractor / wimp . py <nl> <nl> from __future__ import unicode_literals <nl> <nl> - from . common import InfoExtractor <nl> from . youtube import YoutubeIE <nl> + from . jwplatform import JWPlatformBaseIE <nl> <nl> <nl> - class WimpIE ( InfoExtractor ) : <nl> + class WimpIE ( JWPlatformBaseIE ) : <nl> _VALID_URL = r ' https ? : / / ( ? : www \\ . ) ? wimp \\ . com / ( ? P < id > [ ^ / ] + ) ' <nl> _TESTS = [ { <nl> - ' url ' : ' http : / / www . wimp . com / maruexhausted / ' , <nl> + ' url ' : ' http : / / www . wimp . com / maru - is - exhausted / ' , <nl> ' md5 ' : ' ee21217ffd66d058e8b16be340b74883 ' , <nl> ' info_dict ' : { <nl> - ' id ' : ' maruexhausted ' , <nl> + ' id ' : ' maru - is - exhausted ' , <nl> ' ext ' : ' mp4 ' , <nl> ' title ' : ' Maru is exhausted . ' , <nl> ' description ' : ' md5 : 57e099e857c0a4ea312542b684a869b8 ' , <nl> } <nl> } , { <nl> ' url ' : ' http : / / www . wimp . com / clowncar / ' , <nl> - ' md5 ' : ' 4e2986c793694b55b37cf92521d12bb4 ' , <nl> + ' md5 ' : ' 5c31ad862a90dc5b1f023956faec13fe ' , <nl> ' info_dict ' : { <nl> - ' id ' : ' clowncar ' , <nl> + ' id ' : ' cG4CEr2aiSg ' , <nl> ' ext ' : ' webm ' , <nl> - ' title ' : ' It \\ ' s like a clown car . ' , <nl> - ' description ' : ' md5 : 0e56db1370a6e49c5c1d19124c0d2fb2 ' , <nl> + ' title ' : ' Basset hound clown car . . . incredible ! ' , <nl> + ' description ' : ' 5 of my Bassets crawled in this dog loo ! www . bellinghambassets . com \\ n \\ nFor licensing / usage please contact : licensing ( at ) jukinmediadotcom ' , <nl> + ' upload_date ' : ' 20140303 ' , <nl> + ' uploader ' : ' Gretchen Hoey ' , <nl> + ' uploader_id ' : ' gretchenandjeff1 ' , <nl> } , <nl> + ' add_ie ' : [ ' Youtube ' ] , <nl> } ] <nl> <nl> def _real_extract ( self , url ) : <nl> def _real_extract ( self , url ) : <nl> ' ie_key ' : YoutubeIE . ie_key ( ) , <nl> } <nl> <nl> - video_url = self . _search_regex ( <nl> - r ' < video [ ^ > ] + > \\ s * < source [ ^ > ] + src = ( [ \" \\ ' ] ) ( ? P < url > . + ? ) \\ 1 ' , <nl> - webpage , ' video URL ' , group = ' url ' ) <nl> + info_dict = self . _extract_jwplayer_data ( <nl> + webpage , video_id , require_title = False ) <nl> <nl> - return { <nl> + info_dict . update ( { <nl> ' id ' : video_id , <nl> - ' url ' : video_url , <nl> ' title ' : self . _og_search_title ( webpage ) , <nl> - ' thumbnail ' : self . _og_search_thumbnail ( webpage ) , <nl> ' description ' : self . _og_search_description ( webpage ) , <nl> - } <nl> + } ) <nl> + <nl> + return info_dict <nl>\n", "msg": "[ wimp ] Fix extraction and update _TESTS\n"}
{"diff_id": 877, "repo": "ansible/ansible\n", "sha": "48223fd268a1c4f4401efb889eaa477b51eae48f\n", "time": "2016-12-08T16:34:01Z\n", "diff": "mmm a / lib / ansible / modules / extras / monitoring / datadog_monitor . py <nl> ppp b / lib / ansible / modules / extras / monitoring / datadog_monitor . py <nl> <nl> required : false <nl> default : False <nl> thresholds : <nl> - description : [ \" A dictionary of thresholds by status . Because service checks can have multiple thresholds , we don ' t define them directly in the query . \" ] <nl> + description : [ \" A dictionary of thresholds by status . This option is only available for service checks and metric alerts . Because each of them can have multiple thresholds , we don ' t define them directly in the query . \" ] <nl> required : false <nl> default : { ' ok ' : 1 , ' critical ' : 1 , ' warning ' : 1 } <nl> ' ' ' <nl> def main ( ) : <nl> renotify_interval = dict ( required = False , default = None ) , <nl> escalation_message = dict ( required = False , default = None ) , <nl> notify_audit = dict ( required = False , default = False , type = ' bool ' ) , <nl> - thresholds = dict ( required = False , type = ' dict ' , default = { ' ok ' : 1 , ' critical ' : 1 , ' warning ' : 1 } ) , <nl> + thresholds = dict ( required = False , type = ' dict ' , default = None ) , <nl> ) <nl> ) <nl> <nl> def install_monitor ( module ) : <nl> } <nl> <nl> if module . params [ ' type ' ] = = \" service check \" : <nl> + options [ \" thresholds \" ] = module . params [ ' thresholds ' ] or { ' ok ' : 1 , ' critical ' : 1 , ' warning ' : 1 } <nl> + if module . params [ ' type ' ] = = \" metric alert \" and module . params [ ' thresholds ' ] is not None : <nl> options [ \" thresholds \" ] = module . params [ ' thresholds ' ] <nl> <nl> monitor = _get_monitor ( module ) <nl>\n", "msg": "Allow Datadog metric alerts to define multiple thresholds\n"}
{"diff_id": 1000, "repo": "soimort/you-get\n", "sha": "1df62c39ffb2ba3ddf115750cd3aa5d37895a81a\n", "time": "2018-12-26T15:48:32Z\n", "diff": "mmm a / src / you_get / extractors / universal . py <nl> ppp b / src / you_get / extractors / universal . py <nl> def universal_download ( url , output_dir = ' . ' , merge = True , info_only = False , * * kwarg <nl> <nl> else : <nl> # direct download <nl> - filename = parse . unquote ( url . split ( ' / ' ) [ - 1 ] ) or parse . unquote ( url . split ( ' / ' ) [ - 2 ] ) <nl> + url_trunk = url . split ( ' ? ' ) [ 0 ] # strip query string <nl> + filename = parse . unquote ( url_trunk . split ( ' / ' ) [ - 1 ] ) or parse . unquote ( url_trunk . split ( ' / ' ) [ - 2 ] ) <nl> title = ' . ' . join ( filename . split ( ' . ' ) [ : - 1 ] ) or filename <nl> _ , ext , size = url_info ( url , faker = True ) <nl> print_info ( site_info , title , ext , size ) <nl>\n", "msg": "[ universal ] strip query string for direct download\n"}
{"diff_id": 1037, "repo": "ipython/ipython\n", "sha": "641206824e24b42a83b1455988efcfe491fd9969\n", "time": "2011-11-10T01:36:59Z\n", "diff": "mmm a / IPython / core / shellapp . py <nl> ppp b / IPython / core / shellapp . py <nl> def init_code ( self ) : <nl> self . _run_exec_lines ( ) <nl> self . _run_exec_files ( ) <nl> self . _run_cmd_line_code ( ) <nl> + <nl> + # Hide variables defined here from % who etc . <nl> + self . shell . user_ns_hidden . update ( self . shell . user_ns ) <nl> <nl> def _run_exec_lines ( self ) : <nl> \" \" \" Run lines of code in IPythonApp . exec_lines in the user ' s namespace . \" \" \" <nl>\n", "msg": "Do not expose variables defined at startup to % who etc .\n"}
{"diff_id": 1089, "repo": "ansible/ansible\n", "sha": "4185ffc43e0453ff6bac9fde8de33fe778d26376\n", "time": "2016-12-08T16:23:11Z\n", "diff": "mmm a / lib / ansible / modules / packaging / language / pip . py <nl> ppp b / lib / ansible / modules / packaging / language / pip . py <nl> def main ( ) : <nl> this_dir = os . path . join ( this_dir , chdir ) <nl> <nl> if module . check_mode : <nl> - if env or extra_args or requirements or state = = ' latest ' or not name : <nl> + if extra_args or requirements or state = = ' latest ' or not name : <nl> module . exit_json ( changed = True ) <nl> elif name . startswith ( ' svn + ' ) or name . startswith ( ' git + ' ) or \\ <nl> name . startswith ( ' hg + ' ) or name . startswith ( ' bzr + ' ) : <nl>\n", "msg": "Correct check mode for pip in virtualenv .\n"}
{"diff_id": 1105, "repo": "deepfakes/faceswap\n", "sha": "d916557d199c9854aab0fe346ff5eace14b64a0e\n", "time": "2019-06-19T23:20:24Z\n", "diff": "mmm a / lib / logger . py <nl> ppp b / lib / logger . py <nl> <nl> from tqdm import tqdm <nl> <nl> from lib . queue_manager import queue_manager <nl> - from lib . sysinfo import sysinfo <nl> <nl> LOG_QUEUE = queue_manager . _log_queue # pylint : disable = protected - access <nl> <nl> def get_loglevel ( loglevel ) : <nl> <nl> def crash_log ( ) : <nl> \" \" \" Write debug_buffer to a crash log on crash \" \" \" <nl> + from lib . sysinfo import sysinfo <nl> path = os . getcwd ( ) <nl> filename = os . path . join ( path , datetime . now ( ) . strftime ( \" crash_report . % Y . % m . % d . % H % M % S % f . log \" ) ) <nl> <nl>\n", "msg": "Move crash logging imports to crash_log function\n"}
{"diff_id": 1133, "repo": "google-research/bert\n", "sha": "ea8737bdd9dd24c0260910b8d8f5a0fb5789fa34\n", "time": "2018-11-07T21:41:34Z\n", "diff": "mmm a / tokenization . py <nl> ppp b / tokenization . py <nl> def load_vocab ( vocab_file ) : <nl> return vocab <nl> <nl> <nl> - def convert_tokens_to_ids ( vocab , tokens ) : <nl> - \" \" \" Converts a sequence of tokens into ids using the vocab . \" \" \" <nl> - ids = [ ] <nl> - for token in tokens : <nl> - ids . append ( vocab [ token ] ) <nl> - return ids <nl> + def convert_by_vocab ( vocab , items ) : <nl> + \" \" \" Converts a sequence of [ tokens | ids ] using the vocab . \" \" \" <nl> + output = [ ] <nl> + for item in items : <nl> + output . append ( vocab [ item ] ) <nl> + return output <nl> <nl> <nl> def whitespace_tokenize ( text ) : <nl> class FullTokenizer ( object ) : <nl> <nl> def __init__ ( self , vocab_file , do_lower_case = True ) : <nl> self . vocab = load_vocab ( vocab_file ) <nl> + self . inv_vocab = { v : k for k , v in self . vocab . items ( ) } <nl> self . basic_tokenizer = BasicTokenizer ( do_lower_case = do_lower_case ) <nl> self . wordpiece_tokenizer = WordpieceTokenizer ( vocab = self . vocab ) <nl> <nl> def tokenize ( self , text ) : <nl> return split_tokens <nl> <nl> def convert_tokens_to_ids ( self , tokens ) : <nl> - return convert_tokens_to_ids ( self . vocab , tokens ) <nl> + return convert_by_vocab ( self . vocab , tokens ) <nl> + <nl> + def convert_ids_to_tokens ( self , ids ) : <nl> + return convert_by_vocab ( self . inv_vocab , ids ) <nl> <nl> <nl> class BasicTokenizer ( object ) : <nl>\n", "msg": "Add method for converting ids to tokens .\n"}
{"diff_id": 1167, "repo": "ansible/ansible\n", "sha": "000df2709d92003a16d2f03a80d66724b74d6485\n", "time": "2017-08-14T11:55:39Z\n", "diff": "mmm a / lib / ansible / modules / packaging / os / yum . py <nl> ppp b / lib / ansible / modules / packaging / os / yum . py <nl> <nl> version_added : historical <nl> short_description : Manages packages with the I ( yum ) package manager <nl> description : <nl> - - Installs , upgrade , removes , and lists packages and groups with the I ( yum ) package manager . <nl> + - Installs , upgrade , downgrades , removes , and lists packages and groups with the I ( yum ) package manager . <nl> options : <nl> name : <nl> description : <nl> - - \" Package name , or package specifier with version , like C ( name - 1 . 0 ) . When using state = latest , this can be ' * ' which means run : yum - y update . <nl> - You can also pass a url or a local path to a rpm file ( using state = present ) . To operate on several packages this can accept a comma separated list <nl> - of packages or ( as of 2 . 0 ) a list of packages . \" <nl> + - Package name , or package specifier with version , like C ( name - 1 . 0 ) . <nl> + If a previous version is specified , the task also needs to turn <nl> + C ( allow_downgrade ) on . See the C ( allow_downgrade ) documentation for <nl> + caveats with downgrading packages . When using state = latest , this can <nl> + be ' * ' which means run C ( yum - y update ) . You can also pass a url <nl> + or a local path to a rpm file ( using state = present ) . To operate on <nl> + several packages this can accept a comma separated list of packages <nl> + or ( as of 2 . 0 ) a list of packages . <nl> required : true <nl> default : null <nl> aliases : [ ' pkg ' ] <nl> <nl> choices : [ \" yes \" , \" no \" ] <nl> version_added : \" 2 . 4 \" <nl> <nl> + allow_downgrade : <nl> + description : <nl> + - Specify if the named package and version is allowed to downgrade <nl> + a maybe already installed higher version of that package . <nl> + Note that setting allow_downgrade = True can make this module <nl> + behave in a non - idempotent way . The task could end up with a set <nl> + of packages that does not match the complete list of specified <nl> + packages to install ( because dependencies between the downgraded <nl> + package and others can cause changes to the packages which were <nl> + in the earlier transaction ) . <nl> + required : false <nl> + default : \" no \" <nl> + choices : [ \" yes \" , \" no \" ] <nl> + version_added : \" 2 . 4 \" <nl> + <nl> notes : <nl> - When used with a loop of package names in a playbook , ansible optimizes <nl> the call to the yum module . Instead of calling the module with a single <nl> <nl> <nl> import os <nl> import re <nl> - import shutil <nl> import tempfile <nl> <nl> try : <nl> <nl> <nl> try : <nl> from yum . misc import find_unfinished_transactions , find_ts_remaining <nl> - from rpmUtils . miscutils import splitFilename <nl> + from rpmUtils . miscutils import splitFilename , compareEVR <nl> transaction_helpers = True <nl> except : <nl> transaction_helpers = False <nl> def list_stuff ( module , repoquerybin , conf_file , stuff , installroot = ' / ' , disabler <nl> return [ pkg_to_dict ( p ) for p in sorted ( is_installed ( module , repoq , stuff , conf_file , qf = is_installed_qf , installroot = installroot ) + <nl> is_available ( module , repoq , stuff , conf_file , qf = qf , installroot = installroot ) ) if p . strip ( ) ] <nl> <nl> - def install ( module , items , repoq , yum_basecmd , conf_file , en_repos , dis_repos , installroot = ' / ' ) : <nl> + <nl> + def exec_install ( module , items , action , pkgs , res , yum_basecmd ) : <nl> + cmd = yum_basecmd + [ action ] + pkgs <nl> + <nl> + if module . check_mode : <nl> + module . exit_json ( changed = True , results = res [ ' results ' ] , changes = dict ( installed = pkgs ) ) <nl> + <nl> + lang_env = dict ( LANG = ' C ' , LC_ALL = ' C ' , LC_MESSAGES = ' C ' ) <nl> + rc , out , err = module . run_command ( cmd , environ_update = lang_env ) <nl> + <nl> + if ( rc = = 1 ) : <nl> + for spec in items : <nl> + # Fail on invalid urls : <nl> + if ( ' : / / ' in spec and ( ' No package % s available . ' % spec in out or ' Cannot open : % s . Skipping . ' % spec in err ) ) : <nl> + err = ' Package at % s could not be installed ' % spec <nl> + module . fail_json ( changed = False , msg = err , rc = 1 ) <nl> + <nl> + res [ ' rc ' ] = rc <nl> + res [ ' results ' ] . append ( out ) <nl> + res [ ' msg ' ] + = err <nl> + res [ ' changed ' ] = True <nl> + <nl> + # special case for groups <nl> + for spec in items : <nl> + if spec . startswith ( ' @ ' ) : <nl> + if ( ' Nothing to do ' in out and rc = = 0 ) or ( ' does not have any packages to install ' in err ) : <nl> + res [ ' changed ' ] = False <nl> + <nl> + if rc ! = 0 : <nl> + res [ ' changed ' ] = False <nl> + module . fail_json ( * * res ) <nl> + <nl> + # FIXME - if we did an install - go and check the rpmdb to see if it actually installed <nl> + # look for each pkg in rpmdb <nl> + # look for each pkg via obsoletes <nl> + <nl> + return res <nl> + <nl> + <nl> + def install ( module , items , repoq , yum_basecmd , conf_file , en_repos , dis_repos , installroot = ' / ' , allow_downgrade = False ) : <nl> <nl> pkgs = [ ] <nl> + downgrade_pkgs = [ ] <nl> res = { } <nl> res [ ' results ' ] = [ ] <nl> res [ ' msg ' ] = ' ' <nl> def install ( module , items , repoq , yum_basecmd , conf_file , en_repos , dis_repos , i <nl> <nl> for spec in items : <nl> pkg = None <nl> + downgrade_candidate = False <nl> <nl> # check if pkgspec is installed ( if possible for idempotence ) <nl> # localpkg <nl> def install ( module , items , repoq , yum_basecmd , conf_file , en_repos , dis_repos , i <nl> if found : <nl> continue <nl> <nl> - # if not - then pass in the spec as what to install <nl> + # Downgrade - The yum install command will only install or upgrade to a spec version , it will <nl> + # not install an older version of an RPM even if specified by the install spec . So we need to <nl> + # determine if this is a downgrade , and then use the yum downgrade command to install the RPM . <nl> + if allow_downgrade : <nl> + for package in pkglist : <nl> + # Get the NEVRA of the requested package using pkglist instead of spec because pkglist <nl> + # contains consistently - formatted package names returned by yum , rather than user input <nl> + # that is often not parsed correctly by splitFilename ( ) . <nl> + ( name , ver , rel , epoch , arch ) = splitFilename ( package ) <nl> + <nl> + # Check if any version of the requested package is installed <nl> + inst_pkgs = is_installed ( module , repoq , name , conf_file , en_repos = en_repos , dis_repos = dis_repos , is_pkg = True ) <nl> + if inst_pkgs : <nl> + ( cur_name , cur_ver , cur_rel , cur_epoch , cur_arch ) = splitFilename ( inst_pkgs [ 0 ] ) <nl> + compare = compareEVR ( ( cur_epoch , cur_ver , cur_rel ) , ( epoch , ver , rel ) ) <nl> + if compare > 0 : <nl> + downgrade_candidate = True <nl> + else : <nl> + downgrade_candidate = False <nl> + break <nl> + <nl> + # If package needs to be installed / upgraded / downgraded , then pass in the spec <nl> # we could get here if nothing provides it but that ' s not <nl> # the error we ' re catching here <nl> pkg = spec <nl> <nl> - pkgs . append ( pkg ) <nl> - <nl> - if pkgs : <nl> - cmd = yum_basecmd + [ ' install ' ] + pkgs <nl> - <nl> - if module . check_mode : <nl> - module . exit_json ( changed = True , results = res [ ' results ' ] , changes = dict ( installed = pkgs ) ) <nl> - <nl> - <nl> - lang_env = dict ( LANG = ' C ' , LC_ALL = ' C ' , LC_MESSAGES = ' C ' ) <nl> - rc , out , err = module . run_command ( cmd , environ_update = lang_env ) <nl> - <nl> - if rc = = 1 : <nl> - for spec in items : <nl> - # Fail on invalid urls : <nl> - if ' : / / ' in spec and ( ' No package % s available . ' % spec in out or ' Cannot open : % s . Skipping . ' % spec in err ) : <nl> - module . fail_json ( msg = ' Package at % s could not be installed ' % spec , rc = 1 , changed = False ) <nl> - <nl> - res [ ' rc ' ] = rc <nl> - res [ ' results ' ] . append ( out ) <nl> - res [ ' msg ' ] + = err <nl> - res [ ' changed ' ] = True <nl> - <nl> - # special case for groups <nl> - if spec . startswith ( ' @ ' ) : <nl> - if ( ' Nothing to do ' in out and rc = = 0 ) or ( ' does not have any packages to install ' in err ) : <nl> - res [ ' changed ' ] = False <nl> + if downgrade_candidate and allow_downgrade : <nl> + downgrade_pkgs . append ( pkg ) <nl> + else : <nl> + pkgs . append ( pkg ) <nl> <nl> - if rc ! = 0 : <nl> - res [ ' changed ' ] = False <nl> - module . fail_json ( * * res ) <nl> + if downgrade_pkgs : <nl> + res = exec_install ( module , items , ' downgrade ' , downgrade_pkgs , res , yum_basecmd ) <nl> <nl> - # FIXME - if we did an install - go and check the rpmdb to see if it actually installed <nl> - # look for each pkg in rpmdb <nl> - # look for each pkg via obsoletes <nl> + if pkgs : <nl> + res = exec_install ( module , items , ' install ' , pkgs , res , yum_basecmd ) <nl> <nl> return res <nl> <nl> def latest ( module , items , repoq , yum_basecmd , conf_file , en_repos , dis_repos , in <nl> return res <nl> <nl> def ensure ( module , state , pkgs , conf_file , enablerepo , disablerepo , <nl> - disable_gpg_check , exclude , repoq , skip_broken , security , installroot = ' / ' ) : <nl> + disable_gpg_check , exclude , repoq , skip_broken , security , <nl> + installroot = ' / ' , allow_downgrade = False ) : <nl> <nl> # fedora will redirect yum to dnf , which has incompatibilities <nl> # with how this module expects yum to operate . If yum - deprecated <nl> def ensure ( module , state , pkgs , conf_file , enablerepo , disablerepo , <nl> if state in [ ' installed ' , ' present ' ] : <nl> if disable_gpg_check : <nl> yum_basecmd . append ( ' - - nogpgcheck ' ) <nl> - res = install ( module , pkgs , repoq , yum_basecmd , conf_file , en_repos , dis_repos , installroot = installroot ) <nl> + res = install ( module , pkgs , repoq , yum_basecmd , conf_file , en_repos , dis_repos , installroot = installroot , allow_downgrade = allow_downgrade ) <nl> elif state in [ ' removed ' , ' absent ' ] : <nl> res = remove ( module , pkgs , repoq , yum_basecmd , conf_file , en_repos , dis_repos , installroot = installroot ) <nl> elif state = = ' latest ' : <nl> def main ( ) : <nl> installroot = dict ( required = False , default = \" / \" , type = ' str ' ) , <nl> # this should not be needed , but exists as a failsafe <nl> install_repoquery = dict ( required = False , default = \" yes \" , type = ' bool ' ) , <nl> + allow_downgrade = dict ( required = False , default = \" no \" , type = ' bool ' ) , <nl> security = dict ( default = \" no \" , type = ' bool ' ) , <nl> ) , <nl> required_one_of = [ [ ' name ' , ' list ' ] ] , <nl> def main ( ) : <nl> disable_gpg_check = params [ ' disable_gpg_check ' ] <nl> skip_broken = params [ ' skip_broken ' ] <nl> security = params [ ' security ' ] <nl> + allow_downgrade = params [ ' allow_downgrade ' ] <nl> results = ensure ( module , state , pkg , params [ ' conf_file ' ] , enablerepo , <nl> disablerepo , disable_gpg_check , exclude , repoquery , <nl> - skip_broken , security , params [ ' installroot ' ] ) <nl> + skip_broken , security , params [ ' installroot ' ] , allow_downgrade ) <nl> if repoquery : <nl> results [ ' msg ' ] = ' % s % s ' % ( results . get ( ' msg ' , ' ' ) , <nl> ' Warning : Due to potential bad behaviour with rhnplugin and certificates , used slower repoquery calls instead of Yum API . ' ) <nl>\n", "msg": "Support downgrade to specific version in yum module .\n"}
{"diff_id": 1170, "repo": "python/cpython\n", "sha": "b8c42c9825d57d35149c078cf950ccde1e8802b4\n", "time": "1997-12-19T04:29:50Z\n", "diff": "mmm a / Lib / tempfile . py <nl> ppp b / Lib / tempfile . py <nl> def gettempdir ( ) : <nl> elif os . name = = ' mac ' : <nl> import macfs , MACFS <nl> try : <nl> - refnum , dirid = macfs . FindFolder ( MACFS . kOnSystemDisk , MACFS . kTemporaryFolderType , 0 ) <nl> + refnum , dirid = macfs . FindFolder ( MACFS . kOnSystemDisk , <nl> + MACFS . kTemporaryFolderType , 0 ) <nl> dirname = macfs . FSSpec ( ( refnum , dirid , ' ' ) ) . as_pathname ( ) <nl> attempdirs . insert ( 0 , dirname ) <nl> except macfs . error : <nl> def gettempprefix ( ) : <nl> <nl> # User - callable function to return a unique temporary file name <nl> <nl> - def mktemp ( ) : <nl> + def mktemp ( suffix = \" \" ) : <nl> global counter <nl> dir = gettempdir ( ) <nl> pre = gettempprefix ( ) <nl> while 1 : <nl> counter = counter + 1 <nl> - file = os . path . join ( dir , pre + ` counter ` ) <nl> + file = os . path . join ( dir , pre + ` counter ` + suffix ) <nl> if not os . path . exists ( file ) : <nl> return file <nl> <nl> class TemporaryFileWrapper : <nl> no longer needed . <nl> \" \" \" <nl> def __init__ ( self , file , path ) : <nl> - self . file = file <nl> - self . path = path <nl> + self . file = file <nl> + self . path = path <nl> <nl> def close ( self ) : <nl> self . file . close ( ) <nl> def __del__ ( self ) : <nl> except : pass <nl> <nl> def __getattr__ ( self , name ) : <nl> - file = self . __dict__ [ ' file ' ] <nl> - a = getattr ( file , name ) <nl> + file = self . __dict__ [ ' file ' ] <nl> + a = getattr ( file , name ) <nl> setattr ( self , name , a ) <nl> return a <nl> <nl> <nl> - def TemporaryFile ( mode = ' w + b ' , bufsize = - 1 ) : <nl> - name = mktemp ( ) <nl> - file = open ( name , mode , bufsize ) <nl> + def TemporaryFile ( mode = ' w + b ' , bufsize = - 1 , suffix = \" \" ) : <nl> + name = mktemp ( suffix ) <nl> + file = open ( name , mode , bufsize ) <nl> try : <nl> os . unlink ( name ) <nl> except os . error : <nl>\n", "msg": "Add new optional parameter ' suffix ' ( default ' ' ) , which is appended to\n"}
{"diff_id": 1216, "repo": "zulip/zulip\n", "sha": "d56b16b275afafdd439a0805d9ea2ec4def56dc3\n", "time": "2019-05-06T19:37:32Z\n", "diff": "mmm a / zerver / lib / url_preview / parsers / open_graph . py <nl> ppp b / zerver / lib / url_preview / parsers / open_graph . py <nl> def extract_data ( self ) - > Dict [ str , str ] : <nl> meta = self . _soup . findAll ( ' meta ' ) <nl> content = { } <nl> for tag in meta : <nl> - if tag . has_attr ( ' property ' ) and ' og : ' in tag [ ' property ' ] : <nl> + if tag . has_attr ( ' property ' ) and ' og : ' in tag [ ' property ' ] and tag . has_attr ( ' content ' ) : <nl> content [ re . sub ( ' og : ' , ' ' , tag [ ' property ' ] ) ] = tag [ ' content ' ] <nl> return content <nl>\n", "msg": "url preview : Ignore open graph tags without a content attribute .\n"}
{"diff_id": 1229, "repo": "getredash/redash\n", "sha": "df362c12b6ae9e5deb471c7840d2646815313c81\n", "time": "2014-10-21T15:51:23Z\n", "diff": "mmm a / manage . py <nl> ppp b / manage . py <nl> def delete ( email ) : <nl> deleted_count = models . User . delete ( ) . where ( models . User . email = = email ) . execute ( ) <nl> print \" Deleted % d users . \" % deleted_count <nl> <nl> + <nl> + @ users_manager . option ( ' password ' , help = \" new password for the user \" ) <nl> + @ users_manager . option ( ' email ' , help = \" email address of the user to change password for \" ) <nl> + def password ( email , password ) : <nl> + try : <nl> + user = models . User . get_by_email ( email ) <nl> + <nl> + user . hash_password ( password ) <nl> + user . save ( ) <nl> + <nl> + print \" User updated . \" <nl> + except models . User . DoesNotExist : <nl> + print \" User [ % s ] not found . \" % email <nl> + <nl> + <nl> + @ users_manager . option ( ' email ' , help = \" email address of the user to grant admin to \" ) <nl> + def grant_admin ( email ) : <nl> + try : <nl> + user = models . User . get_by_email ( email ) <nl> + <nl> + user . groups . append ( ' admin ' ) <nl> + user . save ( ) <nl> + <nl> + print \" User updated . \" <nl> + except models . User . DoesNotExist : <nl> + print \" User [ % s ] not found . \" % email <nl> + <nl> + <nl> @ data_sources_manager . command <nl> def import_from_settings ( name = None ) : <nl> \" \" \" Import data source from settings ( env variables ) . \" \" \" <nl>\n", "msg": "Add commands to change user password and grant admin\n"}
{"diff_id": 1284, "repo": "ansible/ansible\n", "sha": "9dcea1917568dcccadcdbf10dd56612e543dd242\n", "time": "2016-12-08T16:33:43Z\n", "diff": "mmm a / lib / ansible / modules / extras / packaging / language / bower . py <nl> ppp b / lib / ansible / modules / extras / packaging / language / bower . py <nl> <nl> description : <nl> - The base path where to install the bower packages <nl> required : true <nl> + relative_execpath : <nl> + description : <nl> + - Relative path to bower executable from install path <nl> + default : null <nl> + required : false <nl> + version_added : \" 2 . 1 \" <nl> state : <nl> description : <nl> - The state of the bower package <nl> <nl> <nl> description : Update packages based on bower . json to their latest version . <nl> - bower : path = / app / location state = latest <nl> + <nl> + description : install bower locally and run from there <nl> + - npm : path = / app / location name = bower global = no <nl> + - bower : path = / app / location relative_execpath = node_modules / . bin <nl> ' ' ' <nl> <nl> <nl> def __init__ ( self , module , * * kwargs ) : <nl> self . offline = kwargs [ ' offline ' ] <nl> self . production = kwargs [ ' production ' ] <nl> self . path = kwargs [ ' path ' ] <nl> + self . relative_execpath = kwargs [ ' relative_execpath ' ] <nl> self . version = kwargs [ ' version ' ] <nl> <nl> if kwargs [ ' version ' ] : <nl> def __init__ ( self , module , * * kwargs ) : <nl> <nl> def _exec ( self , args , run_in_check_mode = False , check_rc = True ) : <nl> if not self . module . check_mode or ( self . module . check_mode and run_in_check_mode ) : <nl> - cmd = [ \" bower \" ] + args + [ ' - - config . interactive = false ' , ' - - allow - root ' ] <nl> + cmd = [ ] <nl> + <nl> + if self . relative_execpath : <nl> + cmd . append ( os . path . join ( self . path , self . relative_execpath , \" bower \" ) ) <nl> + if not os . path . isfile ( cmd [ - 1 ] ) : <nl> + self . module . fail_json ( msg = \" bower not found at relative path % s \" % self . relative_execpath ) <nl> + else : <nl> + cmd . append ( \" bower \" ) <nl> + <nl> + cmd . extend ( args ) <nl> + cmd . extend ( [ ' - - config . interactive = false ' , ' - - allow - root ' ] ) <nl> <nl> if self . name : <nl> cmd . append ( self . name_version ) <nl> def list ( self ) : <nl> dep_data = data [ ' dependencies ' ] [ dep ] <nl> if dep_data . get ( ' missing ' , False ) : <nl> missing . append ( dep ) <nl> - elif \\ <nl> - ' version ' in dep_data [ ' pkgMeta ' ] and \\ <nl> - ' update ' in dep_data and \\ <nl> - dep_data [ ' pkgMeta ' ] [ ' version ' ] ! = dep_data [ ' update ' ] [ ' latest ' ] : <nl> + elif ( ' version ' in dep_data [ ' pkgMeta ' ] and <nl> + ' update ' in dep_data and <nl> + dep_data [ ' pkgMeta ' ] [ ' version ' ] ! = dep_data [ ' update ' ] [ ' latest ' ] ) : <nl> outdated . append ( dep ) <nl> elif dep_data . get ( ' incompatible ' , False ) : <nl> outdated . append ( dep ) <nl> def main ( ) : <nl> name = dict ( default = None ) , <nl> offline = dict ( default = ' no ' , type = ' bool ' ) , <nl> production = dict ( default = ' no ' , type = ' bool ' ) , <nl> - path = dict ( required = True ) , <nl> + path = dict ( required = True , type = ' path ' ) , <nl> + relative_execpath = dict ( default = None , required = False , type = ' path ' ) , <nl> state = dict ( default = ' present ' , choices = [ ' present ' , ' absent ' , ' latest ' , ] ) , <nl> version = dict ( default = None ) , <nl> ) <nl> def main ( ) : <nl> offline = module . params [ ' offline ' ] <nl> production = module . params [ ' production ' ] <nl> path = os . path . expanduser ( module . params [ ' path ' ] ) <nl> + relative_execpath = module . params [ ' relative_execpath ' ] <nl> state = module . params [ ' state ' ] <nl> version = module . params [ ' version ' ] <nl> <nl> if state = = ' absent ' and not name : <nl> module . fail_json ( msg = ' uninstalling a package is only available for named packages ' ) <nl> <nl> - bower = Bower ( module , name = name , offline = offline , production = production , path = path , version = version ) <nl> + bower = Bower ( module , name = name , offline = offline , production = production , path = path , relative_execpath = relative_execpath , version = version ) <nl> <nl> changed = False <nl> if state = = ' present ' : <nl> def main ( ) : <nl> <nl> # Import module snippets <nl> from ansible . module_utils . basic import * <nl> - main ( ) <nl> + if __name__ = = ' __main__ ' : <nl> + main ( ) <nl>\n", "msg": "Allow relative path for bower executable\n"}
{"diff_id": 1446, "repo": "ansible/ansible\n", "sha": "3c1dea1933b96cc3d9486c659fe4ce38f4a3c85e\n", "time": "2016-12-08T16:34:59Z\n", "diff": "mmm a / lib / ansible / modules / extras / cloud / misc / proxmox . py <nl> ppp b / lib / ansible / modules / extras / cloud / misc / proxmox . py <nl> <nl> default : present <nl> notes : <nl> - Requires proxmoxer and requests modules on host . This modules can be installed with pip . <nl> - requirements : [ \" proxmoxer \" , \" requests \" ] <nl> + requirements : [ \" proxmoxer \" , \" python > = 2 . 7 \" , \" requests \" ] <nl> author : \" Sergei Antipov @ UnderGreen \" <nl> ' ' ' <nl> <nl>\n", "msg": "Add python - 2 . 6 requirement to the proxmox module\n"}
{"diff_id": 1557, "repo": "celery/celery\n", "sha": "cc778116b99031e4fdf60de5737b50b6a800af54\n", "time": "2012-07-08T13:50:10Z\n", "diff": "mmm a / celery / beat . py <nl> ppp b / celery / beat . py <nl> def setup_schedule ( self ) : <nl> else : <nl> if ' __version__ ' not in self . _store : <nl> self . _store . clear ( ) # remove schedule at 2 . 2 . 2 upgrade . <nl> + if ' utc ' not in self . _store : <nl> + self . _store . clear ( ) # remove schedule at 3 . 0 . 1 upgrade . <nl> entries = self . _store . setdefault ( ' entries ' , { } ) <nl> self . merge_inplace ( self . app . conf . CELERYBEAT_SCHEDULE ) <nl> self . install_default_entries ( self . schedule ) <nl> - self . _store [ ' __version__ ' ] = __version__ <nl> + self . _store . update ( __version__ = __version__ , utc = True ) <nl> self . sync ( ) <nl> debug ( ' Current schedule : \\ n ' + ' \\ n ' . join ( repr ( entry ) <nl> for entry in entries . itervalues ( ) ) ) <nl>\n", "msg": "Reset beat schedule to upgrade to UTC . Closes\n"}
{"diff_id": 1625, "repo": "ansible/ansible\n", "sha": "5ba67396036bb87d6b76f726569314ff43955a86\n", "time": "2014-01-21T18:41:58Z\n", "diff": "mmm a / lib / ansible / utils / plugins . py <nl> ppp b / lib / ansible / utils / plugins . py <nl> <nl> <nl> def push_basedir ( basedir ) : <nl> # avoid pushing the same absolute dir more than once <nl> - basedir = os . path . abspath ( basedir ) <nl> + basedir = os . path . realpath ( basedir ) <nl> if basedir not in _basedirs : <nl> _basedirs . insert ( 0 , basedir ) <nl> <nl> def _get_paths ( self ) : <nl> ret = [ ] <nl> ret + = self . _extra_dirs <nl> for basedir in _basedirs : <nl> - fullpath = os . path . abspath ( os . path . join ( basedir , self . subdir ) ) <nl> + fullpath = os . path . realpath ( os . path . join ( basedir , self . subdir ) ) <nl> if os . path . isdir ( fullpath ) : <nl> files = glob . glob ( \" % s / * \" % fullpath ) <nl> for file in files : <nl> def _get_paths ( self ) : <nl> # look in any configured plugin paths , allow one level deep for subcategories <nl> configured_paths = self . config . split ( os . pathsep ) <nl> for path in configured_paths : <nl> - path = os . path . abspath ( os . path . expanduser ( path ) ) <nl> + path = os . path . realpath ( os . path . expanduser ( path ) ) <nl> contents = glob . glob ( \" % s / * \" % path ) <nl> for c in contents : <nl> if os . path . isdir ( c ) and c not in ret : <nl> def add_directory ( self , directory , with_subdir = False ) : <nl> ' ' ' Adds an additional directory to the search path ' ' ' <nl> <nl> self . _paths = None <nl> - directory = os . path . abspath ( directory ) <nl> + directory = os . path . realpath ( directory ) <nl> <nl> if directory is not None : <nl> if with_subdir : <nl>\n", "msg": "Use realpath for plugin directories instead of abspath\n"}
{"diff_id": 1779, "repo": "scikit-learn/scikit-learn\n", "sha": "42c246b7b5098f8f69fe5a4c7ce0f71ebaa9aef4\n", "time": "2010-12-08T10:17:18Z\n", "diff": "mmm a / examples / applications / plot_face_recognition . py <nl> ppp b / examples / applications / plot_face_recognition . py <nl> <nl> <nl> precision recall f1 - score support <nl> <nl> - Gerhard_Schroeder 0 . 87 0 . 71 0 . 78 28 <nl> - Donald_Rumsfeld 0 . 94 0 . 88 0 . 91 33 <nl> - Tony_Blair 0 . 78 0 . 85 0 . 82 34 <nl> - Colin_Powell 0 . 84 0 . 88 0 . 86 58 <nl> - George_W_Bush 0 . 91 0 . 91 0 . 91 129 <nl> + Gerhard_Schroeder 0 . 91 0 . 75 0 . 82 28 <nl> + Donald_Rumsfeld 0 . 84 0 . 82 0 . 83 33 <nl> + Tony_Blair 0 . 65 0 . 82 0 . 73 34 <nl> + Colin_Powell 0 . 78 0 . 88 0 . 83 58 <nl> + George_W_Bush 0 . 93 0 . 86 0 . 90 129 <nl> <nl> - avg / total 0 . 88 0 . 88 0 . 88 282 <nl> + avg / total 0 . 86 0 . 84 0 . 85 282 <nl> <nl> \" \" \" <nl> print __doc__ <nl> <nl> import numpy as np <nl> import pylab as pl <nl> <nl> + from scikits . learn . grid_search import GridSearchCV <nl> from scikits . learn . metrics import classification_report <nl> from scikits . learn . metrics import confusion_matrix <nl> from scikits . learn . pca import PCA <nl> <nl> n_components = 150 <nl> <nl> print \" Extracting the top % d eigenfaces \" % n_components <nl> - pca = PCA ( n_comp = n_components , do_fast_svd = True ) . fit ( X_train ) <nl> + pca = PCA ( n_comp = n_components , whiten = True , do_fast_svd = True ) . fit ( X_train ) <nl> <nl> eigenfaces = pca . components_ . T . reshape ( ( n_components , 64 , 64 ) ) <nl> <nl> <nl> # Train a SVM classification model <nl> <nl> print \" Fitting the classifier to the training set \" <nl> - clf = SVC ( C = 1 , gamma = 5 ) . fit ( X_train_pca , y_train , class_weight = \" auto \" ) <nl> + param_grid = { <nl> + ' C ' : [ 1 , 5 , 10 , 50 , 100 ] , <nl> + ' gamma ' : [ 0 . 0001 , 0 . 0005 , 0 . 001 , 0 . 005 , 0 . 01 , 0 . 1 ] , <nl> + } <nl> + clf = GridSearchCV ( SVC ( kernel = ' rbf ' ) , param_grid , <nl> + fit_params = { ' class_weight ' : ' auto ' } ) <nl> + clf = clf . fit ( X_train_pca , y_train ) <nl> + print \" Best estimator found by grid search : \" <nl> + print clf . best_estimator <nl> <nl> <nl> # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # <nl>\n", "msg": "use a grid search for the SVM params in the faces example\n"}
{"diff_id": 1820, "repo": "pypa/pipenv\n", "sha": "bec324a194a6caf442d34fbce1e63ae2860f3dbd\n", "time": "2017-02-17T03:16:40Z\n", "diff": "mmm a / pipenv / cli . py <nl> ppp b / pipenv / cli . py <nl> def pip_install ( package_name = None , r = None , allow_global = False ) : <nl> <nl> <nl> def pip_download ( package_name ) : <nl> - c = delegator . run ( ' { 0 } download \" { 1 } \" - d { 2 } ' . format ( which_pip ( ) , package_name , project . download_location ) ) <nl> + for source in project . sources : <nl> + c = delegator . run ( <nl> + ' { 0 } download \" { 1 } \" - i { 2 } - d { 3 } ' . format ( <nl> + which_pip ( ) , <nl> + package_name , <nl> + source [ ' url ' ] , <nl> + project . download_location <nl> + ) <nl> + ) <nl> + if c . return_code = = 0 : <nl> + break <nl> + <nl> return c <nl> <nl> <nl>\n", "msg": "making pip_download use all available sources by passing the ` - i ` arg to pip for each source\n"}
{"diff_id": 1898, "repo": "ansible/ansible\n", "sha": "97621852db237c71e611b33e52e6c48138c3574f\n", "time": "2019-01-04T10:59:36Z\n", "diff": "mmm a / lib / ansible / plugins / terminal / nxos . py <nl> ppp b / lib / ansible / plugins / terminal / nxos . py <nl> def on_become ( self , passwd = None ) : <nl> if ' 15 ' in out : <nl> return <nl> <nl> + if self . validate_user_role ( ) : <nl> + return <nl> + <nl> cmd = { u ' command ' : u ' enable ' } <nl> if passwd : <nl> cmd [ u ' prompt ' ] = to_text ( r \" ( ? i ) [ \\ r \\ n ] ? Password : $ \" , errors = ' surrogate_or_strict ' ) <nl> def on_open_shell ( self ) : <nl> self . _exec_cli_command ( cmd ) <nl> except AnsibleConnectionFailure : <nl> raise AnsibleConnectionFailure ( ' unable to set terminal parameters ' ) <nl> + <nl> + def validate_user_role ( self ) : <nl> + user = self . _connection . _play_context . remote_user <nl> + <nl> + out = self . _exec_cli_command ( ' show user - account % s ' % user ) <nl> + out = to_text ( out , errors = ' surrogate_then_replace ' ) . strip ( ) <nl> + <nl> + match = re . search ( r ' roles : ( . + ) $ ' , out , re . M ) <nl> + if match : <nl> + roles = match . group ( 1 ) . split ( ) <nl> + if ' network - admin ' in roles : <nl> + return True <nl> + return False <nl>\n", "msg": "add privileged role validation for nxos become ( )\n"}
{"diff_id": 2003, "repo": "celery/celery\n", "sha": "75ab5c3656c5fd04e6d86506cd4995a363813edd\n", "time": "2015-06-25T01:30:57Z\n", "diff": "mmm a / celery / backends / redis . py <nl> ppp b / celery / backends / redis . py <nl> def _get ( key ) : <nl> ' port ' : _get ( ' PORT ' ) or 6379 , <nl> ' db ' : _get ( ' DB ' ) or 0 , <nl> ' password ' : _get ( ' PASSWORD ' ) , <nl> + ' socket_timeout ' : _get ( ' SOCKET_TIMEOUT ' ) , <nl> ' max_connections ' : self . max_connections , <nl> } <nl> if url : <nl> def _params_from_url ( self , url , defaults ) : <nl> <nl> # Query parameters override other parameters <nl> connparams . update ( query ) <nl> - connparams . update ( socket_timeout = 5 ) <nl> return connparams <nl> <nl> def get ( self , key ) : <nl>\n", "msg": "Better way for setting it through CELERY_REDIS_SOCKET_TIMEOUT\n"}
{"diff_id": 2199, "repo": "matplotlib/matplotlib\n", "sha": "30b4a6c8c807a17e647cb92531a7aaea3213ecc0\n", "time": "2017-09-25T21:34:21Z\n", "diff": "mmm a / lib / matplotlib / projections / polar . py <nl> ppp b / lib / matplotlib / projections / polar . py <nl> <nl> import matplotlib . axis as maxis <nl> from matplotlib import cbook <nl> from matplotlib import docstring <nl> + import matplotlib . markers as mmarkers <nl> import matplotlib . patches as mpatches <nl> import matplotlib . path as mpath <nl> from matplotlib import rcParams <nl> def zoom ( self , direction ) : <nl> return self . base . zoom ( direction ) <nl> <nl> <nl> + class ThetaTick ( maxis . XTick ) : <nl> + \" \" \" <nl> + A theta - axis tick . <nl> + <nl> + This subclass of ` XTick ` provides angular ticks with some small <nl> + modification to their re - positioning such that ticks are rotated based on <nl> + tick location . This results in ticks that are correctly perpendicular to <nl> + the arc spine . Labels are also rotated to be parallel to the spine . <nl> + \" \" \" <nl> + def _get_text1 ( self ) : <nl> + t = maxis . XTick . _get_text1 ( self ) <nl> + t . set_rotation_mode ( ' anchor ' ) <nl> + return t <nl> + <nl> + def _get_text2 ( self ) : <nl> + t = maxis . XTick . _get_text2 ( self ) <nl> + t . set_rotation_mode ( ' anchor ' ) <nl> + return t <nl> + <nl> + def update_position ( self , loc ) : <nl> + maxis . XTick . update_position ( self , loc ) <nl> + axes = self . axes <nl> + angle = ( loc * axes . get_theta_direction ( ) + <nl> + axes . get_theta_offset ( ) - np . pi / 2 ) <nl> + <nl> + if self . tick1On : <nl> + marker = self . tick1line . get_marker ( ) <nl> + if marker in ( mmarkers . TICKUP , ' | ' ) : <nl> + trans = mtransforms . Affine2D ( ) . scale ( 1 . 0 , 1 . 0 ) . rotate ( angle ) <nl> + elif marker = = mmarkers . TICKDOWN : <nl> + trans = mtransforms . Affine2D ( ) . scale ( 1 . 0 , - 1 . 0 ) . rotate ( angle ) <nl> + else : <nl> + # Don ' t modify custom tick line markers . <nl> + trans = self . tick1line . _marker . _transform <nl> + self . tick1line . _marker . _transform = trans <nl> + if self . tick2On : <nl> + marker = self . tick2line . get_marker ( ) <nl> + if marker in ( mmarkers . TICKUP , ' | ' ) : <nl> + trans = mtransforms . Affine2D ( ) . scale ( 1 . 0 , 1 . 0 ) . rotate ( angle ) <nl> + elif marker = = mmarkers . TICKDOWN : <nl> + trans = mtransforms . Affine2D ( ) . scale ( 1 . 0 , - 1 . 0 ) . rotate ( angle ) <nl> + else : <nl> + # Don ' t modify custom tick line markers . <nl> + trans = self . tick2line . _marker . _transform <nl> + self . tick2line . _marker . _transform = trans <nl> + <nl> + if self . label1On : <nl> + self . label1 . set_rotation ( np . rad2deg ( angle ) + self . _labelrotation ) <nl> + if self . label2On : <nl> + self . label2 . set_rotation ( np . rad2deg ( angle ) + self . _labelrotation ) <nl> + <nl> + <nl> class ThetaAxis ( maxis . XAxis ) : <nl> \" \" \" <nl> A theta Axis . <nl> def _get_tick ( self , major ) : <nl> tick_kw = self . _major_tick_kw <nl> else : <nl> tick_kw = self . _minor_tick_kw <nl> - return maxis . XTick ( self . axes , 0 , ' ' , major = major , * * tick_kw ) <nl> + return ThetaTick ( self . axes , 0 , ' ' , major = major , * * tick_kw ) <nl> <nl> def _wrap_locator_formatter ( self ) : <nl> self . set_major_locator ( ThetaLocator ( self . get_major_locator ( ) ) ) <nl> def get_xaxis_transform ( self , which = ' grid ' ) : <nl> return self . _xaxis_transform <nl> <nl> def get_xaxis_text1_transform ( self , pad ) : <nl> - return self . _xaxis_text1_transform , ' center ' , ' center ' <nl> + return self . _xaxis_text1_transform , ' bottom ' , ' center ' <nl> <nl> def get_xaxis_text2_transform ( self , pad ) : <nl> - return self . _xaxis_text2_transform , ' center ' , ' center ' <nl> + return self . _xaxis_text2_transform , ' top ' , ' center ' <nl> <nl> def get_yaxis_transform ( self , which = ' grid ' ) : <nl> if which in ( ' tick1 ' , ' tick2 ' ) : <nl>\n", "msg": "Add a custom ThetaTick for polar plots .\n"}
{"diff_id": 2204, "repo": "ansible/ansible\n", "sha": "b49d39c5dbdbf3faf959c3bdee10c245b502e1e3\n", "time": "2017-02-13T20:02:05Z\n", "diff": "mmm a / lib / ansible / modules / packaging / language / bower . py <nl> ppp b / lib / ansible / modules / packaging / language / bower . py <nl> def main ( ) : <nl> name = module . params [ ' name ' ] <nl> offline = module . params [ ' offline ' ] <nl> production = module . params [ ' production ' ] <nl> - path = os . path . expanduser ( module . params [ ' path ' ] ) <nl> + path = module . params [ ' path ' ] <nl> relative_execpath = module . params [ ' relative_execpath ' ] <nl> state = module . params [ ' state ' ] <nl> version = module . params [ ' version ' ] <nl>\n", "msg": "Removes usage of expanduser because of type path\n"}
{"diff_id": 2250, "repo": "zulip/zulip\n", "sha": "114b0a2982075f9436c3a0e673872e2352b6cf0c\n", "time": "2020-08-11T17:47:13Z\n", "diff": "mmm a / zerver / views / message_edit . py <nl> ppp b / zerver / views / message_edit . py <nl> def get_message_edit_history ( request : HttpRequest , user_profile : UserProfile , <nl> <nl> # Fill in all the extra data that will make it usable <nl> fill_edit_history_entries ( message_edit_history , message ) <nl> - return json_success ( { \" message_history \" : reversed ( message_edit_history ) } ) <nl> + return json_success ( { \" message_history \" : list ( reversed ( message_edit_history ) ) } ) <nl> <nl> PROPAGATE_MODE_VALUES = [ \" change_later \" , \" change_one \" , \" change_all \" ] <nl> @ has_request_variables <nl>\n", "msg": "message_edit : Output a list , not a reversed iterator .\n"}
{"diff_id": 2252, "repo": "zulip/zulip\n", "sha": "84b148728fa745c54af7da3c37dc98b2a980d9e3\n", "time": "2018-08-04T23:17:34Z\n", "diff": "mmm a / zilencer / tests / test_stripe . py <nl> ppp b / zilencer / tests / test_stripe . py <nl> def test_get_seat_count ( self ) - > None : <nl> do_deactivate_user ( user2 ) <nl> self . assertEqual ( get_seat_count ( realm ) , initial_count ) <nl> <nl> - @ mock . patch ( \" stripe . Customer . retrieve \" , side_effect = mock_customer_with_active_subscription ) <nl> - @ mock . patch ( \" stripe . Customer . create \" , side_effect = mock_create_customer ) <nl> - def test_extract_current_subscription ( self , mock_create_customer : mock . Mock , <nl> - mock_customer_with_active_subscription : mock . Mock ) - > None : <nl> + def test_extract_current_subscription ( self ) - > None : <nl> # Only the most basic test . In particular , doesn ' t include testing with a <nl> # canceled subscription , because we don ' t have a fixture for it . <nl> - customer_without_subscription = stripe . Customer . create ( ) # type : ignore # Mocked out function call <nl> - self . assertIsNone ( extract_current_subscription ( customer_without_subscription ) ) <nl> + self . assertIsNone ( extract_current_subscription ( mock_create_customer ( ) ) ) <nl> <nl> - customer_with_subscription = stripe . Customer . retrieve ( ) # type : ignore # Mocked out function call <nl> - subscription = extract_current_subscription ( customer_with_subscription ) <nl> + subscription = extract_current_subscription ( mock_customer_with_active_subscription ( ) ) <nl> self . assertEqual ( subscription [ \" id \" ] [ : 4 ] , \" sub_ \" ) <nl> <nl> @ mock . patch ( \" stripe . Customer . retrieve \" , side_effect = mock_customer_with_active_subscription ) <nl>\n", "msg": "billing : Don ' t mock functions in test_extract_current_subscription .\n"}
{"diff_id": 2334, "repo": "bokeh/bokeh\n", "sha": "5fd9a2a58266d1c495b92d00bbb049c9152eac43\n", "time": "2014-02-21T22:29:01Z\n", "diff": "mmm a / extensions / bokeh_magic . py <nl> ppp b / extensions / bokeh_magic . py <nl> def bokeh ( self , arg , line = None ) : <nl> mmmmmm - - <nl> <nl> <nl> - In [ 1 ] : % install_ext url_fot_bokeh_extension <nl> + In [ 1 ] : % install_ext url_for_bokeh_extension <nl> <nl> In [ 2 ] : % load_ext bokeh_magic <nl> <nl> def bokeh ( self , arg , line = None ) : <nl> <nl> To enable bokeh for usage with the IPython Notebook : : <nl> <nl> - In [ 3 ] : % bokeh <nl> + In [ 3 ] : % bokeh - - notebook <nl> + <nl> + Then you can use several ` modes ` listed below : : <nl> + <nl> + In [ 4 ] : % bokeh - - show [ - s ] # to enable the autoshow function <nl> + <nl> + In [ 5 ] : % bokeh - - show - off [ - s - off ] to disable the autoshow function <nl> <nl> Note : In order to actually use this magic , you need to have <nl> get_ipython ( ) , so you need to have a running IPython kernel . <nl> def bokeh ( self , arg , line = None ) : <nl> # Activate / deactivate the execution of func accordingly with the args . <nl> if args . notebook : <nl> # Configuring embedded BokehJS mode . <nl> - output_notebook ( ) <nl> - self . has_run = True <nl> + self . notebook_output ( ) <nl> elif args . show : <nl> if not self . has_run : <nl> - # Configuring embedded BokehJS mode . <nl> - output_notebook ( ) <nl> - self . has_run = True <nl> + self . notebook_output ( ) <nl> # Register a function for calling after code execution . <nl> ip . register_post_execute ( self . notebook_show ) <nl> print \" Automatic show ( ) is enable . \" <nl> elif args . show_off : <nl> try : <nl> if not self . has_run : <nl> - # Configuring embedded BokehJS mode . <nl> - output_notebook ( ) <nl> - self . has_run = True <nl> + self . notebook_output ( ) <nl> # Unregister a function from the _post_execute dict . <nl> del ip . _post_execute [ self . notebook_show ] <nl> print \" Automatic show ( ) is disable . \" <nl> except KeyError : <nl> raise UsageError ( \" You have to enable the - - show mode before trying to disable it . \" ) <nl> <nl> + def notebook_output ( self ) : <nl> + output_notebook ( ) <nl> + self . has_run = True <nl> + <nl> def notebook_show ( self ) : <nl> try : <nl> show ( ) <nl>\n", "msg": "Some refactoring and better docstrings .\n"}
{"diff_id": 2400, "repo": "sanic-org/sanic\n", "sha": "a811c84e9947a731bb54f55c90f5d0a10cb5886b\n", "time": "2017-01-21T20:40:34Z\n", "diff": "mmm a / sanic / response . py <nl> ppp b / sanic / response . py <nl> def cookies ( self ) : <nl> return self . _cookies <nl> <nl> <nl> - def json ( body , status = 200 , headers = None ) : <nl> + def json ( body , status = 200 , headers = None , * * kwargs ) : <nl> \" \" \" <nl> Returns response object with body in json format . <nl> : param body : Response data to be serialized . <nl> : param status : Response code . <nl> : param headers : Custom Headers . <nl> + : param \\ * * kwargs : Remaining arguments that are passed to the json encoder . <nl> \" \" \" <nl> - return HTTPResponse ( json_dumps ( body ) , headers = headers , status = status , <nl> + return HTTPResponse ( json_dumps ( body , * * kwargs ) , headers = headers , status = status , <nl> content_type = \" application / json \" ) <nl> <nl> <nl>\n", "msg": "allowed passing arguments to json response encoder\n"}
{"diff_id": 2431, "repo": "ansible/ansible\n", "sha": "f85f575a58e9dde66cb176ed3fccd1bbea0dac9e\n", "time": "2016-12-08T16:33:57Z\n", "diff": "mmm a / lib / ansible / modules / extras / clustering / znode . py <nl> ppp b / lib / ansible / modules / extras / clustering / znode . py <nl> <nl> - The amount of time to wait for a node to appear . <nl> default : 300 <nl> required : false <nl> + recursive : <nl> + description : <nl> + - Recursively delete node and all its children . <nl> + default : False <nl> + required : false <nl> + version_added : \" 2 . 1 \" <nl> requirements : <nl> - kazoo > = 2 . 1 <nl> - python > = 2 . 6 <nl> def main ( ) : <nl> value = dict ( required = False , default = None , type = ' str ' ) , <nl> op = dict ( required = False , default = None , choices = [ ' get ' , ' wait ' , ' list ' ] ) , <nl> state = dict ( choices = [ ' present ' , ' absent ' ] ) , <nl> - timeout = dict ( required = False , default = 300 , type = ' int ' ) <nl> + timeout = dict ( required = False , default = 300 , type = ' int ' ) , <nl> + recursive = dict ( required = False , default = False , type = ' bool ' ) <nl> ) , <nl> supports_check_mode = False <nl> ) <nl> def wait ( self ) : <nl> <nl> def _absent ( self , znode ) : <nl> if self . exists ( znode ) : <nl> - self . zk . delete ( znode ) <nl> + self . zk . delete ( znode , recursive = self . module . params [ ' recursive ' ] ) <nl> return True , { ' changed ' : True , ' msg ' : ' The znode was deleted . ' } <nl> else : <nl> return True , { ' changed ' : False , ' msg ' : ' The znode does not exist . ' } <nl>\n", "msg": "Add support for recursive znode deletion\n"}
{"diff_id": 2451, "repo": "scrapy/scrapy\n", "sha": "8123c427373778398036bb375bf03b3cd6b240de\n", "time": "2020-08-29T07:01:58Z\n", "diff": "mmm a / tests / test_crawl . py <nl> ppp b / tests / test_crawl . py <nl> def setUp ( self ) : <nl> def tearDown ( self ) : <nl> self . mockserver . __exit__ ( None , None , None ) <nl> <nl> + @ defer . inlineCallbacks <nl> + def _run_spider ( self , spider_cls ) : <nl> + items = [ ] <nl> + <nl> + def _on_item_scraped ( item ) : <nl> + items . append ( item ) <nl> + <nl> + crawler = self . runner . create_crawler ( spider_cls ) <nl> + crawler . signals . connect ( _on_item_scraped , signals . item_scraped ) <nl> + with LogCapture ( ) as log : <nl> + yield crawler . crawl ( self . mockserver . url ( \" / status ? n = 200 \" ) , mockserver = self . mockserver ) <nl> + return log , items , crawler . stats <nl> + <nl> @ defer . inlineCallbacks <nl> def test_crawlspider_with_parse ( self ) : <nl> self . runner . crawl ( CrawlSpiderWithParseMethod , mockserver = self . mockserver ) <nl> def test_async_def_asyncio_parse ( self ) : <nl> @ mark . only_asyncio ( ) <nl> @ defer . inlineCallbacks <nl> def test_async_def_asyncio_parse_items_list ( self ) : <nl> - items = [ ] <nl> - <nl> - def _on_item_scraped ( item ) : <nl> - items . append ( item ) <nl> - <nl> - crawler = self . runner . create_crawler ( AsyncDefAsyncioReturnSpider ) <nl> - crawler . signals . connect ( _on_item_scraped , signals . item_scraped ) <nl> - with LogCapture ( ) as log : <nl> - yield crawler . crawl ( self . mockserver . url ( \" / status ? n = 200 \" ) , mockserver = self . mockserver ) <nl> + log , items , _ = yield self . _run_spider ( AsyncDefAsyncioReturnSpider ) <nl> self . assertIn ( \" Got response 200 \" , str ( log ) ) <nl> self . assertIn ( { ' id ' : 1 } , items ) <nl> self . assertIn ( { ' id ' : 2 } , items ) <nl> def _on_item_scraped ( item ) : <nl> @ mark . only_asyncio ( ) <nl> @ defer . inlineCallbacks <nl> def test_async_def_asyncgen_parse ( self ) : <nl> - crawler = self . runner . create_crawler ( AsyncDefAsyncioGenSpider ) <nl> - with LogCapture ( ) as log : <nl> - yield crawler . crawl ( self . mockserver . url ( \" / status ? n = 200 \" ) , mockserver = self . mockserver ) <nl> + log , _ , stats = yield self . _run_spider ( AsyncDefAsyncioGenSpider ) <nl> self . assertIn ( \" Got response 200 \" , str ( log ) ) <nl> - itemcount = crawler . stats . get_value ( ' item_scraped_count ' ) <nl> + itemcount = stats . get_value ( ' item_scraped_count ' ) <nl> self . assertEqual ( itemcount , 1 ) <nl> <nl> @ mark . only_asyncio ( ) <nl> @ defer . inlineCallbacks <nl> def test_async_def_asyncgen_parse_loop ( self ) : <nl> - items = [ ] <nl> - <nl> - def _on_item_scraped ( item ) : <nl> - items . append ( item ) <nl> - <nl> - crawler = self . runner . create_crawler ( AsyncDefAsyncioGenLoopSpider ) <nl> - crawler . signals . connect ( _on_item_scraped , signals . item_scraped ) <nl> - with LogCapture ( ) as log : <nl> - yield crawler . crawl ( self . mockserver . url ( \" / status ? n = 200 \" ) , mockserver = self . mockserver ) <nl> + log , items , stats = yield self . _run_spider ( AsyncDefAsyncioGenLoopSpider ) <nl> self . assertIn ( \" Got response 200 \" , str ( log ) ) <nl> - itemcount = crawler . stats . get_value ( ' item_scraped_count ' ) <nl> + itemcount = stats . get_value ( ' item_scraped_count ' ) <nl> self . assertEqual ( itemcount , 10 ) <nl> for i in range ( 10 ) : <nl> self . assertIn ( { ' foo ' : i } , items ) <nl> def _on_item_scraped ( item ) : <nl> @ mark . only_asyncio ( ) <nl> @ defer . inlineCallbacks <nl> def test_async_def_asyncgen_parse_complex ( self ) : <nl> - items = [ ] <nl> - <nl> - def _on_item_scraped ( item ) : <nl> - items . append ( item ) <nl> - <nl> - crawler = self . runner . create_crawler ( AsyncDefAsyncioGenComplexSpider ) <nl> - crawler . signals . connect ( _on_item_scraped , signals . item_scraped ) <nl> - yield crawler . crawl ( mockserver = self . mockserver ) <nl> - itemcount = crawler . stats . get_value ( ' item_scraped_count ' ) <nl> + _ , items , stats = yield self . _run_spider ( AsyncDefAsyncioGenComplexSpider ) <nl> + itemcount = stats . get_value ( ' item_scraped_count ' ) <nl> self . assertEqual ( itemcount , 156 ) <nl> # some random items <nl> for i in [ 1 , 4 , 21 , 22 , 207 , 311 ] : <nl> def _on_item_scraped ( item ) : <nl> @ mark . only_asyncio ( ) <nl> @ defer . inlineCallbacks <nl> def test_async_def_asyncio_parse_reqs_list ( self ) : <nl> - crawler = self . runner . create_crawler ( AsyncDefAsyncioReqsReturnSpider ) <nl> - with LogCapture ( ) as log : <nl> - yield crawler . crawl ( self . mockserver . url ( \" / status ? n = 200 \" ) , mockserver = self . mockserver ) <nl> + log , * _ = yield self . _run_spider ( AsyncDefAsyncioReqsReturnSpider ) <nl> for req_id in range ( 3 ) : <nl> self . assertIn ( \" Got response 200 , req_id % d \" % req_id , str ( log ) ) <nl> <nl>\n", "msg": "Simplify running spiders in CrawlSpiderTestCase .\n"}
{"diff_id": 2599, "repo": "zulip/zulip\n", "sha": "8487a24a6c3ea57939764aa8f4b44d3505fe6f9f\n", "time": "2017-02-17T23:20:32Z\n", "diff": "mmm a / zerver / tornado / socket . py <nl> ppp b / zerver / tornado / socket . py <nl> def on_close ( self ) : <nl> <nl> def fake_message_sender ( event ) : <nl> # type : ( Dict [ str , Any ] ) - > None <nl> + \" \" \" This function is used only for Casper and backend tests , where <nl> + rabbitmq is disabled \" \" \" <nl> log_data = dict ( ) # type : Dict [ str , Any ] <nl> record_request_start_data ( log_data ) <nl> <nl> req = event [ ' request ' ] <nl> try : <nl> sender = get_user_profile_by_id ( event [ ' server_meta ' ] [ ' user_id ' ] ) <nl> - client = get_client ( req [ ' client ' ] ) <nl> + client = get_client ( \" website \" ) <nl> <nl> msg_id = check_send_message ( sender , client , req [ ' type ' ] , <nl> extract_recipients ( req [ ' to ' ] ) , <nl>\n", "msg": "socket : Hardcode website message sender for fake messages .\n"}
{"diff_id": 2701, "repo": "python/cpython\n", "sha": "50bb7e12ec32da69b89a9b7cfdfdd899d2c8c685\n", "time": "2008-08-02T03:15:20Z\n", "diff": "mmm a / Lib / tokenize . py <nl> ppp b / Lib / tokenize . py <nl> class TokenError ( Exception ) : pass <nl> <nl> class StopTokenizing ( Exception ) : pass <nl> <nl> - def printtoken ( type , token , ( srow , scol ) , ( erow , ecol ) , line ) : # for testing <nl> + def printtoken ( type , token , srow_scol , erow_ecol , line ) : # for testing <nl> + srow , scol = srow_scol <nl> + erow , ecol = erow_ecol <nl> print \" % d , % d - % d , % d : \\ t % s \\ t % s \" % \\ <nl> ( srow , scol , erow , ecol , tok_name [ type ] , repr ( token ) ) <nl> <nl>\n", "msg": "Remove a tuple unpacking in a parameter list to remove a SyntaxWarning raised\n"}
{"diff_id": 2702, "repo": "ansible/ansible\n", "sha": "1683d44d2e4669499bd74f035a31c65ac6502368\n", "time": "2013-07-04T20:04:31Z\n", "diff": "mmm a / lib / ansible / runner / connection_plugins / ssh . py <nl> ppp b / lib / ansible / runner / connection_plugins / ssh . py <nl> def _send_password ( self ) : <nl> os . write ( self . wfd , \" % s \\ n \" % self . password ) <nl> os . close ( self . wfd ) <nl> <nl> + def not_in_host_file ( self , host ) : <nl> + host_file = os . path . expanduser ( \" ~ / . ssh / known_hosts \" ) <nl> + if not os . path . exists ( host_file ) : <nl> + print \" previous known host file not found \" <nl> + return True <nl> + host_fh = open ( host_file ) <nl> + data = host_fh . read ( ) <nl> + host_fh . close ( ) <nl> + for line in data . split ( \" \\ n \" ) : <nl> + if line is None or line . find ( \" \" ) = = - 1 : <nl> + continue <nl> + tokens = line . split ( ) <nl> + if host in tokens [ 0 ] : <nl> + return False <nl> + return True <nl> + <nl> def exec_command ( self , cmd , tmp_path , sudo_user , sudoable = False , executable = ' / bin / sh ' ) : <nl> ' ' ' run a command on the remote host ' ' ' <nl> <nl> def exec_command ( self , cmd , tmp_path , sudo_user , sudoable = False , executable = ' / bin <nl> ssh_cmd . append ( sudocmd ) <nl> <nl> vvv ( \" EXEC % s \" % ssh_cmd , host = self . host ) <nl> + <nl> + not_in_host_file = self . not_in_host_file ( self . host ) <nl> + <nl> + if C . HOST_KEY_CHECKING and not_in_host_file : <nl> + # lock around the initial SSH connectivity so the user prompt about whether to add <nl> + # the host to known hosts is not intermingled with multiprocess output . <nl> + KEY_LOCK = self . runner . lockfile <nl> + fcntl . lockf ( KEY_LOCK , fcntl . LOCK_EX ) <nl> + <nl> + <nl> try : <nl> # Make sure stdin is a proper ( pseudo ) pty to avoid : tcgetattr errors <nl> import pty <nl> def exec_command ( self , cmd , tmp_path , sudo_user , sudoable = False , executable = ' / bin <nl> elif p . poll ( ) is not None : <nl> break <nl> stdin . close ( ) # close stdin after we read from stdout ( see also issue # 848 ) <nl> + <nl> + if C . HOST_KEY_CHECKING and not_in_host_file : <nl> + # lock around the initial SSH connectivity so the user prompt about whether to add <nl> + # the host to known hosts is not intermingled with multiprocess output . <nl> + KEY_LOCK = self . runner . lockfile <nl> + fcntl . lockf ( KEY_LOCK , fcntl . LOCK_EX ) <nl> <nl> if p . returncode ! = 0 and stderr . find ( ' Bad configuration option : ControlPersist ' ) ! = - 1 : <nl> raise errors . AnsibleError ( ' using - c ssh on certain older ssh versions may not support ControlPersist , set ANSIBLE_SSH_ARGS = \" \" ( or ansible_ssh_args in the config file ) before running again ' ) <nl>\n", "msg": "Lock around SSH connectivity to new hosts in host checking mode such that prompts for host approval\n"}
{"diff_id": 2760, "repo": "zulip/zulip\n", "sha": "2eb8c6aae3709f48c6a32248f2362565ff8666ee\n", "time": "2013-09-10T17:25:59Z\n", "diff": "mmm a / zerver / management / commands / analyze_user_activity . py <nl> ppp b / zerver / management / commands / analyze_user_activity . py <nl> <nl> <nl> def analyze_activity ( options ) : <nl> day_start = datetime . datetime . strptime ( options [ \" date \" ] , \" % Y - % m - % d \" ) . replace ( tzinfo = utc ) <nl> - day_end = day_start + datetime . timedelta ( days = 1 ) <nl> + day_end = day_start + datetime . timedelta ( days = options [ \" duration \" ] ) <nl> <nl> user_profile_query = UserProfile . objects . all ( ) <nl> if options [ \" realm \" ] : <nl> user_profile_query = user_profile_query . filter ( realm__domain = options [ \" realm \" ] ) <nl> <nl> + print \" Per - user online duration : \\ n \" <nl> total_duration = datetime . timedelta ( 0 ) <nl> for user_profile in user_profile_query : <nl> intervals = UserActivityInterval . objects . filter ( user_profile = user_profile , <nl> def analyze_activity ( options ) : <nl> duration + = end - start <nl> <nl> total_duration + = duration <nl> - print user_profile . email , duration <nl> + print \" % - * s % s \" % ( 37 , user_profile . email , duration , ) <nl> <nl> - print \" Total Duration : % s \" % ( total_duration , ) <nl> - print \" Total Duration in minutes : % s \" % ( total_duration . total_seconds ( ) / 60 . , ) <nl> + print \" \\ nTotal Duration : % s \" % ( total_duration , ) <nl> + print \" \\ nTotal Duration in minutes : % s \" % ( total_duration . total_seconds ( ) / 60 . , ) <nl> print \" Total Duration amortized to a month : % s \" % ( total_duration . total_seconds ( ) * 30 . / 60 . , ) <nl> <nl> class Command ( BaseCommand ) : <nl> + help = \" \" \" Report analytics of user activity on a per - user and realm basis . <nl> + <nl> + This command aggregates user activity data that is collected by each user using Zulip . It attempts <nl> + to approximate how much each user has been using Zulip per day , measured by recording each 15 minute <nl> + period where some activity has occurred ( mouse move or keyboard activity ) . <nl> + <nl> + It will correctly not count server - initiated reloads in the activity statistics . <nl> + <nl> + The duration flag can be used to control how many days to show usage duration for <nl> + <nl> + Usage : python manage . py analyze_user_activity [ - - realm = zulip . com ] [ - - date = 2013 - 09 - 10 ] [ - - duration = 1 ] <nl> + <nl> + By default , if no date is selected 2013 - 09 - 10 is used . If no realm is provided , information <nl> + is shown for all realms \" \" \" <nl> + <nl> option_list = BaseCommand . option_list + ( <nl> make_option ( ' - - realm ' , action = ' store ' ) , <nl> make_option ( ' - - date ' , action = ' store ' , default = \" 2013 - 09 - 06 \" ) , <nl> + make_option ( ' - - duration ' , action = ' store ' , default = 1 , type = int , help = \" How many days to show usage information for \" ) , <nl> ) <nl> <nl> def handle ( self , * args , * * options ) : <nl>\n", "msg": "Add documentation and a duration argument to analyze_user_activity . py\n"}
{"diff_id": 2820, "repo": "ansible/ansible\n", "sha": "95dc4ec5ec3bca59f7ef1d61a9540d4efd38435c\n", "time": "2016-12-08T16:33:06Z\n", "diff": "mmm a / lib / ansible / modules / extras / packaging / os / apk . py <nl> ppp b / lib / ansible / modules / extras / packaging / os / apk . py <nl> <nl> # Update repositories and install \" foo \" package <nl> - apk : name = foo update_cache = yes <nl> <nl> + # Update repositories and install \" foo \" and \" bar \" packages <nl> + - apk : name = foo , bar update_cache = yes <nl> + <nl> # Remove \" foo \" package <nl> - apk : name = foo state = absent <nl> <nl> <nl> # Install the package \" foo \" <nl> - apk : name = foo state = present <nl> <nl> + # Install the packages \" foo \" and \" bar \" <nl> + - apk : name = foo , bar state = present <nl> + <nl> # Update repositories and update package \" foo \" to latest version <nl> - apk : name = foo state = latest update_cache = yes <nl> <nl> + # Update repositories and update packages \" foo \" and \" bar \" to latest versions <nl> + - apk : name = foo , bar state = latest update_cache = yes <nl> + <nl> # Update all installed packages to the latest versions <nl> - apk : upgrade = yes <nl> <nl> def upgrade_packages ( module ) : <nl> module . exit_json ( changed = False , msg = \" packages already upgraded \" ) <nl> module . exit_json ( changed = True , msg = \" upgraded packages \" ) <nl> <nl> - def install_package ( module , name , state ) : <nl> + def install_packages ( module , names , state ) : <nl> upgrade = False <nl> - installed = query_package ( module , name ) <nl> - latest = query_latest ( module , name ) <nl> - if state = = ' latest ' and not latest : <nl> - upgrade = True <nl> - if installed and not upgrade : <nl> - module . exit_json ( changed = False , msg = \" package already installed \" ) <nl> + uninstalled = [ ] <nl> + for name in names : <nl> + if not query_package ( module , name ) : <nl> + uninstalled . append ( name ) <nl> + elif state = = ' latest ' and not query_latest ( module , name ) : <nl> + upgrade = True <nl> + if not uninstalled and not upgrade : <nl> + module . exit_json ( changed = False , msg = \" package ( s ) already installed \" ) <nl> + names = \" \" . join ( uninstalled ) <nl> if upgrade : <nl> if module . check_mode : <nl> - cmd = \" apk add - - upgrade - - simulate % s \" % ( name ) <nl> + cmd = \" apk add - - upgrade - - simulate % s \" % ( names ) <nl> else : <nl> - cmd = \" apk add - - upgrade % s \" % ( name ) <nl> + cmd = \" apk add - - upgrade % s \" % ( names ) <nl> else : <nl> if module . check_mode : <nl> - cmd = \" apk add - - simulate % s \" % ( name ) <nl> + cmd = \" apk add - - simulate % s \" % ( names ) <nl> else : <nl> - cmd = \" apk add % s \" % ( name ) <nl> + cmd = \" apk add % s \" % ( names ) <nl> rc , stdout , stderr = module . run_command ( cmd , check_rc = False ) <nl> if rc ! = 0 : <nl> - module . fail_json ( msg = \" failed to install % s \" % ( name ) ) <nl> - module . exit_json ( changed = True , msg = \" installed % s package \" % ( name ) ) <nl> + module . fail_json ( msg = \" failed to install % s \" % ( names ) ) <nl> + module . exit_json ( changed = True , msg = \" installed % s package ( s ) \" % ( names ) ) <nl> <nl> def remove_packages ( module , names ) : <nl> installed = [ ] <nl> def main ( ) : <nl> names = filter ( ( lambda x : x ! = ' ' ) , p [ ' name ' ] . split ( ' , ' ) ) <nl> <nl> if p [ ' state ' ] in [ ' present ' , ' latest ' ] : <nl> - install_package ( module , p [ ' name ' ] , p [ ' state ' ] ) <nl> + install_packages ( module , names , p [ ' state ' ] ) <nl> elif p [ ' state ' ] = = ' absent ' : <nl> remove_packages ( module , names ) <nl> <nl>\n", "msg": "Allow multiple packages to be installed at the same time\n"}
{"diff_id": 2822, "repo": "ytdl-org/youtube-dl\n", "sha": "dc016bf5216d4c0d5b5fb2cd707e1d08fa4b0517\n", "time": "2015-12-22T08:55:25Z\n", "diff": "mmm a / youtube_dl / extractor / viki . py <nl> ppp b / youtube_dl / extractor / viki . py <nl> class VikiBaseIE ( InfoExtractor ) : <nl> <nl> _token = None <nl> <nl> + _ERRORS = { <nl> + ' geo ' : ' Sorry , this content is not available in your region . ' , <nl> + ' upcoming ' : ' Sorry , this content is not yet available . ' , <nl> + # ' paywall ' : ' paywall ' , <nl> + } <nl> + <nl> def _prepare_call ( self , path , timestamp = None , post_data = None ) : <nl> path + = ' ? ' if ' ? ' not in path else ' & ' <nl> if not timestamp : <nl> def _raise_error ( self , error ) : <nl> ' % s returned error : % s ' % ( self . IE_NAME , error ) , <nl> expected = True ) <nl> <nl> + def _check_errors ( self , data ) : <nl> + for reason , status in data . get ( ' blocking ' , { } ) . items ( ) : <nl> + if status and reason in self . _ERRORS : <nl> + raise ExtractorError ( ' % s said : % s ' % ( <nl> + self . IE_NAME , self . _ERRORS [ reason ] ) , expected = True ) <nl> + <nl> def _real_initialize ( self ) : <nl> self . _login ( ) <nl> <nl> class VikiIE ( VikiBaseIE ) : <nl> ' timestamp ' : 1321985454 , <nl> ' description ' : ' md5 : 44b1e46619df3a072294645c770cef36 ' , <nl> ' title ' : ' Love In Magic ' , <nl> + ' age_limit ' : 13 , <nl> } , <nl> } ] <nl> <nl> def _real_extract ( self , url ) : <nl> video = self . _call_api ( <nl> ' videos / % s . json ' % video_id , video_id , ' Downloading video JSON ' ) <nl> <nl> + self . _check_errors ( video ) <nl> + <nl> title = self . dict_selection ( video . get ( ' titles ' , { } ) , ' en ' ) <nl> if not title : <nl> title = ' Episode % d ' % video . get ( ' number ' ) if video . get ( ' type ' ) = = ' episode ' else video . get ( ' id ' ) or video_id <nl> def _real_extract ( self , url ) : <nl> r ' ^ ( \\ d + ) [ pP ] $ ' , format_id , ' height ' , default = None ) ) <nl> for protocol , format_dict in stream_dict . items ( ) : <nl> if format_id = = ' m3u8 ' : <nl> - formats = self . _extract_m3u8_formats ( <nl> - format_dict [ ' url ' ] , video_id , ' mp4 ' , m3u8_id = ' m3u8 - % s ' % protocol ) <nl> + m3u8_formats = self . _extract_m3u8_formats ( <nl> + format_dict [ ' url ' ] , video_id , ' mp4 ' , ' m3u8_native ' , <nl> + m3u8_id = ' m3u8 - % s ' % protocol , fatal = None ) <nl> + if m3u8_formats : <nl> + formats . extend ( m3u8_formats ) <nl> else : <nl> formats . append ( { <nl> ' url ' : format_dict [ ' url ' ] , <nl> def _real_extract ( self , url ) : <nl> ' containers / % s . json ' % channel_id , channel_id , <nl> ' Downloading channel JSON ' ) <nl> <nl> + self . _check_errors ( channel ) <nl> + <nl> title = self . dict_selection ( channel [ ' titles ' ] , ' en ' ) <nl> <nl> description = self . dict_selection ( channel [ ' descriptions ' ] , ' en ' ) <nl>\n", "msg": "[ viki ] detect errors and fix formats extraction\n"}
{"diff_id": 2832, "repo": "ipython/ipython\n", "sha": "9a985feca5d447559e4d7a631b1f635991a53b0a\n", "time": "2012-02-03T19:57:49Z\n", "diff": "mmm a / IPython / parallel / client / view . py <nl> ppp b / IPython / parallel / client / view . py <nl> def importer ( self ) : <nl> return self . sync_imports ( True ) <nl> <nl> @ contextmanager <nl> - def sync_imports ( self , local = True ) : <nl> + def sync_imports ( self , local = True , quiet = False ) : <nl> \" \" \" Context Manager for performing simultaneous local and remote imports . <nl> <nl> ' import x as y ' will * not * work . The ' as y ' part will simply be ignored . <nl> <nl> If ` local = True ` , then the package will also be imported locally . <nl> <nl> + If ` quiet = True ` , then no message concerning the success of import will be <nl> + reported . <nl> + <nl> Note that remote - only ( ` local = False ` ) imports have not been implemented . <nl> <nl> > > > with view . sync_imports ( ) : <nl> def view_import ( name , globals = { } , locals = { } , fromlist = [ ] , level = - 1 ) : <nl> key = name + ' : ' + ' , ' . join ( fromlist or [ ] ) <nl> if level = = - 1 and key not in modules : <nl> modules . add ( key ) <nl> - if fromlist : <nl> - print \" importing % s from % s on engine ( s ) \" % ( ' , ' . join ( fromlist ) , name ) <nl> - else : <nl> - print \" importing % s on engine ( s ) \" % name <nl> + if not quiet : <nl> + if fromlist : <nl> + print \" importing % s from % s on engine ( s ) \" % ( ' , ' . join ( fromlist ) , name ) <nl> + else : <nl> + print \" importing % s on engine ( s ) \" % name <nl> results . append ( self . apply_async ( remote_import , name , fromlist , level ) ) <nl> # restore override <nl> __builtin__ . __import__ = save_import <nl>\n", "msg": "Adds a quiet keyword to sync_imports to allow users to surpress messages about imports on remote engines .\n"}
{"diff_id": 2971, "repo": "pypa/pipenv\n", "sha": "fafb4b9f507877322afa3771034a0b3bfc88537f\n", "time": "2017-09-14T17:21:58Z\n", "diff": "mmm a / pipenv / utils . py <nl> ppp b / pipenv / utils . py <nl> def python_version ( path_to_python ) : <nl> if not path_to_python : <nl> return None <nl> <nl> + # Quote the path to Python , for Windows . <nl> + path_to_python = shellquote ( path_to_python ) <nl> + <nl> try : <nl> TEMPLATE = ' Python { } . { } . { } ' <nl> print ( ' { 0 } - - version ' . format ( path_to_python ) ) <nl>\n", "msg": "quote the path to python , for windows\n"}
{"diff_id": 2997, "repo": "scikit-learn/scikit-learn\n", "sha": "e7d86c77471d3b0890287e0ca32ecfb94b80abda\n", "time": "2010-03-03T17:08:05Z\n", "diff": "new file mode 100644 <nl> index 000000000000 . . 4e777d62a43e <nl> mmm / dev / null <nl> ppp b / scikits / learn / utils / crossval . py <nl> <nl> + # Author : Alexandre Gramfort < alexandre . gramfort @ inria . fr > <nl> + # License : BSD Style . <nl> + <nl> + # $ Id : cd . py 473 2010 - 03 - 03 16 : 27 : 38Z twigster $ <nl> + <nl> + import numpy as np <nl> + import exceptions <nl> + <nl> + class LOO : <nl> + \" \" \" <nl> + Leave - One - Out cross validation : <nl> + Provides train / test indexes to split data in train test sets <nl> + <nl> + Examples : <nl> + import scikits . learn . utils . crossval <nl> + import numpy as np <nl> + n_samples , n_features = 5 , 10 <nl> + X = np . random . randn ( n_samples , n_features ) <nl> + print X <nl> + loo = crossval . LOO ( n_samples ) <nl> + print loo [ 1 ] <nl> + for train_index , test_index in loo : <nl> + print \" TRAIN : \" , train_index , \" TEST : \" , test_index <nl> + \" \" \" <nl> + <nl> + def __init__ ( self , n ) : <nl> + \" \" \" <nl> + n : is the size of the dataset to split <nl> + \" \" \" <nl> + self . n_folds = n <nl> + self . iter = 0 <nl> + <nl> + def __getitem__ ( self , item ) : <nl> + test_index = np . zeros ( self . n_folds , dtype = np . bool ) <nl> + test_index [ item ] = True <nl> + train_index = np . logical_not ( test_index ) <nl> + return train_index , test_index <nl> + <nl> + def next ( self ) : <nl> + if self . iter < self . n_folds : <nl> + self . iter + = 1 <nl> + return self . __getitem__ ( self . iter - 1 ) <nl> + raise StopIteration <nl> + <nl> + def __iter__ ( self ) : <nl> + return self <nl> + <nl> + def crossval_split ( train_indexes , test_indexes , * args ) : <nl> + \" \" \" <nl> + For each arg return a train and test subsets defined by indexes provided <nl> + in train_indexes and test_indexes <nl> + \" \" \" <nl> + ret = [ ] <nl> + for arg in args : <nl> + arg_train = arg [ trainIndexes , : ] <nl> + arg_test = arg [ testIndexes , : ] <nl> + ret . append ( arg_train ) <nl> + ret . append ( arg_test ) <nl> + return ret <nl> + <nl> + if __name__ = = \" __main__ \" : <nl> + print \" Leave One Out crossvalidation \" <nl> + n_samples , n_features = 5 , 10 <nl> + X = np . random . randn ( n_samples , n_features ) <nl> + print X <nl> + loo = LOO ( n_samples ) <nl> + print loo [ 1 ] <nl> + for train_index , test_index in loo : <nl> + print \" TRAIN : \" , train_index , \" TEST : \" , test_index <nl>\n", "msg": "add util method for Leave One Out crossvalidation\n"}
{"diff_id": 3058, "repo": "tornadoweb/tornado\n", "sha": "2f0fbd9ba98440d0266f2e7726156a4a5cedb27c\n", "time": "2012-03-24T22:19:47Z\n", "diff": "mmm a / tornado / ioloop . py <nl> ppp b / tornado / ioloop . py <nl> def close ( self , all_fds = False ) : <nl> \" \" \" Closes the IOLoop , freeing any resources used . <nl> <nl> If ` ` all_fds ` ` is true , all file descriptors registered on the <nl> - IOLoop will be closed ( not just the ones created by the IOLoop itself . <nl> + IOLoop will be closed ( not just the ones created by the IOLoop itself ) . <nl> + <nl> + Many applications will only use a single IOLoop that runs for the <nl> + entire lifetime of the process . In that case closing the IOLoop <nl> + is not necessary since everything will be cleaned up when the <nl> + process exits . ` IOLoop . close ` is provided mainly for scenarios <nl> + such as unit tests , which create and destroy a large number of <nl> + IOLoops . <nl> + <nl> + An IOLoop must be completely stopped before it can be closed . This <nl> + means that ` IOLoop . stop ( ) ` must be called * and * ` IOLoop . start ( ) ` must <nl> + be allowed to return before attempting to call ` IOLoop . close ( ) ` . <nl> + Therefore the call to ` close ` will usually appear just after <nl> + the call to ` start ` rather than near the call to ` stop ` . <nl> \" \" \" <nl> self . remove_handler ( self . _waker . fileno ( ) ) <nl> if all_fds : <nl> def stop ( self ) : <nl> <nl> ioloop . start ( ) will return after async_method has run its callback , <nl> whether that callback was invoked before or after ioloop . start . <nl> + <nl> + Note that even after ` stop ` has been called , the IOLoop is not <nl> + completely stopped until ` IOLoop . start ` has also returned . <nl> \" \" \" <nl> self . _running = False <nl> self . _stopped = True <nl>\n", "msg": "Expand documentation for IOLoop . stop and IOLoop . close\n"}
{"diff_id": 3086, "repo": "ansible/ansible\n", "sha": "c3c4ae87dda2aeff708bf0b2c7af0911228b1610\n", "time": "2016-12-20T12:27:26Z\n", "diff": "mmm a / lib / ansible / modules / network / junos / junos_config . py <nl> ppp b / lib / ansible / modules / network / junos / junos_config . py <nl> <nl> in the corresponding hierarchy of the source configuration loaded <nl> from this module . <nl> - Note this argument should be considered deprecated . To achieve <nl> - the equivalent , set the I ( update ) argument to C ( replace ) . This argument <nl> - will be removed in a future release . <nl> + the equivalent , set the I ( update ) argument to C ( replace ) . This argument <nl> + will be removed in a future release . The C ( replace ) and C ( update ) argument <nl> + is mutually exclusive . <nl> required : false <nl> choices : [ ' yes ' , ' no ' ] <nl> default : false <nl> <nl> default : no <nl> choices : [ ' yes ' , ' no ' ] <nl> version_added : \" 2 . 2 \" <nl> + update : <nl> + description : <nl> + - This argument will decide how to load the configuration <nl> + data particulary when the candidate configuration and loaded <nl> + configuration contain conflicting statements . Following are <nl> + accepted values . <nl> + C ( merge ) combines the data in the loaded configuration with the <nl> + candidate configuration . If statements in the loaded configuration <nl> + conflict with statements in the candidate configuration , the loaded <nl> + statements replace the candidate ones . <nl> + C ( overwrite ) discards the entire candidate configuration and replaces <nl> + it with the loaded configuration . <nl> + C ( replace ) substitutes each hierarchy level in the loaded configuration <nl> + for the corresponding level . <nl> + required : false <nl> + default : merge <nl> + choices : [ ' merge ' , ' overwrite ' , ' replace ' ] <nl> + version_added : \" 2 . 3 \" <nl> requirements : <nl> - junos - eznc <nl> notes : <nl> def load_config ( module , result ) : <nl> kwargs = dict ( ) <nl> kwargs [ ' comment ' ] = module . params [ ' comment ' ] <nl> kwargs [ ' confirm ' ] = module . params [ ' confirm ' ] <nl> - kwargs [ ' replace ' ] = module . params [ ' replace ' ] <nl> + kwargs [ module . params [ ' update ' ] ] = True <nl> kwargs [ ' commit ' ] = not module . check_mode <nl> + kwargs [ ' replace ' ] = module . params [ ' replace ' ] <nl> <nl> if module . params [ ' src ' ] : <nl> config_format = module . params [ ' src_format ' ] or guess_format ( str ( candidate ) ) <nl> def main ( ) : <nl> src_format = dict ( choices = [ ' xml ' , ' text ' , ' set ' , ' json ' ] ) , <nl> <nl> # update operations <nl> + update = dict ( default = ' merge ' , choices = [ ' merge ' , ' overwrite ' , ' replace ' ] ) , <nl> replace = dict ( default = False , type = ' bool ' ) , <nl> + <nl> confirm = dict ( default = 0 , type = ' int ' ) , <nl> comment = dict ( default = DEFAULT_COMMENT ) , <nl> <nl> def main ( ) : <nl> <nl> mutually_exclusive = [ ( ' lines ' , ' rollback ' ) , ( ' lines ' , ' zeroize ' ) , <nl> ( ' rollback ' , ' zeroize ' ) , ( ' lines ' , ' src ' ) , <nl> - ( ' src ' , ' zeroize ' ) , ( ' src ' , ' rollback ' ) ] <nl> + ( ' src ' , ' zeroize ' ) , ( ' src ' , ' rollback ' ) , <nl> + ( ' update ' , ' replace ' ) ] <nl> <nl> - required_if = [ ( ' replace ' , True , [ ' src ' ] ) ] <nl> + required_if = [ ( ' replace ' , True , [ ' src ' ] ) , <nl> + ( ' update ' , ' merge ' , [ ' src ' ] ) , <nl> + ( ' update ' , ' overwrite ' , [ ' src ' ] ) , <nl> + ( ' update ' , ' replace ' , [ ' src ' ] ) ] <nl> <nl> module = NetworkModule ( argument_spec = argument_spec , <nl> mutually_exclusive = mutually_exclusive , <nl>\n", "msg": "Add ' update ' parameter in junos_config module ( )\n"}
{"diff_id": 3132, "repo": "ansible/ansible\n", "sha": "66f654cb6417d0f41f1e5f9a90ef8d9165cccb3b\n", "time": "2018-01-17T17:49:49Z\n", "diff": "mmm a / lib / ansible / plugins / filter / ipaddr . py <nl> ppp b / lib / ansible / plugins / filter / ipaddr . py <nl> def _need_netaddr ( f_name , * args , * * kwargs ) : <nl> ' installed on the ansible controller ' % f_name ) <nl> <nl> <nl> - def ip4_hex ( arg ) : <nl> + def ip4_hex ( arg , delimiter = ' ' ) : <nl> ' ' ' Convert an IPv4 address to Hexadecimal notation ' ' ' <nl> numbers = list ( map ( int , arg . split ( ' . ' ) ) ) <nl> - return ' { : 02x } { : 02x } { : 02x } { : 02x } ' . format ( * numbers ) <nl> + return ' { 0 : 02x } { sep } { 1 : 02x } { sep } { 2 : 02x } { sep } { 3 : 02x } ' . format ( * numbers , sep = delimiter ) <nl> <nl> <nl> # mmm - Ansible filters mmm - <nl>\n", "msg": "filter ipaddr : add custom delimiter option to ip4_hex ( ) ; fix format ( )\n"}
{"diff_id": 3138, "repo": "zulip/zulip\n", "sha": "9576ce589d3c03d92e9d680456b6d8aacea1b02d\n", "time": "2019-06-20T18:04:17Z\n", "diff": "mmm a / zerver / management / commands / create_user . py <nl> ppp b / zerver / management / commands / create_user . py <nl> def handle ( self , * args : Any , * * options : Any ) - > None : <nl> user_initial_password = initial_password ( email ) <nl> if user_initial_password is None : <nl> raise CommandError ( \" Password is unusable . \" ) <nl> - pw = user_initial_password . encode ( ) <nl> + pw = user_initial_password <nl> do_create_user ( email , pw , realm , full_name , email_to_username ( email ) ) <nl> except IntegrityError : <nl> raise CommandError ( \" User already exists . \" ) <nl>\n", "msg": "check_user : Get rid of incorrect encode call for initial password .\n"}
{"diff_id": 3160, "repo": "ipython/ipython\n", "sha": "2f7c1a47bc0a84692e4101f3fa7255816002a0c6\n", "time": "2016-11-08T22:55:17Z\n", "diff": "mmm a / IPython / extensions / autoreload . py <nl> ppp b / IPython / extensions / autoreload . py <nl> <nl> <nl> Import module ' foo ' and mark it to be autoreloaded for ` ` % autoreload 1 ` ` <nl> <nl> - ` ` % aimport foo bar ` ` <nl> + ` ` % aimport foo , bar ` ` <nl> <nl> Import modules ' foo ' , ' bar ' and mark them to be autoreloaded for ` ` % autoreload 1 ` ` <nl> <nl> def aimport ( self , parameter_s = ' ' , stream = None ) : <nl> % aimport foo <nl> Import module ' foo ' and mark it to be autoreloaded for % autoreload 1 <nl> <nl> - % aimport foo bar <nl> + % aimport foo , bar <nl> Import modules ' foo ' , ' bar ' and mark them to be autoreloaded for % autoreload 1 <nl> <nl> % aimport - foo <nl> def aimport ( self , parameter_s = ' ' , stream = None ) : <nl> modname = modname [ 1 : ] <nl> self . _reloader . mark_module_skipped ( modname ) <nl> else : <nl> - for _module in modname . split ( ) : <nl> + for _module in ( [ _ . strip ( ) for _ in modname . split ( ' , ' ) ] ) : <nl> top_module , top_name = self . _reloader . aimport_module ( _module ) <nl> <nl> # Inject module to user namespace <nl>\n", "msg": "Switch module separator to comas to be consistent .\n"}
{"diff_id": 3206, "repo": "ansible/ansible\n", "sha": "88ac3eca78f177f6a89ddc26e7520ffd15cfbf29\n", "time": "2016-12-08T16:33:29Z\n", "diff": "mmm a / lib / ansible / modules / extras / web_infrastructure / deploy_helper . py <nl> ppp b / lib / ansible / modules / extras / web_infrastructure / deploy_helper . py <nl> <nl> module : deploy_helper <nl> version_added : \" 1 . 8 \" <nl> author : Ramon de la Fuente , Jasper N . Brouwer <nl> - short_description : Manages the folders for deploy of a project <nl> + short_description : Manages some of the steps common in deploying projects . <nl> description : <nl> - - Manages some of the steps common in deploying projects . <nl> - It creates a folder structure , cleans up old releases and manages a symlink for the current release . <nl> - <nl> - For more information , see the : doc : ` guide_deploy_helper ` <nl> + - The Deploy Helper manages some of the steps common in deploying software . <nl> + It creates a folder structure , manages a symlink for the current release <nl> + and cleans up old releases . <nl> + - \" Running it with the C ( state = query ) or C ( state = present ) will return the C ( deploy_helper ) fact . <nl> + C ( project_path ) , whatever you set in the path parameter , <nl> + C ( current_path ) , the path to the symlink that points to the active release , <nl> + C ( releases_path ) , the path to the folder to keep releases in , <nl> + C ( shared_path ) , the path to the folder to keep shared resources in , <nl> + C ( unfinished_filename ) , the file to check for to recognize unfinished builds , <nl> + C ( previous_release ) , the release the ' current ' symlink is pointing to , <nl> + C ( previous_release_path ) , the full path to the ' current ' symlink target , <nl> + C ( new_release ) , either the ' release ' parameter or a generated timestamp , <nl> + C ( new_release_path ) , the path to the new release folder ( not created by the module ) . \" <nl> <nl> options : <nl> path : <nl> <nl> aliases : [ ' dest ' ] <nl> description : <nl> - the root path of the project . Alias I ( dest ) . <nl> + Returned in the C ( deploy_helper . project_path ) fact . <nl> <nl> state : <nl> required : false <nl> <nl> description : <nl> - the state of the project . <nl> C ( query ) will only gather facts , <nl> - C ( present ) will create the project , <nl> - C ( finalize ) will create a symlink to the newly deployed release , <nl> + C ( present ) will create the project I ( root ) folder , and in it the I ( releases ) and I ( shared ) folders , <nl> + C ( finalize ) will remove the unfinished_filename file , create a symlink to the newly <nl> + deployed release and optionally clean old releases , <nl> C ( clean ) will remove failed & old releases , <nl> - C ( absent ) will remove the project folder ( synonymous to M ( file ) with state = absent ) <nl> + C ( absent ) will remove the project folder ( synonymous to the M ( file ) module with C ( state = absent ) ) <nl> <nl> release : <nl> required : false <nl> description : <nl> - - the release version that is being deployed ( defaults to a timestamp % Y % m % d % H % M % S ) . This parameter is <nl> - optional during C ( state = present ) , but needs to be set explicitly for C ( state = finalize ) . You can use the <nl> - generated fact C ( release = { { deploy_helper . new_release } } ) <nl> + - the release version that is being deployed . Defaults to a timestamp format % Y % m % d % H % M % S ( i . e . ' 20141119223359 ' ) . <nl> + This parameter is optional during C ( state = present ) , but needs to be set explicitly for C ( state = finalize ) . <nl> + You can use the generated fact C ( release = { { deploy_helper . new_release } } ) . <nl> <nl> releases_path : <nl> required : false <nl> default : releases <nl> description : <nl> - the name of the folder that will hold the releases . This can be relative to C ( path ) or absolute . <nl> + Returned in the C ( deploy_helper . releases_path ) fact . <nl> <nl> shared_path : <nl> required : false <nl> <nl> description : <nl> - the name of the folder that will hold the shared resources . This can be relative to C ( path ) or absolute . <nl> If this is set to an empty string , no shared folder will be created . <nl> + Returned in the C ( deploy_helper . shared_path ) fact . <nl> <nl> current_path : <nl> required : false <nl> default : current <nl> description : <nl> - the name of the symlink that is created when the deploy is finalized . Used in C ( finalize ) and C ( clean ) . <nl> + Returned in the C ( deploy_helper . current_path ) fact . <nl> <nl> unfinished_filename : <nl> required : false <nl> <nl> notes : <nl> - Facts are only returned for C ( state = query ) and C ( state = present ) . If you use both , you should pass any overridden <nl> parameters to both calls , otherwise the second call will overwrite the facts of the first one . <nl> - - When using C ( state = clean ) , the releases are ordered by creation date . You should be able to switch to a <nl> + - When using C ( state = clean ) , the releases are ordered by I ( creation date ) . You should be able to switch to a <nl> new naming strategy without problems . <nl> - Because of the default behaviour of generating the I ( new_release ) fact , this module will not be idempotent <nl> unless you pass your own release name with C ( release ) . Due to the nature of deploying software , this should not <nl> <nl> ' ' ' <nl> <nl> EXAMPLES = ' ' ' <nl> - Example usage for the deploy_helper module . <nl> - <nl> - tasks : <nl> - # Typical usage : <nl> - - deploy_helper : path = / path / to / root state = present <nl> - . . . some_build_steps_here , like a git clone to { { deploy_helper . new_release_path } } for example . . . <nl> - - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize <nl> - <nl> - # Gather information only <nl> - - deploy_helper : path = / path / to / root state = query <nl> - # Remember to set the ' release = ' when you actually call state = present later <nl> - - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = present <nl> - <nl> - # all paths can be absolute or relative ( to ' path ' ) <nl> - - deploy_helper : path = / path / to / root <nl> - releases_path = / var / www / project / releases <nl> - shared_path = / var / www / shared <nl> - current_path = / var / www / active <nl> - <nl> - # Using your own naming strategy : <nl> - - deploy_helper : path = / path / to / root release = v1 . 1 . 1 state = present <nl> - - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize <nl> - <nl> - # Postponing the cleanup of older builds : <nl> - - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize clean = False <nl> - . . . anything you do before actually deleting older releases . . . <nl> - - deploy_helper : path = / path / to / root state = clean <nl> - <nl> - # Keeping more old releases : <nl> - - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize keep_releases = 10 <nl> - # Or : <nl> - - deploy_helper : path = / path / to / root state = clean keep_releases = 10 <nl> - <nl> - # Using a different unfinished_filename : <nl> - - deploy_helper : path = / path / to / root unfinished_filename = README . md release = { { deploy_helper . new_release } } state = finalize <nl> + <nl> + # Typical usage : <nl> + - name : Initialize the deploy root and gather facts <nl> + deploy_helper : path = / path / to / root <nl> + - name : Clone the project to the new release folder <nl> + git : repo = git : / / foosball . example . org / path / to / repo . git dest = { { deploy . new_release_path } } version = v1 . 1 . 1 \" <nl> + - name : Add an unfinished file , to allow cleanup on successful finalize <nl> + file : path = { { deploy . new_release_path } } / { { deploy . unfinished_filename } } state = touch <nl> + - name : Perform some build steps , like running your dependency manager for example <nl> + composer : command = install working_dir = { { deploy . new_release_path } } <nl> + - name : Finalize the deploy , removing the unfinished file and switching the symlink <nl> + deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize <nl> + <nl> + # Retrieving facts before running a deploy <nl> + - name : Run query to gather facts without changing anything <nl> + deploy_helper : path = / path / to / root state = query <nl> + # Remember to set the ' release ' parameter when you actually call state = present <nl> + - name : Initialize the deploy root <nl> + deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = present <nl> + <nl> + # all paths can be absolute or relative ( to the ' path ' parameter ) <nl> + - deploy_helper : path = / path / to / root <nl> + releases_path = / var / www / project / releases <nl> + shared_path = / var / www / shared <nl> + current_path = / var / www / active <nl> + <nl> + # Using your own naming strategy : <nl> + - deploy_helper : path = / path / to / root release = v1 . 1 . 1 state = present <nl> + - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize <nl> + <nl> + # Using a different unfinished_filename : <nl> + - deploy_helper : path = / path / to / root <nl> + unfinished_filename = README . md <nl> + release = { { deploy_helper . new_release } } <nl> + state = finalize <nl> + <nl> + # Postponing the cleanup of older builds : <nl> + - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize clean = False <nl> + - deploy_helper : path = / path / to / root state = clean <nl> + <nl> + # Keeping more old releases : <nl> + - deploy_helper : path = / path / to / root release = { { deploy_helper . new_release } } state = finalize keep_releases = 10 <nl> + # Or : <nl> + - deploy_helper : path = / path / to / root state = clean keep_releases = 10 <nl> <nl> ' ' ' <nl> <nl>\n", "msg": "removed link to guide , and added more documentation and examples\n"}
{"diff_id": 3243, "repo": "matplotlib/matplotlib\n", "sha": "d9142f519f336bed99af51269c4e23c730bc99bf\n", "time": "2011-11-11T02:21:22Z\n", "diff": "mmm a / lib / matplotlib / cbook . py <nl> ppp b / lib / matplotlib / cbook . py <nl> <nl> <nl> major , minor1 , minor2 , s , tmp = sys . version_info <nl> <nl> + # Handle the transition from urllib2 in Python 2 to urllib in Python 3 <nl> if major > = 3 : <nl> import types <nl> import urllib . request , urllib . error , urllib . parse <nl> def urllib_quote ( ) : <nl> def addinfourl ( data , headers , url , code = None ) : <nl> return urllib . request . addinfourl ( io . BytesIO ( data ) , <nl> headers , url , code ) <nl> + urllib_HTTPSHandler = urllib . request . HTTPSHandler <nl> + urllib_build_opener = urllib . request . build_opener <nl> + urllib_URLError = urllib . error . URLError <nl> else : <nl> import new <nl> import urllib2 <nl> def urllib_quote ( ) : <nl> def addinfourl ( data , headers , url , code = None ) : <nl> return urllib2 . addinfourl ( io . StringIO ( data ) , <nl> headers , url , code ) <nl> + urllib_HTTPSHandler = urllib2 . HTTPSHandler <nl> + urllib_build_opener = urllib2 . build_opener <nl> + urllib_URLError = urllib2 . URLError <nl> <nl> import matplotlib <nl> <nl> def is_scalar_or_string ( val ) : <nl> return is_string_like ( val ) or not iterable ( val ) <nl> <nl> def _get_data_server ( cache_dir , baseurl ) : <nl> - class ViewVCCachedServer ( urllib2 . HTTPSHandler ) : <nl> + class ViewVCCachedServer ( urllib_HTTPSHandler ) : <nl> \" \" \" <nl> - Urllib2 handler that takes care of caching files . <nl> + Urllib handler that takes care of caching files . <nl> The file cache . pck holds the directory of files that have been cached . <nl> \" \" \" <nl> def __init__ ( self , cache_dir , baseurl ) : <nl> - urllib2 . HTTPSHandler . __init__ ( self ) <nl> + urllib_HTTPSHandler . __init__ ( self ) <nl> self . cache_dir = cache_dir <nl> self . baseurl = baseurl <nl> self . read_cache ( ) <nl> self . remove_stale_files ( ) <nl> - self . opener = urllib2 . build_opener ( self ) <nl> + self . opener = urllib_build_opener ( self ) <nl> <nl> def in_cache_dir ( self , fn ) : <nl> # make sure the datadir exists <nl> def cache_file ( self , url , data , headers ) : <nl> headers . get ( ' Last - Modified ' ) ) <nl> self . write_cache ( ) <nl> <nl> - # These urllib2 entry points are used : <nl> + # These urllib entry points are used : <nl> # http_request for preprocessing requests <nl> # http_error_304 for handling 304 Not Modified responses <nl> # http_response for postprocessing requests <nl> def get_sample_data ( self , fname , asfileobj = True ) : <nl> path to the file as a string will be returned . <nl> \" \" \" <nl> # TODO : time out if the connection takes forever <nl> - # ( may not be possible with urllib2 only - spawn a helper process ? ) <nl> + # ( may not be possible with urllib only - spawn a helper process ? ) <nl> <nl> quote = urllib_quote ( ) <nl> <nl> def get_sample_data ( self , fname , asfileobj = True ) : <nl> % url , ' debug ' ) <nl> try : <nl> response = self . opener . open ( url ) <nl> - except urllib2 . URLError , e : <nl> + except urllib_URLError as e : <nl> # could be a missing network connection <nl> error = str ( e ) <nl> <nl>\n", "msg": "Make the conversion of urllib2 in Python 2 to urllib in Python 3 more obvious , with less magic happening in 2to3 .\n"}
{"diff_id": 3257, "repo": "python/cpython\n", "sha": "808b94eb453603fcfcd6208e0003cfe37ea3f604\n", "time": "2001-09-13T19:33:07Z\n", "diff": "mmm a / Lib / test / test_descr . py <nl> ppp b / Lib / test / test_descr . py <nl> def readline ( self ) : <nl> except : <nl> pass <nl> <nl> + def keywords ( ) : <nl> + if verbose : <nl> + print \" Testing keyword args to basic type constructors . . . \" <nl> + verify ( int ( x = 1 ) = = 1 ) <nl> + verify ( float ( x = 2 ) = = 2 . 0 ) <nl> + verify ( long ( x = 3 ) = = 3L ) <nl> + verify ( complex ( imag = 42 , real = 666 ) = = complex ( 666 , 42 ) ) <nl> + verify ( str ( object = 500 ) = = ' 500 ' ) <nl> + verify ( unicode ( string = ' abc ' , errors = ' strict ' ) = = u ' abc ' ) <nl> + verify ( tuple ( sequence = range ( 3 ) ) = = ( 0 , 1 , 2 ) ) <nl> + verify ( list ( sequence = ( 0 , 1 , 2 ) ) = = range ( 3 ) ) <nl> + verify ( dictionary ( mapping = { 1 : 2 } ) = = { 1 : 2 } ) <nl> + <nl> + for constructor in ( int , float , long , complex , str , unicode , <nl> + tuple , list , dictionary , file ) : <nl> + try : <nl> + constructor ( bogus_keyword_arg = 1 ) <nl> + except TypeError : <nl> + pass <nl> + else : <nl> + raise TestFailed ( \" expected TypeError from bogus keyword \" <nl> + \" argument to % r \" % constructor ) <nl> + <nl> def all ( ) : <nl> lists ( ) <nl> dicts ( ) <nl> def all ( ) : <nl> properties ( ) <nl> supers ( ) <nl> inherits ( ) <nl> + keywords ( ) <nl> <nl> all ( ) <nl> <nl>\n", "msg": "Added simple tests of keyword arguments in the basic type constructors .\n"}
{"diff_id": 3422, "repo": "python/cpython\n", "sha": "49c994239f81b3362033620762a82b84c9247c55\n", "time": "2001-01-26T18:00:48Z\n", "diff": "mmm a / Lib / distutils / command / build_ext . py <nl> ppp b / Lib / distutils / command / build_ext . py <nl> def get_outputs ( self ) : <nl> <nl> # get_outputs ( ) <nl> <nl> - <nl> - def build_extensions ( self ) : <nl> + def build_extensions ( self ) : <nl> <nl> # First , sanity - check the ' extensions ' list <nl> self . check_extensions_list ( self . extensions ) <nl> <nl> for ext in self . extensions : <nl> - sources = ext . sources <nl> - if sources is None or type ( sources ) not in ( ListType , TupleType ) : <nl> - raise DistutilsSetupError , \\ <nl> - ( \" in ' ext_modules ' option ( extension ' % s ' ) , \" + <nl> - \" ' sources ' must be present and must be \" + <nl> - \" a list of source filenames \" ) % ext . name <nl> - sources = list ( sources ) <nl> + self . build_extension ( ext ) <nl> <nl> - fullname = self . get_ext_fullname ( ext . name ) <nl> - if self . inplace : <nl> - # ignore build - lib - - put the compiled extension into <nl> - # the source tree along with pure Python modules <nl> - <nl> - modpath = string . split ( fullname , ' . ' ) <nl> - package = string . join ( modpath [ 0 : - 1 ] , ' . ' ) <nl> - base = modpath [ - 1 ] <nl> - <nl> - build_py = self . get_finalized_command ( ' build_py ' ) <nl> - package_dir = build_py . get_package_dir ( package ) <nl> - ext_filename = os . path . join ( package_dir , <nl> - self . get_ext_filename ( base ) ) <nl> - else : <nl> - ext_filename = os . path . join ( self . build_lib , <nl> - self . get_ext_filename ( fullname ) ) <nl> + def build_extension ( self , ext ) : <nl> <nl> - if not ( self . force or newer_group ( sources , ext_filename , ' newer ' ) ) : <nl> - self . announce ( \" skipping ' % s ' extension ( up - to - date ) \" % <nl> - ext . name ) <nl> - continue # ' for ' loop over all extensions <nl> - else : <nl> - self . announce ( \" building ' % s ' extension \" % ext . name ) <nl> - <nl> - # First , scan the sources for SWIG definition files ( . i ) , run <nl> - # SWIG on ' em to create . c files , and modify the sources list <nl> - # accordingly . <nl> - sources = self . swig_sources ( sources ) <nl> - <nl> - # Next , compile the source code to object files . <nl> - <nl> - # XXX not honouring ' define_macros ' or ' undef_macros ' - - the <nl> - # CCompiler API needs to change to accommodate this , and I <nl> - # want to do one thing at a time ! <nl> - <nl> - # Two possible sources for extra compiler arguments : <nl> - # - ' extra_compile_args ' in Extension object <nl> - # - CFLAGS environment variable ( not particularly <nl> - # elegant , but people seem to expect it and I <nl> - # guess it ' s useful ) <nl> - # The environment variable should take precedence , and <nl> - # any sensible compiler will give precedence to later <nl> - # command line args . Hence we combine them in order : <nl> - extra_args = ext . extra_compile_args or [ ] <nl> - <nl> - macros = ext . define_macros [ : ] <nl> - for undef in ext . undef_macros : <nl> - macros . append ( ( undef , ) ) <nl> - <nl> - # XXX and if we support CFLAGS , why not CC ( compiler <nl> - # executable ) , CPPFLAGS ( pre - processor options ) , and LDFLAGS <nl> - # ( linker options ) too ? <nl> - # XXX should we use shlex to properly parse CFLAGS ? <nl> - <nl> - if os . environ . has_key ( ' CFLAGS ' ) : <nl> - extra_args . extend ( string . split ( os . environ [ ' CFLAGS ' ] ) ) <nl> - <nl> - objects = self . compiler . compile ( sources , <nl> - output_dir = self . build_temp , <nl> - macros = macros , <nl> - include_dirs = ext . include_dirs , <nl> - debug = self . debug , <nl> - extra_postargs = extra_args ) <nl> - <nl> - # Now link the object files together into a \" shared object \" - - <nl> - # of course , first we have to figure out all the other things <nl> - # that go into the mix . <nl> - if ext . extra_objects : <nl> - objects . extend ( ext . extra_objects ) <nl> - extra_args = ext . extra_link_args or [ ] <nl> - <nl> - <nl> - self . compiler . link_shared_object ( <nl> - objects , ext_filename , <nl> - libraries = self . get_libraries ( ext ) , <nl> - library_dirs = ext . library_dirs , <nl> - runtime_library_dirs = ext . runtime_library_dirs , <nl> - extra_postargs = extra_args , <nl> - export_symbols = self . get_export_symbols ( ext ) , <nl> - debug = self . debug , <nl> - build_temp = self . build_temp ) <nl> - <nl> - # build_extensions ( ) <nl> + sources = ext . sources <nl> + if sources is None or type ( sources ) not in ( ListType , TupleType ) : <nl> + raise DistutilsSetupError , \\ <nl> + ( \" in ' ext_modules ' option ( extension ' % s ' ) , \" + <nl> + \" ' sources ' must be present and must be \" + <nl> + \" a list of source filenames \" ) % ext . name <nl> + sources = list ( sources ) <nl> + <nl> + fullname = self . get_ext_fullname ( ext . name ) <nl> + if self . inplace : <nl> + # ignore build - lib - - put the compiled extension into <nl> + # the source tree along with pure Python modules <nl> + <nl> + modpath = string . split ( fullname , ' . ' ) <nl> + package = string . join ( modpath [ 0 : - 1 ] , ' . ' ) <nl> + base = modpath [ - 1 ] <nl> + <nl> + build_py = self . get_finalized_command ( ' build_py ' ) <nl> + package_dir = build_py . get_package_dir ( package ) <nl> + ext_filename = os . path . join ( package_dir , <nl> + self . get_ext_filename ( base ) ) <nl> + else : <nl> + ext_filename = os . path . join ( self . build_lib , <nl> + self . get_ext_filename ( fullname ) ) <nl> + <nl> + if not ( self . force or newer_group ( sources , ext_filename , ' newer ' ) ) : <nl> + self . announce ( \" skipping ' % s ' extension ( up - to - date ) \" % <nl> + ext . name ) <nl> + return <nl> + else : <nl> + self . announce ( \" building ' % s ' extension \" % ext . name ) <nl> + <nl> + # First , scan the sources for SWIG definition files ( . i ) , run <nl> + # SWIG on ' em to create . c files , and modify the sources list <nl> + # accordingly . <nl> + sources = self . swig_sources ( sources ) <nl> + <nl> + # Next , compile the source code to object files . <nl> + <nl> + # XXX not honouring ' define_macros ' or ' undef_macros ' - - the <nl> + # CCompiler API needs to change to accommodate this , and I <nl> + # want to do one thing at a time ! <nl> + <nl> + # Two possible sources for extra compiler arguments : <nl> + # - ' extra_compile_args ' in Extension object <nl> + # - CFLAGS environment variable ( not particularly <nl> + # elegant , but people seem to expect it and I <nl> + # guess it ' s useful ) <nl> + # The environment variable should take precedence , and <nl> + # any sensible compiler will give precedence to later <nl> + # command line args . Hence we combine them in order : <nl> + extra_args = ext . extra_compile_args or [ ] <nl> + <nl> + macros = ext . define_macros [ : ] <nl> + for undef in ext . undef_macros : <nl> + macros . append ( ( undef , ) ) <nl> + <nl> + # XXX and if we support CFLAGS , why not CC ( compiler <nl> + # executable ) , CPPFLAGS ( pre - processor options ) , and LDFLAGS <nl> + # ( linker options ) too ? <nl> + # XXX should we use shlex to properly parse CFLAGS ? <nl> + <nl> + if os . environ . has_key ( ' CFLAGS ' ) : <nl> + extra_args . extend ( string . split ( os . environ [ ' CFLAGS ' ] ) ) <nl> + <nl> + objects = self . compiler . compile ( sources , <nl> + output_dir = self . build_temp , <nl> + macros = macros , <nl> + include_dirs = ext . include_dirs , <nl> + debug = self . debug , <nl> + extra_postargs = extra_args ) <nl> + <nl> + # Now link the object files together into a \" shared object \" - - <nl> + # of course , first we have to figure out all the other things <nl> + # that go into the mix . <nl> + if ext . extra_objects : <nl> + objects . extend ( ext . extra_objects ) <nl> + extra_args = ext . extra_link_args or [ ] <nl> + <nl> + <nl> + self . compiler . link_shared_object ( <nl> + objects , ext_filename , <nl> + libraries = self . get_libraries ( ext ) , <nl> + library_dirs = ext . library_dirs , <nl> + runtime_library_dirs = ext . runtime_library_dirs , <nl> + extra_postargs = extra_args , <nl> + export_symbols = self . get_export_symbols ( ext ) , <nl> + debug = self . debug , <nl> + build_temp = self . build_temp ) <nl> <nl> <nl> def swig_sources ( self , sources ) : <nl>\n", "msg": "Added an execution layer to be able to customize per - extension\n"}
{"diff_id": 3630, "repo": "mitmproxy/mitmproxy\n", "sha": "8352c0278f468be30a9765a61f369ceb317aafe9\n", "time": "2012-06-24T23:34:29Z\n", "diff": "mmm a / libpathod / pathod . py <nl> ppp b / libpathod / pathod . py <nl> def handle ( self ) : <nl> self . server . ssloptions [ \" keyfile \" ] , <nl> ) <nl> <nl> - while 1 : <nl> + while not self . finished : <nl> line = self . rfile . readline ( ) <nl> if line = = \" \\ r \\ n \" or line = = \" \\ n \" : # Possible leftover from previous message <nl> line = self . rfile . readline ( ) <nl>\n", "msg": "Handle client close more gracefully .\n"}
{"diff_id": 3659, "repo": "bokeh/bokeh\n", "sha": "063d45a483df91326d781f5aa39ebb1bb84ca626\n", "time": "2015-12-20T15:08:43Z\n", "diff": "mmm a / bokeh / server / protocol / __init__ . py <nl> ppp b / bokeh / server / protocol / __init__ . py <nl> <nl> ' ' ' <nl> from __future__ import absolute_import <nl> <nl> + import logging <nl> + log = logging . getLogger ( __name__ ) <nl> + <nl> from tornado . escape import json_decode <nl> <nl> from . . exceptions import ProtocolError <nl> def assemble ( self , header_json , metadata_json , content_json ) : <nl> <nl> ' ' ' <nl> header = json_decode ( header_json ) <nl> + if ' msgtype ' not in header : <nl> + log . error ( \" Bad header with no msgtype was : % r \" , header ) <nl> + raise ProtocolError ( \" No ' msgtype ' in header \" ) <nl> return self . _messages [ header [ ' msgtype ' ] ] . assemble ( <nl> header_json , metadata_json , content_json <nl> ) <nl>\n", "msg": "raise ProtocolError if we get a header with no msgtype\n"}
{"diff_id": 3671, "repo": "python/cpython\n", "sha": "ee248110428d523f7e73a4918508ad48d91a36d2\n", "time": "1995-01-12T12:40:48Z\n", "diff": "mmm a / Demo / scripts / mboxconvert . py <nl> ppp b / Demo / scripts / mboxconvert . py <nl> def mmdf ( f ) : <nl> ' Bad line in MMFD mailbox : % s \\ n ' % ` line ` ) <nl> return sts <nl> <nl> + counter = 0 # for generating unique Message - ID headers <nl> + <nl> def message ( f , delimiter = ' ' ) : <nl> sts = 0 <nl> # Parse RFC822 header <nl> def message ( f , delimiter = ' ' ) : <nl> # Copy RFC822 header <nl> for line in m . headers : <nl> print line , <nl> + # Invent Message - ID header if none is present <nl> + if not m . has_key ( ' message - id ' ) : <nl> + global counter <nl> + counter = counter + 1 <nl> + msgid = \" < % s . % d > \" % ( hex ( t ) , counter ) <nl> + sys . stderr . write ( \" Adding Message - ID % s ( From % s ) \\ n \" % <nl> + ( msgid , email ) ) <nl> + print \" Message - ID : \" , msgid <nl> print <nl> # Copy body <nl> while 1 : <nl>\n", "msg": "Invent Message - ID header if none is present\n"}
{"diff_id": 3677, "repo": "python/cpython\n", "sha": "97043c3c02d0a531685ce2256eeaf5c4c5d8dc44\n", "time": "2007-08-04T02:34:24Z\n", "diff": "mmm a / Lib / httplib . py <nl> ppp b / Lib / httplib . py <nl> def readheaders ( self ) : <nl> self . status = self . status + ' ; bad seek ' <nl> break <nl> <nl> - class HTTPResponse ( io . IOBase ) : <nl> + class HTTPResponse : <nl> <nl> # strict : If true , raise BadStatusLine if the status line can ' t be <nl> # parsed as a valid HTTP / 1 . 0 or 1 . 1 status line . By default it is <nl> def getreply ( self ) : <nl> try : <nl> response = self . _conn . getresponse ( ) <nl> except BadStatusLine as e : <nl> - # # # hmm . if getresponse ( ) ever closes the socket on a bad request , <nl> - # # # then we are going to have problems with self . sock <nl> - <nl> - # # # should we keep this behavior ? do people use it ? <nl> # keep the socket open ( as a file ) , and return it <nl> self . file = self . _conn . sock . makefile ( ' rb ' , 0 ) <nl> <nl> def test ( ) : <nl> status , reason , headers = h . getreply ( ) <nl> print ( ' status = ' , status ) <nl> print ( ' reason = ' , reason ) <nl> - print ( \" read \" , len ( h . getfile ( ) . read ( ) ) ) <nl> + print ( ' read ' , len ( h . getfile ( ) . read ( ) ) ) <nl> print ( ) <nl> if headers : <nl> for header in headers . headers : print ( header . strip ( ) ) <nl>\n", "msg": "HTTPResponse should not inherit from io . IOBase .\n"}
{"diff_id": 3699, "repo": "bokeh/bokeh\n", "sha": "5d22ee4c454e13b195cc26c2e8cc144557fa217b\n", "time": "2013-05-03T17:05:21Z\n", "diff": "mmm a / bokeh / properties . py <nl> ppp b / bokeh / properties . py <nl> def __init__ ( self , * args , * * kwargs ) : <nl> setattr ( self , kw , val ) <nl> else : <nl> newkwargs [ kw ] = val <nl> - super ( HasProps , self ) . __init__ ( * args , * * newkwargs ) <nl> - <nl> + # Dump the rest of the kwargs in self . dict <nl> + self . __dict__ . update ( newkwargs ) <nl> + super ( HasProps , self ) . __init__ ( * args ) <nl> <nl> def clone ( self ) : <nl> \" \" \" Returns a duplicate of this object with all its properties <nl> class String ( BaseProperty ) : pass <nl> class List ( BaseProperty ) : <nl> \" \" \" If a default value is passed in , then a shallow copy of it will be <nl> used for each new use of this property . <nl> + <nl> + People will also frequently pass in some other kind of property or a <nl> + class ( to indicate a list of instances ) . In those cases , we want to <nl> + just create an empty list <nl> \" \" \" <nl> <nl> - def __init__ ( self , default = [ ] ) : <nl> + def __init__ ( self , default = None ) : <nl> + if isinstance ( default , type ) or isinstance ( default , BaseProperty ) : <nl> + default = None <nl> BaseProperty . __init__ ( self , default ) <nl> <nl> def __get__ ( self , obj , type = None ) : <nl> - if not hasattr ( obj , \" _ \" + self . name ) and isinstance ( self . default , list ) : <nl> - setattr ( obj , \" _ \" + self . name , copy ( self . default ) ) <nl> - return getattr ( obj , \" _ \" + self . name ) and self . default is None <nl> + if hasattr ( obj , \" _ \" + self . name ) : <nl> + return getattr ( obj , \" _ \" + self . name ) <nl> + if self . default is None : <nl> + val = [ ] <nl> + elif isinstance ( self . default , list ) : <nl> + val = copy ( self . default ) <nl> else : <nl> - return getattr ( obj , \" _ \" + self . name , self . default ) <nl> + val = self . default <nl> + setattr ( obj , \" _ \" + self . name , val ) <nl> + return val <nl> <nl> class Dict ( BaseProperty ) : <nl> \" \" \" If a default value is passed in , then a shallow copy of it will be <nl> def __get__ ( self , obj , type = None ) : <nl> <nl> # OOP things <nl> class Class ( BaseProperty ) : pass <nl> - class Instance ( BaseProperty ) : pass <nl> - class This ( BaseProperty ) : pass <nl> + class Instance ( BaseProperty ) : <nl> + <nl> + def __get__ ( self , obj , type = None ) : <nl> + # If the constructor for Instance ( ) supplied a class name , we should <nl> + # instantiate that class here , instead of returning the class as the <nl> + # default object <nl> + if not hasattr ( obj , \" _ \" + self . name ) : <nl> + if self . default and isinstance ( self . default , type ) : <nl> + setattr ( obj , \" _ \" + self . name , self . default ( ) ) <nl> + return getattr ( obj , \" _ \" + self . name , None ) <nl> + <nl> + class This ( BaseProperty ) : <nl> + \" \" \" A reference to an instance of the class being defined <nl> + \" \" \" <nl> + pass <nl> <nl> # Fake types , ABCs <nl> class Any ( BaseProperty ) : pass <nl>\n", "msg": "Fixing List and Instance properties , and adding default kwarg handling to __init__\n"}
{"diff_id": 3861, "repo": "explosion/spaCy\n", "sha": "4fa96705379b10b761a7097b1adb12145402cb1f\n", "time": "2020-05-20T08:15:43Z\n", "diff": "mmm a / spacy / lemmatizer . py <nl> ppp b / spacy / lemmatizer . py <nl> <nl> from . symbols import NOUN , VERB , ADJ , PUNCT , PROPN <nl> from . errors import Errors <nl> from . lookups import Lookups <nl> + from . parts_of_speech import NAMES as UPOS_NAMES <nl> <nl> <nl> class Lemmatizer ( object ) : <nl> def __call__ ( self , string , univ_pos , morphology = None ) : <nl> lookup_table = self . lookups . get_table ( \" lemma_lookup \" , { } ) <nl> if \" lemma_rules \" not in self . lookups : <nl> return [ lookup_table . get ( string , string ) ] <nl> - if univ_pos in ( NOUN , \" NOUN \" , \" noun \" ) : <nl> - univ_pos = \" noun \" <nl> - elif univ_pos in ( VERB , \" VERB \" , \" verb \" ) : <nl> - univ_pos = \" verb \" <nl> - elif univ_pos in ( ADJ , \" ADJ \" , \" adj \" ) : <nl> - univ_pos = \" adj \" <nl> - elif univ_pos in ( PUNCT , \" PUNCT \" , \" punct \" ) : <nl> - univ_pos = \" punct \" <nl> - elif univ_pos in ( PROPN , \" PROPN \" ) : <nl> - return [ string ] <nl> - else : <nl> + if isinstance ( univ_pos , int ) : <nl> + univ_pos = UPOS_NAMES . get ( univ_pos , \" X \" ) <nl> + univ_pos = univ_pos . lower ( ) <nl> + <nl> + if univ_pos in ( \" \" , \" eol \" , \" space \" ) : <nl> return [ string . lower ( ) ] <nl> # See Issue # 435 for example of where this logic is requied . <nl> if self . is_base_form ( univ_pos , morphology ) : <nl>\n", "msg": "Extend lemmatizer rules for all UPOS tags\n"}
{"diff_id": 3983, "repo": "zulip/zulip\n", "sha": "43241250b6fd1cf5bcbf58e2e78236f2e7ed191b\n", "time": "2020-04-18T23:19:15Z\n", "diff": "mmm a / zerver / tests / test_alert_words . py <nl> ppp b / zerver / tests / test_alert_words . py <nl> def test_remove_word ( self ) - > None : <nl> \" \" \" <nl> user = self . example_user ( ' cordelia ' ) <nl> <nl> + expected_remaining_alerts = set ( self . interesting_alert_word_list ) <nl> add_user_alert_words ( user , self . interesting_alert_word_list ) <nl> <nl> - theoretical_remaining_alerts = self . interesting_alert_word_list [ : ] <nl> - <nl> for alert_word in self . interesting_alert_word_list : <nl> remove_user_alert_words ( user , alert_word ) <nl> - theoretical_remaining_alerts . remove ( alert_word ) <nl> + expected_remaining_alerts . remove ( alert_word ) <nl> actual_remaining_alerts = user_alert_words ( user ) <nl> self . assertEqual ( set ( actual_remaining_alerts ) , <nl> - set ( theoretical_remaining_alerts ) ) <nl> + expected_remaining_alerts ) <nl> <nl> def test_realm_words ( self ) - > None : <nl> \" \" \" <nl>\n", "msg": "test_alert_words : Use better variable names .\n"}
{"diff_id": 3993, "repo": "3b1b/manim\n", "sha": "f2b5c21c968bdf5eb172c42c7e4ef08f349c882e\n", "time": "2015-12-25T16:28:17Z\n", "diff": "mmm a / mobject / image_mobject . py <nl> ppp b / mobject / image_mobject . py <nl> class ImageMobject ( Mobject ) : <nl> Automatically filters out black pixels <nl> \" \" \" <nl> DEFAULT_CONFIG = { <nl> - \" filter_color \" : \" black \" , <nl> - \" invert \" : True , <nl> - \" use_cache \" : True , <nl> + \" filter_color \" : \" black \" , <nl> + \" invert \" : True , <nl> + \" use_cache \" : True , <nl> \" point_thickness \" : 1 , <nl> - \" scale_value \" : 1 . 0 , <nl> - \" should_center \" : True <nl> + \" scale_value \" : 1 . 0 , <nl> + \" should_center \" : True , <nl> } <nl> def __init__ ( self , image_file , * * kwargs ) : <nl> digest_locals ( self ) <nl> def __init__ ( self , image_array , * * kwargs ) : <nl> <nl> <nl> class MobjectFromRegion ( MobjectFromPixelArray ) : <nl> - def __init__ ( self , region , * * kwargs ) : <nl> + def __init__ ( self , region , color = None , * * kwargs ) : <nl> MobjectFromPixelArray . __init__ ( <nl> self , <nl> - disp . paint_region ( region ) , <nl> + disp . paint_region ( region , color = color ) , <nl> * * kwargs <nl> ) <nl> <nl>\n", "msg": "Why did MobjectFromRegion not specify color ?\n"}
{"diff_id": 4101, "repo": "ipython/ipython\n", "sha": "2903df30ec5ce51ac0abcdf12a90ce5d5fb380c1\n", "time": "2018-02-10T12:10:24Z\n", "diff": "mmm a / IPython / core / completer . py <nl> ppp b / IPython / core / completer . py <nl> <nl> <nl> try : <nl> import jedi <nl> + jedi . settings . case_insensitive_completion = False <nl> import jedi . api . helpers <nl> import jedi . api . classes <nl> JEDI_INSTALLED = True <nl>\n", "msg": "Change defaults to be consistent with original defaults and REPL performance .\n"}
{"diff_id": 4111, "repo": "matplotlib/matplotlib\n", "sha": "e53ee2a40328482f7818af3708a48ff81248b1b2\n", "time": "2017-10-26T17:17:59Z\n", "diff": "mmm a / lib / matplotlib / axes / _axes . py <nl> ppp b / lib / matplotlib / axes / _axes . py <nl> def fill_between ( self , x , y1 , y2 = 0 , where = None , interpolate = False , <nl> step : { ' pre ' , ' post ' , ' mid ' } , optional <nl> If not None , fill with step logic . <nl> <nl> + Returns <nl> + mmmmmm - <nl> + ` PolyCollection ` <nl> + Plotted polygon collection <nl> <nl> Notes <nl> mmm - - <nl> def fill_betweenx ( self , y , x1 , x2 = 0 , where = None , <nl> end points of the filled region will only occur on explicit <nl> values in the * x * array . <nl> <nl> + <nl> + Returns <nl> + mmmmmm - <nl> + ` PolyCollection ` <nl> + Plotted polygon collection <nl> + <nl> Notes <nl> mmm - - <nl> <nl>\n", "msg": "Add returns documentation to fill_between methods ( )\n"}
{"diff_id": 4135, "repo": "ipython/ipython\n", "sha": "0d0b44fddf028f7a481c41b63fe69ba49262e824\n", "time": "2014-10-17T18:04:13Z\n", "diff": "mmm a / IPython / nbconvert / filters / highlight . py <nl> ppp b / IPython / nbconvert / filters / highlight . py <nl> def _pygments_highlight ( source , output_formatter , language = ' ipython ' , metadata = N <nl> \" \" \" <nl> from pygments import highlight <nl> from pygments . lexers import get_lexer_by_name <nl> + from pygments . util import ClassNotFound <nl> from IPython . nbconvert . utils . lexers import IPythonLexer , IPython3Lexer <nl> <nl> # If the cell uses a magic extension language , <nl> def _pygments_highlight ( source , output_formatter , language = ' ipython ' , metadata = N <nl> elif language = = ' ipython3 ' : <nl> lexer = IPython3Lexer ( ) <nl> else : <nl> - lexer = get_lexer_by_name ( language , stripall = True ) <nl> + try : <nl> + lexer = get_lexer_by_name ( language , stripall = True ) <nl> + except ClassNotFound : <nl> + warn ( \" No lexer found for language % r . Treating as plain text . \" % language ) <nl> + from pygments . lexers . special import TextLexer <nl> + lexer = TextLexer ( ) <nl> + <nl> <nl> return highlight ( source , lexer , output_formatter ) <nl>\n", "msg": "Fallback to plain text if higlighting lexer not found .\n"}
{"diff_id": 4159, "repo": "ansible/ansible\n", "sha": "e9f98c0efc8de619035c6c3bbd2673ddcd1f59b3\n", "time": "2016-12-08T16:32:18Z\n", "diff": "mmm a / lib / ansible / modules / extras / database / mysql_replication . py <nl> ppp b / lib / ansible / modules / extras / database / mysql_replication . py <nl> def main ( ) : <nl> <nl> try : <nl> if module . params [ \" login_unix_socket \" ] : <nl> - db_connection = MySQLdb . connect ( host = module . params [ \" login_host \" ] , unix_socket = module . params [ \" login_unix_socket \" ] , user = login_user , passwd = login_password , db = \" mysql \" ) <nl> + db_connection = MySQLdb . connect ( host = module . params [ \" login_host \" ] , unix_socket = module . params [ \" login_unix_socket \" ] , user = login_user , passwd = login_password ) <nl> else : <nl> - db_connection = MySQLdb . connect ( host = module . params [ \" login_host \" ] , user = login_user , passwd = login_password , db = \" mysql \" ) <nl> + db_connection = MySQLdb . connect ( host = module . params [ \" login_host \" ] , user = login_user , passwd = login_password ) <nl> except Exception , e : <nl> module . fail_json ( msg = \" unable to connect to database , check login_user and login_password are correct or ~ / . my . cnf has the credentials \" ) <nl> try : <nl>\n", "msg": "mysql_replication should not connect to the ' mysql ' database\n"}
{"diff_id": 4193, "repo": "ansible/ansible\n", "sha": "9a401e73a697bf6dd45e3cad75c11c5884fb4894\n", "time": "2013-08-15T14:29:51Z\n", "diff": "mmm a / lib / ansible / playbook / play . py <nl> ppp b / lib / ansible / playbook / play . py <nl> def __init__ ( self , playbook , ds , basedir ) : <nl> <nl> # * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * <nl> <nl> - def _load_roles ( self , roles , ds ) : <nl> - <nl> + def _get_role_path ( self , role ) : <nl> + \" \" \" <nl> + Returns the path on disk to the directory containing <nl> + the role directories like tasks , templates , etc . Also <nl> + returns any variables that were included with the role <nl> + \" \" \" <nl> + orig_path = template ( self . basedir , role , self . vars ) <nl> + <nl> + role_vars = { } <nl> + if type ( orig_path ) = = dict : <nl> + # what , not a path ? <nl> + role_name = orig_path . get ( ' role ' , None ) <nl> + if role_name is None : <nl> + raise errors . AnsibleError ( \" expected a role name in dictionary : % s \" % orig_path ) <nl> + role_vars = orig_path <nl> + orig_path = role_name <nl> + <nl> + path = utils . path_dwim ( self . basedir , os . path . join ( ' roles ' , orig_path ) ) <nl> + if not os . path . isdir ( path ) and not orig_path . startswith ( \" . \" ) and not orig_path . startswith ( \" / \" ) : <nl> + path2 = utils . path_dwim ( self . basedir , orig_path ) <nl> + if not os . path . isdir ( path2 ) : <nl> + raise errors . AnsibleError ( \" cannot find role in % s or % s \" % ( path , path2 ) ) <nl> + path = path2 <nl> + elif not os . path . isdir ( path ) : <nl> + raise errors . AnsibleError ( \" cannot find role in % s \" % ( path ) ) <nl> + <nl> + return ( path , role_vars ) <nl> + <nl> + def _build_role_dependencies ( self , roles , dep_stack , vars = { } , level = 0 ) : <nl> + # this number is arbitrary , but it seems sane <nl> + if level > 20 : <nl> + raise errors . AnsibleError ( \" too many levels of recursion while resolving role dependencies \" ) <nl> + for role in roles : <nl> + path , role_vars = self . _get_role_path ( role ) <nl> + # the meta directory contains the yaml that should <nl> + # hold the list of dependencies ( if any ) <nl> + meta = self . _resolve_main ( utils . path_dwim ( self . basedir , os . path . join ( path , ' meta ' ) ) ) <nl> + if os . path . isfile ( meta ) : <nl> + data = utils . parse_yaml_from_file ( meta ) <nl> + if data : <nl> + dependencies = data . get ( ' dependencies ' , [ ] ) <nl> + for dep in dependencies : <nl> + ( dep_path , dep_vars ) = self . _get_role_path ( dep ) <nl> + dep_vars . update ( role_vars ) <nl> + for k in vars . keys ( ) : <nl> + if not k in dep_vars : <nl> + dep_vars [ k ] = vars [ k ] <nl> + if ' role ' in dep_vars : <nl> + del dep_vars [ ' role ' ] <nl> + self . _build_role_dependencies ( [ dep ] , dep_stack , vars = dep_vars , level = level + 1 ) <nl> + dep_stack . append ( [ dep , dep_vars ] ) <nl> + # only add the current role when we ' re at the top level , <nl> + # otherwise we ' ll end up in a recursive loop <nl> + if level = = 0 : <nl> + dep_stack . append ( [ role , role_vars ] ) <nl> + return dep_stack <nl> <nl> + def _load_roles ( self , roles , ds ) : <nl> # a role is a name that auto - includes the following if they exist <nl> # < rolename > / tasks / main . yml <nl> # < rolename > / handlers / main . yml <nl> def _load_roles ( self , roles , ds ) : <nl> # flush handlers after pre_tasks <nl> new_tasks . append ( dict ( meta = ' flush_handlers ' ) ) <nl> <nl> - # variables if the role was parameterized ( i . e . given as a hash ) <nl> - has_dict = { } <nl> + roles = self . _build_role_dependencies ( roles , [ ] , self . vars ) <nl> <nl> - for role_path in roles : <nl> - orig_path = template ( self . basedir , role_path , self . vars ) <nl> - <nl> - if type ( orig_path ) = = dict : <nl> - # what , not a path ? <nl> - role_name = orig_path . get ( ' role ' , None ) <nl> - if role_name is None : <nl> - raise errors . AnsibleError ( \" expected a role name in dictionary : % s \" % orig_path ) <nl> - has_dict = orig_path <nl> - orig_path = role_name <nl> + for role , role_vars in roles : <nl> + path , ignore = self . _get_role_path ( role ) <nl> <nl> # special vars must be extracted from the dict to the included tasks <nl> special_keys = [ \" sudo \" , \" sudo_user \" , \" when \" , \" with_items \" ] <nl> special_vars = { } <nl> for k in special_keys : <nl> - if k in has_dict : <nl> - special_vars [ k ] = has_dict [ k ] <nl> - <nl> - path = utils . path_dwim ( self . basedir , os . path . join ( ' roles ' , orig_path ) ) <nl> - if not os . path . isdir ( path ) and not orig_path . startswith ( \" . \" ) and not orig_path . startswith ( \" / \" ) : <nl> - path2 = utils . path_dwim ( self . basedir , orig_path ) <nl> - if not os . path . isdir ( path2 ) : <nl> - raise errors . AnsibleError ( \" cannot find role in % s or % s \" % ( path , path2 ) ) <nl> - path = path2 <nl> - elif not os . path . isdir ( path ) : <nl> - raise errors . AnsibleError ( \" cannot find role in % s \" % ( path ) ) <nl> + if k in role_vars : <nl> + special_vars [ k ] = role_vars [ k ] <nl> + <nl> task_basepath = utils . path_dwim ( self . basedir , os . path . join ( path , ' tasks ' ) ) <nl> handler_basepath = utils . path_dwim ( self . basedir , os . path . join ( path , ' handlers ' ) ) <nl> vars_basepath = utils . path_dwim ( self . basedir , os . path . join ( path , ' vars ' ) ) <nl> + <nl> task = self . _resolve_main ( task_basepath ) <nl> handler = self . _resolve_main ( handler_basepath ) <nl> vars_file = self . _resolve_main ( vars_basepath ) <nl> library = utils . path_dwim ( self . basedir , os . path . join ( path , ' library ' ) ) <nl> + <nl> if not os . path . isfile ( task ) and not os . path . isfile ( handler ) and not os . path . isfile ( vars_file ) and not os . path . isdir ( library ) : <nl> raise errors . AnsibleError ( \" found role at % s , but cannot find % s or % s or % s or % s \" % ( path , task , handler , vars_file , library ) ) <nl> if os . path . isfile ( task ) : <nl> - nt = dict ( include = pipes . quote ( task ) , vars = has_dict ) <nl> + nt = dict ( include = pipes . quote ( task ) , vars = role_vars ) <nl> for k in special_keys : <nl> if k in special_vars : <nl> nt [ k ] = special_vars [ k ] <nl> new_tasks . append ( nt ) <nl> if os . path . isfile ( handler ) : <nl> - nt = dict ( include = pipes . quote ( handler ) , vars = has_dict ) <nl> + nt = dict ( include = pipes . quote ( handler ) , vars = role_vars ) <nl> for k in special_keys : <nl> if k in special_vars : <nl> nt [ k ] = special_vars [ k ] <nl> def _load_roles ( self , roles , ds ) : <nl> if os . path . isdir ( library ) : <nl> utils . plugins . module_finder . add_directory ( library ) <nl> <nl> - tasks = ds . get ( ' tasks ' , None ) <nl> + tasks = ds . get ( ' tasks ' , None ) <nl> post_tasks = ds . get ( ' post_tasks ' , None ) <nl> - <nl> - handlers = ds . get ( ' handlers ' , None ) <nl> + handlers = ds . get ( ' handlers ' , None ) <nl> vars_files = ds . get ( ' vars_files ' , None ) <nl> <nl> if type ( tasks ) ! = list : <nl> def _load_roles ( self , roles , ds ) : <nl> new_tasks . extend ( post_tasks ) <nl> # flush handlers after post tasks <nl> new_tasks . append ( dict ( meta = ' flush_handlers ' ) ) <nl> + <nl> new_handlers . extend ( handlers ) <nl> new_vars_files . extend ( vars_files ) <nl> + <nl> ds [ ' tasks ' ] = new_tasks <nl> ds [ ' handlers ' ] = new_handlers <nl> ds [ ' vars_files ' ] = new_vars_files <nl>\n", "msg": "Adding support for role dependencies .\n"}
{"diff_id": 4216, "repo": "zulip/zulip\n", "sha": "8ae35211b5e4048051c83717c81733bfe554389f\n", "time": "2017-03-21T07:53:22Z\n", "diff": "new file mode 100644 <nl> index 000000000000 . . d4c8b4421b18 <nl> mmm / dev / null <nl> ppp b / zerver / migrations / 0064_sync_uploads_filesize_with_db . py <nl> <nl> + # - * - coding : utf - 8 - * - <nl> + # Generated by Django 1 . 10 . 5 on 2017 - 03 - 18 12 : 38 <nl> + from __future__ import unicode_literals <nl> + <nl> + from django . db import migrations <nl> + from django . db . backends . postgresql_psycopg2 . schema import DatabaseSchemaEditor <nl> + from django . db . migrations . state import StateApps <nl> + from django . conf import settings <nl> + from boto . s3 . bucket import Bucket <nl> + from boto . s3 . key import Key <nl> + from boto . s3 . connection import S3Connection <nl> + from typing import Text <nl> + import os <nl> + <nl> + class MissingUploadFileException ( Exception ) : <nl> + pass <nl> + <nl> + def get_file_size_local ( path_id ) : <nl> + # type : ( Text ) - > int <nl> + file_path = os . path . join ( settings . LOCAL_UPLOADS_DIR , ' files ' , path_id ) <nl> + try : <nl> + size = os . path . getsize ( file_path ) <nl> + except OSError : <nl> + raise MissingUploadFileException <nl> + return size <nl> + <nl> + def sync_filesizes ( apps , schema_editor ) : <nl> + # type : ( StateApps , DatabaseSchemaEditor ) - > None <nl> + attachments = apps . get_model ( ' zerver ' , ' Attachment ' ) <nl> + if settings . LOCAL_UPLOADS_DIR is not None : <nl> + for attachment in attachments . objects . all ( ) : <nl> + if attachment . size is None : <nl> + try : <nl> + new_size = get_file_size_local ( attachment . path_id ) <nl> + except MissingUploadFileException : <nl> + new_size = 0 <nl> + attachment . size = new_size <nl> + attachment . save ( update_fields = [ \" size \" ] ) <nl> + else : <nl> + conn = S3Connection ( settings . S3_KEY , settings . S3_SECRET_KEY ) <nl> + bucket_name = settings . S3_AUTH_UPLOADS_BUCKET <nl> + bucket = conn . get_bucket ( bucket_name , validate = False ) <nl> + for attachment in attachments . objects . all ( ) : <nl> + if attachment . size is None : <nl> + file_key = bucket . get_key ( attachment . path_id ) <nl> + if file_key is None : <nl> + new_size = 0 <nl> + else : <nl> + new_size = file_key . size <nl> + attachment . size = new_size <nl> + attachment . save ( update_fields = [ \" size \" ] ) <nl> + <nl> + def reverse_sync_filesizes ( apps , schema_editor ) : <nl> + # type : ( StateApps , DatabaseSchemaEditor ) - > None <nl> + \" \" \" Does nothing \" \" \" <nl> + return None <nl> + <nl> + class Migration ( migrations . Migration ) : <nl> + <nl> + dependencies = [ <nl> + ( ' zerver ' , ' 0063_realm_description ' ) , <nl> + ] <nl> + <nl> + operations = [ <nl> + migrations . RunPython ( sync_filesizes , reverse_sync_filesizes ) , <nl> + ] <nl>\n", "msg": "migration : Sync sizes for existing Attachment objects with actual files .\n"}
{"diff_id": 4378, "repo": "ipython/ipython\n", "sha": "5e95b1e664262c80a9fefe9ebda821a63002e220\n", "time": "2011-11-23T03:12:46Z\n", "diff": "similarity index 95 % <nl> rename from IPython / extensions / pspersistence . py <nl> rename to IPython / extensions / storemagic . py <nl> mmm a / IPython / extensions / pspersistence . py <nl> ppp b / IPython / extensions / storemagic . py <nl> <nl> % store magic for lightweight persistence . <nl> <nl> Stores variables , aliases etc . in PickleShare database . <nl> + <nl> + To enable this functionality , run : : <nl> + <nl> + % load_ext storemagic <nl> + <nl> + in your IPython session . If you always want it enabled , you can list it in <nl> + your default profile ` ipython_config . py ` file : : <nl> + <nl> + c . InteractiveShellApp . extensions = [ ' storemagic ' ] <nl> \" \" \" <nl> <nl> from IPython . core . error import TryNext , UsageError <nl>\n", "msg": "Add docstring explaining how to enable the ` storemagic ` extension .\n"}
{"diff_id": 4469, "repo": "zulip/zulip\n", "sha": "0198736e77c4bc3205df9d39796702c81672993d\n", "time": "2019-07-24T23:26:10Z\n", "diff": "mmm a / zerver / tests / test_upload . py <nl> ppp b / zerver / tests / test_upload . py <nl> def test_non_existing_file_download ( self ) - > None : <nl> ' ' ' <nl> Trying to download a file that was never uploaded will return a json_error <nl> ' ' ' <nl> - self . login ( self . example_email ( \" hamlet \" ) ) <nl> - response = self . client_get ( \" http : / / localhost : 9991 / user_uploads / 1 / ff / gg / abc . py \" ) <nl> + hamlet = self . example_user ( \" hamlet \" ) <nl> + self . login ( hamlet . email ) <nl> + response = self . client_get ( \" http : / / localhost : 9991 / user_uploads / % s / ff / gg / abc . py \" % ( <nl> + hamlet . realm_id , ) ) <nl> self . assertEqual ( response . status_code , 404 ) <nl> self . assert_in_response ( ' File not found . ' , response ) <nl> <nl> def test_delete_old_unclaimed_attachments ( self ) - > None : <nl> self . assertTrue ( not delete_message_image ( d2_path_id ) ) <nl> <nl> def test_attachment_url_without_upload ( self ) - > None : <nl> - self . login ( self . example_email ( \" hamlet \" ) ) <nl> - body = \" Test message . . . [ zulip . txt ] ( http : / / localhost : 9991 / user_uploads / 1 / 64 / fake_path_id . txt ) \" <nl> + hamlet = self . example_user ( \" hamlet \" ) <nl> + self . login ( hamlet . email ) <nl> + body = \" Test message . . . [ zulip . txt ] ( http : / / localhost : 9991 / user_uploads / % s / 64 / fake_path_id . txt ) \" % ( <nl> + hamlet . realm_id , ) <nl> self . send_stream_message ( self . example_email ( \" hamlet \" ) , \" Denmark \" , body , \" test \" ) <nl> self . assertFalse ( Attachment . objects . filter ( path_id = \" 1 / 64 / fake_path_id . txt \" ) . exists ( ) ) <nl> <nl> def test_get_user_avatar ( self ) - > None : <nl> hamlet = self . example_email ( \" hamlet \" ) <nl> self . login ( hamlet ) <nl> cordelia = self . example_user ( ' cordelia ' ) <nl> - cross_realm_bot = self . example_user ( ' welcome_bot ' ) <nl> + cross_realm_bot = get_system_bot ( settings . WELCOME_BOT ) <nl> <nl> cordelia . avatar_source = UserProfile . AVATAR_FROM_USER <nl> cordelia . save ( ) <nl> def test_tarball_upload_local ( self ) - > None : <nl> result = re . search ( re . compile ( r \" ( [ A - Za - z0 - 9 \\ - _ ] { 24 } ) \" ) , uri ) <nl> if result is not None : <nl> random_name = result . group ( 1 ) <nl> - expected_url = \" http : / / zulip . testserver / user_avatars / exports / 1 / { random_name } / tarball . tar . gz \" . format ( <nl> + expected_url = \" http : / / zulip . testserver / user_avatars / exports / { realm_id } / { random_name } / tarball . tar . gz \" . format ( <nl> + realm_id = user_profile . realm_id , <nl> random_name = random_name , <nl> ) <nl> self . assertEqual ( expected_url , uri ) <nl> def test_tarball_upload ( self ) - > None : <nl> <nl> class UploadTitleTests ( TestCase ) : <nl> def test_upload_titles ( self ) - > None : <nl> - self . assertEqual ( url_filename ( \" http : / / localhost : 9991 / user_uploads / 1 / LUeQZUG5jxkagzVzp1Ox_amr / dummy . txt \" ) , \" dummy . txt \" ) <nl> - self . assertEqual ( url_filename ( \" http : / / localhost : 9991 / user_uploads / 1 / 94 / SzGYe0RFT - tEcOhQ6n - ZblFZ / zulip . txt \" ) , \" zulip . txt \" ) <nl> + zulip_realm = get_realm ( \" zulip \" ) <nl> + self . assertEqual ( url_filename ( \" http : / / localhost : 9991 / user_uploads / % s / LUeQZUG5jxkagzVzp1Ox_amr / dummy . txt \" % ( <nl> + zulip_realm . id , ) ) , \" dummy . txt \" ) <nl> + self . assertEqual ( url_filename ( \" http : / / localhost : 9991 / user_uploads / % s / 94 / SzGYe0RFT - tEcOhQ6n - ZblFZ / zulip . txt \" % ( <nl> + zulip_realm . id , ) ) , \" zulip . txt \" ) <nl> self . assertEqual ( url_filename ( \" https : / / zulip . com / user_uploads / 4142 / LUeQZUG5jxkagzVzp1Ox_amr / pasted_image . png \" ) , \" pasted_image . png \" ) <nl> self . assertEqual ( url_filename ( \" https : / / zulipchat . com / integrations \" ) , \" https : / / zulipchat . com / integrations \" ) <nl> self . assertEqual ( url_filename ( \" https : / / example . com \" ) , \" https : / / example . com \" ) <nl>\n", "msg": "test_upload : Prepare for moving system bots to zulipinternal .\n"}
{"diff_id": 4605, "repo": "zulip/zulip\n", "sha": "0f684118590d17787707dc3bf5f8d2bcf7ef16e9\n", "time": "2013-03-13T18:14:46Z\n", "diff": "mmm a / zephyr / management / commands / populate_db . py <nl> ppp b / zephyr / management / commands / populate_db . py <nl> def handle ( self , * * options ) : <nl> ] <nl> create_users ( realms , internal_humbug_users_nosubs ) <nl> <nl> + # Mark all messages as read <nl> + with transaction . commit_on_success ( ) : <nl> + UserMessage . objects . all ( ) . update ( flags = 0 ) <nl> + <nl> self . stdout . write ( \" Successfully populated test database . \\ n \" ) <nl> if options [ \" replay_old_messages \" ] : <nl> restore_saved_messages ( ) <nl>\n", "msg": "Mark messages as read when creating them in populate_db\n"}
{"diff_id": 4664, "repo": "python/cpython\n", "sha": "68faf5b0fc24e11af95a60cef3f9d13fc85b2711\n", "time": "2008-05-24T09:00:04Z\n", "diff": "mmm a / Lib / distutils / config . py <nl> ppp b / Lib / distutils / config . py <nl> def _read_pypirc ( self ) : <nl> \" \" \" Reads the . pypirc file . \" \" \" <nl> rc = self . _get_rc_file ( ) <nl> if os . path . exists ( rc ) : <nl> - print ' Using PyPI login from % s ' % rc <nl> + self . announce ( ' Using PyPI login from % s ' % rc ) <nl> repository = self . repository or self . DEFAULT_REPOSITORY <nl> realm = self . realm or self . DEFAULT_REALM <nl> <nl>\n", "msg": "Use announce instead of print , to suppress output in\n"}
{"diff_id": 4675, "repo": "ansible/ansible\n", "sha": "3c135ef3f2b7234e1ba20cc30822ebaca45a2bac\n", "time": "2016-04-25T03:42:14Z\n", "diff": "mmm a / lib / ansible / executor / module_common . py <nl> ppp b / lib / ansible / executor / module_common . py <nl> def debug ( command , zipped_mod , json_params ) : <nl> # $ / usr / bin / python / home / badger / . ansible / tmp / ansible - tmp - 1461173013 . 93 - 9076457629738 / ping execute <nl> <nl> # Okay to use __file__ here because we ' re running from a kept file <nl> - basedir = os . path . abspath ( os . path . dirname ( __file__ ) ) <nl> + basedir = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , ' debug_dir ' ) <nl> args_path = os . path . join ( basedir , ' args ' ) <nl> + script_path = os . path . join ( basedir , ' ansible_module_ % ( ansible_module ) s . py ' ) <nl> + <nl> if command = = ' explode ' : <nl> # transform the ZIPDATA into an exploded directory of code and then <nl> # print the path to the code . This is an easy way for people to look <nl> def debug ( command , zipped_mod , json_params ) : <nl> # This differs slightly from default Ansible execution of Python modules <nl> # as it passes the arguments to the module via a file instead of stdin . <nl> <nl> + # Set pythonpath to the debug dir <nl> pythonpath = os . environ . get ( ' PYTHONPATH ' ) <nl> if pythonpath : <nl> os . environ [ ' PYTHONPATH ' ] = ' : ' . join ( ( basedir , pythonpath ) ) <nl> else : <nl> os . environ [ ' PYTHONPATH ' ] = basedir <nl> <nl> - p = subprocess . Popen ( [ ' % ( interpreter ) s ' , ' ansible_module_ % ( ansible_module ) s . py ' , args_path ] , env = os . environ , shell = False , stdout = subprocess . PIPE , stderr = subprocess . PIPE , stdin = subprocess . PIPE ) <nl> + p = subprocess . Popen ( [ ' % ( interpreter ) s ' , script_path , args_path ] , env = os . environ , shell = False , stdout = subprocess . PIPE , stderr = subprocess . PIPE , stdin = subprocess . PIPE ) <nl> ( stdout , stderr ) = p . communicate ( ) <nl> <nl> if not isinstance ( stderr , ( bytes , unicode ) ) : <nl> def debug ( command , zipped_mod , json_params ) : <nl> # not actual bugs ( as they don ' t affect the real way that we invoke <nl> # ansible modules ) <nl> <nl> - # stub the <nl> + # stub the args and python path <nl> sys . argv = [ ' % ( ansible_module ) s ' , args_path ] <nl> + sys . path . insert ( 0 , basedir ) <nl> + <nl> from ansible_module_ % ( ansible_module ) s import main <nl> main ( ) <nl> print ( ' WARNING : Module returned to wrapper instead of exiting ' ) <nl> def debug ( command , zipped_mod , json_params ) : <nl> if PY3 : <nl> ZIPLOADER_PARAMS = ZIPLOADER_PARAMS . encode ( ' utf - 8 ' ) <nl> try : <nl> + # There ' s a race condition with the controller removing the <nl> + # remote_tmpdir and this module executing under async . So we cannot <nl> + # store this in remote_tmpdir ( use system tempdir instead ) <nl> temp_path = tempfile . mkdtemp ( prefix = ' ansible_ ' ) <nl> zipped_mod = os . path . join ( temp_path , ' ansible_modlib . zip ' ) <nl> modlib = open ( zipped_mod , ' wb ' ) <nl>\n", "msg": "Push debug files into a subdirectory to keep things cleaner .\n"}
{"diff_id": 4793, "repo": "ipython/ipython\n", "sha": "f4e744063a902b33ca3d778a5ba018850e66a111\n", "time": "2017-09-27T16:13:44Z\n", "diff": "mmm a / IPython / core / magics / script . py <nl> ppp b / IPython / core / magics / script . py <nl> def _script_magics_default ( self ) : <nl> ' python2 ' , <nl> ' python3 ' , <nl> ' pypy ' , <nl> - ' julia ' , <nl> ] <nl> if os . name = = ' nt ' : <nl> defaults . extend ( [ <nl>\n", "msg": "Revert \" Add Julia to default script magics . \"\n"}
{"diff_id": 4856, "repo": "zulip/zulip\n", "sha": "908025bdad3ce77ea45b353983948377e7efed84\n", "time": "2020-12-20T20:27:51Z\n", "diff": "mmm a / zerver / management / commands / runtornado . py <nl> ppp b / zerver / management / commands / runtornado . py <nl> def inner_run ( ) - > None : <nl> # We pass display_num_errors = False , since Django will <nl> # likely display similar output anyway . <nl> self . check ( display_num_errors = False ) <nl> - print ( f \" Tornado server is running at http : / / { addr } : { port } / \" ) <nl> + print ( f \" Tornado server ( re ) started on port { port } \" ) <nl> <nl> if settings . USING_RABBITMQ : <nl> queue_client = get_queue_client ( ) <nl>\n", "msg": "runtornado : Avoid providing a URL for Tornado on startup .\n"}
{"diff_id": 4875, "repo": "ansible/ansible\n", "sha": "677008bef7f179ab411dd6e17c44522a4d898195\n", "time": "2014-03-13T19:31:32Z\n", "diff": "mmm a / lib / ansible / module_utils / basic . py <nl> ppp b / lib / ansible / module_utils / basic . py <nl> def run_command ( self , args , check_rc = False , close_fds = False , executable = None , da <nl> <nl> shell = False <nl> if isinstance ( args , list ) : <nl> - pass <nl> + if use_unsafe_shell : <nl> + args = \" \" . join ( [ pipes . quote ( x ) for x in args ] ) <nl> + shell = True <nl> elif isinstance ( args , basestring ) and use_unsafe_shell : <nl> shell = True <nl> elif isinstance ( args , basestring ) : <nl>\n", "msg": "Rejoin args list into a string for run_command when using an unsafe shell\n"}
{"diff_id": 4918, "repo": "tornadoweb/tornado\n", "sha": "9ec87c2ba25e202fbce5ff1f35184fcbabba26f3\n", "time": "2010-11-05T18:19:18Z\n", "diff": "mmm a / tornado / httpserver . py <nl> ppp b / tornado / httpserver . py <nl> <nl> from tornado import httputil <nl> from tornado import ioloop <nl> from tornado import iostream <nl> + from tornado import stack_context <nl> <nl> try : <nl> import fcntl <nl> def __init__ ( self , stream , address , request_callback , no_keep_alive = False , <nl> self . xheaders = xheaders <nl> self . _request = None <nl> self . _request_finished = False <nl> - self . stream . read_until ( \" \\ r \\ n \\ r \\ n \" , self . _on_headers ) <nl> + # Save stack context here , outside of any request . This keeps <nl> + # contexts from one request from leaking into the next . <nl> + self . _header_callback = stack_context . wrap ( self . _on_headers ) <nl> + self . stream . read_until ( \" \\ r \\ n \\ r \\ n \" , self . _header_callback ) <nl> <nl> def write ( self , chunk ) : <nl> assert self . _request , \" Request closed \" <nl> def _finish_request ( self ) : <nl> if disconnect : <nl> self . stream . close ( ) <nl> return <nl> - self . stream . read_until ( \" \\ r \\ n \\ r \\ n \" , self . _on_headers ) <nl> + self . stream . read_until ( \" \\ r \\ n \\ r \\ n \" , self . _header_callback ) <nl> <nl> def _on_headers ( self , data ) : <nl> eol = data . find ( \" \\ r \\ n \" ) <nl>\n", "msg": "Capture StackContext explicitly in HTTPConnection to prevent leaks from\n"}
{"diff_id": 4923, "repo": "bokeh/bokeh\n", "sha": "c8d84bedfe45b93693d325d810816895e1743be2\n", "time": "2014-04-27T22:15:59Z\n", "diff": "mmm a / bokeh / mpl . py <nl> ppp b / bokeh / mpl . py <nl> def open_figure ( self , fig , props ) : <nl> height = self . height ) <nl> <nl> def close_figure ( self , fig ) : <nl> + # # Add plot props <nl> + # self . plot_props ( ) <nl> + <nl> + # Add tools <nl> + pantool = PanTool ( dimensions = [ \" width \" , \" height \" ] ) <nl> + wheelzoom = WheelZoomTool ( dimensions = [ \" width \" , \" height \" ] ) <nl> + previewsave = PreviewSaveTool ( plot = self . plot ) <nl> + objectexplorer = ObjectExplorerTool ( ) <nl> + self . plot . tools = [ pantool , wheelzoom , previewsave , objectexplorer ] <nl> + <nl> + if _PLOTLIST is not None : <nl> + _PLOTLIST . append ( self . plot ) <nl> + <nl> self . sess . add_plot ( self . plot ) <nl> <nl> def open_axes ( self , ax , props ) : <nl> def open_axes ( self , ax , props ) : <nl> xgrid = self . make_grid ( bxaxis , 0 ) <nl> ygrid = self . make_grid ( byaxis , 1 ) <nl> <nl> + def close_axes ( self , ax ) : <nl> + pass <nl> + <nl> + def open_legend ( self , legend , props ) : <nl> + pass <nl> + <nl> + def close_legend ( self , legend ) : <nl> + pass <nl> + <nl> def draw_line ( self , data , coordinates , style , label , mplobj = None ) : <nl> x = data [ : , 0 ] <nl> y = data [ : , 1 ] <nl>\n", "msg": "Added plot tools and some empty methods to be filled later .\n"}
{"diff_id": 4930, "repo": "home-assistant/core\n", "sha": "c00647ace06848cbc3788a25087f051db6b60033\n", "time": "2017-10-09T04:05:49Z\n", "diff": "mmm a / homeassistant / components / light / yeelight . py <nl> ppp b / homeassistant / components / light / yeelight . py <nl> <nl> SUPPORT_EFFECT | <nl> SUPPORT_COLOR_TEMP ) <nl> <nl> + YEELIGHT_MIN_KELVIN = YEELIGHT_MAX_KELVIN = 2700 <nl> + YEELIGHT_RGB_MIN_KELVIN = 1700 <nl> + YEELIGHT_RGB_MAX_KELVIN = 6500 <nl> + <nl> EFFECT_DISCO = \" Disco \" <nl> EFFECT_TEMP = \" Slow Temp \" <nl> EFFECT_STROBE = \" Strobe epilepsy ! \" <nl> def brightness ( self ) - > int : <nl> \" \" \" Return the brightness of this light between 1 . . 255 . \" \" \" <nl> return self . _brightness <nl> <nl> + @ property <nl> + def min_mireds ( self ) : <nl> + \" \" \" Return minimum supported color temperature . \" \" \" <nl> + if self . supported_features & SUPPORT_COLOR_TEMP : <nl> + return kelvin_to_mired ( YEELIGHT_RGB_MAX_KELVIN ) <nl> + return kelvin_to_mired ( YEELIGHT_MAX_KELVIN ) <nl> + <nl> + @ property <nl> + def max_mireds ( self ) : <nl> + \" \" \" Return maximum supported color temperature . \" \" \" <nl> + if self . supported_features & SUPPORT_COLOR_TEMP : <nl> + return kelvin_to_mired ( YEELIGHT_RGB_MIN_KELVIN ) <nl> + return kelvin_to_mired ( YEELIGHT_MIN_KELVIN ) <nl> + <nl> def _get_rgb_from_properties ( self ) : <nl> rgb = self . _properties . get ( ' rgb ' , None ) <nl> color_mode = self . _properties . get ( ' color_mode ' , None ) <nl>\n", "msg": "yeelight : implement min_mireds and max_mireds , fixes ( )\n"}
{"diff_id": 4960, "repo": "python/cpython\n", "sha": "31fd86c4f1ff17f615141c73f2ae61c61ea1b06c\n", "time": "2002-03-08T02:36:18Z\n", "diff": "mmm a / Lib / BaseHTTPServer . py <nl> ppp b / Lib / BaseHTTPServer . py <nl> def send_error ( self , code , message = None ) : <nl> explain = long <nl> self . log_error ( \" code % d , message % s \" , code , message ) <nl> self . send_response ( code , message ) <nl> + self . send_header ( \" Content - Type \" , \" text / html \" ) <nl> self . end_headers ( ) <nl> self . wfile . write ( self . error_message_format % <nl> { ' code ' : code , <nl>\n", "msg": "add Content - Type header to error responses\n"}
{"diff_id": 5068, "repo": "ytdl-org/youtube-dl\n", "sha": "84e4682f0eb2e67da23645e94abfa08e79ffc0df\n", "time": "2013-05-04T05:49:25Z\n", "diff": "mmm a / youtube_dl / InfoExtractors . py <nl> ppp b / youtube_dl / InfoExtractors . py <nl> class YoutubeIE ( InfoExtractor ) : <nl> ( [ 0 - 9A - Za - z_ - ] + ) # here is it ! the YouTube video ID <nl> ( ? ( 1 ) . + ) ? # if we found the ID , everything can follow <nl> $ \" \" \" <nl> - _LANG_URL = r ' http : / / www . youtube . com / ? hl = en & persist_hl = 1 & gl = US & persist_gl = 1 & opt_out_ackd = 1 ' <nl> + _LANG_URL = r ' https : / / www . youtube . com / ? hl = en & persist_hl = 1 & gl = US & persist_gl = 1 & opt_out_ackd = 1 ' <nl> _LOGIN_URL = ' https : / / accounts . google . com / ServiceLogin ' <nl> _AGE_URL = ' http : / / www . youtube . com / verify_age ? next_url = / & gl = US & hl = en ' <nl> _NEXT_URL_RE = r ' [ \\ ? & ] next_url = ( [ ^ & ] + ) ' <nl> def _real_initialize ( self ) : <nl> <nl> # Log in <nl> login_form_strs = { <nl> - u ' continue ' : u ' http : / / www . youtube . com / signin ? action_handle_signin = true & feature = sign_in_button & hl = en_US & nomobiletemp = 1 ' , <nl> + u ' continue ' : u ' https : / / www . youtube . com / signin ? action_handle_signin = true & feature = sign_in_button & hl = en_US & nomobiletemp = 1 ' , <nl> u ' Email ' : username , <nl> u ' GALX ' : galx , <nl> u ' Passwd ' : password , <nl> def _real_extract ( self , url ) : <nl> # Extract original video URL from URL with redirection , like age verification , using next_url parameter <nl> mobj = re . search ( self . _NEXT_URL_RE , url ) <nl> if mobj : <nl> - url = ' http : / / www . youtube . com / ' + compat_urllib_parse . unquote ( mobj . group ( 1 ) ) . lstrip ( ' / ' ) <nl> + url = ' https : / / www . youtube . com / ' + compat_urllib_parse . unquote ( mobj . group ( 1 ) ) . lstrip ( ' / ' ) <nl> video_id = self . _extract_id ( url ) <nl> <nl> # Get video webpage <nl> self . report_video_webpage_download ( video_id ) <nl> - url = ' http : / / www . youtube . com / watch ? v = % s & gl = US & hl = en & has_verified = 1 ' % video_id <nl> + url = ' https : / / www . youtube . com / watch ? v = % s & gl = US & hl = en & has_verified = 1 ' % video_id <nl> request = compat_urllib_request . Request ( url ) <nl> try : <nl> video_webpage_bytes = compat_urllib_request . urlopen ( request ) . read ( ) <nl>\n", "msg": "Always use HTTPS for youtube ( Fixes )\n"}
{"diff_id": 5125, "repo": "python/cpython\n", "sha": "51cc72c6c01a40f14ab84bfaf918dca81631db15\n", "time": "2003-01-03T21:02:36Z\n", "diff": "mmm a / Lib / mimetypes . py <nl> ppp b / Lib / mimetypes . py <nl> def read ( self , filename , strict = True ) : <nl> types . <nl> \" \" \" <nl> fp = open ( filename ) <nl> - self . readfp ( fp ) <nl> + self . readfp ( fp , strict ) <nl> fp . close ( ) <nl> <nl> def readfp ( self , fp , strict = True ) : <nl>\n", "msg": "Pass the strict argument from read ( ) on to readfp ( ) , so the\n"}
{"diff_id": 5206, "repo": "ipython/ipython\n", "sha": "d4efed3e53b56dd33bfe653a196d1cb9ff9bb05c\n", "time": "2013-04-15T20:27:04Z\n", "diff": "mmm a / IPython / frontend / html / notebook / handlers . py <nl> ppp b / IPython / frontend / html / notebook / handlers . py <nl> def get ( self , path , include_body = True ) : <nl> raise HTTPError ( 403 , \" % s is not a file \" , path ) <nl> <nl> stat_result = os . stat ( abspath ) <nl> - modified = datetime . datetime . fromtimestamp ( stat_result [ stat . ST_MTIME ] ) <nl> + modified = datetime . datetime . utcfromtimestamp ( stat_result [ stat . ST_MTIME ] ) <nl> <nl> self . set_header ( \" Last - Modified \" , modified ) <nl> <nl> def get ( self , path , include_body = True ) : <nl> ims_value = self . request . headers . get ( \" If - Modified - Since \" ) <nl> if ims_value is not None : <nl> date_tuple = email . utils . parsedate ( ims_value ) <nl> - if_since = datetime . datetime . fromtimestamp ( time . mktime ( date_tuple ) ) <nl> + if_since = datetime . datetime ( * date_tuple [ : 6 ] ) <nl> if if_since > = modified : <nl> self . set_status ( 304 ) <nl> return <nl>\n", "msg": "backport If - Modified - Since fix from tornado\n"}
{"diff_id": 5224, "repo": "matplotlib/matplotlib\n", "sha": "910b325f38f01d1465f4c353672cfac8f8c10616\n", "time": "2013-07-02T18:55:50Z\n", "diff": "mmm a / lib / matplotlib / tests / test_contour . py <nl> ppp b / lib / matplotlib / tests / test_contour . py <nl> <nl> from matplotlib . testing . decorators import cleanup , image_comparison <nl> from matplotlib import pyplot as plt <nl> <nl> + import re <nl> + <nl> <nl> @ cleanup <nl> def test_contour_shape_1d_valid ( ) : <nl> def test_contour_shape_mismatch_4 ( ) : <nl> ax . contour ( b , g , z ) <nl> except TypeError as exc : <nl> print exc . args [ 0 ] <nl> - assert exc . args [ 0 ] = = ' Shape of x does not match that of z : ' + \\ <nl> - ' found ( 9 , 9 ) instead of ( 9 , 10 ) . ' <nl> + assert re . match ( <nl> + r ' Shape of x does not match that of z : ' + <nl> + r ' found \\ ( 9L ? , 9L ? \\ ) instead of \\ ( 9L ? , 10L ? \\ ) \\ . ' , <nl> + exc . args [ 0 ] ) is not None <nl> <nl> try : <nl> ax . contour ( g , b , z ) <nl> except TypeError as exc : <nl> - assert exc . args [ 0 ] = = ' Shape of y does not match that of z : ' + \\ <nl> - ' found ( 9 , 9 ) instead of ( 9 , 10 ) . ' <nl> + assert re . match ( <nl> + r ' Shape of y does not match that of z : ' + <nl> + r ' found \\ ( 9L ? , 9L ? \\ ) instead of \\ ( 9L ? , 10L ? \\ ) \\ . ' , <nl> + exc . args [ 0 ] ) is not None <nl> <nl> <nl> @ cleanup <nl> def test_given_colors_levels_and_extends ( ) : <nl> _ , axes = plt . subplots ( 2 , 4 ) <nl> <nl> data = np . arange ( 12 ) . reshape ( 3 , 4 ) <nl> - <nl> + <nl> colors = [ ' red ' , ' yellow ' , ' pink ' , ' blue ' , ' black ' ] <nl> levels = [ 2 , 4 , 8 , 10 ] <nl> - <nl> + <nl> for i , ax in enumerate ( axes . flatten ( ) ) : <nl> plt . sca ( ax ) <nl> - <nl> + <nl> filled = i % 2 = = 0 . <nl> extend = [ ' neither ' , ' min ' , ' max ' , ' both ' ] [ i / / 2 ] <nl> - <nl> + <nl> if filled : <nl> last_color = - 1 if extend in [ ' min ' , ' max ' ] else None <nl> plt . contourf ( data , colors = colors [ : last_color ] , levels = levels , extend = extend ) <nl> else : <nl> last_level = - 1 if extend = = ' both ' else None <nl> plt . contour ( data , colors = colors , levels = levels [ : last_level ] , extend = extend ) <nl> - <nl> + <nl> plt . colorbar ( ) <nl> <nl> <nl>\n", "msg": "Use a regular expression to handle the different output of integers on Unix and Windows .\n"}
{"diff_id": 5336, "repo": "ipython/ipython\n", "sha": "af94ba2dfbd9f195bc39127c12b1844f248335fb\n", "time": "2008-06-24T00:29:58Z\n", "diff": "mmm a / IPython / genutils . py <nl> ppp b / IPython / genutils . py <nl> <nl> except ImportError : <nl> pass <nl> import os <nl> + import platform <nl> import re <nl> import shlex <nl> import shutil <nl> + import subprocess <nl> import sys <nl> import tempfile <nl> import time <nl> def newFunc ( * args , * * kwargs ) : <nl> stacklevel = 2 ) <nl> return func ( * args , * * kwargs ) <nl> return newFunc <nl> - <nl> - # * * * * * * * * * * * * * * * * * * * * * * * * * * * end of file < genutils . py > * * * * * * * * * * * * * * * * * * * * * * <nl> <nl> + <nl> + def _num_cpus_unix ( ) : <nl> + \" \" \" Return the number of active CPUs on a Unix system . \" \" \" <nl> + return os . sysconf ( \" SC_NPROCESSORS_ONLN \" ) <nl> + <nl> + <nl> + def _num_cpus_darwin ( ) : <nl> + \" \" \" Return the number of active CPUs on a Darwin system . \" \" \" <nl> + p = subprocess . Popen ( [ ' sysctl ' , ' - n ' , ' hw . ncpu ' ] , stdout = subprocess . PIPE ) <nl> + return p . stdout . read ( ) <nl> + <nl> + <nl> + def _num_cpus_windows ( ) : <nl> + \" \" \" Return the number of active CPUs on a Windows system . \" \" \" <nl> + return os . environ . get ( \" NUMBER_OF_PROCESSORS \" ) <nl> + <nl> + <nl> + def num_cpus ( ) : <nl> + \" \" \" Return the effective number of CPUs in the system as an integer . <nl> + <nl> + This cross - platform function makes an attempt at finding the total number of <nl> + available CPUs in the system , as returned by various underlying system and <nl> + python calls . <nl> + <nl> + If it can ' t find a sensible answer , it returns 1 ( though an error * may * make <nl> + it return a large positive number that ' s actually incorrect ) . <nl> + \" \" \" <nl> + <nl> + # Many thanks to the Parallel Python project ( http : / / www . parallelpython . com ) <nl> + # for the names of the keys we needed to look up for this function . This <nl> + # code was inspired by their equivalent function . <nl> + <nl> + ncpufuncs = { ' Linux ' : _num_cpus_unix , <nl> + ' Darwin ' : _num_cpus_darwin , <nl> + ' Windows ' : _num_cpus_windows , <nl> + # On Vista , python < 2 . 5 . 2 has a bug and returns ' Microsoft ' <nl> + # See http : / / bugs . python . org / issue1082 for details . <nl> + ' Microsoft ' : _num_cpus_windows , <nl> + } <nl> + <nl> + ncpufunc = ncpufuncs . get ( platform . system ( ) , <nl> + # default to unix version ( Solaris , AIX , etc ) <nl> + _num_cpus_unix ) <nl> + <nl> + try : <nl> + ncpus = max ( 1 , int ( ncpufunc ( ) ) ) <nl> + except : <nl> + ncpus = 1 <nl> + return ncpus <nl> + <nl> + # * * * * * * * * * * * * * * * * * * * * * * * * * * * end of file < genutils . py > * * * * * * * * * * * * * * * * * * * * * * <nl>\n", "msg": "Add num_cpus ( ) functions to detect total number of CPUs active .\n"}
{"diff_id": 5338, "repo": "python/cpython\n", "sha": "1269be5f4a0975748409eee3ac6ce3e01620523b\n", "time": "2003-03-29T22:54:00Z\n", "diff": "mmm a / Mac / scripts / gensuitemodule . py <nl> ppp b / Mac / scripts / gensuitemodule . py <nl> <nl> import distutils . sysconfig <nl> import OSATerminology <nl> from Carbon . Res import * <nl> + import Carbon . Folder <nl> import MacOS <nl> import getopt <nl> + import plistlib <nl> <nl> _MAC_LIB_FOLDER = os . path . dirname ( aetools . __file__ ) <nl> DEFAULT_STANDARD_PACKAGEFOLDER = os . path . join ( _MAC_LIB_FOLDER , ' lib - scriptpackages ' ) <nl> def main ( ) : <nl> process_func ( filename , output = output , basepkgname = basepkgname , <nl> edit_modnames = edit_modnames , creatorsignature = creatorsignature ) <nl> else : <nl> - # The dialogOptionFlags below allows selection of . app bundles . <nl> - filename = EasyDialogs . AskFileForOpen ( <nl> - message = ' Select scriptable application ' , <nl> - dialogOptionFlags = 0x1056 ) <nl> - if not filename : <nl> - sys . exit ( 0 ) <nl> - try : <nl> - processfile ( filename ) <nl> - except MacOS . Error , arg : <nl> - print \" Error getting terminology : \" , arg <nl> - print \" Retry , manually parsing resources \" <nl> - processfile_fromresource ( filename ) <nl> + main_interactive ( ) <nl> + <nl> + def main_interactive ( interact = 0 , basepkgname = ' StdSuites ' ) : <nl> + if interact : <nl> + # Ask for save - filename for each module <nl> + edit_modnames = None <nl> + else : <nl> + # Use default filenames for each module <nl> + edit_modnames = [ ] <nl> + appsfolder = Carbon . Folder . FSFindFolder ( - 32765 , ' apps ' , 0 ) <nl> + filename = EasyDialogs . AskFileForOpen ( <nl> + message = ' Select scriptable application ' , <nl> + dialogOptionFlags = 0x1056 , # allow selection of . app bundles <nl> + defaultLocation = appsfolder ) <nl> + if not filename : <nl> + return <nl> + if not is_scriptable ( filename ) : <nl> + if EasyDialogs . AskYesNoCancel ( <nl> + \" Warning : application does not seem scriptable \" , <nl> + yes = \" Continue \" , default = 2 , no = \" \" ) < = 0 : <nl> + return <nl> + try : <nl> + processfile ( filename , edit_modnames = edit_modnames , basepkgname = basepkgname ) <nl> + except MacOS . Error , arg : <nl> + print \" Error getting terminology : \" , arg <nl> + print \" Retry , manually parsing resources \" <nl> + processfile_fromresource ( filename , edit_modnames = edit_modnames , <nl> + basepkgname = basepkgname ) <nl> + <nl> + def is_scriptable ( application ) : <nl> + \" \" \" Return true if the application is scriptable \" \" \" <nl> + if os . path . isdir ( application ) : <nl> + plistfile = os . path . join ( application , ' Contents ' , ' Info . plist ' ) <nl> + if not os . path . exists ( plistfile ) : <nl> + return False <nl> + plist = plistlib . Plist . fromFile ( plistfile ) <nl> + return plist . get ( ' NSAppleScriptEnabled ' , False ) <nl> + # If it is a file test for an aete / aeut resource . <nl> + currf = CurResFile ( ) <nl> + try : <nl> + refno = macresource . open_pathname ( application ) <nl> + except MacOS . Error : <nl> + return False <nl> + UseResFile ( refno ) <nl> + n_terminology = Count1Resources ( ' aete ' ) + Count1Resources ( ' aeut ' ) + \\ <nl> + Count1Resources ( ' scsz ' ) + Count1Resources ( ' osiz ' ) <nl> + CloseResFile ( refno ) <nl> + UseResFile ( currf ) <nl> + return n_terminology > 0 <nl> <nl> def processfile_fromresource ( fullname , output = None , basepkgname = None , <nl> edit_modnames = None , creatorsignature = None ) : <nl> \" \" \" Process all resources in a single file \" \" \" <nl> + if not is_scriptable ( fullname ) : <nl> + print \" Warning : app does not seem scriptable : % s \" % fullname <nl> cur = CurResFile ( ) <nl> print \" Processing \" , fullname <nl> rf = macresource . open_pathname ( fullname ) <nl> def processfile_fromresource ( fullname , output = None , basepkgname = None , <nl> def processfile ( fullname , output = None , basepkgname = None , <nl> edit_modnames = None , creatorsignature = None ) : <nl> \" \" \" Ask an application for its terminology and process that \" \" \" <nl> + if not is_scriptable ( fullname ) : <nl> + print \" Warning : app does not seem scriptable : % s \" % fullname <nl> print \" \\ nASKING FOR aete DICTIONARY IN \" , ` fullname ` <nl> try : <nl> aedescobj , launched = OSATerminology . GetAppTerminology ( fullname ) <nl>\n", "msg": "- Added an is_scriptable method to test applications for having\n"}
{"diff_id": 5509, "repo": "zulip/zulip\n", "sha": "319e2231b8db60100a70d8fc0fbc1e5621d8d0cb\n", "time": "2020-01-16T20:36:24Z\n", "diff": "mmm a / zerver / lib / thumbnail . py <nl> ppp b / zerver / lib / thumbnail . py <nl> <nl> import os <nl> import sys <nl> import urllib <nl> - from urllib . parse import urljoin , urlsplit , urlunsplit <nl> + from urllib . parse import urljoin <nl> from django . conf import settings <nl> + from django . utils . http import is_safe_url <nl> from libthumbor import CryptoURL <nl> <nl> ZULIP_PATH = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( ' __file__ ' ) ) ) ) <nl> def is_thumbor_enabled ( ) - > bool : <nl> return settings . THUMBOR_URL ! = ' ' <nl> <nl> def user_uploads_or_external ( url : str ) - > bool : <nl> - u = urlsplit ( url ) <nl> - return u . scheme ! = \" \" or u . netloc ! = \" \" or u . path . startswith ( \" / user_uploads / \" ) <nl> + return not is_safe_url ( url ) or url . startswith ( \" / user_uploads / \" ) <nl> <nl> def get_source_type ( url : str ) - > str : <nl> if not url . startswith ( ' / user_uploads / ' ) : <nl> def generate_thumbnail_url ( path : str , <nl> size : str = ' 0x0 ' , <nl> is_camo_url : bool = False ) - > str : <nl> path = urljoin ( \" / \" , path ) <nl> - u = urlsplit ( path ) <nl> <nl> if not is_thumbor_enabled ( ) : <nl> - if u . scheme = = \" \" and u . netloc = = \" \" : <nl> - return urlunsplit ( u ) <nl> + if is_safe_url ( path ) : <nl> + return path <nl> return get_camo_url ( path ) <nl> <nl> - if u . scheme = = \" \" and u . netloc = = \" \" and not u . path . startswith ( \" / user_uploads / \" ) : <nl> - return urlunsplit ( u ) <nl> + if is_safe_url ( path ) and not path . startswith ( \" / user_uploads / \" ) : <nl> + return path <nl> <nl> source_type = get_source_type ( path ) <nl> safe_url = base64 . urlsafe_b64encode ( path . encode ( ) ) . decode ( ' utf - 8 ' ) <nl>\n", "msg": "thumbnail : Tighten fix for CVE - 2019 - 19775 open redirect .\n"}
{"diff_id": 5510, "repo": "matplotlib/matplotlib\n", "sha": "80ca8da9ed35b2197941b283fd295361d80c8051\n", "time": "2010-06-08T21:57:14Z\n", "diff": "mmm a / lib / matplotlib / backends / backend_wx . py <nl> ppp b / lib / matplotlib / backends / backend_wx . py <nl> def draw_if_interactive ( ) : <nl> <nl> def show ( ) : <nl> \" \" \" <nl> - Current implementation assumes that matplotlib is executed in a PyCrust <nl> - shell . It appears to be possible to execute wxPython applications from <nl> - within a PyCrust without having to ensure that wxPython has been created <nl> - in a secondary thread ( e . g . SciPy gui_thread ) . <nl> - <nl> - Unfortunately , gui_thread seems to introduce a number of further <nl> - dependencies on SciPy modules , which I do not wish to introduce <nl> - into the backend at this point . If there is a need I will look <nl> - into this in a later release . <nl> + Show all the figures and enter the wx main loop . <nl> + This should be the last line of your script . <nl> \" \" \" <nl> DEBUG_MSG ( \" show ( ) \" , 3 , None ) <nl> <nl> for figwin in Gcf . get_all_fig_managers ( ) : <nl> figwin . frame . Show ( ) <nl> <nl> - if show . _needmain and not matplotlib . is_interactive ( ) : <nl> - # start the wxPython gui event if there is not already one running <nl> + needmain = not wx . App . IsMainLoopRunning ( ) <nl> + if needmain and len ( Gcf . get_all_fig_managers ( ) ) > 0 : <nl> wxapp = wx . GetApp ( ) <nl> if wxapp is not None : <nl> - # wxPython 2 . 4 has no wx . App . IsMainLoopRunning ( ) method <nl> - imlr = getattr ( wxapp , ' IsMainLoopRunning ' , lambda : False ) <nl> - if not imlr ( ) : <nl> - wxapp . MainLoop ( ) <nl> - show . _needmain = False <nl> - show . _needmain = True <nl> + wxapp . MainLoop ( ) <nl> + # start the wxPython gui event if there is not already one running <nl> + <nl> <nl> def new_figure_manager ( num , * args , * * kwargs ) : <nl> \" \" \" <nl>\n", "msg": "backend_wx : modernize show ; multiple calls to show work now\n"}
{"diff_id": 5538, "repo": "TheAlgorithms/Python\n", "sha": "b05081a717546f021014454d03bcf2f6145148aa\n", "time": "2020-09-19T06:58:08Z\n", "diff": "similarity index 96 % <nl> rename from conversions / bin_to_octal . py <nl> rename to conversions / binary_to_octal . py <nl> mmm a / conversions / bin_to_octal . py <nl> ppp b / conversions / binary_to_octal . py <nl> def bin_to_octal ( bin_string : str ) - > str : <nl> while len ( bin_string ) % 3 ! = 0 : <nl> bin_string = \" 0 \" + bin_string <nl> bin_string_in_3_list = [ <nl> - bin_string [ index : index + 3 ] <nl> + bin_string [ index : index + 3 ] <nl> for index , value in enumerate ( bin_string ) <nl> if index % 3 = = 0 <nl> ] <nl>\n", "msg": "Update and rename bin_to_octal . py to binary_to_octal . py ( )\n"}
{"diff_id": 5668, "repo": "zulip/zulip\n", "sha": "190f481f495d914c7feb1bf1d5c1ab3901e2a554\n", "time": "2020-10-01T21:40:48Z\n", "diff": "mmm a / zerver / lib / stream_subscription . py <nl> ppp b / zerver / lib / stream_subscription . py <nl> <nl> - from typing import Any , Dict , List , Tuple <nl> + from typing import Any , Dict , List , Optional , Tuple <nl> <nl> from django . db . models . query import QuerySet <nl> <nl> def num_subscribers_for_stream_id ( stream_id : int ) - > int : <nl> ) . count ( ) <nl> <nl> <nl> - def handle_stream_notifications_compatibility ( user_profile : UserProfile , <nl> + def handle_stream_notifications_compatibility ( user_profile : Optional [ UserProfile ] , <nl> stream_dict : Dict [ str , Any ] , <nl> notification_settings_null : bool ) - > None : <nl> # Old versions of the mobile apps don ' t support ` None ` as a <nl> def handle_stream_notifications_compatibility ( user_profile : UserProfile , <nl> if stream_dict [ notification_type ] is not None : <nl> continue <nl> target_attr = \" enable_stream_ \" + notification_type <nl> - stream_dict [ notification_type ] = getattr ( user_profile , target_attr ) <nl> + stream_dict [ notification_type ] = False if user_profile is None else getattr ( user_profile , target_attr ) <nl>\n", "msg": "stream_subscription : Mark notifications disabled for web public users .\n"}
{"diff_id": 5762, "repo": "ansible/ansible\n", "sha": "a82fea65c1deed4a2726f4a703172b368b76991c\n", "time": "2018-03-26T23:15:05Z\n", "diff": "mmm a / lib / ansible / plugins / loader . py <nl> ppp b / lib / ansible / plugins / loader . py <nl> def all ( self , * args , * * kwargs ) : <nl> if not found_in_cache : <nl> self . _load_config_defs ( basename , path ) <nl> <nl> - self . _update_object ( obj , name , path ) <nl> + self . _update_object ( obj , basename , path ) <nl> yield obj <nl> <nl> <nl>\n", "msg": "We need to save the basename into an attribute for calling code\n"}
{"diff_id": 5795, "repo": "python/cpython\n", "sha": "199e0857f9040c56d7a37fac62fdc5b0df7a08a7\n", "time": "2011-05-09T03:41:55Z\n", "diff": "mmm a / Lib / test / support . py <nl> ppp b / Lib / test / support . py <nl> def import_module ( name , deprecated = False ) : <nl> def _save_and_remove_module ( name , orig_modules ) : <nl> \" \" \" Helper function to save and remove a module from sys . modules <nl> <nl> - Return value is True if the module was in sys . modules and <nl> - False otherwise . \" \" \" <nl> + Return True if the module was in sys . modules , False otherwise . <nl> + Raise ImportError if the module can ' t be imported . \" \" \" <nl> saved = True <nl> try : <nl> orig_modules [ name ] = sys . modules [ name ] <nl> except KeyError : <nl> + # try to import the module and raise an error if it can ' t be imported <nl> + __import__ ( name ) <nl> saved = False <nl> else : <nl> del sys . modules [ name ] <nl> def _save_and_remove_module ( name , orig_modules ) : <nl> def _save_and_block_module ( name , orig_modules ) : <nl> \" \" \" Helper function to save and block a module in sys . modules <nl> <nl> - Return value is True if the module was in sys . modules and <nl> - False otherwise . \" \" \" <nl> + Return True if the module was in sys . modules , False otherwise . \" \" \" <nl> saved = True <nl> try : <nl> orig_modules [ name ] = sys . modules [ name ] <nl> def import_fresh_module ( name , fresh = ( ) , blocked = ( ) , deprecated = False ) : <nl> the sys . modules cache is restored to its original state . <nl> <nl> Modules named in fresh are also imported anew if needed by the import . <nl> + If one of these modules can ' t be imported , None is returned . <nl> <nl> Importing of modules named in blocked is prevented while the fresh import <nl> takes place . <nl> def import_fresh_module ( name , fresh = ( ) , blocked = ( ) , deprecated = False ) : <nl> if not _save_and_block_module ( blocked_name , orig_modules ) : <nl> names_to_remove . append ( blocked_name ) <nl> fresh_module = importlib . import_module ( name ) <nl> + except ImportError : <nl> + fresh_module = None <nl> finally : <nl> for orig_name , module in orig_modules . items ( ) : <nl> sys . modules [ orig_name ] = module <nl>\n", "msg": ": change import_fresh_module to return None when one of the \" fresh \" modules can not be imported .\n"}
{"diff_id": 5865, "repo": "numpy/numpy\n", "sha": "876176a0412c4fb22645edc531110d2a0a2fed16\n", "time": "2006-07-07T16:46:58Z\n", "diff": "mmm a / numpy / __init__ . py <nl> ppp b / numpy / __init__ . py <nl> <nl> <nl> def test ( level = 1 , verbosity = 1 ) : <nl> return NumpyTest ( ) . test ( level , verbosity ) <nl> + test . __doc__ = NumpyTest . test . __doc__ <nl> <nl> import add_newdocs <nl> <nl>\n", "msg": "Don ' t want to assign test to NumpyTest ( ) . test ( which instantiates NumpyTest ) ,\n"}
{"diff_id": 5909, "repo": "zulip/zulip\n", "sha": "5441e361675653074494df1403807f686e742415\n", "time": "2013-09-18T17:27:58Z\n", "diff": "mmm a / zerver / lib / actions . py <nl> ppp b / zerver / lib / actions . py <nl> def do_update_message ( user_profile , message_id , subject , propagate_mode , content <nl> event [ ' message_ids ' ] = [ ] <nl> for changed_message in changed_messages : <nl> event [ ' message_ids ' ] . append ( changed_message . id ) <nl> - cache_save_message ( changed_message ) <nl> + items_for_memcached [ message_cache_key ( changed_message . id ) ] = ( changed_message , ) <nl> items_for_memcached [ to_dict_cache_key ( changed_message , True ) ] = \\ <nl> ( stringify_message_dict ( changed_message . to_dict_uncached ( apply_markdown = True ) ) , ) <nl> items_for_memcached [ to_dict_cache_key ( changed_message , False ) ] = \\ <nl>\n", "msg": "Bulk update messages in deprecated cache when updating message\n"}
{"diff_id": 5913, "repo": "ansible/ansible\n", "sha": "352b620665e369843eb890fc24a8f84482e42320\n", "time": "2016-12-08T16:25:22Z\n", "diff": "mmm a / lib / ansible / modules / cloud / google / gce . py <nl> ppp b / lib / ansible / modules / cloud / google / gce . py <nl> <nl> aliases : [ ] <nl> name : <nl> description : <nl> - - identifier when working with a single instance . Will be deprecated in a future release . <nl> - Please ' instance_names ' instead . <nl> + - either a name of a single instance or when used with ' num_instances ' , <nl> + the base name of a cluster of nodes <nl> required : false <nl> - aliases : [ ] <nl> + aliases : [ ' base_name ' ] <nl> + num_instances : <nl> + description : <nl> + - can be used with ' name ' , specifies <nl> + the number of nodes to provision using ' name ' <nl> + as a base name <nl> + required : false <nl> + version_added : \" 2 . 3 \" <nl> network : <nl> description : <nl> - name of the network , ' default ' will be used if not specified <nl> def get_instance_info ( inst ) : <nl> } ) <nl> <nl> <nl> - def create_instances ( module , gce , instance_names ) : <nl> + def create_instances ( module , gce , instance_names , number ) : <nl> \" \" \" Creates new instances . Attributes other than instance_names are picked <nl> up from ' module ' <nl> <nl> def create_instances ( module , gce , instance_names ) : <nl> module . fail_json ( msg = ' Missing required create instance variable ' , <nl> changed = False ) <nl> <nl> - for name in instance_names : <nl> - pd = None <nl> - if lc_disks : <nl> - pd = lc_disks [ 0 ] <nl> - elif persistent_boot_disk : <nl> + gce_args = dict ( <nl> + location = lc_zone , <nl> + ex_network = network , ex_tags = tags , ex_metadata = metadata , <nl> + ex_can_ip_forward = ip_forward , <nl> + external_ip = instance_external_ip , ex_disk_auto_delete = disk_auto_delete , <nl> + ex_service_accounts = ex_sa_perms <nl> + ) <nl> + if preemptible is not None : <nl> + gce_args [ ' ex_preemptible ' ] = preemptible <nl> + if subnetwork is not None : <nl> + gce_args [ ' ex_subnetwork ' ] = subnetwork <nl> + <nl> + if isinstance ( instance_names , str ) and not number : <nl> + instance_names = [ instance_names ] <nl> + <nl> + if isinstance ( instance_names , str ) and number : <nl> + instance_responses = gce . ex_create_multiple_nodes ( instance_names , lc_machine_type , <nl> + lc_image ( ) , number , * * gce_args ) <nl> + for resp in instance_responses : <nl> + n = resp <nl> + if isinstance ( resp , libcloud . compute . drivers . gce . GCEFailedNode ) : <nl> + try : <nl> + n = gce . ex_get_node ( n . name , lc_zone ) <nl> + except ResourceNotFoundError : <nl> + pass <nl> + else : <nl> + # Assure that at least one node has been created to set changed = True <nl> + changed = True <nl> + new_instances . append ( n ) <nl> + else : <nl> + for instance in instance_names : <nl> + pd = None <nl> + if lc_disks : <nl> + pd = lc_disks [ 0 ] <nl> + elif persistent_boot_disk : <nl> + try : <nl> + pd = gce . ex_get_volume ( \" % s \" % instance , lc_zone ) <nl> + except ResourceNotFoundError : <nl> + pd = gce . create_volume ( None , \" % s \" % instance , image = lc_image ( ) ) <nl> + gce_args [ ' ex_boot_disk ' ] = pd <nl> + <nl> + inst = None <nl> try : <nl> - pd = gce . ex_get_volume ( \" % s \" % name , lc_zone ) <nl> + inst = gce . ex_get_node ( instance , lc_zone ) <nl> except ResourceNotFoundError : <nl> - pd = gce . create_volume ( None , \" % s \" % name , image = lc_image ( ) ) <nl> - <nl> - gce_args = dict ( <nl> - location = lc_zone , <nl> - ex_network = network , ex_tags = tags , ex_metadata = metadata , <nl> - ex_boot_disk = pd , ex_can_ip_forward = ip_forward , <nl> - external_ip = instance_external_ip , ex_disk_auto_delete = disk_auto_delete , <nl> - ex_service_accounts = ex_sa_perms <nl> - ) <nl> - if preemptible is not None : <nl> - gce_args [ ' ex_preemptible ' ] = preemptible <nl> - if subnetwork is not None : <nl> - gce_args [ ' ex_subnetwork ' ] = subnetwork <nl> - <nl> - inst = None <nl> - try : <nl> - inst = gce . ex_get_node ( name , lc_zone ) <nl> - except ResourceNotFoundError : <nl> - inst = gce . create_node ( <nl> - name , lc_machine_type , lc_image ( ) , * * gce_args <nl> - ) <nl> - changed = True <nl> - except GoogleBaseError as e : <nl> - module . fail_json ( msg = ' Unexpected error attempting to create ' + <nl> - ' instance % s , error : % s ' % ( name , e . value ) ) <nl> + inst = gce . create_node ( <nl> + instance , lc_machine_type , lc_image ( ) , * * gce_args <nl> + ) <nl> + changed = True <nl> + except GoogleBaseError as e : <nl> + module . fail_json ( msg = ' Unexpected error attempting to create ' + <nl> + ' instance % s , error : % s ' % ( instance , e . value ) ) <nl> + if inst : <nl> + new_instances . append ( inst ) <nl> <nl> + for inst in new_instances : <nl> for i , lc_disk in enumerate ( lc_disks ) : <nl> # Check whether the disk is already attached <nl> if ( len ( inst . extra [ ' disks ' ] ) > i ) : <nl> def create_instances ( module , gce , instance_names ) : <nl> inst . extra [ ' disks ' ] . append ( <nl> { ' source ' : lc_disk . extra [ ' selfLink ' ] , ' index ' : i } ) <nl> <nl> - if inst : <nl> - new_instances . append ( inst ) <nl> - <nl> instance_names = [ ] <nl> instance_json_data = [ ] <nl> for inst in new_instances : <nl> def create_instances ( module , gce , instance_names ) : <nl> <nl> return ( changed , instance_json_data , instance_names ) <nl> <nl> - def change_instance_state ( module , gce , instance_names , zone_name , state ) : <nl> + def change_instance_state ( module , gce , instance_names , number , zone_name , state ) : <nl> \" \" \" Changes the state of a list of instances . For example , <nl> change from started to stopped , or started to absent . <nl> <nl> def change_instance_state ( module , gce , instance_names , zone_name , state ) : <nl> <nl> \" \" \" <nl> changed = False <nl> - changed_instance_names = [ ] <nl> - for name in instance_names : <nl> + nodes = [ ] <nl> + state_instance_names = [ ] <nl> + <nl> + if isinstance ( instance_names , str ) and number : <nl> + node_names = [ ' % s - % 03d ' % ( instance_names , i ) for i in range ( number ) ] <nl> + elif isinstance ( instance_names , str ) and not number : <nl> + node_names = [ instance_names ] <nl> + else : <nl> + node_names = instance_names <nl> + <nl> + for name in node_names : <nl> inst = None <nl> try : <nl> inst = gce . ex_get_node ( name , zone_name ) <nl> except ResourceNotFoundError : <nl> - pass <nl> + state_instance_names . append ( name ) <nl> except Exception as e : <nl> module . fail_json ( msg = unexpected_error_msg ( e ) , changed = False ) <nl> - if inst and state in [ ' absent ' , ' deleted ' ] : <nl> - gce . destroy_node ( inst ) <nl> - changed_instance_names . append ( inst . name ) <nl> - changed = True <nl> - elif inst and state = = ' started ' and \\ <nl> - inst . state = = libcloud . compute . types . NodeState . STOPPED : <nl> - gce . ex_start_node ( inst ) <nl> - changed_instance_names . append ( inst . name ) <nl> - changed = True <nl> - elif inst and state in [ ' stopped ' , ' terminated ' ] and \\ <nl> - inst . state = = libcloud . compute . types . NodeState . RUNNING : <nl> - gce . ex_stop_node ( inst ) <nl> - changed_instance_names . append ( inst . name ) <nl> - changed = True <nl> - <nl> - return ( changed , changed_instance_names ) <nl> + else : <nl> + nodes . append ( inst ) <nl> + state_instance_names . append ( name ) <nl> + <nl> + if state in [ ' absent ' , ' deleted ' ] and number : <nl> + changed_nodes = gce . ex_destroy_multiple_nodes ( nodes ) or [ False ] <nl> + changed = reduce ( lambda x , y : x or y , changed_nodes ) <nl> + else : <nl> + for node in nodes : <nl> + if state in [ ' absent ' , ' deleted ' ] : <nl> + gce . destroy_node ( node ) <nl> + changed = True <nl> + elif state = = ' started ' and \\ <nl> + node . state = = libcloud . compute . types . NodeState . STOPPED : <nl> + gce . ex_start_node ( node ) <nl> + changed = True <nl> + elif state in [ ' stopped ' , ' terminated ' ] and \\ <nl> + node . state = = libcloud . compute . types . NodeState . RUNNING : <nl> + gce . ex_stop_node ( node ) <nl> + changed = True <nl> + <nl> + return ( changed , state_instance_names ) <nl> <nl> def main ( ) : <nl> module = AnsibleModule ( <nl> def main ( ) : <nl> instance_names = dict ( ) , <nl> machine_type = dict ( default = ' n1 - standard - 1 ' ) , <nl> metadata = dict ( ) , <nl> - name = dict ( ) , <nl> + name = dict ( aliases = [ ' base_name ' ] ) , <nl> + num_instances = dict ( type = ' int ' ) , <nl> network = dict ( default = ' default ' ) , <nl> subnetwork = dict ( ) , <nl> persistent_boot_disk = dict ( type = ' bool ' , default = False ) , <nl> def main ( ) : <nl> external_ip = dict ( default = ' ephemeral ' ) , <nl> disk_auto_delete = dict ( type = ' bool ' , default = True ) , <nl> preemptible = dict ( type = ' bool ' , default = None ) , <nl> - ) <nl> + ) , <nl> + mutually_exclusive = [ ( ' instance_names ' , ' name ' ) ] <nl> ) <nl> <nl> if not HAS_PYTHON26 : <nl> def main ( ) : <nl> machine_type = module . params . get ( ' machine_type ' ) <nl> metadata = module . params . get ( ' metadata ' ) <nl> name = module . params . get ( ' name ' ) <nl> + number = module . params . get ( ' num_instances ' ) <nl> network = module . params . get ( ' network ' ) <nl> subnetwork = module . params . get ( ' subnetwork ' ) <nl> persistent_boot_disk = module . params . get ( ' persistent_boot_disk ' ) <nl> def main ( ) : <nl> preemptible = module . params . get ( ' preemptible ' ) <nl> changed = False <nl> <nl> - inames = [ ] <nl> + inames = None <nl> if isinstance ( instance_names , list ) : <nl> inames = instance_names <nl> elif isinstance ( instance_names , str ) : <nl> inames = instance_names . split ( ' , ' ) <nl> if name : <nl> - inames . append ( name ) <nl> + inames = name <nl> if not inames : <nl> module . fail_json ( msg = ' Must specify a \" name \" or \" instance_names \" ' , <nl> changed = False ) <nl> def main ( ) : <nl> json_output = { ' zone ' : zone } <nl> if state in [ ' absent ' , ' deleted ' , ' started ' , ' stopped ' , ' terminated ' ] : <nl> json_output [ ' state ' ] = state <nl> - ( changed , changed_instance_names ) = change_instance_state ( <nl> - module , gce , inames , zone , state ) <nl> + ( changed , state_instance_names ) = change_instance_state ( <nl> + module , gce , inames , number , zone , state ) <nl> <nl> # based on what user specified , return the same variable , although <nl> # value could be different if an instance could not be destroyed <nl> - if instance_names : <nl> - json_output [ ' instance_names ' ] = changed_instance_names <nl> + if instance_names or name and number : <nl> + json_output [ ' instance_names ' ] = state_instance_names <nl> elif name : <nl> json_output [ ' name ' ] = name <nl> <nl> elif state in [ ' active ' , ' present ' ] : <nl> json_output [ ' state ' ] = ' present ' <nl> ( changed , instance_data , instance_name_list ) = create_instances ( <nl> - module , gce , inames ) <nl> + module , gce , inames , number ) <nl> json_output [ ' instance_data ' ] = instance_data <nl> if instance_names : <nl> json_output [ ' instance_names ' ] = instance_name_list <nl>\n", "msg": "GCE : Add support for ' number ' parameter for manually provisioned Google Compute clusters ( )\n"}
{"diff_id": 5919, "repo": "zulip/zulip\n", "sha": "5fd9748e13f79b98cd5b0bab3aab45bfdfd9ce4d\n", "time": "2019-01-31T23:04:09Z\n", "diff": "mmm a / analytics / management / commands / stream_stats . py <nl> ppp b / analytics / management / commands / stream_stats . py <nl> def handle ( self , * args : Any , * * options : str ) - > None : <nl> realms = Realm . objects . all ( ) <nl> <nl> for realm in realms : <nl> - print ( realm . string_id ) <nl> - print ( \" mmmmmmmmmmmm \" ) <nl> - print ( \" % 25s % 15s % 10s \" % ( \" stream \" , \" subscribers \" , \" messages \" ) ) <nl> streams = Stream . objects . filter ( realm = realm ) . exclude ( Q ( name__istartswith = \" tutorial - \" ) ) <nl> - invite_only_count = 0 <nl> + # private stream count <nl> + private_count = 0 <nl> + # public stream count <nl> + public_count = 0 <nl> for stream in streams : <nl> if stream . invite_only : <nl> - invite_only_count + = 1 <nl> - continue <nl> + private_count + = 1 <nl> + else : <nl> + public_count + = 1 <nl> + print ( \" mmmmmmmmmmmm \" ) <nl> + print ( realm . string_id , end = ' ' ) <nl> + print ( \" % 10s % d public streams and \" % ( \" ( \" , public_count ) , end = ' ' ) <nl> + print ( \" % d private streams ) \" % ( private_count , ) ) <nl> + print ( \" mmmmmmmmmmmm \" ) <nl> + print ( \" % 25s % 15s % 10s \" % ( \" stream \" , \" subscribers \" , \" messages \" ) ) <nl> + <nl> + for stream in streams : <nl> print ( \" % 25s \" % ( stream . name , ) , end = ' ' ) <nl> recipient = Recipient . objects . filter ( type = Recipient . STREAM , type_id = stream . id ) <nl> print ( \" % 10d \" % ( len ( Subscription . objects . filter ( recipient = recipient , <nl> active = True ) ) , ) , end = ' ' ) <nl> num_messages = len ( Message . objects . filter ( recipient = recipient ) ) <nl> print ( \" % 12d \" % ( num_messages , ) ) <nl> - print ( \" % d private streams \" % ( invite_only_count , ) ) <nl> print ( \" \" ) <nl>\n", "msg": "stream_stats : List number of private and public streams for each realm .\n"}
{"diff_id": 6191, "repo": "ansible/ansible\n", "sha": "c9d28e10ad792a6942a74c7fc25bb036f8e785b9\n", "time": "2013-08-19T20:27:20Z\n", "diff": "mmm a / lib / ansible / inventory / __init__ . py <nl> ppp b / lib / ansible / inventory / __init__ . py <nl> def __init__ ( self , host_list = C . DEFAULT_HOST_LIST ) : <nl> self . parser = None <nl> all = Group ( ' all ' ) <nl> self . groups = [ all ] <nl> + ipv6_re = re . compile ( ' \\ [ ( [ a - f : A - F0 - 9 ] * ) \\ ] ( ? : : ( \\ d + ) ) ? ' ) <nl> for x in host_list : <nl> - if \" : \" in x : <nl> - tokens = x . split ( \" : \" , 1 ) <nl> - all . add_host ( Host ( tokens [ 0 ] , tokens [ 1 ] ) ) <nl> + m = ipv6_re . match ( x ) <nl> + if m : <nl> + all . add_host ( Host ( m . groups ( ) [ 0 ] , m . groups ( ) [ 1 ] ) ) <nl> else : <nl> - all . add_host ( Host ( x ) ) <nl> + if \" : \" in x : <nl> + tokens = x . rsplit ( \" : \" , 1 ) <nl> + # if there is ' : ' in the address , then this is a ipv6 <nl> + if ' : ' in tokens [ 0 ] : <nl> + all . add_host ( Host ( x ) ) <nl> + else : <nl> + all . add_host ( Host ( tokens [ 0 ] , tokens [ 1 ] ) ) <nl> + else : <nl> + all . add_host ( Host ( x ) ) <nl> elif os . path . exists ( host_list ) : <nl> if os . path . isdir ( host_list ) : <nl> # Ensure basedir is inside the directory <nl>\n", "msg": "add support for using a ipv6 in - i\n"}
{"diff_id": 6194, "repo": "python/cpython\n", "sha": "446974687877fd1f3fb754d64a4740ace1b6a745\n", "time": "2012-05-28T15:33:01Z\n", "diff": "mmm a / Lib / venv / __init__ . py <nl> ppp b / Lib / venv / __init__ . py <nl> <nl> <nl> Copyright ( C ) 20011 - 2012 Vinay Sajip . All Rights Reserved . <nl> <nl> - usage : python - m venv [ - h ] [ - - no - distribute ] [ - - system - site - packages ] <nl> - [ - - symlinks ] [ - - clear ] [ - - upgrade ] <nl> + usage : python - m venv [ - h ] [ - - system - site - packages ] [ - - symlinks ] [ - - clear ] <nl> + [ - - upgrade ] <nl> ENV_DIR [ ENV_DIR . . . ] <nl> <nl> Creates virtual Python environments in one or more target directories . <nl> <nl> <nl> optional arguments : <nl> - h , - - help show this help message and exit <nl> - - - no - distribute Don ' t install Distribute in the virtual environment . * <nl> - - system - site - packages <nl> Give the virtual environment access to the system <nl> site - packages dir . <nl> <nl> raised . <nl> - - upgrade Upgrade the environment directory to use this version <nl> of Python , assuming Python has been upgraded in - place . <nl> - <nl> - * Note : Distribute support will be available during the alpha phase to <nl> - facilitate testing third - party packages with venvs created using this package . <nl> - This support will be removed after the alpha phase . <nl> \" \" \" <nl> import base64 <nl> import io <nl> <nl> import os . path <nl> import shutil <nl> import sys <nl> + import sysconfig <nl> try : <nl> import threading <nl> except ImportError : <nl> def create ( self , env_dir ) : <nl> \" \" \" <nl> if ( self . symlinks and <nl> sys . platform = = ' darwin ' and <nl> - ' Library / Framework ' in sys . base_prefix ) : <nl> + sysconfig . get_config_var ( ' PYTHONFRAMEWORK ' ) ) : <nl> # Symlinking the stub executable in an OSX framework build will <nl> # result in a broken virtual environment . <nl> raise ValueError ( <nl> - \" Symlinking is not supported on OSX framework Python . \" ) <nl> + ' Symlinking is not supported on OSX framework Python . ' ) <nl> env_dir = os . path . abspath ( env_dir ) <nl> context = self . ensure_directories ( env_dir ) <nl> self . create_configuration ( context ) <nl> def setup_python ( self , context ) : <nl> if os . name ! = ' nt ' : <nl> if not os . path . islink ( path ) : <nl> os . chmod ( path , 0o755 ) <nl> - path = os . path . join ( binpath , ' python ' ) <nl> - if not os . path . exists ( path ) : <nl> - os . symlink ( exename , path ) <nl> + for suffix in ( ' python ' , ' python3 ' ) : <nl> + path = os . path . join ( binpath , suffix ) <nl> + if not os . path . exists ( path ) : <nl> + os . symlink ( exename , path ) <nl> else : <nl> subdir = ' DLLs ' <nl> include = self . include_binary <nl> def install_scripts ( self , context , path ) : <nl> os . chmod ( dstfile , 0o755 ) <nl> <nl> <nl> - # This class will not be included in Python core ; it ' s here for now to <nl> - # facilitate experimentation and testing , and as proof - of - concept of what could <nl> - # be done by external extension tools . <nl> - class DistributeEnvBuilder ( EnvBuilder ) : <nl> - \" \" \" <nl> - By default , this builder installs Distribute so that you can pip or <nl> - easy_install other packages into the created environment . <nl> - <nl> - : param nodist : If True , Distribute is not installed into the created <nl> - environment . <nl> - : param progress : If Distribute is installed , the progress of the <nl> - installation can be monitored by passing a progress <nl> - callable . If specified , it is called with two <nl> - arguments : a string indicating some progress , and a <nl> - context indicating where the string is coming from . <nl> - The context argument can have one of three values : <nl> - ' main ' , indicating that it is called from virtualize ( ) <nl> - itself , and ' stdout ' and ' stderr ' , which are obtained <nl> - by reading lines from the output streams of a subprocess <nl> - which is used to install Distribute . <nl> - <nl> - If a callable is not specified , default progress <nl> - information is output to sys . stderr . <nl> - \" \" \" <nl> - <nl> - def __init__ ( self , * args , * * kwargs ) : <nl> - self . nodist = kwargs . pop ( \" nodist \" , False ) <nl> - self . progress = kwargs . pop ( \" progress \" , None ) <nl> - super ( ) . __init__ ( * args , * * kwargs ) <nl> - <nl> - def post_setup ( self , context ) : <nl> - \" \" \" <nl> - Set up any packages which need to be pre - installed into the <nl> - environment being created . <nl> - <nl> - : param context : The information for the environment creation request <nl> - being processed . <nl> - \" \" \" <nl> - if not self . nodist : <nl> - if threading : <nl> - self . install_distribute ( context ) <nl> - <nl> - def reader ( self , stream , context ) : <nl> - \" \" \" <nl> - Read lines from a subprocess ' output stream and either pass to a progress <nl> - callable ( if specified ) or write progress information to sys . stderr . <nl> - \" \" \" <nl> - progress = self . progress <nl> - while True : <nl> - s = stream . readline ( ) <nl> - if not s : <nl> - break <nl> - if progress is not None : <nl> - progress ( s , context ) <nl> - else : <nl> - sys . stderr . write ( ' . ' ) <nl> - # sys . stderr . write ( s . decode ( ' utf - 8 ' ) ) <nl> - sys . stderr . flush ( ) <nl> - stream . close ( ) <nl> - <nl> - def install_distribute ( self , context ) : <nl> - \" \" \" <nl> - Install Distribute in the environment . <nl> - <nl> - : param context : The information for the environment creation request <nl> - being processed . <nl> - \" \" \" <nl> - from subprocess import Popen , PIPE <nl> - from urllib . request import urlretrieve <nl> - <nl> - url = ' http : / / python - distribute . org / distribute_setup . py ' <nl> - binpath = context . bin_path <nl> - distpath = os . path . join ( binpath , ' distribute_setup . py ' ) <nl> - # Download Distribute in the env <nl> - urlretrieve ( url , distpath ) <nl> - progress = self . progress <nl> - if progress is not None : <nl> - progress ( ' Installing distribute ' , ' main ' ) <nl> - else : <nl> - sys . stderr . write ( ' Installing distribute ' ) <nl> - sys . stderr . flush ( ) <nl> - # Install Distribute in the env <nl> - args = [ context . env_exe , ' distribute_setup . py ' ] <nl> - p = Popen ( args , stdout = PIPE , stderr = PIPE , cwd = binpath ) <nl> - t1 = threading . Thread ( target = self . reader , args = ( p . stdout , ' stdout ' ) ) <nl> - t1 . start ( ) <nl> - t2 = threading . Thread ( target = self . reader , args = ( p . stderr , ' stderr ' ) ) <nl> - t2 . start ( ) <nl> - p . wait ( ) <nl> - t1 . join ( ) <nl> - t2 . join ( ) <nl> - if progress is not None : <nl> - progress ( ' done . ' , ' main ' ) <nl> - else : <nl> - sys . stderr . write ( ' done . \\ n ' ) <nl> - # Clean up - no longer needed <nl> - os . unlink ( distpath ) <nl> - <nl> def create ( env_dir , system_site_packages = False , clear = False , symlinks = False ) : <nl> \" \" \" <nl> Create a virtual environment in a directory . <nl> def create ( env_dir , system_site_packages = False , clear = False , symlinks = False ) : <nl> : param symlinks : If True , attempt to symlink rather than copy files into <nl> virtual environment . <nl> \" \" \" <nl> - # XXX This should be changed to EnvBuilder . <nl> - builder = DistributeEnvBuilder ( system_site_packages = system_site_packages , <nl> + builder = EnvBuilder ( system_site_packages = system_site_packages , <nl> clear = clear , symlinks = symlinks ) <nl> builder . create ( env_dir ) <nl> <nl> def main ( args = None ) : <nl> ' directories . ' ) <nl> parser . add_argument ( ' dirs ' , metavar = ' ENV_DIR ' , nargs = ' + ' , <nl> help = ' A directory to create the environment in . ' ) <nl> - # XXX This option will be removed . <nl> - parser . add_argument ( ' - - no - distribute ' , default = False , <nl> - action = ' store_true ' , dest = ' nodist ' , <nl> - help = \" Don ' t install Distribute in the virtual \" <nl> - \" environment . \" ) <nl> parser . add_argument ( ' - - system - site - packages ' , default = False , <nl> action = ' store_true ' , dest = ' system_site ' , <nl> - help = \" Give the virtual environment access to the \" <nl> - \" system site - packages dir . \" ) <nl> + help = ' Give the virtual environment access to the ' <nl> + ' system site - packages dir . ' ) <nl> if os . name = = ' nt ' or ( sys . platform = = ' darwin ' and <nl> - ' Library / Framework ' in sys . base_prefix ) : <nl> + sysconfig . get_config_var ( ' PYTHONFRAMEWORK ' ) ) : <nl> use_symlinks = False <nl> else : <nl> use_symlinks = True <nl> def main ( args = None ) : <nl> options = parser . parse_args ( args ) <nl> if options . upgrade and options . clear : <nl> raise ValueError ( ' you cannot supply - - upgrade and - - clear together . ' ) <nl> - # XXX This will be changed to EnvBuilder <nl> - builder = DistributeEnvBuilder ( system_site_packages = options . system_site , <nl> - clear = options . clear , <nl> - symlinks = options . symlinks , <nl> - upgrade = options . upgrade , <nl> - nodist = options . nodist ) <nl> + builder = EnvBuilder ( system_site_packages = options . system_site , <nl> + clear = options . clear , symlinks = options . symlinks , <nl> + upgrade = options . upgrade ) <nl> for d in options . dirs : <nl> builder . create ( d ) <nl> <nl>\n", "msg": "Added python3 symlink ; removed Distribute - related code , docs and comments ; changed Mac OS X computation to determine framework builds .\n"}
{"diff_id": 6240, "repo": "python/cpython\n", "sha": "9824509d3ee84f00ba658e249bad81e00e3b98bf\n", "time": "1998-02-19T21:15:44Z\n", "diff": "mmm a / Lib / ftplib . py <nl> ppp b / Lib / ftplib . py <nl> def transfercmd ( self , cmd ) : <nl> def login ( self , user = ' ' , passwd = ' ' , acct = ' ' ) : <nl> ' ' ' Login , default anonymous . ' ' ' <nl> if not user : user = ' anonymous ' <nl> + if not passwd : passwd = ' ' <nl> + if not acct : acct = ' ' <nl> if user = = ' anonymous ' and passwd in ( ' ' , ' - ' ) : <nl> thishost = socket . gethostname ( ) <nl> # Make sure it is fully qualified <nl> def mkd ( self , dirname ) : <nl> resp = self . sendcmd ( ' MKD ' + dirname ) <nl> return parse257 ( resp ) <nl> <nl> + def rmd ( self , dirname ) : <nl> + ' ' ' Remove a directory . ' ' ' <nl> + return self . voidcmd ( ' RMD ' + dirname ) <nl> + <nl> def pwd ( self ) : <nl> ' ' ' Return current working directory . ' ' ' <nl> resp = self . sendcmd ( ' PWD ' ) <nl> def parse227 ( resp ) : <nl> <nl> <nl> def parse257 ( resp ) : <nl> - ' ' ' Parse the ' 257 ' response for a MKD or RMD request . <nl> - This is a response to a MKD or RMD request : a directory name . <nl> + ' ' ' Parse the ' 257 ' response for a MKD or PWD request . <nl> + This is a response to a MKD or PWD request : a directory name . <nl> Returns the directoryname in the 257 reply . ' ' ' <nl> <nl> if resp [ : 3 ] < > ' 257 ' : <nl>\n", "msg": "Add rmd ( ) ( remove directory command ) ; fix comment in parse257 .\n"}
{"diff_id": 6284, "repo": "numpy/numpy\n", "sha": "d7f761650aeca14772d3eeb11dcac4788e53e0a3\n", "time": "2014-04-21T18:22:57Z\n", "diff": "mmm a / numpy / doc / misc . py <nl> ppp b / numpy / doc / misc . py <nl> <nl> Miscellaneous <nl> = = = = = = = = = = = = = <nl> <nl> - IEEE 754 Floating Point Special Values : <nl> mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm <nl> + IEEE 754 Floating Point Special Values <nl> + mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> <nl> Special values defined in numpy : nan , inf , <nl> <nl> <nl> > > > np . nansum ( x ) <nl> 42 . 0 <nl> <nl> - How numpy handles numerical exceptions : <nl> mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - <nl> + How numpy handles numerical exceptions <nl> + mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> <nl> The default is to ` ` ' warn ' ` ` for ` ` invalid ` ` , ` ` divide ` ` , and ` ` overflow ` ` <nl> and ` ` ' ignore ' ` ` for ` ` underflow ` ` . But this can be changed , and it can be <nl> <nl> Note that integer divide - by - zero is handled by the same machinery . <nl> These behaviors are set on a per - thread basis . <nl> <nl> - Examples : <nl> mmmmmmmmmmmm - <nl> + Examples <nl> + mmmmmm - - <nl> <nl> : : <nl> <nl> <nl> > > > j = np . seterr ( * * oldsettings ) # restore previous <nl> . . . # error - handling settings <nl> <nl> - Interfacing to C : <nl> mmmmmmmmmmmmmmmmmm <nl> + Interfacing to C <nl> + mmmmmmmmmmmmmmm - <nl> Only a survey of the choices . Little detail on how each works . <nl> <nl> 1 ) Bare metal , wrap your own C - code manually . <nl> <nl> <nl> - API will change for Python 3 . 0 ! <nl> <nl> - 2 ) pyrex <nl> + 2 ) Cython <nl> <nl> - Plusses : <nl> <nl> - avoid learning C API ' s <nl> - no dealing with reference counting <nl> - - can code in psuedo python and generate C code <nl> + - can code in pseudo python and generate C code <nl> - can also interface to existing C code <nl> - should shield you from changes to Python C api <nl> - - become pretty popular within Python community <nl> + - has become the de - facto standard within the scientific Python community <nl> + - fast indexing support for arrays <nl> <nl> - Minuses : <nl> <nl> - Can write code in non - standard form which may become obsolete <nl> - Not as flexible as manual wrapping <nl> - - Maintainers not easily adaptable to new features <nl> - <nl> - Thus : <nl> - <nl> - 3 ) cython - fork of pyrex to allow needed features for SAGE <nl> - <nl> - - being considered as the standard scipy / numpy wrapping tool <nl> - - fast indexing support for arrays <nl> <nl> 4 ) ctypes <nl> <nl> <nl> <nl> - generates lots of code between Python and the C code <nl> - can cause performance problems that are nearly impossible to optimize <nl> - out <nl> + out <nl> - interface files can be hard to write <nl> - doesn ' t necessarily avoid reference counting issues or needing to know <nl> API ' s <nl> <nl> - 7 ) Weave <nl> + 7 ) scipy . weave <nl> <nl> - Plusses : <nl> <nl> - - Phenomenal tool <nl> - can turn many numpy expressions into C code <nl> - dynamic compiling and loading of generated C code <nl> - can embed pure C code in Python module and have weave extract , generate <nl> <nl> <nl> - Minuses : <nl> <nl> - - Future uncertain - - lacks a champion <nl> + - Future very uncertain : it ' s the only part of Scipy not ported to Python 3 <nl> + and is effectively deprecated in favor of Cython . <nl> <nl> 8 ) Psyco <nl> <nl> <nl> <nl> Interfacing to Fortran : <nl> mmmmmmmmmmmmmmmmmmmmm - - <nl> - Fortran : Clear choice is f2py . ( Pyfort is an older alternative , but not <nl> - supported any longer ) <nl> + The clear choice to wrap Fortran code is <nl> + ` f2py < http : / / docs . scipy . org / doc / numpy - dev / f2py / > ` _ . <nl> + <nl> + Pyfort is an older alternative , but not supported any longer . <nl> + Fwrap is a newer project that looked promising but isn ' t being developed any <nl> + longer . <nl> <nl> Interfacing to C + + : <nl> mmmmmmmmmmmmmmmmmm - <nl> - 1 ) CXX <nl> - 2 ) Boost . python <nl> - 3 ) SWIG <nl> - 4 ) Sage has used cython to wrap C + + ( not pretty , but it can be done ) <nl> + 1 ) Cython <nl> + 2 ) CXX <nl> + 3 ) Boost . python <nl> + 4 ) SWIG <nl> 5 ) SIP ( used mainly in PyQT ) <nl> <nl> \" \" \" <nl>\n", "msg": "DOC : clean up docs in userguide for interfacing to compiled code a bit .\n"}
{"diff_id": 6287, "repo": "matplotlib/matplotlib\n", "sha": "6c48f4c02b9e878133a481599a001ac8db3ad4fd\n", "time": "2009-05-19T21:29:19Z\n", "diff": "mmm a / lib / matplotlib / axis . py <nl> ppp b / lib / matplotlib / axis . py <nl> def get_minor_ticks ( self , numticks = None ) : <nl> <nl> def grid ( self , b = None , which = ' major ' , * * kwargs ) : <nl> \" \" \" <nl> - Set the axis grid on or off ; b is a boolean use * which * = <nl> - ' major ' | ' minor ' to set the grid for major or minor ticks <nl> + Set the axis grid on or off ; b is a boolean . Use * which * = <nl> + ' major ' | ' minor ' to set the grid for major or minor ticks . <nl> <nl> - if * b * is * None * and len ( kwargs ) = = 0 , toggle the grid state . If <nl> + If * b * is * None * and len ( kwargs ) = = 0 , toggle the grid state . If <nl> * kwargs * are supplied , it is assumed you want the grid on and * b * <nl> - will be set to True <nl> + will be set to True . <nl> <nl> * kwargs * are used to set the line properties of the grids , eg , <nl> <nl>\n", "msg": "convert Axis . grid ( ) docstrings to use complete sentences\n"}
{"diff_id": 6302, "repo": "python/cpython\n", "sha": "f4033ab245582999fe19423edc5ffa42f7ea505f\n", "time": "2008-02-04T18:48:38Z\n", "diff": "mmm a / Lib / test / test_sys . py <nl> ppp b / Lib / test / test_sys . py <nl> def test_compact_freelists ( self ) : <nl> del floats <nl> # should free more than 200 blocks each <nl> r = sys . _compact_freelists ( ) <nl> + self . assert_ ( r [ 0 ] [ 1 ] > 200 , r [ 0 ] [ 1 ] ) <nl> + self . assert_ ( r [ 1 ] [ 2 ] > 200 , r [ 1 ] [ 1 ] ) <nl> + <nl> self . assert_ ( r [ 0 ] [ 2 ] > 200 , r [ 0 ] [ 2 ] ) <nl> self . assert_ ( r [ 1 ] [ 2 ] > 200 , r [ 1 ] [ 2 ] ) <nl> <nl>\n", "msg": "Increase debugging to investige failing tests on some build bots\n"}
{"diff_id": 6382, "repo": "ansible/ansible\n", "sha": "9ede6f7f494d83cf95b11bb46a8eb4a8234c0a48\n", "time": "2014-03-26T17:04:06Z\n", "diff": "mmm a / lib / ansible / playbook / play . py <nl> ppp b / lib / ansible / playbook / play . py <nl> def __init__ ( self , playbook , ds , basedir , vault_password = None ) : <nl> self . sudo = ds . get ( ' sudo ' , self . playbook . sudo ) <nl> self . sudo_user = ds . get ( ' sudo_user ' , self . playbook . sudo_user ) <nl> self . transport = ds . get ( ' connection ' , self . playbook . transport ) <nl> - self . gather_facts = ds . get ( ' gather_facts ' , None ) <nl> self . remote_port = self . remote_port <nl> self . any_errors_fatal = utils . boolean ( ds . get ( ' any_errors_fatal ' , ' false ' ) ) <nl> self . accelerate = utils . boolean ( ds . get ( ' accelerate ' , ' false ' ) ) <nl> def __init__ ( self , playbook , ds , basedir , vault_password = None ) : <nl> self . su_user = ds . get ( ' su_user ' , self . playbook . su_user ) <nl> # self . vault_password = vault_password <nl> <nl> + # gather_facts is not a simple boolean , as None means that a ' smart ' <nl> + # fact gathering mode will be used , so we need to be careful here as <nl> + # calling utils . boolean ( None ) returns False <nl> + self . gather_facts = ds . get ( ' gather_facts ' , None ) <nl> + if self . gather_facts : <nl> + self . gather_facts = utils . boolean ( self . gather_facts ) <nl> + <nl> # Fail out if user specifies a sudo param with a su param in a given play <nl> if ( ds . get ( ' sudo ' ) or ds . get ( ' sudo_user ' ) ) and ( ds . get ( ' su ' ) or ds . get ( ' su_user ' ) ) : <nl> raise errors . AnsibleError ( ' sudo params ( \" sudo \" , \" sudo_user \" ) and su params ' <nl>\n", "msg": "Convert gather_facts to a boolean value if it is not None\n"}
{"diff_id": 6432, "repo": "pypa/pipenv\n", "sha": "60e0e97a6e8adf419ba3ce2ed382db57d16176fe\n", "time": "2018-03-11T01:19:11Z\n", "diff": "mmm a / pipenv / utils . py <nl> ppp b / pipenv / utils . py <nl> <nl> <nl> from distutils . spawn import find_executable <nl> from contextlib import contextmanager <nl> - from piptools . resolver import Resolver <nl> - from piptools . repositories . pypi import PyPIRepository <nl> - from piptools . scripts . compile import get_pip_command <nl> - from piptools import logging as piptools_logging <nl> - from piptools . exceptions import NoCandidateFound <nl> - from pip . download import is_archive_file <nl> - from pip . exceptions import DistributionNotFound <nl> - from pip . index import Link <nl> + from pipenv . patched . piptools . resolver import Resolver <nl> + from pipenv . patched . piptools . repositories . pypi import PyPIRepository <nl> + from pipenv . patched . piptools . scripts . compile import get_pip_command <nl> + from pipenv . patched . piptools import logging as piptools_logging <nl> + from pipenv . patched . piptools . exceptions import NoCandidateFound <nl> + from pipenv . patched . pip . download import is_archive_file <nl> + from pipenv . patched . pip . exceptions import DistributionNotFound <nl> + from pipenv . patched . pip . index import Link <nl> from requests . exceptions import HTTPError , ConnectionError <nl> <nl> from . pep508checker import lookup <nl> class ResourceWarning ( Warning ) : <nl> <nl> <nl> def get_requirement ( dep ) : <nl> - import pip <nl> + from pipenv . patched . pip . req . req_install import _strip_extras <nl> import requirements <nl> \" \" \" Pre - clean requirement strings passed to the requirements parser . <nl> <nl> def get_requirement ( dep ) : <nl> else : <nl> markers = None <nl> # Strip extras from the requirement so we can make a properly parseable req <nl> - dep , extras = pip . req . req_install . _strip_extras ( dep ) <nl> + dep , extras = _strip_extras ( dep ) <nl> # Only operate on local , existing , non - URI formatted paths which are installable <nl> if is_installable_file ( dep ) : <nl> dep_path = Path ( dep ) <nl> def prepare_pip_source_args ( sources , pip_args = None ) : <nl> <nl> <nl> def actually_resolve_reps ( deps , index_lookup , markers_lookup , project , sources , verbose , clear , pre ) : <nl> - import pip <nl> + from pipenv . patched . pip import basecommand , req <nl> + from pipenv . patched . pip . _vendor import requests as pip_requests <nl> <nl> - class PipCommand ( pip . basecommand . Command ) : <nl> + class PipCommand ( basecommand . Command ) : <nl> \" \" \" Needed for pip - tools . \" \" \" <nl> name = ' PipCommand ' <nl> <nl> class PipCommand ( pip . basecommand . Command ) : <nl> for dep in deps : <nl> if dep : <nl> if dep . startswith ( ' - e ' ) : <nl> - constraint = pip . req . InstallRequirement . from_editable ( dep [ len ( ' - e ' ) : ] ) <nl> + constraint = req . InstallRequirement . from_editable ( dep [ len ( ' - e ' ) : ] ) <nl> else : <nl> fd , t = tempfile . mkstemp ( prefix = ' pipenv - ' , suffix = ' - requirement . txt ' , dir = req_dir ) <nl> with os . fdopen ( fd , ' w ' ) as f : <nl> f . write ( dep ) <nl> <nl> - constraint = [ c for c in pip . req . parse_requirements ( t , session = pip . _vendor . requests ) ] [ 0 ] <nl> + constraint = [ c for c in req . parse_requirements ( t , session = pip_requests ) ] [ 0 ] <nl> <nl> # extra_constraints = [ ] <nl> <nl> def is_vcs ( pipfile_entry ) : <nl> <nl> def is_installable_file ( path ) : <nl> \" \" \" Determine if a path can potentially be installed \" \" \" <nl> - import pip <nl> + from pip . utils import is_installable_dir <nl> + from pip . utils . packaging import specifiers <nl> if hasattr ( path , ' keys ' ) and any ( key for key in path . keys ( ) if key in [ ' file ' , ' path ' ] ) : <nl> path = urlparse ( path [ ' file ' ] ) . path if ' file ' in path else path [ ' path ' ] <nl> if not isinstance ( path , six . string_types ) or path = = ' * ' : <nl> def is_installable_file ( path ) : <nl> # specifier set before making a path object ( to avoid breaking windows ) <nl> if any ( path . startswith ( spec ) for spec in ' ! = < > ~ ' ) : <nl> try : <nl> - pip . utils . packaging . specifiers . SpecifierSet ( path ) <nl> + specifiers . SpecifierSet ( path ) <nl> # If this is not a valid specifier , just move on and try it as a path <nl> - except pip . utils . packaging . specifiers . InvalidSpecifier : <nl> + except specifiers . InvalidSpecifier : <nl> pass <nl> else : <nl> return False <nl> def is_installable_file ( path ) : <nl> return False <nl> lookup_path = Path ( path ) <nl> absolute_path = ' { 0 } ' . format ( lookup_path . absolute ( ) ) <nl> - if lookup_path . is_dir ( ) and pip . utils . is_installable_dir ( absolute_path ) : <nl> + if lookup_path . is_dir ( ) and is_installable_dir ( absolute_path ) : <nl> return True <nl> elif lookup_path . is_file ( ) and is_archive_file ( absolute_path ) : <nl> return True <nl> def is_file ( package ) : <nl> <nl> def pep440_version ( version ) : <nl> \" \" \" Normalize version to PEP 440 standards \" \" \" <nl> - import pip <nl> + from pip . index import parse_version <nl> <nl> # Use pip built - in version parser . <nl> - return str ( pip . index . parse_version ( version ) ) <nl> + return str ( parse_version ( version ) ) <nl> <nl> <nl> def pep423_name ( name ) : <nl>\n", "msg": "Update pip imports to always come from patched dir\n"}
{"diff_id": 6478, "repo": "python-telegram-bot/python-telegram-bot\n", "sha": "f65b6911ea61838e8ced486417453cb9bc26687b\n", "time": "2016-06-24T16:13:40Z\n", "diff": "mmm a / telegram / ext / jobqueue . py <nl> ppp b / telegram / ext / jobqueue . py <nl> class JobQueue ( object ) : <nl> def __init__ ( self , bot ) : <nl> self . queue = PriorityQueue ( ) <nl> self . bot = bot <nl> - self . logger = logging . getLogger ( __name__ ) <nl> + self . logger = logging . getLogger ( self . __class__ . __name__ ) <nl> self . __lock = Lock ( ) <nl> self . __tick = Event ( ) <nl> self . _next_peek = None <nl>\n", "msg": "JobQueue : use class name for the logger name\n"}
{"diff_id": 6533, "repo": "numpy/numpy\n", "sha": "cf3eb93b05da716e16b87c30d7b5e08c22115330\n", "time": "2006-10-11T23:52:53Z\n", "diff": "mmm a / numpy / core / numeric . py <nl> ppp b / numpy / core / numeric . py <nl> <nl> ' load ' , ' loads ' , ' isscalar ' , ' binary_repr ' , ' base_repr ' , <nl> ' ones ' , ' identity ' , ' allclose ' , ' compare_chararrays ' , ' putmask ' , <nl> ' seterr ' , ' geterr ' , ' setbufsize ' , ' getbufsize ' , <nl> - ' seterrcall ' , ' geterrcall ' , ' flatnonzero ' , <nl> + ' seterrcall ' , ' geterrcall ' , ' errstate ' , ' flatnonzero ' , <nl> ' Inf ' , ' inf ' , ' infty ' , ' Infinity ' , <nl> ' nan ' , ' NaN ' , ' False_ ' , ' True_ ' , ' bitwise_not ' , <nl> ' CLIP ' , ' RAISE ' , ' WRAP ' , ' MAXDIMS ' , ' BUFSIZE ' , ' ALLOW_THREADS ' ] <nl> def geterrcall ( ) : <nl> \" \" \" <nl> return umath . geterrobj ( ) [ 2 ] <nl> <nl> + class errstate ( object ) : <nl> + def __init__ ( self , * * kwargs ) : <nl> + self . kwargs = kwargs <nl> + def __enter__ ( self ) : <nl> + self . oldstate = seterr ( * * self . kwargs ) <nl> + def __exit__ ( self , * exc_info ) : <nl> + numpy . seterr ( * * self . oldstate ) <nl> + <nl> def _setdef ( ) : <nl> defval = [ UFUNC_BUFSIZE_DEFAULT , ERR_DEFAULT , None ] <nl> umath . seterrobj ( defval ) <nl>\n", "msg": "Add errstate object to be created in new ' with ' statement\n"}
{"diff_id": 6646, "repo": "python/cpython\n", "sha": "7bc82bb1f0ce5c841382587839c218b6cc63e1ff\n", "time": "2000-08-10T14:02:23Z\n", "diff": "mmm a / Lib / smtplib . py <nl> ppp b / Lib / smtplib . py <nl> def __init__ ( self , recipients ) : <nl> self . args = ( recipients , ) <nl> <nl> <nl> - <nl> class SMTPDataError ( SMTPResponseException ) : <nl> \" \" \" The SMTP server didn ' t accept the data . \" \" \" <nl> <nl> class SMTPConnectError ( SMTPResponseException ) : <nl> class SMTPHeloError ( SMTPResponseException ) : <nl> \" \" \" The server refused our HELO reply . \" \" \" <nl> <nl> + <nl> def quoteaddr ( addr ) : <nl> \" \" \" Quote a subset of the email addresses defined by RFC 821 . <nl> <nl> def quotedata ( data ) : <nl> return re . sub ( r ' ( ? m ) ^ \\ . ' , ' . . ' , <nl> re . sub ( r ' ( ? : \\ r \\ n | \\ n | \\ r ( ? ! \\ n ) ) ' , CRLF , data ) ) <nl> <nl> + def _get_fqdn_hostname ( name ) : <nl> + name = string . strip ( name ) <nl> + if len ( name ) = = 0 : <nl> + name = socket . gethostname ( ) <nl> + try : <nl> + hostname , aliases , ipaddrs = socket . gethostbyaddr ( name ) <nl> + except socket . error : <nl> + pass <nl> + else : <nl> + aliases . insert ( 0 , hostname ) <nl> + for name in aliases : <nl> + if ' . ' in name : <nl> + break <nl> + else : <nl> + name = hostname <nl> + return name <nl> + <nl> + <nl> class SMTP : <nl> \" \" \" This class manages a connection to an SMTP or ESMTP server . <nl> SMTP Objects : <nl> def helo ( self , name = ' ' ) : <nl> Hostname to send for this command defaults to the FQDN of the local <nl> host . <nl> \" \" \" <nl> - name = string . strip ( name ) <nl> - if len ( name ) = = 0 : <nl> - name = socket . gethostname ( ) <nl> - try : <nl> - name = socket . gethostbyaddr ( name ) [ 0 ] <nl> - except socket . error : <nl> - pass <nl> - self . putcmd ( \" helo \" , name ) <nl> + self . putcmd ( \" helo \" , _get_fqdn_hostname ( name ) ) <nl> ( code , msg ) = self . getreply ( ) <nl> self . helo_resp = msg <nl> return ( code , msg ) <nl> def ehlo ( self , name = ' ' ) : <nl> Hostname to send for this command defaults to the FQDN of the local <nl> host . <nl> \" \" \" <nl> - name = string . strip ( name ) <nl> - if len ( name ) = = 0 : <nl> - name = socket . gethostname ( ) <nl> - try : <nl> - name = socket . gethostbyaddr ( name ) [ 0 ] <nl> - except socket . error : <nl> - pass <nl> - self . putcmd ( \" ehlo \" , name ) <nl> + self . putcmd ( \" ehlo \" , _get_fqdn_hostname ( name ) ) <nl> ( code , msg ) = self . getreply ( ) <nl> # According to RFC1869 some ( badly written ) <nl> # MTA ' s will disconnect on an ehlo . Toss an exception if <nl>\n", "msg": "add better algorithm to get fully qualified domain name for localhost\n"}
{"diff_id": 6692, "repo": "3b1b/manim\n", "sha": "865f5c33bf59296d5051468bd2f0cfca66018916\n", "time": "2017-02-09T19:10:48Z\n", "diff": "new file mode 100644 <nl> index 000000000 . . 3bffd8d9f <nl> mmm / dev / null <nl> ppp b / scene / reconfigurable_scene . py <nl> <nl> + import numpy as np <nl> + <nl> + from scene import Scene <nl> + from animation . transform import Transform <nl> + <nl> + from helpers import * <nl> + <nl> + class ReconfigurableScene ( Scene ) : <nl> + CONFIG = { <nl> + \" allow_recursion \" : True , <nl> + } <nl> + def setup ( self ) : <nl> + self . states = [ ] <nl> + self . num_recursions = 0 <nl> + <nl> + def show_alt_config ( self , return_to_original_configuration = True , * * new_config ) : <nl> + original_state = self . get_mobjects ( ) <nl> + state_copy = [ m . copy ( ) for m in original_state ] <nl> + self . states . append ( state_copy ) <nl> + if not self . allow_recursion : <nl> + return <nl> + alt_scene = self . __class__ ( <nl> + skip_animations = True , <nl> + allow_recursion = False , <nl> + * * new_config <nl> + ) <nl> + alt_state = alt_scene . states [ len ( self . states ) - 1 ] <nl> + <nl> + if return_to_original_configuration : <nl> + self . clear ( ) <nl> + self . transition_between_states ( state_copy , alt_state ) <nl> + self . transition_between_states ( state_copy , original_state ) <nl> + self . clear ( ) <nl> + self . add ( * original_state ) <nl> + else : <nl> + self . transition_between_states ( original_state , alt_state ) <nl> + self . __dict__ . update ( new_config ) <nl> + <nl> + def transition_between_states ( self , start_state , target_state ) : <nl> + self . play ( * [ <nl> + Transform ( * pair ) <nl> + for pair in zip ( start_state , target_state ) <nl> + ] ) <nl> + self . dither ( ) <nl> + <nl> + <nl> + <nl>\n", "msg": "Added ReconfigurableScene . Should make for fun times .\n"}
{"diff_id": 6711, "repo": "python/cpython\n", "sha": "cdcc0f0c3c0daac2cd263c36d9b11564e531bbb3\n", "time": "1999-02-15T00:04:05Z\n", "diff": "mmm a / Mac / Lib / EasyDialogs . py <nl> ppp b / Mac / Lib / EasyDialogs . py <nl> <nl> <nl> Message ( msg ) - - display a message and an OK button . <nl> AskString ( prompt , default ) - - ask for a string , display OK and Cancel buttons . <nl> + AskPassword ( prompt , default ) - - like AskString ( ) , but shows text as bullets . <nl> AskYesNoCancel ( question , default ) - - display a question and Yes , No and Cancel buttons . <nl> bar = Progress ( label , maxvalue ) - - Display a progress bar <nl> bar . set ( value ) - - Set value <nl> def AskPassword ( prompt , default = ' ' , id = 257 ) : <nl> string = default <nl> oldschedparams = MacOS . SchedParams ( 0 , 0 ) <nl> while 1 : <nl> - ready , ev = Evt . WaitNextEvent ( - 1 , 6 ) <nl> + ready , ev = Evt . WaitNextEvent ( Events . everyEvent , 6 ) <nl> if not ready : continue <nl> what , msg , when , where , mod = ev <nl> if what = = 0 : Dlg . DialogSelect ( ev ) # for blinking caret <nl> elif Dlg . IsDialogEvent ( ev ) : <nl> - if what = = Events . keyDown : <nl> + if what in ( Events . keyDown , Events . autoKey ) : <nl> charcode = msg & Events . charCodeMask <nl> if ( mod & Events . cmdKey ) : <nl> MacOS . SysBeep ( ) <nl>\n", "msg": "AskPassword ( ) : added reference to the module doc string ; added support for autoKey events . - - jvr\n"}
{"diff_id": 6716, "repo": "quantopian/zipline\n", "sha": "7a6d2fb66e04d13e00517e50285d748e93169ebe\n", "time": "2016-03-11T20:29:44Z\n", "diff": "mmm a / zipline / pipeline / factors / factor . py <nl> ppp b / zipline / pipeline / factors / factor . py <nl> def if_not_float64_tell_caller_to_use_isnull ( f ) : <nl> directing the user to ` isnull ` or ` notnull ` instead . <nl> \" \" \" <nl> @ wraps ( f ) <nl> - def wrapped_method ( self , * args , * * kwargs ) : <nl> + def wrapped_method ( self ) : <nl> if self . dtype ! = float64_dtype : <nl> raise TypeError ( <nl> \" { meth } ( ) was called on a factor of dtype { dtype } . \\ n \" <nl> def wrapped_method ( self , * args , * * kwargs ) : <nl> dtype = self . dtype , <nl> ) , <nl> ) <nl> - return f ( self , * args , * * kwargs ) <nl> + return f ( self ) <nl> return wrapped_method <nl> <nl> <nl>\n", "msg": "MAINT : Use more specific signature in decorator .\n"}
{"diff_id": 6897, "repo": "python/cpython\n", "sha": "d9a8dec1356e19798492c2abb945c22f00299f33\n", "time": "2000-09-22T18:41:50Z\n", "diff": "mmm a / Lib / mailbox . py <nl> ppp b / Lib / mailbox . py <nl> class Maildir : <nl> def __init__ ( self , dirname ) : <nl> import string <nl> self . dirname = dirname <nl> - self . boxes = [ ] <nl> <nl> # check for new mail <nl> newdir = os . path . join ( self . dirname , ' new ' ) <nl> - for file in os . listdir ( newdir ) : <nl> - if len ( string . split ( file , ' . ' ) ) > 2 : <nl> - self . boxes . append ( os . path . join ( newdir , file ) ) <nl> + boxes = [ os . path . join ( newdir , f ) <nl> + for f in os . listdir ( newdir ) if f [ 0 ] ! = ' . ' ] <nl> <nl> # Now check for current mail in this maildir <nl> curdir = os . path . join ( self . dirname , ' cur ' ) <nl> - for file in os . listdir ( curdir ) : <nl> - if len ( string . split ( file , ' . ' ) ) > 2 : <nl> - self . boxes . append ( os . path . join ( curdir , file ) ) <nl> + boxes + = [ os . path . join ( curdir , f ) <nl> + for f in os . listdir ( curdir ) if f [ 0 ] ! = ' . ' ] <nl> <nl> def next ( self ) : <nl> if not self . boxes : <nl>\n", "msg": "Maildir . __init__ ( ) : Use the correct filter for filenames , so that this\n"}
{"diff_id": 6927, "repo": "ansible/ansible\n", "sha": "54232c3b2d4b377739258df7ff8e6f6c851a5802\n", "time": "2016-12-08T16:24:56Z\n", "diff": "mmm a / lib / ansible / modules / network / openswitch / ops_config . py <nl> ppp b / lib / ansible / modules / network / openswitch / ops_config . py <nl> <nl> mmm <nl> module : ops_config <nl> version_added : \" 2 . 1 \" <nl> - author : \" Peter sprygada ( @ privateip ) \" <nl> + author : \" Peter Sprygada ( @ privateip ) \" <nl> short_description : Manage OpenSwitch configuration using CLI <nl> description : <nl> - OpenSwitch configurations use a simple block indent file syntax <nl> <nl> in the device running - config . Be sure to note the configuration <nl> command syntax as some commands are automatically modified by the <nl> device config parser . <nl> - required : true <nl> + required : false <nl> + default : null <nl> parents : <nl> description : <nl> - The ordered set of parents that uniquely identify the section <nl> <nl> level or global commands . <nl> required : false <nl> default : null <nl> + src : <nl> + description : <nl> + - The I ( src ) argument provides a path to the configuration file <nl> + to load into the remote system . The path can either be a full <nl> + system path to the configuration file if the value starts with / <nl> + or relative to the root of the implemented role or playbook . <nl> + This arugment is mutually exclusive with the I ( lines ) and <nl> + I ( parents ) arguments . <nl> + required : false <nl> + default : null <nl> + version_added : \" 2 . 2 \" <nl> before : <nl> description : <nl> - The ordered set of commands to push on to the command stack if <nl> <nl> must be an equal match . <nl> required : false <nl> default : line <nl> - choices : [ ' line ' , ' strict ' , ' exact ' ] <nl> + choices : [ ' line ' , ' strict ' , ' exact ' , ' none ' ] <nl> replace : <nl> description : <nl> - Instructs the module on the way to perform the configuration <nl> <nl> current devices running - config . When set to true , this will <nl> cause the module to push the contents of I ( src ) into the device <nl> without first checking if already configured . <nl> + - Note this argument should be considered deprecated . To achieve <nl> + the equivalent , set the match argument to none . This argument <nl> + will be removed in a future release . <nl> required : false <nl> default : false <nl> - choices : [ ' true ' , ' false ' ] <nl> + choices : [ ' yes ' , ' no ' ] <nl> + update : <nl> + description : <nl> + - The I ( update ) argument controls how the configuration statements <nl> + are processed on the remote device . Valid choices for the I ( update ) <nl> + argument are I ( merge ) I ( replace ) and I ( check ) . When the argument is <nl> + set to I ( merge ) , the configuration changes are merged with the current <nl> + device running configuration . When the argument is set to I ( check ) <nl> + the configuration updates are determined but not actually configured <nl> + on the remote device . <nl> + required : false <nl> + default : merge <nl> + choices : [ ' merge ' , ' check ' ] <nl> + version_added : \" 2 . 2 \" <nl> config : <nl> description : <nl> - The module , by default , will connect to the remote device and <nl> <nl> config for comparison . <nl> required : false <nl> default : null <nl> + save : <nl> + description : <nl> + - The C ( save ) argument instructs the module to save the running - <nl> + config to the startup - config at the conclusion of the module <nl> + running . If check mode is specified , this argument is ignored . <nl> + required : false <nl> + default : no <nl> + choices : [ ' yes ' , ' no ' ] <nl> + version_added : \" 2 . 2 \" <nl> + state : <nl> + description : <nl> + - This argument specifies whether or not the running - config is <nl> + present on the remote device . When set to I ( absent ) the <nl> + running - config on the remote device is erased . <nl> + required : false <nl> + default : no <nl> + choices : [ ' yes ' , ' no ' ] <nl> + version_added : \" 2 . 2 \" <nl> \" \" \" <nl> <nl> EXAMPLES = \" \" \" <nl> + # Note : examples below use the following provider dict to handle <nl> + # transport and authentication to the node . <nl> + vars : <nl> + cli : <nl> + host : \" { { inventory_hostname } } \" <nl> + username : netop <nl> + password : netop <nl> + <nl> - name : configure hostname over cli <nl> ops_config : <nl> - lines : <nl> - - \" hostname { { inventory_hostname } } \" <nl> + lines : <nl> + - \" hostname { { inventory_hostname } } \" <nl> + provider : \" { { cli } } \" <nl> + <nl> <nl> - name : configure vlan 10 over cli <nl> ops_config : <nl> - lines : <nl> - - no shutdown <nl> - parents : <nl> - - vlan 10 <nl> + lines : <nl> + - no shutdown <nl> + parents : <nl> + - vlan 10 <nl> + provider : \" { { cli } } \" <nl> + <nl> + - name : load config from file <nl> + ops_config : <nl> + src : ops01 . cfg <nl> + backup : yes <nl> + provider : \" { { cli } } \" <nl> \" \" \" <nl> <nl> RETURN = \" \" \" <nl> <nl> returned : always <nl> type : list <nl> sample : [ ' . . . ' , ' . . . ' ] <nl> - <nl> + backup_path : <nl> + description : The full path to the backup file <nl> + returned : when backup is yes <nl> + type : path <nl> + sample : / playbooks / ansible / backup / ios_config . 2016 - 07 - 16 @ 22 : 28 : 34 <nl> responses : <nl> description : The set of responses from issuing the commands on the device <nl> - retured : when not check_mode <nl> + returned : when not check_mode <nl> type : list <nl> sample : [ ' . . . ' , ' . . . ' ] <nl> \" \" \" <nl> import re <nl> - import itertools <nl> - <nl> - def get_config ( module ) : <nl> - config = module . params [ ' config ' ] or dict ( ) <nl> - if not config and not module . params [ ' force ' ] : <nl> - config = module . config <nl> - return config <nl> - <nl> - <nl> - def build_candidate ( lines , parents , config , strategy ) : <nl> - candidate = list ( ) <nl> - <nl> - if strategy = = ' strict ' : <nl> - for index , cmd in enumerate ( lines ) : <nl> - try : <nl> - if cmd ! = config [ index ] : <nl> - candidate . append ( cmd ) <nl> - except IndexError : <nl> - candidate . append ( cmd ) <nl> - <nl> - elif strategy = = ' exact ' : <nl> - if len ( lines ) ! = len ( config ) : <nl> - candidate = list ( lines ) <nl> - else : <nl> - for cmd , cfg in itertools . izip ( lines , config ) : <nl> - if cmd ! = cfg : <nl> - candidate = list ( lines ) <nl> - break <nl> <nl> - else : <nl> - for cmd in lines : <nl> - if cmd not in config : <nl> - candidate . append ( cmd ) <nl> + from ansible . module_utils . basic import get_exception <nl> + from ansible . module_utils . openswitch import NetworkModule , NetworkError <nl> + from ansible . module_utils . netcfg import NetworkConfig , dumps <nl> + from ansible . module_utils . netcli import Command <nl> <nl> + def invoke ( name , * args , * * kwargs ) : <nl> + func = globals ( ) . get ( name ) <nl> + if func : <nl> + return func ( * args , * * kwargs ) <nl> + <nl> + def check_args ( module , warnings ) : <nl> + if module . params [ ' parents ' ] : <nl> + if not module . params [ ' lines ' ] or module . params [ ' src ' ] : <nl> + warnings . append ( ' ignoring unnecessary argument parents ' ) <nl> + if module . params [ ' force ' ] : <nl> + warnings . append ( ' The force argument is deprecated , please use ' <nl> + ' match = none instead . This argument will be ' <nl> + ' removed in the future ' ) <nl> + <nl> + def get_config ( module , result ) : <nl> + contents = module . params [ ' config ' ] <nl> + if not contents : <nl> + contents = module . config . get_config ( ) <nl> + return NetworkConfig ( indent = 4 , contents = contents ) <nl> + <nl> + def get_candidate ( module ) : <nl> + candidate = NetworkConfig ( indent = 4 ) <nl> + if module . params [ ' src ' ] : <nl> + candidate . load ( module . params [ ' src ' ] ) <nl> + elif module . params [ ' lines ' ] : <nl> + parents = module . params [ ' parents ' ] or list ( ) <nl> + candidate . add ( module . params [ ' lines ' ] , parents = parents ) <nl> return candidate <nl> <nl> + def load_backup ( module ) : <nl> + try : <nl> + module . cli ( [ ' exit ' , ' config replace flash : / ansible - rollback force ' ] ) <nl> + except NetworkError : <nl> + module . fail_json ( msg = ' unable to rollback configuration ' ) <nl> + <nl> + def backup_config ( module ) : <nl> + cmd = ' copy running - config flash : / ansible - rollback ' <nl> + cmd = Command ( cmd , prompt = re . compile ( ' \\ ? $ ' ) , response = ' \\ n ' ) <nl> + module . cli ( cmd ) <nl> + <nl> + def load_config ( module , commands , result ) : <nl> + if not module . check_mode and module . params [ ' update ' ] ! = ' check ' : <nl> + module . config ( commands ) <nl> + result [ ' changed ' ] = module . params [ ' update ' ] ! = ' check ' <nl> + result [ ' updates ' ] = commands . split ( ' \\ n ' ) <nl> + <nl> + def present ( module , result ) : <nl> + match = module . params [ ' match ' ] <nl> + replace = module . params [ ' replace ' ] <nl> + <nl> + candidate = get_candidate ( module ) <nl> + <nl> + if match ! = ' none ' : <nl> + config = get_config ( module , result ) <nl> + configobjs = candidate . difference ( config , match = match , replace = replace ) <nl> + else : <nl> + config = None <nl> + configobjs = candidate . items <nl> + <nl> + if configobjs : <nl> + commands = dumps ( configobjs , ' commands ' ) <nl> + <nl> + if module . params [ ' before ' ] : <nl> + commands [ : 0 ] = module . params [ ' before ' ] <nl> + <nl> + if module . params [ ' after ' ] : <nl> + commands . extend ( module . params [ ' after ' ] ) <nl> + <nl> + # send the configuration commands to the device and merge <nl> + # them with the current running config <nl> + load_config ( module , commands , result ) <nl> + <nl> + if module . params [ ' save ' ] and not module . check_mode : <nl> + module . config . save_config ( ) <nl> + <nl> + def absent ( module , result ) : <nl> + if not module . check_mode : <nl> + module . cli ( ' erase startup - config ' ) <nl> + result [ ' changed ' ] = True <nl> <nl> def main ( ) : <nl> <nl> argument_spec = dict ( <nl> - lines = dict ( aliases = [ ' commands ' ] , required = True , type = ' list ' ) , <nl> + lines = dict ( aliases = [ ' commands ' ] , type = ' list ' ) , <nl> parents = dict ( type = ' list ' ) , <nl> + <nl> + src = dict ( type = ' path ' ) , <nl> + <nl> before = dict ( type = ' list ' ) , <nl> after = dict ( type = ' list ' ) , <nl> - match = dict ( default = ' line ' , choices = [ ' line ' , ' strict ' , ' exact ' ] ) , <nl> + <nl> + match = dict ( default = ' line ' , choices = [ ' line ' , ' strict ' , ' exact ' , ' none ' ] ) , <nl> replace = dict ( default = ' line ' , choices = [ ' line ' , ' block ' ] ) , <nl> + <nl> + # this argument is deprecated in favor of setting match : none <nl> + # it will be removed in a future version <nl> force = dict ( default = False , type = ' bool ' ) , <nl> - config = dict ( ) , <nl> - transport = dict ( default = ' cli ' , choices = [ ' cli ' ] ) <nl> - ) <nl> <nl> - module = get_module ( argument_spec = argument_spec , <nl> - supports_check_mode = True ) <nl> + update = dict ( choices = [ ' merge ' , ' check ' ] , default = ' merge ' ) , <nl> + backup = dict ( type = ' bool ' , default = False ) , <nl> <nl> - lines = module . params [ ' lines ' ] <nl> - parents = module . params [ ' parents ' ] or list ( ) <nl> + config = dict ( ) , <nl> + default = dict ( type = ' bool ' , default = False ) , <nl> <nl> - before = module . params [ ' before ' ] <nl> - after = module . params [ ' after ' ] <nl> + save = dict ( type = ' bool ' , default = False ) , <nl> <nl> - match = module . params [ ' match ' ] <nl> - replace = module . params [ ' replace ' ] <nl> + state = dict ( choices = [ ' present ' , ' absent ' ] , default = ' present ' ) , <nl> <nl> - contents = get_config ( module ) <nl> - config = module . parse_config ( contents ) <nl> + # ops_config is only supported over Cli transport so force <nl> + # the value of transport to be cli <nl> + transport = dict ( default = ' cli ' , choices = [ ' cli ' ] ) <nl> + ) <nl> <nl> - if parents : <nl> - for parent in parents : <nl> - for item in config : <nl> - if item . text = = parent : <nl> - config = item <nl> + mutually_exclusive = [ ( ' lines ' , ' src ' ) ] <nl> <nl> - try : <nl> - children = [ c . text for c in config . children ] <nl> - except AttributeError : <nl> - children = [ c . text for c in config ] <nl> + module = NetworkModule ( argument_spec = argument_spec , <nl> + connect_on_load = False , <nl> + mutually_exclusive = mutually_exclusive , <nl> + supports_check_mode = True ) <nl> <nl> - else : <nl> - children = [ c . text for c in config if not c . parents ] <nl> + state = module . params [ ' state ' ] <nl> <nl> - result = dict ( changed = False ) <nl> + if module . params [ ' force ' ] is True : <nl> + module . params [ ' match ' ] = ' none ' <nl> <nl> - candidate = build_candidate ( lines , parents , children , match ) <nl> + warnings = list ( ) <nl> + check_args ( module , warnings ) <nl> <nl> - if candidate : <nl> - if replace = = ' line ' : <nl> - candidate [ : 0 ] = parents <nl> - else : <nl> - candidate = list ( parents ) <nl> - candidate . extend ( lines ) <nl> + result = dict ( changed = False , warnings = warnings ) <nl> <nl> - if before : <nl> - candidate [ : 0 ] = before <nl> + if module . params [ ' backup ' ] : <nl> + result [ ' __backup__ ' ] = module . config . get_config ( ) <nl> <nl> - if after : <nl> - candidate . extend ( after ) <nl> + try : <nl> + invoke ( state , module , result ) <nl> + except NetworkError : <nl> + exc = get_exception ( ) <nl> + module . fail_json ( msg = str ( exc ) ) <nl> <nl> - if not module . check_mode : <nl> - response = module . configure ( candidate ) <nl> - result [ ' responses ' ] = response <nl> - result [ ' changed ' ] = True <nl> + module . exit_json ( * * result ) <nl> <nl> - result [ ' updates ' ] = candidate <nl> - return module . exit_json ( * * result ) <nl> <nl> - from ansible . module_utils . basic import * <nl> - from ansible . module_utils . shell import * <nl> - from ansible . module_utils . netcfg import * <nl> - from ansible . module_utils . openswitch import * <nl> if __name__ = = ' __main__ ' : <nl> main ( ) <nl> + <nl>\n", "msg": "update ops_config module with new enhancements\n"}
{"diff_id": 7021, "repo": "matplotlib/matplotlib\n", "sha": "c23ef3bd08e56b59acadd8ab799e583c9d41cd60\n", "time": "2018-01-26T22:05:29Z\n", "diff": "mmm a / lib / matplotlib / axes / _base . py <nl> ppp b / lib / matplotlib / axes / _base . py <nl> def ticklabel_format ( self , * * kwargs ) : <nl> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = <nl> * style * [ ' sci ' ( or ' scientific ' ) | ' plain ' ] <nl> plain turns off scientific notation <nl> - * scilimits * ( m , n ) , pair of integers ; if * style * <nl> + * scilimits * [ ( m , n ) | m ] , a pair of integers or <nl> + a single integer ; if * style * <nl> is ' sci ' , scientific notation will <nl> be used for numbers outside the range <nl> 10 ` m ` : sup : to 10 ` n ` : sup : . <nl> Use ( 0 , 0 ) to include all numbers . <nl> - Use ( m , m ) to fix scaling to 10 ` m ` : sup : . <nl> + Use a single integer m , or equivalently <nl> + ( m , m ) to fix the scale to 10 ` m ` : sup : . <nl> * useOffset * [ bool | offset ] ; if True , <nl> the offset will be calculated as needed ; <nl> if False , no offset will be used ; if a <nl> def ticklabel_format ( self , * * kwargs ) : <nl> useMathText = kwargs . pop ( ' useMathText ' , None ) <nl> axis = kwargs . pop ( ' axis ' , ' both ' ) . lower ( ) <nl> if scilimits is not None : <nl> - try : <nl> - m , n = scilimits <nl> - m + n + 1 # check that both are numbers <nl> - except ( ValueError , TypeError ) : <nl> - raise ValueError ( \" scilimits must be a sequence of 2 integers \" ) <nl> + if type ( scilimits ) = = int : <nl> + m = n = scilimits <nl> + else : <nl> + try : <nl> + m , n = scilimits <nl> + m + n + 1 # check that both are numbers <nl> + except ( ValueError , TypeError ) : <nl> + raise ValueError ( <nl> + \" scilimits must be a single integer or \" + <nl> + \" a sequence of 2 integers \" <nl> + ) <nl> if style [ : 3 ] = = ' sci ' : <nl> sb = True <nl> elif style = = ' plain ' : <nl>\n", "msg": "Take scilimits = single integer for fixed - scaling\n"}
{"diff_id": 7139, "repo": "open-mmlab/mmdetection\n", "sha": "7ac6ebee18012fa1380d01be58fcaba791bbd911\n", "time": "2020-07-26T14:00:28Z\n", "diff": "mmm a / mmdet / models / dense_heads / gfl_head . py <nl> ppp b / mmdet / models / dense_heads / gfl_head . py <nl> def forward ( self , x ) : <nl> offsets from the box center in four directions , shape ( N , 4 ) . <nl> \" \" \" <nl> x = F . softmax ( x . reshape ( - 1 , self . reg_max + 1 ) , dim = 1 ) <nl> - x = F . linear ( x , self . project ) . reshape ( - 1 , 4 ) <nl> + x = F . linear ( x , self . project . type_as ( x ) ) . reshape ( - 1 , 4 ) <nl> return x <nl> <nl> <nl>\n", "msg": "Update to support fp16 training of GFL ( )\n"}
{"diff_id": 7172, "repo": "zulip/zulip\n", "sha": "10d3d94a2aeb702bb76ecfd4bd15f51ce0b444e8\n", "time": "2018-11-29T06:29:29Z\n", "diff": "mmm a / corporate / tests / test_stripe . py <nl> ppp b / corporate / tests / test_stripe . py <nl> class StripeTest ( ZulipTestCase ) : <nl> def setUp ( self , mock3 : Mock , mock2 : Mock , mock1 : Mock ) - > None : <nl> call_command ( \" setup_stripe \" ) <nl> <nl> - self . token = ' token ' <nl> self . quantity = 8 <nl> self . signed_seat_count , self . salt = sign_string ( str ( self . quantity ) ) <nl> <nl> def get_salt_from_response ( self , response : HttpResponse ) - > Optional [ str ] : <nl> match = re . search ( r ' name = \\ \" salt \\ \" value = \\ \" ( \\ w + ) \\ \" ' , response . content . decode ( \" utf - 8 \" ) ) <nl> return match . group ( 1 ) if match else None <nl> <nl> + def upgrade ( self , invoice : bool = False , talk_to_stripe : bool = True , <nl> + realm : Optional [ Realm ] = None , * * kwargs : Any ) - > HttpResponse : <nl> + host_args = { } <nl> + if realm is not None : <nl> + host_args [ ' HTTP_HOST ' ] = realm . host <nl> + response = self . client_get ( \" / upgrade / \" , * * host_args ) <nl> + params = { <nl> + ' signed_seat_count ' : self . get_signed_seat_count_from_response ( response ) , <nl> + ' salt ' : self . get_salt_from_response ( response ) , <nl> + ' plan ' : Plan . CLOUD_ANNUAL } # type : Dict [ str , Any ] <nl> + if invoice : # send_invoice <nl> + params . update ( { <nl> + ' invoiced_seat_count ' : 123 , <nl> + ' billing_modality ' : ' send_invoice ' } ) <nl> + else : # charge_automatically <nl> + stripe_token = None <nl> + if not talk_to_stripe : <nl> + stripe_token = ' token ' <nl> + stripe_token = kwargs . get ( ' stripe_token ' , stripe_token ) <nl> + if stripe_token is None : <nl> + stripe_token = stripe_create_token ( ) . id <nl> + params . update ( { <nl> + ' stripeToken ' : stripe_token , <nl> + ' billing_modality ' : ' charge_automatically ' , <nl> + } ) <nl> + params . update ( kwargs ) <nl> + return self . client_post ( \" / upgrade / \" , params , * * host_args ) <nl> + <nl> @ patch ( \" corporate . lib . stripe . billing_logger . error \" ) <nl> def test_catch_stripe_errors ( self , mock_billing_logger_error : Mock ) - > None : <nl> @ catch_stripe_errors <nl> def test_initial_upgrade ( self , mock5 : Mock , mock4 : Mock , mock3 : Mock , mock2 : Moc <nl> self . assertFalse ( Customer . objects . filter ( realm = user . realm ) . exists ( ) ) <nl> <nl> # Click \" Make payment \" in Stripe Checkout <nl> - self . client_post ( \" / upgrade / \" , { <nl> - ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . get_signed_seat_count_from_response ( response ) , <nl> - ' salt ' : self . get_salt_from_response ( response ) , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> <nl> # Check that we correctly created Customer and Subscription objects in Stripe <nl> stripe_customer = stripe_get_customer ( Customer . objects . get ( realm = user . realm ) . stripe_customer_id ) <nl> def test_billing_page_permissions ( self , mock5 : Mock , mock4 : Mock , mock3 : Mock , <nl> self . assertEqual ( response . status_code , 302 ) <nl> self . assertEqual ( ' / upgrade / ' , response . url ) <nl> # Check that non - admins can sign up and pay <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> # Check that the non - admin hamlet can still access / billing <nl> response = self . client_get ( \" / billing / \" ) <nl> self . assert_in_success_response ( [ \" for billing history or to make changes \" ] , response ) <nl> def test_upgrade_with_outdated_seat_count ( <nl> self . login ( self . example_email ( \" hamlet \" ) ) <nl> new_seat_count = 123 <nl> # Change the seat count while the user is going through the upgrade flow <nl> - response = self . client_get ( \" / upgrade / \" ) <nl> with patch ( ' corporate . lib . stripe . get_seat_count ' , return_value = new_seat_count ) : <nl> - self . client_post ( \" / upgrade / \" , { <nl> - ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . get_signed_seat_count_from_response ( response ) , <nl> - ' salt ' : self . get_salt_from_response ( response ) , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> # Check that the subscription call used the old quantity , not new_seat_count <nl> stripe_customer = stripe_get_customer ( <nl> Customer . objects . get ( realm = get_realm ( ' zulip ' ) ) . stripe_customer_id ) <nl> def test_upgrade_where_subscription_save_fails_at_first ( <nl> self . login ( user . email ) <nl> # From https : / / stripe . com / docs / testing # cards : Attaching this card to <nl> # a Customer object succeeds , but attempts to charge the customer fail . <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ' 4000000000000341 ' ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( stripe_token = stripe_create_token ( ' 4000000000000341 ' ) . id ) <nl> # Check that we created a Customer object with has_billing_relationship False <nl> customer = Customer . objects . get ( realm = get_realm ( ' zulip ' ) ) <nl> self . assertFalse ( customer . has_billing_relationship ) <nl> def test_upgrade_where_subscription_save_fails_at_first ( <nl> self . assertEqual ( ' / upgrade / ' , response . url ) <nl> <nl> # Try again , with a valid card <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> customer = Customer . objects . get ( realm = get_realm ( ' zulip ' ) ) <nl> # Impossible to create two Customers , but check that we didn ' t <nl> # change stripe_customer_id and that we updated has_billing_relationship <nl> def test_upgrade_where_subscription_save_fails_at_first ( <nl> <nl> def test_upgrade_with_tampered_seat_count ( self ) - > None : <nl> self . login ( self . example_email ( \" hamlet \" ) ) <nl> - response = self . client_post ( \" / upgrade / \" , { <nl> - ' stripeToken ' : self . token , <nl> - ' signed_seat_count ' : \" randomsalt \" , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' , <nl> - } ) <nl> + response = self . upgrade ( talk_to_stripe = False , salt = ' badsalt ' ) <nl> self . assert_in_success_response ( [ \" Upgrade to Zulip Standard \" ] , response ) <nl> self . assertEqual ( response [ ' error_description ' ] , ' tampered seat count ' ) <nl> <nl> def test_upgrade_with_tampered_plan ( self ) - > None : <nl> # Test with an unknown plan <nl> self . login ( self . example_email ( \" hamlet \" ) ) <nl> - response = self . client_post ( \" / upgrade / \" , { <nl> - ' stripeToken ' : self . token , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : \" invalid \" , <nl> - ' billing_modality ' : ' charge_automatically ' , <nl> - } ) <nl> + response = self . upgrade ( talk_to_stripe = False , plan = ' badplan ' ) <nl> self . assert_in_success_response ( [ \" Upgrade to Zulip Standard \" ] , response ) <nl> self . assertEqual ( response [ ' error_description ' ] , ' tampered plan ' ) <nl> # Test with a plan that ' s valid , but not if you ' re paying by invoice <nl> - response = self . client_post ( \" / upgrade / \" , { <nl> - ' invoiced_seat_count ' : 123 , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_MONTHLY , <nl> - ' billing_modality ' : ' send_invoice ' , <nl> - } ) <nl> + response = self . upgrade ( invoice = True , talk_to_stripe = False , plan = Plan . CLOUD_MONTHLY ) <nl> self . assert_in_success_response ( [ \" Upgrade to Zulip Standard \" ] , response ) <nl> self . assertEqual ( response [ ' error_description ' ] , ' tampered plan ' ) <nl> <nl> def test_upgrade_with_insufficient_invoiced_seat_count ( self ) - > None : <nl> self . login ( self . example_email ( \" hamlet \" ) ) <nl> # Test invoicing for less than MIN_INVOICED_SEAT_COUNT <nl> - response = self . client_post ( \" / upgrade / \" , { <nl> - ' invoiced_seat_count ' : self . quantity , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' send_invoice ' , <nl> - } ) <nl> + response = self . upgrade ( invoice = True , talk_to_stripe = False , <nl> + invoiced_seat_count = MIN_INVOICED_SEAT_COUNT - 1 ) <nl> self . assert_in_success_response ( [ \" Upgrade to Zulip Standard \" , <nl> \" at least % d users \" % ( MIN_INVOICED_SEAT_COUNT , ) ] , response ) <nl> self . assertEqual ( response [ ' error_description ' ] , ' lowball seat count ' ) <nl> # Test invoicing for less than your user count <nl> with patch ( \" corporate . views . MIN_INVOICED_SEAT_COUNT \" , 3 ) : <nl> - response = self . client_post ( \" / upgrade / \" , { <nl> - ' invoiced_seat_count ' : self . quantity - 1 , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' send_invoice ' , <nl> - } ) <nl> + response = self . upgrade ( invoice = True , talk_to_stripe = False , invoiced_seat_count = 4 ) <nl> self . assert_in_success_response ( [ \" Upgrade to Zulip Standard \" , <nl> \" at least % d users \" % ( self . quantity , ) ] , response ) <nl> self . assertEqual ( response [ ' error_description ' ] , ' lowball seat count ' ) <nl> # Test not setting an invoiced_seat_count <nl> - response = self . client_post ( \" / upgrade / \" , { <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' send_invoice ' , <nl> - } ) <nl> + response = self . upgrade ( invoice = True , talk_to_stripe = False , invoiced_seat_count = None ) <nl> self . assert_in_success_response ( [ \" Upgrade to Zulip Standard \" , <nl> \" at least % d users \" % ( MIN_INVOICED_SEAT_COUNT , ) ] , response ) <nl> self . assertEqual ( response [ ' error_description ' ] , ' lowball seat count ' ) <nl> def test_upgrade_with_insufficient_invoiced_seat_count ( self ) - > None : <nl> def test_upgrade_with_uncaught_exception ( self , mock1 : Mock ) - > None : <nl> self . login ( self . example_email ( \" hamlet \" ) ) <nl> with patch ( \" corporate . views . process_initial_upgrade \" , side_effect = Exception ) : <nl> - response = self . client_post ( \" / upgrade / \" , { <nl> - ' stripeToken ' : self . token , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' , <nl> - } ) <nl> + response = self . upgrade ( talk_to_stripe = False ) <nl> self . assert_in_success_response ( [ \" Upgrade to Zulip Standard \" , <nl> \" Something went wrong . Please contact \" ] , response ) <nl> self . assertEqual ( response [ ' error_description ' ] , ' uncaught exception during upgrade ' ) <nl> def test_upgrade_billing_by_invoice ( self , mock6 : Mock , mock5 : Mock , mock4 : Mock , <nl> mock2 : Mock , mock1 : Mock ) - > None : <nl> user = self . example_user ( \" hamlet \" ) <nl> self . login ( user . email ) <nl> - self . client_post ( \" / upgrade / \" , { <nl> - ' invoiced_seat_count ' : 123 , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' send_invoice ' } ) <nl> + self . upgrade ( invoice = True ) <nl> process_all_billing_log_entries ( ) <nl> <nl> # Check that we correctly created a Customer in Stripe <nl> def test_payment_method_string ( self , mock5 : Mock , mock4 : Mock , mock3 : Mock , mock <nl> user = self . example_user ( \" hamlet \" ) <nl> do_create_customer ( user , stripe_create_token ( ) . id ) <nl> self . login ( user . email ) <nl> - self . client_post ( \" / upgrade / \" , { ' invoiced_seat_count ' : 123 , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' send_invoice ' } ) <nl> + self . upgrade ( invoice = True ) <nl> stripe_customer = stripe_get_customer ( Customer . objects . get ( realm = user . realm ) . stripe_customer_id ) <nl> self . assertEqual ( ' Billed by invoice ' , payment_method_string ( stripe_customer ) ) <nl> <nl> def test_payment_method_string ( self , mock5 : Mock , mock4 : Mock , mock3 : Mock , mock <nl> realm = do_create_realm ( ' realm1 ' , ' realm1 ' ) <nl> user = do_create_user ( ' name @ realm1 . com ' , ' password ' , realm , ' name ' , ' name ' ) <nl> self . login ( user . email , password = ' password ' , realm = realm ) <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } , <nl> - HTTP_HOST = realm . host ) <nl> + self . upgrade ( realm = realm ) <nl> with patch ( ' corporate . lib . stripe . preview_invoice_total_for_downgrade ' , return_value = 1 ) : <nl> process_downgrade ( user ) <nl> stripe_customer = stripe_get_customer ( Customer . objects . get ( realm = user . realm ) . stripe_customer_id ) <nl> def test_payment_method_string ( self , mock5 : Mock , mock4 : Mock , mock3 : Mock , mock <nl> realm = do_create_realm ( ' realm2 ' , ' realm2 ' ) <nl> user = do_create_user ( ' name @ realm2 . com ' , ' password ' , realm , ' name ' , ' name ' ) <nl> self . login ( user . email , password = ' password ' , realm = realm ) <nl> - self . client_post ( \" / upgrade / \" , { ' invoiced_seat_count ' : 123 , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' send_invoice ' } , <nl> - HTTP_HOST = realm . host ) <nl> + self . upgrade ( invoice = True , realm = realm ) <nl> with patch ( ' corporate . lib . stripe . preview_invoice_total_for_downgrade ' , return_value = 1 ) : <nl> process_downgrade ( user ) <nl> stripe_customer = stripe_get_customer ( Customer . objects . get ( realm = user . realm ) . stripe_customer_id ) <nl> def test_attach_discount_to_realm ( self , mock7 : Mock , mock6 : Mock , mock5 : Mock , m <nl> user = self . example_user ( ' hamlet ' ) <nl> attach_discount_to_realm ( user , 85 ) <nl> self . login ( user . email ) <nl> - response = self . client_get ( \" / upgrade / \" ) <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . get_signed_seat_count_from_response ( response ) , <nl> - ' salt ' : self . get_salt_from_response ( response ) , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> stripe_customer = stripe_get_customer ( Customer . objects . get ( realm = user . realm ) . stripe_customer_id ) <nl> assert ( stripe_customer . discount is not None ) # for mypy <nl> self . assertEqual ( stripe_customer . discount . coupon . percent_off , 85 . 0 ) <nl> def test_downgrade ( self , mock9 : Mock , mock8 : Mock , mock7 : Mock , mock6 : Mock , moc <nl> mock4 : Mock , mock3 : Mock , mock2 : Mock , mock1 : Mock ) - > None : <nl> user = self . example_user ( ' iago ' ) <nl> self . login ( user . email ) <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> realm = get_realm ( ' zulip ' ) <nl> self . assertEqual ( realm . has_seat_based_plan , True ) <nl> self . assertEqual ( realm . plan_type , Realm . STANDARD ) <nl> def test_downgrade_with_money_owed ( self , mock10 : Mock , mock9 : Mock , mock8 : Mock , <nl> mock2 : Mock , mock1 : Mock ) - > None : <nl> user = self . example_user ( ' iago ' ) <nl> self . login ( user . email ) <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> stripe_customer = stripe_get_customer ( Customer . objects . get ( realm = user . realm ) . stripe_customer_id ) <nl> self . assertEqual ( stripe_customer . account_balance , 0 ) <nl> stripe_subscription = extract_current_subscription ( stripe_customer ) <nl> def test_replace_payment_source ( self , mock5 : Mock , mock4 : Mock , mock3 : Mock , <nl> mock2 : Mock , mock1 : Mock ) - > None : <nl> user = self . example_user ( \" hamlet \" ) <nl> self . login ( user . email ) <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> # Try replacing with a valid card <nl> stripe_token = stripe_create_token ( card_number = ' 5555555555554444 ' ) . id <nl> response = self . client_post ( \" / json / billing / sources / change \" , <nl> def check_subscription_save ( subscription : stripe . Subscription , idempotency_key : <nl> new_seat_count = 123 <nl> # change the seat count while the user is going through the upgrade flow <nl> with patch ( ' corporate . lib . stripe . get_seat_count ' , return_value = new_seat_count ) : <nl> - self . client_post ( \" / upgrade / \" , { ' stripeToken ' : stripe_create_token ( ) . id , <nl> - ' signed_seat_count ' : self . signed_seat_count , <nl> - ' salt ' : self . salt , <nl> - ' plan ' : Plan . CLOUD_ANNUAL , <nl> - ' billing_modality ' : ' charge_automatically ' } ) <nl> + self . upgrade ( ) <nl> check_billing_processor_update ( RealmAuditLog . STRIPE_PLAN_QUANTITY_RESET , new_seat_count ) <nl> <nl> # Test USER_CREATED <nl>\n", "msg": "billing : Use common pathway for upgrading in tests .\n"}
{"diff_id": 7228, "repo": "zulip/zulip\n", "sha": "a180f01e6b122f7b018ea5cc6b0b3341204a8dfe\n", "time": "2019-12-28T18:47:08Z\n", "diff": "mmm a / zproject / backends . py <nl> ppp b / zproject / backends . py <nl> def authenticate ( self , * , username : str , password : str , realm : Realm , <nl> return None <nl> <nl> try : <nl> - # We want to apss the user ' s LDAP username into <nl> + # We want to pass the user ' s LDAP username into <nl> # authenticate ( ) below . If an email address was entered <nl> # in the login form , we need to use <nl> # django_to_ldap_username to translate the email address <nl> def authenticate ( self , * , username : str , password : str , realm : Realm , <nl> # against the LDAP database , and assuming those are correct , <nl> # end up calling ` self . get_or_build_user ` with the <nl> # authenticated user ' s data from LDAP . <nl> - return ZulipLDAPAuthBackendBase . authenticate ( self , <nl> - request = None , <nl> - username = username , <nl> - password = password ) <nl> + return super ( ) . authenticate ( request = None , username = username , password = password ) <nl> <nl> def get_or_build_user ( self , username : str , ldap_user : _LDAPUser ) - > Tuple [ UserProfile , bool ] : <nl> \" \" \" The main function of our authentication backend extension of <nl>\n", "msg": "ldap : Use a cleaner super ( ) . authenticate ( ) call in ZulipLDAPAuthBackend .\n"}
{"diff_id": 7349, "repo": "python/cpython\n", "sha": "066af1074dfed4f7a6538692433ba04995dd7509\n", "time": "2000-03-22T00:30:54Z\n", "diff": "mmm a / Lib / distutils / command / install . py <nl> ppp b / Lib / distutils / command / install . py <nl> class install ( Command ) : <nl> ( ' install - data = ' , None , <nl> \" installation directory for data files \" ) , <nl> <nl> - # Build directories : where to find the files to install <nl> - ( ' build - base = ' , None , <nl> - \" base build directory \" ) , <nl> - ( ' build - lib = ' , None , <nl> - \" build directory for all Python modules \" ) , <nl> - <nl> # Where to install documentation ( eventually ! ) <nl> # ( ' doc - format = ' , None , \" format of documentation to generate \" ) , <nl> # ( ' install - man = ' , None , \" directory for Unix man pages \" ) , <nl> def initialize_options ( self ) : <nl> self . install_scripts = None <nl> self . install_data = None <nl> <nl> - self . build_base = None <nl> - self . build_lib = None <nl> - self . build_platlib = None <nl> - <nl> self . extra_path = None <nl> self . install_path_file = 0 <nl> <nl> + # These are only here as a conduit from the ' build ' command to the <nl> + # ' install_ * ' commands that do the real work . ( ' build_base ' isn ' t <nl> + # actually used anywhere , but it might be useful in future . ) They <nl> + # are not user options , because if the user told the install <nl> + # command where the build directory is , that wouldn ' t affect the <nl> + # build command . <nl> + self . build_base = None <nl> + self . build_lib = None <nl> + <nl> # self . install_man = None <nl> # self . install_html = None <nl> # self . install_info = None <nl> def finalize_options ( self ) : <nl> self . install_lib = os . path . join ( self . install_lib , self . extra_dirs ) <nl> <nl> # Figure out the build directories , ie . where to install from <nl> - self . set_peer_option ( ' build ' , ' build_base ' , self . build_base ) <nl> self . set_undefined_options ( ' build ' , <nl> ( ' build_base ' , ' build_base ' ) , <nl> - ( ' build_lib ' , ' build_lib ' ) , <nl> - ( ' build_platlib ' , ' build_platlib ' ) ) <nl> + ( ' build_lib ' , ' build_lib ' ) ) <nl> <nl> # Punt on doc directories for now - - after all , we ' re punting on <nl> # documentation completely ! <nl>\n", "msg": "Dropped any notion of allowing the user to specify the build directories :\n"}
{"diff_id": 7483, "repo": "ansible/ansible\n", "sha": "34e9c0f3a5c617fd41d54a0d9abeadbbc34a6090\n", "time": "2019-04-24T20:14:58Z\n", "diff": "mmm a / lib / ansible / modules / network / meraki / meraki_mr_l3_firewall . py <nl> ppp b / lib / ansible / modules / network / meraki / meraki_mr_l3_firewall . py <nl> def assemble_payload ( meraki ) : <nl> <nl> <nl> def get_rules ( meraki , net_id , number ) : <nl> - path = meraki . construct_path ( ' get_all ' , net_id = net_id ) <nl> - path = path + number + ' / l3FirewallRules ' <nl> + path = meraki . construct_path ( ' get_all ' , net_id = net_id , custom = { ' number ' : number } ) <nl> response = meraki . request ( path , method = ' GET ' ) <nl> if meraki . status = = 200 : <nl> return response <nl> def main ( ) : <nl> <nl> meraki . params [ ' follow_redirects ' ] = ' all ' <nl> <nl> - query_urls = { ' mr_l3_firewall ' : ' / networks / { net_id } / ssids / ' } <nl> - update_urls = { ' mr_l3_firewall ' : ' / networks / { net_id } / ssids / ' } <nl> + query_urls = { ' mr_l3_firewall ' : ' / networks / { net_id } / ssids / { number } / l3FirewallRules ' } <nl> + update_urls = { ' mr_l3_firewall ' : ' / networks / { net_id } / ssids / { number } / l3FirewallRules ' } <nl> <nl> meraki . url_catalog [ ' get_all ' ] . update ( query_urls ) <nl> meraki . url_catalog [ ' update ' ] = update_urls <nl> def main ( ) : <nl> meraki . result [ ' data ' ] = get_rules ( meraki , net_id , number ) <nl> elif meraki . params [ ' state ' ] = = ' present ' : <nl> rules = get_rules ( meraki , net_id , number ) <nl> - path = meraki . construct_path ( ' get_all ' , net_id = net_id ) <nl> - path = path + number + ' / l3FirewallRules ' <nl> + path = meraki . construct_path ( ' get_all ' , net_id = net_id , custom = { ' number ' : number } ) <nl> if meraki . params [ ' rules ' ] : <nl> payload = assemble_payload ( meraki ) <nl> else : <nl>\n", "msg": "Migrated path creation to custom parameters ( )\n"}
{"diff_id": 7526, "repo": "ytdl-org/youtube-dl\n", "sha": "6ac7f082c469b3b2153735ae8475e1d0fc8b5439\n", "time": "2013-03-05T19:14:32Z\n", "diff": "mmm a / youtube_dl / FileDownloader . py <nl> ppp b / youtube_dl / FileDownloader . py <nl> def extract_info ( self , url ) : <nl> <nl> # Extract information from URL and process it <nl> try : <nl> - videos = ie . extract ( url ) <nl> - for video in videos or [ ] : <nl> - if not ' extractor ' in video : <nl> - # The extractor has already been set somewher else <nl> - video [ ' extractor ' ] = ie . IE_NAME <nl> - return videos <nl> + ie_results = ie . extract ( url ) <nl> + results = self . process_ie_results ( ie_results , ie ) <nl> + return results <nl> except ExtractorError as de : # An error we somewhat expected <nl> self . trouble ( u ' ERROR : ' + compat_str ( de ) , de . format_traceback ( ) ) <nl> break <nl> def extract_info_iterable ( self , urls ) : <nl> for url in urls : <nl> results . extend ( self . extract_info ( url ) ) <nl> return results <nl> + <nl> + def process_ie_results ( self , ie_results , ie ) : <nl> + \" \" \" <nl> + Take the results of the ie and return a list of videos . <nl> + For url elements it will seartch the suitable ie and get the videos <nl> + For playlist elements it will process each of the elements of the ' entries ' key <nl> + \" \" \" <nl> + results = [ ] <nl> + for result in ie_results or [ ] : <nl> + result_type = result . get ( ' _type ' , ' video ' ) # If not given we suppose it ' s a video , support the dafault old system <nl> + if result_type = = ' video ' : <nl> + if not ' extractor ' in result : <nl> + # The extractor has already been set somewhere else <nl> + result [ ' extractor ' ] = ie . IE_NAME <nl> + results . append ( result ) <nl> + elif result_type = = ' url ' : <nl> + # We get the videos pointed by the url <nl> + results . extend ( self . extract_info ( result [ ' url ' ] ) ) <nl> + elif result_type = = ' playlist ' : <nl> + # We process each entry in the playlist <nl> + entries_result = self . process_ie_results ( result [ ' entries ' ] , ie ) <nl> + results . extend ( entries_result ) <nl> + return results <nl> <nl> def process_info ( self , info_dict ) : <nl> \" \" \" Process a single dictionary returned by an InfoExtractor . \" \" \" <nl>\n", "msg": "` extract_info ` now expects ` ie . extract ` to return a list in the format proposed in issue 608 .\n"}
{"diff_id": 7573, "repo": "python/cpython\n", "sha": "d0bb6aa27507fede6cde3e536bd5edec5cd5dfa4\n", "time": "2012-04-25T12:17:54Z\n", "diff": "new file mode 100755 <nl> index 0000000000000 . . 1aa12b3e061c8 <nl> mmm / dev / null <nl> ppp b / Tools / scripts / import_diagnostics . py <nl> <nl> + # ! / usr / bin / env python3 <nl> + \" \" \" Miscellaneous diagnostics for the import system \" \" \" <nl> + <nl> + import sys <nl> + import argparse <nl> + from pprint import pprint <nl> + <nl> + def _dump_state ( args ) : <nl> + print ( sys . version ) <nl> + print ( \" sys . path : \" ) <nl> + pprint ( sys . path ) <nl> + print ( \" sys . meta_path \" ) <nl> + pprint ( sys . meta_path ) <nl> + print ( \" sys . path_hooks \" ) <nl> + pprint ( sys . path_hooks ) <nl> + print ( \" sys . path_importer_cache \" ) <nl> + pprint ( sys . path_importer_cache ) <nl> + print ( \" sys . modules : \" ) <nl> + pprint ( sys . modules ) <nl> + <nl> + COMMANDS = ( <nl> + ( \" dump \" , \" Dump import state \" , _dump_state ) , <nl> + ) <nl> + <nl> + def _make_parser ( ) : <nl> + parser = argparse . ArgumentParser ( ) <nl> + sub = parser . add_subparsers ( title = \" Commands \" ) <nl> + for name , description , implementation in COMMANDS : <nl> + cmd = sub . add_parser ( name , help = description ) <nl> + cmd . set_defaults ( command = implementation ) <nl> + return parser <nl> + <nl> + def main ( args ) : <nl> + parser = _make_parser ( ) <nl> + args = parser . parse_args ( args ) <nl> + return args . command ( args ) <nl> + <nl> + if __name__ = = \" __main__ \" : <nl> + sys . exit ( main ( sys . argv [ 1 : ] ) ) <nl>\n", "msg": "Start a shared utility script for poking around at the import internals\n"}
{"diff_id": 7684, "repo": "numpy/numpy\n", "sha": "c7eb19932b673ffffcf49a55d52e005ec996cdda\n", "time": "2006-09-25T17:51:52Z\n", "diff": "mmm a / numpy / oldnumeric / typeconv . py <nl> ppp b / numpy / oldnumeric / typeconv . py <nl> <nl> <nl> - __all__ = [ ' oldtype2dtype ' , ' convtypecode ' , ' convtypecode2 ' ] <nl> + __all__ = [ ' oldtype2dtype ' , ' convtypecode ' , ' convtypecode2 ' , ' oldtypecodes ' ] <nl> <nl> import numpy as N <nl> <nl> def convtypecode2 ( typecode , dtype = None ) : <nl> return N . dtype ( typecode ) <nl> else : <nl> return dtype <nl> + <nl> + _changedtypes = { ' B ' : ' b ' , <nl> + ' b ' : ' 1 ' , <nl> + ' h ' : ' s ' , <nl> + ' H ' : ' w ' , <nl> + ' I ' : ' u ' } <nl> + <nl> + class _oldtypecodes ( dict ) : <nl> + def __getitem__ ( self , obj ) : <nl> + char = N . dtype ( obj ) . char <nl> + try : <nl> + return _changedtypes [ char ] <nl> + except KeyError : <nl> + return char <nl> + <nl> + <nl> + oldtypecodes = _oldtypecodes ( ) <nl>\n", "msg": "Add oldtypecodes to oldnumeric . typeconv so that old Numeric character codes can be identified .\n"}
{"diff_id": 8029, "repo": "ansible/ansible\n", "sha": "77b4b2cdc9fe4dd10b2e5cff767cff6ad3c88689\n", "time": "2018-08-30T19:51:44Z\n", "diff": "new file mode 100644 <nl> index 0000000000000 . . e69de29bb2d1d <nl> new file mode 100644 <nl> index 0000000000000 . . b352ae4c73374 <nl> mmm / dev / null <nl> ppp b / lib / ansible / modules / remote_management / cpm / cpm_user . py <nl> <nl> + # ! / usr / bin / python <nl> + # - * - coding : utf - 8 - * - <nl> + # <nl> + # ( C ) 2018 Red Hat Inc . <nl> + # Copyright ( C ) 2018 Western Telematic Inc . <nl> + # <nl> + # GNU General Public License v3 . 0 + <nl> + # <nl> + # This program is distributed in the hope that it will be useful , <nl> + # but WITHOUT ANY WARRANTY ; without even the implied warranty of <nl> + # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the <nl> + # GNU General Public License for more details . <nl> + # <nl> + # ( see COPYING or https : / / www . gnu . org / licenses / gpl - 3 . 0 . txt ) <nl> + # <nl> + # Module to execute CPM User Commands on WTI OOB and PDU devices . <nl> + # CPM remote_management <nl> + # <nl> + from __future__ import absolute_import , division , print_function <nl> + __metaclass__ = type <nl> + <nl> + ANSIBLE_METADATA = { <nl> + ' metadata_version ' : ' 1 . 1 ' , <nl> + ' status ' : [ ' preview ' ] , <nl> + ' supported_by ' : ' community ' <nl> + } <nl> + <nl> + DOCUMENTATION = \" \" \" <nl> + mmm <nl> + module : cpm_user <nl> + version_added : \" 2 . 7 \" <nl> + author : \" Western Telematic Inc . ( @ wtinetworkgear ) \" <nl> + short_description : Get various status and parameters from WTI OOB and PDU devices <nl> + description : <nl> + - \" Get / Add / Edit Delete Users from WTI OOB and PDU devices \" <nl> + options : <nl> + cpm_action : <nl> + description : <nl> + - This is the Action to send the module . <nl> + required : true <nl> + choices : [ \" getuser \" , \" adduser \" , \" edituser \" , \" deleteuser \" ] <nl> + cpm_url : <nl> + description : <nl> + - This is the URL of the WTI device to send the module . <nl> + required : true <nl> + cpm_username : <nl> + description : <nl> + - This is the Basic Authentication Username of the WTI device to send the module . <nl> + required : true <nl> + cpm_password : <nl> + description : <nl> + - This is the Basic Authentication Password of the WTI device to send the module . <nl> + required : true <nl> + use_https : <nl> + description : <nl> + - Designates to use an https connection or http connection . <nl> + required : false <nl> + type : bool <nl> + default : true <nl> + validate_certs : <nl> + description : <nl> + - If false , SSL certificates will not be validated . This should only be used <nl> + - on personally controlled sites using self - signed certificates . <nl> + required : false <nl> + type : bool <nl> + default : true <nl> + use_proxy : <nl> + description : Flag to control if the lookup will observe HTTP proxy environment variables when present . <nl> + required : false <nl> + type : bool <nl> + default : false <nl> + user_name : <nl> + description : <nl> + - This is the User Name that needs to be create / modified / deleted <nl> + required : true <nl> + user_pass : <nl> + description : <nl> + - This is the User Password that needs to be create / modified / deleted <nl> + - If the user is being Created this parameter is required <nl> + required : false <nl> + user_accesslevel : <nl> + description : <nl> + - This is the access level that needs to be create / modified / deleted <nl> + - 0 View , 1 User , 2 SuperUser , 3 Adminstrator <nl> + required : false <nl> + choices : [ 0 , 1 , 2 , 3 ] <nl> + user_accessssh : <nl> + description : <nl> + - If the user has access to the WTI device via SSH <nl> + - 0 No , 1 Yes <nl> + required : false <nl> + choices : [ 0 , 1 ] <nl> + user_accessserial : <nl> + description : <nl> + - If the user has access to the WTI device via Serial ports <nl> + - 0 No , 1 Yes <nl> + required : false <nl> + choices : [ 0 , 1 ] <nl> + user_accessweb : <nl> + description : <nl> + - If the user has access to the WTI device via Web <nl> + - 0 No , 1 Yes <nl> + required : false <nl> + choices : [ 0 , 1 ] <nl> + user_accessapi : <nl> + description : <nl> + - If the user has access to the WTI device via RESTful APIs <nl> + - 0 No , 1 Yes <nl> + required : false <nl> + choices : [ 0 , 1 ] <nl> + user_accessmonitor : <nl> + description : <nl> + - If the user has ability to monitor connection sessions <nl> + - 0 No , 1 Yes <nl> + required : false <nl> + choices : [ 0 , 1 ] <nl> + user_accessoutbound : <nl> + description : <nl> + - If the user has ability to initiate Outbound connection <nl> + - 0 No , 1 Yes <nl> + required : false <nl> + choices : [ 0 , 1 ] <nl> + user_portaccess : <nl> + description : <nl> + - If AccessLevel is lower than Administrator , which ports the user has access <nl> + required : false <nl> + user_plugaccess : <nl> + description : <nl> + - If AccessLevel is lower than Administrator , which plugs the user has access <nl> + required : false <nl> + user_groupaccess : <nl> + description : <nl> + - If AccessLevel is lower than Administrator , which Groups the user has access <nl> + required : false <nl> + user_callbackphone : <nl> + description : <nl> + - This is the Call Back phone number used for POTS modem connections <nl> + required : false <nl> + \" \" \" <nl> + <nl> + EXAMPLES = \" \" \" <nl> + # Get User Parameters <nl> + - name : Get the User Parameters for the given user of a WTI device <nl> + cpm_user : <nl> + cpm_action : \" getuser \" <nl> + cpm_url : \" rest . wti . com \" <nl> + cpm_username : \" restuser \" <nl> + cpm_password : \" restfuluserpass12 \" <nl> + use_https : true <nl> + validate_certs : true <nl> + user_name : \" usernumberone \" <nl> + <nl> + # Create User <nl> + - name : Create a User on a given WTI device <nl> + cpm_user : <nl> + cpm_action : \" adduser \" <nl> + cpm_url : \" rest . wti . com \" <nl> + cpm_username : \" restuser \" <nl> + cpm_password : \" restfuluserpass12 \" <nl> + use_https : true <nl> + validate_certs : false <nl> + user_name : \" usernumberone \" <nl> + user_pass : \" complicatedpassword \" <nl> + user_accesslevel : 2 <nl> + user_accessssh : 1 <nl> + user_accessserial : 1 <nl> + user_accessweb : 0 <nl> + user_accessapi : 1 <nl> + user_accessmonitor : 0 <nl> + user_accessoutbound : 0 <nl> + user_portaccess : \" 10011111 \" <nl> + user_plugaccess : \" 00000111 \" <nl> + user_groupaccess : \" 00000000 \" <nl> + <nl> + # Edit User <nl> + - name : Edit a User on a given WTI device <nl> + cpm_user : <nl> + cpm_action : \" edituser \" <nl> + cpm_url : \" rest . wti . com \" <nl> + cpm_username : \" restuser \" <nl> + cpm_password : \" restfuluserpass12 \" <nl> + use_https : true <nl> + validate_certs : false <nl> + user_name : \" usernumberone \" <nl> + user_pass : \" newpasswordcomplicatedpassword \" <nl> + <nl> + # Delete User <nl> + - name : Delete a User from a given WTI device <nl> + cpm_user : <nl> + cpm_action : \" deleteuser \" <nl> + cpm_url : \" rest . wti . com \" <nl> + cpm_username : \" restuser \" <nl> + cpm_password : \" restfuluserpass12 \" <nl> + use_https : true <nl> + validate_certs : true <nl> + user_name : \" usernumberone \" <nl> + \" \" \" <nl> + <nl> + RETURN = \" \" \" <nl> + data : <nl> + description : The output JSON returned from the commands sent <nl> + returned : always <nl> + type : str <nl> + \" \" \" <nl> + <nl> + import base64 <nl> + <nl> + from ansible . module_utils . basic import AnsibleModule <nl> + from ansible . module_utils . _text import to_text , to_bytes , to_native <nl> + from ansible . module_utils . six . moves . urllib . error import HTTPError , URLError <nl> + from ansible . module_utils . urls import open_url , ConnectionError , SSLValidationError <nl> + <nl> + <nl> + def assemble_json ( cpmmodule ) : <nl> + json_load = \" \" <nl> + <nl> + json_load = ' { \" users \" : ' <nl> + json_load = json_load + ' { \" username \" : \" ' + cpmmodule . params [ \" user_name \" ] + ' \" ' <nl> + <nl> + # for Adding there must be a password present <nl> + if cpmmodule . params [ \" user_pass \" ] is not None and ( len ( cpmmodule . params [ \" user_pass \" ] ) > 0 ) : <nl> + json_load = json_load + ' , \" newpasswd \" : \" ' + cpmmodule . params [ \" user_pass \" ] + ' \" ' <nl> + if cpmmodule . params [ \" user_accesslevel \" ] is not None : <nl> + json_load = json_load + ' , \" accesslevel \" : ' + str ( cpmmodule . params [ \" user_accesslevel \" ] ) + ' ' <nl> + if cpmmodule . params [ \" user_portaccess \" ] is not None : <nl> + json_load = json_load + ' , \" portaccess \" : ' + cpmmodule . params [ \" user_portaccess \" ] + ' ' <nl> + if cpmmodule . params [ \" user_plugaccess \" ] is not None : <nl> + json_load = json_load + ' , \" plugaccess \" : ' + cpmmodule . params [ \" user_plugaccess \" ] + ' ' <nl> + if cpmmodule . params [ \" user_groupaccess \" ] is not None : <nl> + json_load = json_load + ' , \" groupaccess \" : ' + cpmmodule . params [ \" user_groupaccess \" ] + ' ' <nl> + if cpmmodule . params [ \" user_accessserial \" ] is not None : <nl> + json_load = json_load + ' , \" accessserial \" : ' + str ( cpmmodule . params [ \" user_accessserial \" ] ) + ' ' <nl> + if cpmmodule . params [ \" user_accessssh \" ] is not None : <nl> + json_load = json_load + ' , \" accessssh \" : ' + str ( cpmmodule . params [ \" user_accessssh \" ] ) + ' ' <nl> + if cpmmodule . params [ \" user_accessweb \" ] is not None : <nl> + json_load = json_load + ' , \" accessweb \" : ' + str ( cpmmodule . params [ \" user_accessweb \" ] ) + ' ' <nl> + if cpmmodule . params [ \" user_accessoutbound \" ] is not None : <nl> + json_load = json_load + ' , \" accessoutbound \" : ' + str ( cpmmodule . params [ \" user_accessoutbound \" ] ) + ' ' <nl> + if cpmmodule . params [ \" user_accessapi \" ] is not None : <nl> + json_load = json_load + ' , \" accessapi \" : ' + str ( cpmmodule . params [ \" user_accessapi \" ] ) + ' ' <nl> + if cpmmodule . params [ \" user_accessmonitor \" ] is not None : <nl> + json_load = json_load + ' , \" accessmonitor \" : ' + str ( cpmmodule . params [ \" user_accessmonitor \" ] ) + ' ' <nl> + if cpmmodule . params [ \" user_callbackphone \" ] is not None : <nl> + json_load = json_load + ' , \" callbackphone \" : \" ' + cpmmodule . params [ \" user_callbackphone \" ] + ' \" ' <nl> + <nl> + json_load = json_load + ' } ' <nl> + json_load = json_load + ' } ' <nl> + return json_load <nl> + <nl> + <nl> + def run_module ( ) : <nl> + <nl> + module_args = dict ( <nl> + cpm_action = dict ( choices = [ ' getuser ' , ' adduser ' , ' edituser ' , ' deleteuser ' ] , required = True ) , <nl> + cpm_url = dict ( type = ' str ' , required = True ) , <nl> + cpm_username = dict ( type = ' str ' , required = True ) , <nl> + cpm_password = dict ( type = ' str ' , required = True , no_log = True ) , <nl> + user_name = dict ( type = ' str ' , required = True ) , <nl> + user_pass = dict ( type = ' str ' , required = False , default = None , no_log = True ) , <nl> + user_accesslevel = dict ( type = ' int ' , required = False , default = None , choices = [ 0 , 1 , 2 , 3 ] ) , <nl> + user_accessssh = dict ( type = ' int ' , required = False , default = None , choices = [ 0 , 1 ] ) , <nl> + user_accessserial = dict ( type = ' int ' , required = False , default = None , choices = [ 0 , 1 ] ) , <nl> + user_accessweb = dict ( type = ' int ' , required = False , default = None , choices = [ 0 , 1 ] ) , <nl> + user_accessapi = dict ( type = ' int ' , required = False , default = None , choices = [ 0 , 1 ] ) , <nl> + user_accessmonitor = dict ( type = ' int ' , required = False , default = None , choices = [ 0 , 1 ] ) , <nl> + user_accessoutbound = dict ( type = ' int ' , required = False , default = None , choices = [ 0 , 1 ] ) , <nl> + user_portaccess = dict ( type = ' str ' , required = False , default = None ) , <nl> + user_plugaccess = dict ( type = ' str ' , required = False , default = None ) , <nl> + user_groupaccess = dict ( type = ' str ' , required = False , default = None ) , <nl> + user_callbackphone = dict ( type = ' str ' , required = False , default = None ) , <nl> + use_https = dict ( type = ' bool ' , default = True ) , <nl> + validate_certs = dict ( type = ' bool ' , default = True ) , <nl> + use_proxy = dict ( type = ' bool ' , default = False ) <nl> + ) <nl> + <nl> + result = dict ( <nl> + changed = False , <nl> + data = ' ' <nl> + ) <nl> + <nl> + module = AnsibleModule ( argument_spec = module_args , supports_check_mode = True ) <nl> + <nl> + if module . check_mode : <nl> + return result <nl> + <nl> + auth = to_text ( base64 . b64encode ( to_bytes ( ' { 0 } : { 1 } ' . format ( module . params [ ' cpm_username ' ] , module . params [ ' cpm_password ' ] ) , <nl> + errors = ' surrogate_or_strict ' ) ) ) <nl> + <nl> + if module . params [ ' use_https ' ] is True : <nl> + protocol = \" https : / / \" <nl> + else : <nl> + protocol = \" http : / / \" <nl> + <nl> + payload = None <nl> + if ( module . params [ ' cpm_action ' ] = = ' getuser ' ) : <nl> + fullurl = ( \" % s % s / api / v2 / config / users ? username = % s \" % ( protocol , module . params [ ' cpm_url ' ] , module . params [ ' user_name ' ] ) ) <nl> + method = ' GET ' <nl> + elif ( module . params [ ' cpm_action ' ] = = ' adduser ' ) : <nl> + if module . params [ \" user_pass \" ] is None or ( len ( module . params [ \" user_pass \" ] ) = = 0 ) : <nl> + module . fail_json ( msg = ' user_pass not defined . ' , * * result ) <nl> + <nl> + payload = assemble_json ( module ) <nl> + fullurl = ( \" % s % s / api / v2 / config / users \" % ( protocol , module . params [ ' cpm_url ' ] ) ) <nl> + method = ' POST ' <nl> + elif ( module . params [ ' cpm_action ' ] = = ' edituser ' ) : <nl> + payload = assemble_json ( module ) <nl> + fullurl = ( \" % s % s / api / v2 / config / users \" % ( protocol , module . params [ ' cpm_url ' ] ) ) <nl> + method = ' PUT ' <nl> + elif ( module . params [ ' cpm_action ' ] = = ' deleteuser ' ) : <nl> + fullurl = ( \" % s % s / api / v2 / config / users ? username = % s \" % ( protocol , module . params [ ' cpm_url ' ] , module . params [ ' user_name ' ] ) ) <nl> + method = ' DELETE ' <nl> + <nl> + try : <nl> + response = open_url ( fullurl , data = payload , method = method , validate_certs = module . params [ ' validate_certs ' ] , use_proxy = module . params [ ' use_proxy ' ] , <nl> + headers = { ' Content - Type ' : ' application / json ' , ' Authorization ' : \" Basic % s \" % auth } ) <nl> + if ( method ! = ' GET ' ) : <nl> + result [ ' changed ' ] = True <nl> + <nl> + except HTTPError as e : <nl> + fail_json = dict ( msg = ' Received HTTP error for { 0 } : { 1 } ' . format ( fullurl , to_native ( e ) ) , changed = False ) <nl> + module . fail_json ( * * fail_json ) <nl> + except URLError as e : <nl> + fail_json = dict ( msg = ' Failed lookup url for { 0 } : { 1 } ' . format ( fullurl , to_native ( e ) ) , changed = False ) <nl> + module . fail_json ( * * fail_json ) <nl> + except SSLValidationError as e : <nl> + fail_json = dict ( msg = ' Error validating the server ' ' s certificate for { 0 } : { 1 } ' . format ( fullurl , to_native ( e ) ) , changed = False ) <nl> + module . fail_json ( * * fail_json ) <nl> + except ConnectionError as e : <nl> + fail_json = dict ( msg = ' Error connecting to for { 0 } : { 1 } ' . format ( fullurl , to_native ( e ) ) , changed = False ) <nl> + module . fail_json ( * * fail_json ) <nl> + <nl> + result [ ' data ' ] = to_text ( response . read ( ) ) <nl> + <nl> + module . exit_json ( * * result ) <nl> + <nl> + <nl> + def main ( ) : <nl> + run_module ( ) <nl> + <nl> + <nl> + if __name__ = = ' __main__ ' : <nl> + main ( ) <nl>\n", "msg": "Add module cpm_user to read , create , edit and delete users on WTI devices ( )\n"}
{"diff_id": 8054, "repo": "python/cpython\n", "sha": "63b5b6fd450260a4239371e96263020587648ad9\n", "time": "2016-10-02T18:16:38Z\n", "diff": "mmm a / Lib / test / test_unicode . py <nl> ppp b / Lib / test / test_unicode . py <nl> def __str__ ( self ) : <nl> self . assertEqual ( \" % s \" % s , ' __str__ overridden ' ) <nl> self . assertEqual ( \" { } \" . format ( s ) , ' __str__ overridden ' ) <nl> <nl> + def test_subclass_add ( self ) : <nl> + class S ( str ) : <nl> + def __add__ ( self , o ) : <nl> + return \" 3 \" <nl> + self . assertEqual ( S ( \" 4 \" ) + S ( \" 5 \" ) , \" 3 \" ) <nl> + class S ( str ) : <nl> + def __iadd__ ( self , o ) : <nl> + return \" 3 \" <nl> + s = S ( \" 1 \" ) <nl> + s + = \" 4 \" <nl> + self . assertEqual ( s , \" 3 \" ) <nl> + <nl> + def test_getnewargs ( self ) : <nl> + text = ' abc ' <nl> + args = text . __getnewargs__ ( ) <nl> + self . assertIsNot ( args [ 0 ] , text ) <nl> + self . assertEqual ( args [ 0 ] , text ) <nl> + self . assertEqual ( len ( args ) , 1 ) <nl> + <nl> + def test_resize ( self ) : <nl> + for length in range ( 1 , 100 , 7 ) : <nl> + # generate a fresh string ( refcount = 1 ) <nl> + text = ' a ' * length + ' b ' <nl> + <nl> + with support . check_warnings ( ( ' unicode_internal codec has been ' <nl> + ' deprecated ' , DeprecationWarning ) ) : <nl> + # fill wstr internal field <nl> + abc = text . encode ( ' unicode_internal ' ) <nl> + self . assertEqual ( abc . decode ( ' unicode_internal ' ) , text ) <nl> + <nl> + # resize text : wstr field must be cleared and then recomputed <nl> + text + = ' c ' <nl> + abcdef = text . encode ( ' unicode_internal ' ) <nl> + self . assertNotEqual ( abc , abcdef ) <nl> + self . assertEqual ( abcdef . decode ( ' unicode_internal ' ) , text ) <nl> + <nl> + def test_compare ( self ) : <nl> + # Issue # 17615 <nl> + N = 10 <nl> + ascii = ' a ' * N <nl> + ascii2 = ' z ' * N <nl> + latin = ' \\ x80 ' * N <nl> + latin2 = ' \\ xff ' * N <nl> + bmp = ' \\ u0100 ' * N <nl> + bmp2 = ' \\ uffff ' * N <nl> + astral = ' \\ U00100000 ' * N <nl> + astral2 = ' \\ U0010ffff ' * N <nl> + strings = ( <nl> + ascii , ascii2 , <nl> + latin , latin2 , <nl> + bmp , bmp2 , <nl> + astral , astral2 ) <nl> + for text1 , text2 in itertools . combinations ( strings , 2 ) : <nl> + equal = ( text1 is text2 ) <nl> + self . assertEqual ( text1 = = text2 , equal ) <nl> + self . assertEqual ( text1 ! = text2 , not equal ) <nl> + <nl> + if equal : <nl> + self . assertTrue ( text1 < = text2 ) <nl> + self . assertTrue ( text1 > = text2 ) <nl> + <nl> + # text1 is text2 : duplicate strings to skip the \" str1 = = str2 \" <nl> + # optimization in unicode_compare_eq ( ) and really compare <nl> + # character per character <nl> + copy1 = duplicate_string ( text1 ) <nl> + copy2 = duplicate_string ( text2 ) <nl> + self . assertIsNot ( copy1 , copy2 ) <nl> + <nl> + self . assertTrue ( copy1 = = copy2 ) <nl> + self . assertFalse ( copy1 ! = copy2 ) <nl> + <nl> + self . assertTrue ( copy1 < = copy2 ) <nl> + self . assertTrue ( copy2 > = copy2 ) <nl> + <nl> + self . assertTrue ( ascii < ascii2 ) <nl> + self . assertTrue ( ascii < latin ) <nl> + self . assertTrue ( ascii < bmp ) <nl> + self . assertTrue ( ascii < astral ) <nl> + self . assertFalse ( ascii > = ascii2 ) <nl> + self . assertFalse ( ascii > = latin ) <nl> + self . assertFalse ( ascii > = bmp ) <nl> + self . assertFalse ( ascii > = astral ) <nl> + <nl> + self . assertFalse ( latin < ascii ) <nl> + self . assertTrue ( latin < latin2 ) <nl> + self . assertTrue ( latin < bmp ) <nl> + self . assertTrue ( latin < astral ) <nl> + self . assertTrue ( latin > = ascii ) <nl> + self . assertFalse ( latin > = latin2 ) <nl> + self . assertFalse ( latin > = bmp ) <nl> + self . assertFalse ( latin > = astral ) <nl> + <nl> + self . assertFalse ( bmp < ascii ) <nl> + self . assertFalse ( bmp < latin ) <nl> + self . assertTrue ( bmp < bmp2 ) <nl> + self . assertTrue ( bmp < astral ) <nl> + self . assertTrue ( bmp > = ascii ) <nl> + self . assertTrue ( bmp > = latin ) <nl> + self . assertFalse ( bmp > = bmp2 ) <nl> + self . assertFalse ( bmp > = astral ) <nl> + <nl> + self . assertFalse ( astral < ascii ) <nl> + self . assertFalse ( astral < latin ) <nl> + self . assertFalse ( astral < bmp2 ) <nl> + self . assertTrue ( astral < astral2 ) <nl> + self . assertTrue ( astral > = ascii ) <nl> + self . assertTrue ( astral > = latin ) <nl> + self . assertTrue ( astral > = bmp2 ) <nl> + self . assertFalse ( astral > = astral2 ) <nl> + <nl> + def test_free_after_iterating ( self ) : <nl> + support . check_free_after_iterating ( self , iter , str ) <nl> + support . check_free_after_iterating ( self , reversed , str ) <nl> + <nl> + <nl> + class CAPITest ( unittest . TestCase ) : <nl> + <nl> # Test PyUnicode_FromFormat ( ) <nl> def test_from_format ( self ) : <nl> support . import_module ( ' ctypes ' ) <nl> def test_aswidecharstring ( self ) : <nl> self . assertEqual ( size , nchar ) <nl> self . assertEqual ( wchar , nonbmp + ' \\ 0 ' ) <nl> <nl> - def test_subclass_add ( self ) : <nl> - class S ( str ) : <nl> - def __add__ ( self , o ) : <nl> - return \" 3 \" <nl> - self . assertEqual ( S ( \" 4 \" ) + S ( \" 5 \" ) , \" 3 \" ) <nl> - class S ( str ) : <nl> - def __iadd__ ( self , o ) : <nl> - return \" 3 \" <nl> - s = S ( \" 1 \" ) <nl> - s + = \" 4 \" <nl> - self . assertEqual ( s , \" 3 \" ) <nl> - <nl> @ support . cpython_only <nl> def test_encode_decimal ( self ) : <nl> from _testcapi import unicode_encodedecimal <nl> def test_transform_decimal ( self ) : <nl> self . assertEqual ( transform_decimal ( ' 123 \\ u20ac ' ) , <nl> ' 123 \\ u20ac ' ) <nl> <nl> - def test_getnewargs ( self ) : <nl> - text = ' abc ' <nl> - args = text . __getnewargs__ ( ) <nl> - self . assertIsNot ( args [ 0 ] , text ) <nl> - self . assertEqual ( args [ 0 ] , text ) <nl> - self . assertEqual ( len ( args ) , 1 ) <nl> - <nl> - def test_resize ( self ) : <nl> - for length in range ( 1 , 100 , 7 ) : <nl> - # generate a fresh string ( refcount = 1 ) <nl> - text = ' a ' * length + ' b ' <nl> - <nl> - with support . check_warnings ( ( ' unicode_internal codec has been ' <nl> - ' deprecated ' , DeprecationWarning ) ) : <nl> - # fill wstr internal field <nl> - abc = text . encode ( ' unicode_internal ' ) <nl> - self . assertEqual ( abc . decode ( ' unicode_internal ' ) , text ) <nl> - <nl> - # resize text : wstr field must be cleared and then recomputed <nl> - text + = ' c ' <nl> - abcdef = text . encode ( ' unicode_internal ' ) <nl> - self . assertNotEqual ( abc , abcdef ) <nl> - self . assertEqual ( abcdef . decode ( ' unicode_internal ' ) , text ) <nl> - <nl> - def test_compare ( self ) : <nl> - # Issue # 17615 <nl> - N = 10 <nl> - ascii = ' a ' * N <nl> - ascii2 = ' z ' * N <nl> - latin = ' \\ x80 ' * N <nl> - latin2 = ' \\ xff ' * N <nl> - bmp = ' \\ u0100 ' * N <nl> - bmp2 = ' \\ uffff ' * N <nl> - astral = ' \\ U00100000 ' * N <nl> - astral2 = ' \\ U0010ffff ' * N <nl> - strings = ( <nl> - ascii , ascii2 , <nl> - latin , latin2 , <nl> - bmp , bmp2 , <nl> - astral , astral2 ) <nl> - for text1 , text2 in itertools . combinations ( strings , 2 ) : <nl> - equal = ( text1 is text2 ) <nl> - self . assertEqual ( text1 = = text2 , equal ) <nl> - self . assertEqual ( text1 ! = text2 , not equal ) <nl> - <nl> - if equal : <nl> - self . assertTrue ( text1 < = text2 ) <nl> - self . assertTrue ( text1 > = text2 ) <nl> - <nl> - # text1 is text2 : duplicate strings to skip the \" str1 = = str2 \" <nl> - # optimization in unicode_compare_eq ( ) and really compare <nl> - # character per character <nl> - copy1 = duplicate_string ( text1 ) <nl> - copy2 = duplicate_string ( text2 ) <nl> - self . assertIsNot ( copy1 , copy2 ) <nl> - <nl> - self . assertTrue ( copy1 = = copy2 ) <nl> - self . assertFalse ( copy1 ! = copy2 ) <nl> - <nl> - self . assertTrue ( copy1 < = copy2 ) <nl> - self . assertTrue ( copy2 > = copy2 ) <nl> - <nl> - self . assertTrue ( ascii < ascii2 ) <nl> - self . assertTrue ( ascii < latin ) <nl> - self . assertTrue ( ascii < bmp ) <nl> - self . assertTrue ( ascii < astral ) <nl> - self . assertFalse ( ascii > = ascii2 ) <nl> - self . assertFalse ( ascii > = latin ) <nl> - self . assertFalse ( ascii > = bmp ) <nl> - self . assertFalse ( ascii > = astral ) <nl> - <nl> - self . assertFalse ( latin < ascii ) <nl> - self . assertTrue ( latin < latin2 ) <nl> - self . assertTrue ( latin < bmp ) <nl> - self . assertTrue ( latin < astral ) <nl> - self . assertTrue ( latin > = ascii ) <nl> - self . assertFalse ( latin > = latin2 ) <nl> - self . assertFalse ( latin > = bmp ) <nl> - self . assertFalse ( latin > = astral ) <nl> - <nl> - self . assertFalse ( bmp < ascii ) <nl> - self . assertFalse ( bmp < latin ) <nl> - self . assertTrue ( bmp < bmp2 ) <nl> - self . assertTrue ( bmp < astral ) <nl> - self . assertTrue ( bmp > = ascii ) <nl> - self . assertTrue ( bmp > = latin ) <nl> - self . assertFalse ( bmp > = bmp2 ) <nl> - self . assertFalse ( bmp > = astral ) <nl> - <nl> - self . assertFalse ( astral < ascii ) <nl> - self . assertFalse ( astral < latin ) <nl> - self . assertFalse ( astral < bmp2 ) <nl> - self . assertTrue ( astral < astral2 ) <nl> - self . assertTrue ( astral > = ascii ) <nl> - self . assertTrue ( astral > = latin ) <nl> - self . assertTrue ( astral > = bmp2 ) <nl> - self . assertFalse ( astral > = astral2 ) <nl> - <nl> @ support . cpython_only <nl> def test_pep393_utf8_caching_bug ( self ) : <nl> # Issue # 25709 : Problem with string concatenation and utf - 8 cache <nl> def test_pep393_utf8_caching_bug ( self ) : <nl> # Check that the second call returns the same result <nl> self . assertEqual ( getargs_s_hash ( s ) , chr ( k ) . encode ( ) * ( i + 1 ) ) <nl> <nl> - def test_free_after_iterating ( self ) : <nl> - support . check_free_after_iterating ( self , iter , str ) <nl> - support . check_free_after_iterating ( self , reversed , str ) <nl> - <nl> <nl> class StringModuleTest ( unittest . TestCase ) : <nl> def test_formatter_parser ( self ) : <nl>\n", "msg": "Moved Unicode C API related tests to separate test class .\n"}
{"diff_id": 8065, "repo": "ipython/ipython\n", "sha": "ca72370db0d97db969f27087ad8cf245f3affe0b\n", "time": "2012-01-11T00:52:07Z\n", "diff": "mmm a / IPython / frontend / html / notebook / notebookapp . py <nl> ppp b / IPython / frontend / html / notebook / notebookapp . py <nl> class NotebookApp ( BaseIPythonApplication ) : <nl> config = True , <nl> help = \" Set the log level by value or name . \" ) <nl> <nl> + # create requested profiles by default , if they don ' t exist : <nl> + auto_create = Bool ( True ) <nl> + <nl> # Network related information . <nl> <nl> ip = Unicode ( LOCALHOST , config = True , <nl>\n", "msg": "set auto_create flag for notebook apps\n"}
{"diff_id": 8104, "repo": "scikit-learn/scikit-learn\n", "sha": "d1a9af884d66c9b035790337edc91b62a02d6da4\n", "time": "2014-07-14T13:45:21Z\n", "diff": "mmm a / sklearn / decomposition / sparse_pca . py <nl> ppp b / sklearn / decomposition / sparse_pca . py <nl> def transform ( self , X , ridge_alpha = None ) : <nl> \" \" \" <nl> ridge_alpha = self . ridge_alpha if ridge_alpha is None else ridge_alpha <nl> U = ridge_regression ( self . components_ . T , X . T , ridge_alpha , <nl> - solver = ' dense_cholesky ' ) <nl> + solver = ' cholesky ' ) <nl> s = np . sqrt ( ( U * * 2 ) . sum ( axis = 0 ) ) <nl> s [ s = = 0 ] = 1 <nl> U / = s <nl>\n", "msg": "Changed solver from ' dense_cholesky ' to ' cholesky ' to eliminate deprecation warning .\n"}
{"diff_id": 8194, "repo": "home-assistant/core\n", "sha": "c69f37500a9d14bf2e16f275cdc3d8d8e3a52ee7\n", "time": "2018-04-15T13:25:30Z\n", "diff": "mmm a / homeassistant / components / switch / edimax . py <nl> ppp b / homeassistant / components / switch / edimax . py <nl> def update ( self ) : <nl> \" \" \" Update edimax switch . \" \" \" <nl> try : <nl> self . _now_power = float ( self . smartplug . now_power ) <nl> - except ValueError : <nl> + except ( TypeError , ValueError ) : <nl> self . _now_power = None <nl> <nl> try : <nl> self . _now_energy_day = float ( self . smartplug . now_energy_day ) <nl> - except ValueError : <nl> + except ( TypeError , ValueError ) : <nl> self . _now_energy_day = None <nl> <nl> self . _state = self . smartplug . state = = ' ON ' <nl>\n", "msg": "Restore typeerror check for units sans energy tracking ( )\n"}
{"diff_id": 8347, "repo": "getredash/redash\n", "sha": "3c8a3caa1da85d16ea9173b96a5ac51be4cba414\n", "time": "2018-12-12T08:10:13Z\n", "diff": "mmm a / redash / authentication / __init__ . py <nl> ppp b / redash / authentication / __init__ . py <nl> def sign ( key , path , expires ) : <nl> @ login_manager . user_loader <nl> def load_user ( user_id_with_identity ) : <nl> org = current_org . _get_current_object ( ) <nl> - user_id , _ = user_id_with_identity . split ( \" - \" ) <nl> + is_legacy_session_identifier = user_id_with_identity . find ( ' - ' ) < 0 <nl> + <nl> + if is_legacy_session_identifier : <nl> + user_id = user_id_with_identity <nl> + else : <nl> + user_id , _ = user_id_with_identity . split ( \" - \" ) <nl> <nl> try : <nl> user = models . User . get_by_id_and_org ( user_id , org ) <nl> - if user . is_disabled or user . get_id ( ) ! = user_id_with_identity : <nl> + if user . is_disabled : <nl> + return None <nl> + <nl> + if not is_legacy_session_identifier and user . get_id ( ) ! = user_id_with_identity : <nl> return None <nl> <nl> return user <nl>\n", "msg": "backward compatibility so users who have the old session identifier don ' t get logged out\n"}
{"diff_id": 8396, "repo": "docker/compose\n", "sha": "60d005b055119ce976265eaf34e4daa6483ead58\n", "time": "2016-10-24T20:58:45Z\n", "diff": "mmm a / tests / integration / project_test . py <nl> ppp b / tests / integration / project_test . py <nl> def test_up_with_network_link_local_ips ( self ) : <nl> name = ' composetest ' , <nl> config_data = config_data <nl> ) <nl> - project . up ( ) <nl> + project . up ( detached = True ) <nl> <nl> - service_container = project . get_service ( ' web ' ) . containers ( ) [ 0 ] <nl> + service_container = project . get_service ( ' web ' ) . containers ( stopped = True ) [ 0 ] <nl> ipam_config = service_container . inspect ( ) . get ( <nl> ' NetworkSettings ' , { } <nl> ) . get ( <nl> def test_up_with_isolation ( self ) : <nl> name = ' composetest ' , <nl> config_data = config_data <nl> ) <nl> - project . up ( ) <nl> - service_container = project . get_service ( ' web ' ) . containers ( ) [ 0 ] <nl> + project . up ( detached = True ) <nl> + service_container = project . get_service ( ' web ' ) . containers ( stopped = True ) [ 0 ] <nl> assert service_container . inspect ( ) [ ' HostConfig ' ] [ ' Isolation ' ] = = ' default ' <nl> <nl> @ v2_1_only ( ) <nl>\n", "msg": "Improve robustness of a couple integration tests with occasional failures\n"}
{"diff_id": 8465, "repo": "encode/django-rest-framework\n", "sha": "ff7725f05e8ca624e54d707f7c655e3d5c8b8888\n", "time": "2012-10-31T14:30:01Z\n", "diff": "mmm a / rest_framework / fields . py <nl> ppp b / rest_framework / fields . py <nl> def __init__ ( self , * args , * * kwargs ) : <nl> self . view_name = kwargs . pop ( ' view_name ' ) <nl> except : <nl> raise ValueError ( \" Hyperlinked field requires ' view_name ' kwarg \" ) <nl> + self . slug_url_kwarg = kwargs . pop ( ' slug_url_kwargs ' , self . slug_url_kwarg ) <nl> + self . slug_field = kwargs . pop ( ' slug_field ' , self . slug_field ) <nl> self . format = kwargs . pop ( ' format ' , None ) <nl> super ( HyperlinkedRelatedField , self ) . __init__ ( * args , * * kwargs ) <nl> <nl>\n", "msg": "added support for custom slug field and kwargs\n"}
{"diff_id": 8616, "repo": "python/cpython\n", "sha": "d9b7d48a82a694ecf57633914d15e3e107de8311\n", "time": "2010-03-19T21:42:45Z\n", "diff": "mmm a / Mac / BuildScript / build - installer . py <nl> ppp b / Mac / BuildScript / build - installer . py <nl> def grepValue ( fn , variable ) : <nl> if ln . startswith ( variable ) : <nl> value = ln [ len ( variable ) : ] . strip ( ) <nl> return value [ 1 : - 1 ] <nl> + raise RuntimeError , \" Cannot find variable % s \" % variable [ : - 1 ] <nl> <nl> def getVersion ( ) : <nl> return grepValue ( os . path . join ( SRCDIR , ' configure ' ) , ' PACKAGE_VERSION ' ) <nl> <nl> + def getVersionTuple ( ) : <nl> + return tuple ( [ int ( n ) for n in getVersion ( ) . split ( ' . ' ) ] ) <nl> + <nl> def getFullVersion ( ) : <nl> fn = os . path . join ( SRCDIR , ' Include ' , ' patchlevel . h ' ) <nl> for ln in open ( fn ) : <nl> if ' PY_VERSION ' in ln : <nl> return ln . split ( ) [ - 1 ] [ 1 : - 1 ] <nl> - <nl> raise RuntimeError , \" Cannot find full version ? ? \" <nl> <nl> # The directory we ' ll use to create the build ( will be erased and recreated ) <nl> def getFullVersion ( ) : <nl> DEPSRC = os . path . expanduser ( ' ~ / Universal / other - sources ' ) <nl> <nl> # Location of the preferred SDK <nl> + <nl> + # # # There are some issues with the SDK selection below here , <nl> + # # # The resulting binary doesn ' t work on all platforms that <nl> + # # # it should . Always default to the 10 . 4u SDK until that <nl> + # # # isue is resolved . <nl> + # # # <nl> + # # if int ( os . uname ( ) [ 2 ] . split ( ' . ' ) [ 0 ] ) = = 8 : <nl> + # # # Explicitly use the 10 . 4u ( universal ) SDK when <nl> + # # # building on 10 . 4 , the system headers are not <nl> + # # # useable for a universal build <nl> + # # SDKPATH = \" / Developer / SDKs / MacOSX10 . 4u . sdk \" <nl> + # # else : <nl> + # # SDKPATH = \" / \" <nl> + <nl> SDKPATH = \" / Developer / SDKs / MacOSX10 . 4u . sdk \" <nl> - # SDKPATH = \" / \" <nl> <nl> universal_opts_map = { ' 32 - bit ' : ( ' i386 ' , ' ppc ' , ) , <nl> ' 64 - bit ' : ( ' x86_64 ' , ' ppc64 ' , ) , <nl> - ' all ' : ( ' i386 ' , ' ppc ' , ' x86_64 ' , ' ppc64 ' , ) } <nl> + ' intel ' : ( ' i386 ' , ' x86_64 ' ) , <nl> + ' 3 - way ' : ( ' ppc ' , ' i386 ' , ' x86_64 ' ) , <nl> + ' all ' : ( ' i386 ' , ' ppc ' , ' x86_64 ' , ' ppc64 ' , ) } <nl> + default_target_map = { <nl> + ' 64 - bit ' : ' 10 . 5 ' , <nl> + ' 3 - way ' : ' 10 . 5 ' , <nl> + ' intel ' : ' 10 . 5 ' , <nl> + ' all ' : ' 10 . 5 ' , <nl> + } <nl> <nl> UNIVERSALOPTS = tuple ( universal_opts_map . keys ( ) ) <nl> <nl> def getFullVersion ( ) : <nl> # $ MACOSX_DEPLOYMENT_TARGET - > minimum OS X level <nl> DEPTARGET = ' 10 . 3 ' <nl> <nl> + target_cc_map = { <nl> + ' 10 . 3 ' : ' gcc - 4 . 0 ' , <nl> + ' 10 . 4 ' : ' gcc - 4 . 0 ' , <nl> + ' 10 . 5 ' : ' gcc - 4 . 0 ' , <nl> + ' 10 . 6 ' : ' gcc - 4 . 2 ' , <nl> + } <nl> + <nl> + CC = target_cc_map [ DEPTARGET ] <nl> + <nl> + PYTHON_3 = getVersionTuple ( ) > = ( 3 , 0 ) <nl> + <nl> USAGE = textwrap . dedent ( \" \" \" \\ <nl> Usage : build_python [ options ] <nl> <nl> def getFullVersion ( ) : <nl> # [ The recipes are defined here for convenience but instantiated later after <nl> # command line options have been processed . ] <nl> def library_recipes ( ) : <nl> - return [ <nl> - dict ( <nl> - name = \" Bzip2 1 . 0 . 4 \" , <nl> - url = \" http : / / www . bzip . org / 1 . 0 . 4 / bzip2 - 1 . 0 . 4 . tar . gz \" , <nl> - checksum = ' fc310b254f6ba5fbb5da018f04533688 ' , <nl> - configure = None , <nl> - install = ' make install PREFIX = % s / usr / local / CFLAGS = \" - arch % s - isysroot % s \" ' % ( <nl> - shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> - ' - arch ' . join ( ARCHLIST ) , <nl> - SDKPATH , <nl> + result = [ ] <nl> + <nl> + if DEPTARGET < ' 10 . 5 ' : <nl> + result . extend ( [ <nl> + dict ( <nl> + name = \" Bzip2 1 . 0 . 5 \" , <nl> + url = \" http : / / www . bzip . org / 1 . 0 . 5 / bzip2 - 1 . 0 . 5 . tar . gz \" , <nl> + checksum = ' 3c15a0c8d1d3ee1c46a1634d00617b1a ' , <nl> + configure = None , <nl> + install = ' make install CC = % s PREFIX = % s / usr / local / CFLAGS = \" - arch % s - isysroot % s \" ' % ( <nl> + CC , <nl> + shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> + ' - arch ' . join ( ARCHLIST ) , <nl> + SDKPATH , <nl> + ) , <nl> ) , <nl> - ) , <nl> - dict ( <nl> - name = \" ZLib 1 . 2 . 3 \" , <nl> - url = \" http : / / www . gzip . org / zlib / zlib - 1 . 2 . 3 . tar . gz \" , <nl> - checksum = ' debc62758716a169df9f62e6ab2bc634 ' , <nl> - configure = None , <nl> - install = ' make install prefix = % s / usr / local / CFLAGS = \" - arch % s - isysroot % s \" ' % ( <nl> - shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> - ' - arch ' . join ( ARCHLIST ) , <nl> - SDKPATH , <nl> + dict ( <nl> + name = \" ZLib 1 . 2 . 3 \" , <nl> + url = \" http : / / www . gzip . org / zlib / zlib - 1 . 2 . 3 . tar . gz \" , <nl> + checksum = ' debc62758716a169df9f62e6ab2bc634 ' , <nl> + configure = None , <nl> + install = ' make install CC = % s prefix = % s / usr / local / CFLAGS = \" - arch % s - isysroot % s \" ' % ( <nl> + CC , <nl> + shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> + ' - arch ' . join ( ARCHLIST ) , <nl> + SDKPATH , <nl> + ) , <nl> ) , <nl> - ) , <nl> - dict ( <nl> - # Note that GNU readline is GPL ' d software <nl> - name = \" GNU Readline 5 . 1 . 4 \" , <nl> - url = \" http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 . tar . gz \" , <nl> - checksum = ' 7ee5a692db88b30ca48927a13fd60e46 ' , <nl> - patchlevel = ' 0 ' , <nl> - patches = [ <nl> - # The readline maintainers don ' t do actual micro releases , but <nl> - # just ship a set of patches . <nl> - ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 001 ' , <nl> - ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 002 ' , <nl> - ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 003 ' , <nl> - ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 004 ' , <nl> - ] <nl> - ) , <nl> + dict ( <nl> + # Note that GNU readline is GPL ' d software <nl> + name = \" GNU Readline 5 . 1 . 4 \" , <nl> + url = \" http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 . tar . gz \" , <nl> + checksum = ' 7ee5a692db88b30ca48927a13fd60e46 ' , <nl> + patchlevel = ' 0 ' , <nl> + patches = [ <nl> + # The readline maintainers don ' t do actual micro releases , but <nl> + # just ship a set of patches . <nl> + ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 001 ' , <nl> + ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 002 ' , <nl> + ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 003 ' , <nl> + ' http : / / ftp . gnu . org / pub / gnu / readline / readline - 5 . 1 - patches / readline51 - 004 ' , <nl> + ] <nl> + ) , <nl> + dict ( <nl> + name = \" SQLite 3 . 6 . 11 \" , <nl> + url = \" http : / / www . sqlite . org / sqlite - 3 . 6 . 11 . tar . gz \" , <nl> + checksum = ' 7ebb099696ab76cc6ff65dd496d17858 ' , <nl> + configure_pre = [ <nl> + ' - - enable - threadsafe ' , <nl> + ' - - enable - tempstore ' , <nl> + ' - - enable - shared = no ' , <nl> + ' - - enable - static = yes ' , <nl> + ' - - disable - tcl ' , <nl> + ] <nl> + ) , <nl> + dict ( <nl> + name = \" NCurses 5 . 5 \" , <nl> + url = \" http : / / ftp . gnu . org / pub / gnu / ncurses / ncurses - 5 . 5 . tar . gz \" , <nl> + checksum = ' e73c1ac10b4bfc46db43b2ddfd6244ef ' , <nl> + configure_pre = [ <nl> + \" - - without - cxx \" , <nl> + \" - - without - ada \" , <nl> + \" - - without - progs \" , <nl> + \" - - without - curses - h \" , <nl> + \" - - enable - shared \" , <nl> + \" - - with - shared \" , <nl> + \" - - datadir = / usr / share \" , <nl> + \" - - sysconfdir = / etc \" , <nl> + \" - - sharedstatedir = / usr / com \" , <nl> + \" - - with - terminfo - dirs = / usr / share / terminfo \" , <nl> + \" - - with - default - terminfo - dir = / usr / share / terminfo \" , <nl> + \" - - libdir = / Library / Frameworks / Python . framework / Versions / % s / lib \" % ( getVersion ( ) , ) , <nl> + \" - - enable - termcap \" , <nl> + ] , <nl> + patches = [ <nl> + \" ncurses - 5 . 5 . patch \" , <nl> + ] , <nl> + useLDFlags = False , <nl> + install = ' make & & make install DESTDIR = % s & & cd % s / usr / local / lib & & ln - fs . . / . . / . . / Library / Frameworks / Python . framework / Versions / % s / lib / lib * . ' % ( <nl> + shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> + shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> + getVersion ( ) , <nl> + ) , <nl> + ) , <nl> + ] ) <nl> <nl> + result . extend ( [ <nl> dict ( <nl> - name = \" SQLite 3 . 6 . 11 \" , <nl> - url = \" http : / / www . sqlite . org / sqlite - 3 . 6 . 11 . tar . gz \" , <nl> - checksum = ' 7ebb099696ab76cc6ff65dd496d17858 ' , <nl> + name = \" Sleepycat DB 4 . 7 . 25 \" , <nl> + url = \" http : / / download . oracle . com / berkeley - db / db - 4 . 7 . 25 . tar . gz \" , <nl> + checksum = ' ec2b87e833779681a0c3a814aa71359e ' , <nl> + buildDir = \" build_unix \" , <nl> + configure = \" . . / dist / configure \" , <nl> configure_pre = [ <nl> - ' - - enable - threadsafe ' , <nl> - ' - - enable - tempstore ' , <nl> - ' - - enable - shared = no ' , <nl> - ' - - enable - static = yes ' , <nl> - ' - - disable - tcl ' , <nl> + ' - - includedir = / usr / local / include / db4 ' , <nl> ] <nl> ) , <nl> + ] ) <nl> + <nl> + return result <nl> <nl> - dict ( <nl> - name = \" NCurses 5 . 5 \" , <nl> - url = \" http : / / ftp . gnu . org / pub / gnu / ncurses / ncurses - 5 . 5 . tar . gz \" , <nl> - checksum = ' e73c1ac10b4bfc46db43b2ddfd6244ef ' , <nl> - configure_pre = [ <nl> - \" - - without - cxx \" , <nl> - \" - - without - ada \" , <nl> - \" - - without - progs \" , <nl> - \" - - without - curses - h \" , <nl> - \" - - enable - shared \" , <nl> - \" - - with - shared \" , <nl> - \" - - datadir = / usr / share \" , <nl> - \" - - sysconfdir = / etc \" , <nl> - \" - - sharedstatedir = / usr / com \" , <nl> - \" - - with - terminfo - dirs = / usr / share / terminfo \" , <nl> - \" - - with - default - terminfo - dir = / usr / share / terminfo \" , <nl> - \" - - libdir = / Library / Frameworks / Python . framework / Versions / % s / lib \" % ( getVersion ( ) , ) , <nl> - \" - - enable - termcap \" , <nl> - ] , <nl> - patches = [ <nl> - \" ncurses - 5 . 5 . patch \" , <nl> - ] , <nl> - useLDFlags = False , <nl> - install = ' make & & make install DESTDIR = % s & & cd % s / usr / local / lib & & ln - fs . . / . . / . . / Library / Frameworks / Python . framework / Versions / % s / lib / lib * . ' % ( <nl> - shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> - shellQuote ( os . path . join ( WORKDIR , ' libraries ' ) ) , <nl> - getVersion ( ) , <nl> - ) , <nl> - ) , <nl> - ] <nl> <nl> # Instructions for building packages inside the . mpkg . <nl> - PKG_RECIPES = [ <nl> - dict ( <nl> - name = \" PythonFramework \" , <nl> - long_name = \" Python Framework \" , <nl> - source = \" / Library / Frameworks / Python . framework \" , <nl> - readme = \" \" \" \\ <nl> - This package installs Python . framework , that is the python <nl> - interpreter and the standard library . This also includes Python <nl> - wrappers for lots of Mac OS X API ' s . <nl> - \" \" \" , <nl> - postflight = \" scripts / postflight . framework \" , <nl> - selected = ' selected ' , <nl> - ) , <nl> - dict ( <nl> - name = \" PythonApplications \" , <nl> - long_name = \" GUI Applications \" , <nl> - source = \" / Applications / Python % ( VER ) s \" , <nl> - readme = \" \" \" \\ <nl> - This package installs IDLE ( an interactive Python IDE ) , <nl> - Python Launcher and Build Applet ( create application bundles <nl> - from python scripts ) . <nl> - <nl> - It also installs a number of examples and demos . <nl> - \" \" \" , <nl> - required = False , <nl> - selected = ' selected ' , <nl> - ) , <nl> - dict ( <nl> - name = \" PythonUnixTools \" , <nl> - long_name = \" UNIX command - line tools \" , <nl> - source = \" / usr / local / bin \" , <nl> - readme = \" \" \" \\ <nl> - This package installs the unix tools in / usr / local / bin for <nl> - compatibility with older releases of Python . This package <nl> - is not necessary to use Python . <nl> - \" \" \" , <nl> - required = False , <nl> - selected = ' selected ' , <nl> - ) , <nl> - dict ( <nl> - name = \" PythonDocumentation \" , <nl> - long_name = \" Python Documentation \" , <nl> - topdir = \" / Library / Frameworks / Python . framework / Versions / % ( VER ) s / Resources / English . lproj / Documentation \" , <nl> - source = \" / pydocs \" , <nl> - readme = \" \" \" \\ <nl> - This package installs the python documentation at a location <nl> - that is useable for pydoc and IDLE . If you have installed Xcode <nl> - it will also install a link to the documentation in <nl> - / Developer / Documentation / Python <nl> - \" \" \" , <nl> - postflight = \" scripts / postflight . documentation \" , <nl> - required = False , <nl> - selected = ' selected ' , <nl> - ) , <nl> - dict ( <nl> - name = \" PythonProfileChanges \" , <nl> - long_name = \" Shell profile updater \" , <nl> - readme = \" \" \" \\ <nl> - This packages updates your shell profile to make sure that <nl> - the Python tools are found by your shell in preference of <nl> - the system provided Python tools . <nl> - <nl> - If you don ' t install this package you ' ll have to add <nl> - \" / Library / Frameworks / Python . framework / Versions / % ( VER ) s / bin \" <nl> - to your PATH by hand . <nl> + def pkg_recipes ( ) : <nl> + unselected_for_python3 = ( ' selected ' , ' unselected ' ) [ PYTHON_3 ] <nl> + result = [ <nl> + dict ( <nl> + name = \" PythonFramework \" , <nl> + long_name = \" Python Framework \" , <nl> + source = \" / Library / Frameworks / Python . framework \" , <nl> + readme = \" \" \" \\ <nl> + This package installs Python . framework , that is the python <nl> + interpreter and the standard library . This also includes Python <nl> + wrappers for lots of Mac OS X API ' s . <nl> \" \" \" , <nl> - postflight = \" scripts / postflight . patch - profile \" , <nl> - topdir = \" / Library / Frameworks / Python . framework \" , <nl> - source = \" / empty - dir \" , <nl> - required = False , <nl> - selected = ' unselected ' , <nl> - ) , <nl> - dict ( <nl> - name = \" PythonSystemFixes \" , <nl> - long_name = \" Fix system Python \" , <nl> - readme = \" \" \" \\ <nl> - This package updates the system python installation on <nl> - Mac OS X 10 . 3 to ensure that you can build new python extensions <nl> - using that copy of python after installing this version . <nl> - \" \" \" , <nl> - postflight = \" . . / Tools / fixapplepython23 . py \" , <nl> - topdir = \" / Library / Frameworks / Python . framework \" , <nl> - source = \" / empty - dir \" , <nl> - required = False , <nl> - selected = ' unselected ' , <nl> - ) <nl> - ] <nl> + postflight = \" scripts / postflight . framework \" , <nl> + selected = ' selected ' , <nl> + ) , <nl> + dict ( <nl> + name = \" PythonApplications \" , <nl> + long_name = \" GUI Applications \" , <nl> + source = \" / Applications / Python % ( VER ) s \" , <nl> + readme = \" \" \" \\ <nl> + This package installs IDLE ( an interactive Python IDE ) , <nl> + Python Launcher and Build Applet ( create application bundles <nl> + from python scripts ) . <nl> + <nl> + It also installs a number of examples and demos . <nl> + \" \" \" , <nl> + required = False , <nl> + selected = ' selected ' , <nl> + ) , <nl> + dict ( <nl> + name = \" PythonUnixTools \" , <nl> + long_name = \" UNIX command - line tools \" , <nl> + source = \" / usr / local / bin \" , <nl> + readme = \" \" \" \\ <nl> + This package installs the unix tools in / usr / local / bin for <nl> + compatibility with older releases of Python . This package <nl> + is not necessary to use Python . <nl> + \" \" \" , <nl> + required = False , <nl> + selected = ' selected ' , <nl> + ) , <nl> + dict ( <nl> + name = \" PythonDocumentation \" , <nl> + long_name = \" Python Documentation \" , <nl> + topdir = \" / Library / Frameworks / Python . framework / Versions / % ( VER ) s / Resources / English . lproj / Documentation \" , <nl> + source = \" / pydocs \" , <nl> + readme = \" \" \" \\ <nl> + This package installs the python documentation at a location <nl> + that is useable for pydoc and IDLE . If you have installed Xcode <nl> + it will also install a link to the documentation in <nl> + / Developer / Documentation / Python <nl> + \" \" \" , <nl> + postflight = \" scripts / postflight . documentation \" , <nl> + required = False , <nl> + selected = ' selected ' , <nl> + ) , <nl> + dict ( <nl> + name = \" PythonProfileChanges \" , <nl> + long_name = \" Shell profile updater \" , <nl> + readme = \" \" \" \\ <nl> + This packages updates your shell profile to make sure that <nl> + the Python tools are found by your shell in preference of <nl> + the system provided Python tools . <nl> + <nl> + If you don ' t install this package you ' ll have to add <nl> + \" / Library / Frameworks / Python . framework / Versions / % ( VER ) s / bin \" <nl> + to your PATH by hand . <nl> + \" \" \" , <nl> + postflight = \" scripts / postflight . patch - profile \" , <nl> + topdir = \" / Library / Frameworks / Python . framework \" , <nl> + source = \" / empty - dir \" , <nl> + required = False , <nl> + selected = unselected_for_python3 , <nl> + ) , <nl> + ] <nl> + <nl> + if DEPTARGET < ' 10 . 4 ' : <nl> + result . append ( <nl> + dict ( <nl> + name = \" PythonSystemFixes \" , <nl> + long_name = \" Fix system Python \" , <nl> + readme = \" \" \" \\ <nl> + This package updates the system python installation on <nl> + Mac OS X 10 . 3 to ensure that you can build new python extensions <nl> + using that copy of python after installing this version . <nl> + \" \" \" , <nl> + postflight = \" . . / Tools / fixapplepython23 . py \" , <nl> + topdir = \" / Library / Frameworks / Python . framework \" , <nl> + source = \" / empty - dir \" , <nl> + required = False , <nl> + selected = unselected_for_python3 , <nl> + ) <nl> + ) <nl> + return result <nl> <nl> def fatal ( msg ) : <nl> \" \" \" <nl> def checkEnvironment ( ) : <nl> \" \" \" <nl> <nl> if platform . system ( ) ! = ' Darwin ' : <nl> - fatal ( \" This script should be run on a Mac OS X 10 . 4 system \" ) <nl> + fatal ( \" This script should be run on a Mac OS X 10 . 4 ( or later ) system \" ) <nl> <nl> - if platform . release ( ) < = ' 8 . ' : <nl> - fatal ( \" This script should be run on a Mac OS X 10 . 4 system \" ) <nl> + if int ( platform . release ( ) . split ( ' . ' ) [ 0 ] ) < 8 : <nl> + fatal ( \" This script should be run on a Mac OS X 10 . 4 ( or later ) system \" ) <nl> <nl> if not os . path . exists ( SDKPATH ) : <nl> fatal ( \" Please install the latest version of Xcode and the % s SDK \" % ( <nl> def parseOptions ( args = None ) : <nl> Parse arguments and update global settings . <nl> \" \" \" <nl> global WORKDIR , DEPSRC , SDKPATH , SRCDIR , DEPTARGET <nl> - global UNIVERSALOPTS , UNIVERSALARCHS , ARCHLIST <nl> + global UNIVERSALOPTS , UNIVERSALARCHS , ARCHLIST , CC <nl> <nl> if args is None : <nl> args = sys . argv [ 1 : ] <nl> def parseOptions ( args = None ) : <nl> print \" Additional arguments \" <nl> sys . exit ( 1 ) <nl> <nl> + deptarget = None <nl> for k , v in options : <nl> if k in ( ' - h ' , ' - ? ' , ' - - help ' ) : <nl> print USAGE <nl> def parseOptions ( args = None ) : <nl> <nl> elif k in ( ' - - dep - target ' , ) : <nl> DEPTARGET = v <nl> + deptarget = v <nl> <nl> elif k in ( ' - - universal - archs ' , ) : <nl> if v in UNIVERSALOPTS : <nl> UNIVERSALARCHS = v <nl> ARCHLIST = universal_opts_map [ UNIVERSALARCHS ] <nl> + if deptarget is None : <nl> + # Select alternate default deployment <nl> + # target <nl> + DEPTARGET = default_target_map . get ( v , ' 10 . 3 ' ) <nl> else : <nl> raise NotImplementedError , v <nl> <nl> def parseOptions ( args = None ) : <nl> SDKPATH = os . path . abspath ( SDKPATH ) <nl> DEPSRC = os . path . abspath ( DEPSRC ) <nl> <nl> + CC = target_cc_map [ DEPTARGET ] <nl> + <nl> print \" Settings : \" <nl> print \" * Source directory : \" , SRCDIR <nl> print \" * Build directory : \" , WORKDIR <nl> def parseOptions ( args = None ) : <nl> print \" * Third - party source : \" , DEPSRC <nl> print \" * Deployment target : \" , DEPTARGET <nl> print \" * Universal architectures : \" , ARCHLIST <nl> + print \" * C compiler : \" , CC <nl> print \" \" <nl> <nl> <nl> def buildPythonDocs ( ) : <nl> runCommand ( ' make update ' ) <nl> runCommand ( ' make html ' ) <nl> os . chdir ( curDir ) <nl> - if os . path . exists ( docdir ) : <nl> - os . rmdir ( docdir ) <nl> + if not os . path . exists ( docdir ) : <nl> + os . mkdir ( docdir ) <nl> os . rename ( os . path . join ( buildDir , ' build ' , ' html ' ) , docdir ) <nl> <nl> <nl> def buildPython ( ) : <nl> ' libraries ' , ' usr ' , ' local ' , ' lib ' ) <nl> print \" Running configure . . . \" <nl> runCommand ( \" % s - C - - enable - framework - - enable - universalsdk = % s \" <nl> - \" - - with - universal - archs = % s - - with - computed - gotos \" <nl> + \" - - with - universal - archs = % s \" <nl> + \" % s \" <nl> \" LDFLAGS = ' - g - L % s / libraries / usr / local / lib ' \" <nl> \" OPT = ' - g - O3 - I % s / libraries / usr / local / include ' 2 > & 1 \" % ( <nl> shellQuote ( os . path . join ( SRCDIR , ' configure ' ) ) , shellQuote ( SDKPATH ) , <nl> UNIVERSALARCHS , <nl> + ( ' ' , ' - - with - computed - gotos ' ) [ PYTHON_3 ] , <nl> shellQuote ( WORKDIR ) [ 1 : - 1 ] , <nl> shellQuote ( WORKDIR ) [ 1 : - 1 ] ) ) <nl> <nl> print \" Running make \" <nl> runCommand ( \" make \" ) <nl> <nl> - print \" Running make frameworkinstall \" <nl> + print \" Running make install \" <nl> runCommand ( \" make install DESTDIR = % s \" % ( <nl> shellQuote ( rootDir ) ) ) <nl> <nl> def buildPython ( ) : <nl> frmDir = os . path . join ( rootDir , ' Library ' , ' Frameworks ' , ' Python . framework ' ) <nl> gid = grp . getgrnam ( ' admin ' ) . gr_gid <nl> <nl> - <nl> - <nl> for dirpath , dirnames , filenames in os . walk ( frmDir ) : <nl> for dn in dirnames : <nl> os . chmod ( os . path . join ( dirpath , dn ) , 0775 ) <nl> def buildPython ( ) : <nl> <nl> os . chdir ( curdir ) <nl> <nl> - # Remove the ' Current ' link , that way we don ' t accidently mess with an already installed <nl> - # version of python <nl> - os . unlink ( os . path . join ( rootDir , ' Library ' , ' Frameworks ' , ' Python . framework ' , ' Versions ' , ' Current ' ) ) <nl> - <nl> - <nl> - <nl> + if PYTHON_3 : <nl> + # Remove the ' Current ' link , that way we don ' t accidently mess <nl> + # with an already installed version of python 2 <nl> + os . unlink ( os . path . join ( rootDir , ' Library ' , ' Frameworks ' , <nl> + ' Python . framework ' , ' Versions ' , ' Current ' ) ) <nl> <nl> def patchFile ( inPath , outPath ) : <nl> data = fileContents ( inPath ) <nl> def makeMpkgPlist ( path ) : <nl> IFPkgFlagPackageList = [ <nl> dict ( <nl> IFPkgFlagPackageLocation = ' % s - % s . pkg ' % ( item [ ' name ' ] , getVersion ( ) ) , <nl> - IFPkgFlagPackageSelection = item [ ' selected ' ] , <nl> + IFPkgFlagPackageSelection = item . get ( ' selected ' , ' selected ' ) , <nl> ) <nl> - for item in PKG_RECIPES <nl> + for item in pkg_recipes ( ) <nl> ] , <nl> IFPkgFormatVersion = 0 . 10000000149011612 , <nl> IFPkgFlagBackgroundScaling = \" proportional \" , <nl> def buildInstaller ( ) : <nl> pkgroot = os . path . join ( outdir , ' Python . mpkg ' , ' Contents ' ) <nl> pkgcontents = os . path . join ( pkgroot , ' Packages ' ) <nl> os . makedirs ( pkgcontents ) <nl> - for recipe in PKG_RECIPES : <nl> + for recipe in pkg_recipes ( ) : <nl> packageFromRecipe ( pkgcontents , recipe ) <nl> <nl> rsrcDir = os . path . join ( pkgroot , ' Resources ' ) <nl> def buildDMG ( ) : <nl> shutil . rmtree ( outdir ) <nl> <nl> imagepath = os . path . join ( outdir , <nl> - ' python - % s - macosx ' % ( getFullVersion ( ) , ) ) <nl> + ' python - % s - macosx % s ' % ( getFullVersion ( ) , DEPTARGET ) ) <nl> if INCLUDE_TIMESTAMP : <nl> - imagepath = imagepath + ' % 04d - % 02d - % 02d ' % ( time . localtime ( ) [ : 3 ] ) <nl> + imagepath = imagepath + ' - % 04d - % 02d - % 02d ' % ( time . localtime ( ) [ : 3 ] ) <nl> imagepath = imagepath + ' . dmg ' <nl> <nl> os . mkdir ( outdir ) <nl> def main ( ) : <nl> checkEnvironment ( ) <nl> <nl> os . environ [ ' MACOSX_DEPLOYMENT_TARGET ' ] = DEPTARGET <nl> + os . environ [ ' CC ' ] = CC <nl> <nl> if os . path . exists ( WORKDIR ) : <nl> shutil . rmtree ( WORKDIR ) <nl> def main ( ) : <nl> print > > fp , \" # By : \" , pwd . getpwuid ( os . getuid ( ) ) . pw_gecos <nl> fp . close ( ) <nl> <nl> - <nl> - <nl> # And copy it to a DMG <nl> buildDMG ( ) <nl> <nl> - <nl> if __name__ = = \" __main__ \" : <nl> main ( ) <nl>\n", "msg": "update mac installer script from the trunk\n"}
{"diff_id": 8745, "repo": "explosion/spaCy\n", "sha": "134d933d67c1515ac5bb4b2d8a57339f04c5ec41\n", "time": "2020-08-09T13:19:28Z\n", "diff": "mmm a / spacy / pipeline / entity_linker . py <nl> ppp b / spacy / pipeline / entity_linker . py <nl> def make_entity_linker ( <nl> incl_prior : bool , <nl> incl_context : bool , <nl> ) : <nl> + \" \" \" Construct an EntityLinker component . <nl> + <nl> + model ( Model [ List [ Doc ] , Floats2d ] ) : A model that learns document vector <nl> + representations . Given a batch of Doc objects , it should return a single <nl> + array , with one row per item in the batch . <nl> + kb ( KnowledgeBase ) : The knowledge - base to link entities to . <nl> + labels_discard ( Iterable [ str ] ) : NER labels that will automatically get a \" NIL \" prediction . <nl> + incl_prior ( bool ) : Whether or not to include prior probabilities from the KB in the model . <nl> + incl_context ( bool ) : Whether or not to include the local context in the model . <nl> + \" \" \" <nl> return EntityLinker ( <nl> nlp . vocab , <nl> model , <nl>\n", "msg": "Add docstring for entity linker factory\n"}
{"diff_id": 8754, "repo": "ansible/ansible\n", "sha": "cfe7de10ef9c6c1d8d5be71993e5f96ace58953d\n", "time": "2018-05-22T02:29:14Z\n", "diff": "mmm a / lib / ansible / release . py <nl> ppp b / lib / ansible / release . py <nl> <nl> from __future__ import ( absolute_import , division , print_function ) <nl> __metaclass__ = type <nl> <nl> - __version__ = ' 2 . 6 . 0a1 ' <nl> + __version__ = ' 2 . 6 . 0dev0 ' <nl> __author__ = ' Ansible , Inc . ' <nl> __codename__ = ' Heartbreaker ' <nl>\n", "msg": "Update Ansible release version to 2 . 6 . 0dev0 .\n"}
{"diff_id": 8822, "repo": "ansible/ansible\n", "sha": "c64eed16fe593cc3685b0fff3a79c9a239b884b3\n", "time": "2017-11-20T17:09:11Z\n", "diff": "mmm a / lib / ansible / modules / web_infrastructure / letsencrypt . py <nl> ppp b / lib / ansible / modules / web_infrastructure / letsencrypt . py <nl> <nl> description : <nl> - \" URI to a terms of service document you agree to when using the <nl> ACME service at C ( acme_directory ) . \" <nl> - default : ' https : / / letsencrypt . org / documents / LE - SA - v1 . 1 . 1 - August - 1 - 2016 . pdf ' <nl> + - Default is latest gathered from C ( acme_directory ) URL . <nl> challenge : <nl> description : The challenge to be performed . <nl> choices : [ ' http - 01 ' , ' dns - 01 ' , ' tls - sni - 02 ' ] <nl> class ACMEAccount ( object ) : <nl> ' ' ' <nl> def __init__ ( self , module ) : <nl> self . module = module <nl> - self . agreement = module . params [ ' agreement ' ] <nl> self . key = module . params [ ' account_key ' ] <nl> self . email = module . params [ ' account_email ' ] <nl> self . data = module . params [ ' data ' ] <nl> self . directory = ACMEDirectory ( module ) <nl> + self . agreement = module . params [ ' agreement ' ] or self . directory [ ' meta ' ] [ ' terms - of - service ' ] <nl> + <nl> self . uri = None <nl> self . changed = False <nl> <nl> def main ( ) : <nl> account_key = dict ( required = True , type = ' path ' ) , <nl> account_email = dict ( required = False , default = None , type = ' str ' ) , <nl> acme_directory = dict ( required = False , default = ' https : / / acme - staging . api . letsencrypt . org / directory ' , type = ' str ' ) , <nl> - agreement = dict ( required = False , default = ' https : / / letsencrypt . org / documents / LE - SA - v1 . 1 . 1 - August - 1 - 2016 . pdf ' , type = ' str ' ) , <nl> + agreement = dict ( required = False , type = ' str ' ) , <nl> challenge = dict ( required = False , default = ' http - 01 ' , choices = [ ' http - 01 ' , ' dns - 01 ' , ' tls - sni - 02 ' ] , type = ' str ' ) , <nl> csr = dict ( required = True , aliases = [ ' src ' ] , type = ' path ' ) , <nl> data = dict ( required = False , no_log = True , default = None , type = ' dict ' ) , <nl>\n", "msg": "letsencrypt : update agreement default to newest gathered\n"}
{"diff_id": 8874, "repo": "scrapy/scrapy\n", "sha": "f8f2f3a542e6bca10d6d894544d767e8af2633f6\n", "time": "2008-09-29T13:21:35Z\n", "diff": "mmm a / scrapy / trunk / scrapy / utils / markup . py <nl> ppp b / scrapy / trunk / scrapy / utils / markup . py <nl> <nl> import re <nl> import htmlentitydefs <nl> <nl> - ent_re = re . compile ( r ' & ( # ? ) ( [ ^ & ; ] + ) ; ' ) <nl> + _ent_re = re . compile ( r ' & ( # ? ) ( [ ^ & ; ] + ) ; ' ) <nl> _tag_re = re . compile ( r ' < [ a - zA - Z \\ / ! ] . * ? > ' , re . DOTALL ) <nl> <nl> def remove_entities ( text , keep = ( ) , remove_illegal = True ) : <nl> def convert_entity ( m ) : <nl> else : <nl> return u ' & % s ; ' % m . group ( 2 ) <nl> <nl> - return ent_re . sub ( convert_entity , text . decode ( ' utf - 8 ' ) ) <nl> + return _ent_re . sub ( convert_entity , text . decode ( ' utf - 8 ' ) ) <nl> <nl> + def has_entities ( text ) : <nl> + return bool ( _ent_re . search ( text ) ) <nl> <nl> def replace_tags ( text , token = ' ' ) : <nl> \" \" \" Replace all markup tags found in the given text by the given token . By <nl>\n", "msg": "rolled back public ent_re to private and added a function has_entities instead\n"}
{"diff_id": 9073, "repo": "zulip/zulip\n", "sha": "fe98a59f5cc203bd84e2cf577e5303ec5e8d553a\n", "time": "2018-01-31T12:30:54Z\n", "diff": "mmm a / zerver / lib / bugdown / api_code_examples . py <nl> ppp b / zerver / lib / bugdown / api_code_examples . py <nl> <nl> <nl> \" \" \" <nl> <nl> + PYTHON_CLIENT_ADMIN_CONFIG = \" \" \" <nl> + # ! / usr / bin / env python <nl> + <nl> + import zulip <nl> + <nl> + # You need a zuliprc - admin with administrator credentials <nl> + client = zulip . Client ( config_file = \" ~ / zuliprc - admin \" ) <nl> + <nl> + \" \" \" <nl> + <nl> <nl> class APICodeExamplesGenerator ( Extension ) : <nl> def extendMarkdown ( self , md : markdown . Markdown , md_globals : Dict [ str , Any ] ) - > None : <nl> def run ( self , lines : List [ str ] ) - > List [ str ] : <nl> text = self . render_fixture ( function ) <nl> elif key = = ' method ' : <nl> text = self . render_code_example ( function ) <nl> + elif key = = ' method ( admin_config = True ) ' : <nl> + text = self . render_code_example ( function , admin_config = True ) <nl> <nl> # The line that contains the directive to include the macro <nl> # may be preceded or followed by text or tags , in that case <nl> def render_fixture ( self , function : str ) - > List [ str ] : <nl> <nl> return fixture <nl> <nl> - def render_code_example ( self , function : str ) - > List [ str ] : <nl> + def render_code_example ( self , function : str , admin_config : Optional [ bool ] = False ) - > List [ str ] : <nl> method = zerver . lib . api_test_helpers . TEST_FUNCTIONS [ function ] <nl> function_source_lines = inspect . getsourcelines ( method ) [ 0 ] <nl> ce_regex = re . compile ( r ' \\ # \\ { code_example \\ | \\ s * ( . + ? ) \\ s * \\ } ' ) <nl> def render_code_example ( self , function : str ) - > List [ str ] : <nl> elif match . group ( 1 ) = = ' end ' : <nl> end = function_source_lines . index ( line ) <nl> <nl> - config = PYTHON_CLIENT_CONFIG_LINES . splitlines ( ) <nl> + if admin_config : <nl> + config = PYTHON_CLIENT_ADMIN_CONFIG . splitlines ( ) <nl> + else : <nl> + config = PYTHON_CLIENT_CONFIG_LINES . splitlines ( ) <nl> + <nl> snippet = function_source_lines [ start + 1 : end ] <nl> <nl> code_example = [ ] <nl>\n", "msg": "bugdown / api_code_examples : Add macro for admin zuliprc lines .\n"}
{"diff_id": 9098, "repo": "python/cpython\n", "sha": "a4265546f9d457ff61e7e4d3fb6d6b3cc54a09d2\n", "time": "2011-08-05T04:34:52Z\n", "diff": "mmm a / Lib / test / test_sysconfig . py <nl> ppp b / Lib / test / test_sysconfig . py <nl> def test_platform_in_subprocess ( self ) : <nl> env = os . environ . copy ( ) <nl> env [ ' MACOSX_DEPLOYMENT_TARGET ' ] = ' 10 . 1 ' <nl> <nl> - p = subprocess . Popen ( [ <nl> - sys . executable , ' - c ' , <nl> - ' import sysconfig ; print ( sysconfig . get_platform ( ) ) ' , <nl> - ] , <nl> - stdout = subprocess . PIPE , <nl> - stderr = open ( ' / dev / null ' ) , <nl> - env = env ) <nl> - test_platform = p . communicate ( ) [ 0 ] . strip ( ) <nl> - test_platform = test_platform . decode ( ' utf - 8 ' ) <nl> - status = p . wait ( ) <nl> + with open ( ' / dev / null ' ) as dev_null : <nl> + p = subprocess . Popen ( [ <nl> + sys . executable , ' - c ' , <nl> + ' import sysconfig ; print ( sysconfig . get_platform ( ) ) ' , <nl> + ] , <nl> + stdout = subprocess . PIPE , <nl> + stderr = dev_null , <nl> + env = env ) <nl> + test_platform = p . communicate ( ) [ 0 ] . strip ( ) <nl> + test_platform = test_platform . decode ( ' utf - 8 ' ) <nl> + status = p . wait ( ) <nl> <nl> - self . assertEqual ( status , 0 ) <nl> - self . assertEqual ( my_platform , test_platform ) <nl> + self . assertEqual ( status , 0 ) <nl> + self . assertEqual ( my_platform , test_platform ) <nl> <nl> <nl> class MakefileTests ( unittest . TestCase ) : <nl>\n", "msg": "Explicitly close a file to stop raising a ResourceWarning .\n"}
{"diff_id": 9181, "repo": "python/cpython\n", "sha": "5749d88243cc9618dff9f829e3084d7e2b736c65\n", "time": "2010-02-06T20:00:43Z\n", "diff": "mmm a / Lib / test / test_logging . py <nl> ppp b / Lib / test / test_logging . py <nl> def setup_via_listener ( self , text ) : <nl> logging . config . stopListening ( ) <nl> t . join ( 2 . 0 ) <nl> <nl> - @ unittest . skip ( \" See issue # 7857 \" ) <nl> + # @ unittest . skip ( \" See issue # 7857 \" ) <nl> def test_listen_config_10_ok ( self ) : <nl> with captured_stdout ( ) as output : <nl> self . setup_via_listener ( json . dumps ( self . config10 ) ) <nl>\n", "msg": "Issue : Tentatively re - enabling one test to see effect on buildbots .\n"}
{"diff_id": 9194, "repo": "matplotlib/matplotlib\n", "sha": "0c6540ba8e036889aec5af85fad75dbd6e4798ad\n", "time": "2013-07-26T13:07:41Z\n", "diff": "mmm a / distribute_setup . py <nl> ppp b / distribute_setup . py <nl> def quote ( arg ) : <nl> args = [ quote ( arg ) for arg in args ] <nl> return os . spawnl ( os . P_WAIT , sys . executable , * args ) = = 0 <nl> <nl> + MINIMUM_VERSION = \" 0 . 6 . 28 \" <nl> DEFAULT_VERSION = \" 0 . 6 . 45 \" <nl> DEFAULT_URL = \" http : / / pypi . python . org / packages / source / d / distribute / \" <nl> SETUPTOOLS_FAKED_VERSION = \" 0 . 6c11 \" <nl> def _do_download ( version , download_base , to_dir , download_delay ) : <nl> setuptools . bootstrap_install_from = egg <nl> <nl> <nl> - def use_setuptools ( version = DEFAULT_VERSION , download_base = DEFAULT_URL , <nl> + def use_setuptools ( version = MINIMUM_VERSION , download_base = DEFAULT_URL , <nl> to_dir = os . curdir , download_delay = 15 , no_fake = True ) : <nl> # making sure we use the absolute path <nl> to_dir = os . path . abspath ( to_dir ) <nl>\n", "msg": "Allow for a minimum distribute version that is older than the downloaded version\n"}
{"diff_id": 9230, "repo": "ansible/ansible\n", "sha": "3a5522a22cfe74e74cb967b172cb8e9eefb45ff0\n", "time": "2015-08-24T16:32:11Z\n", "diff": "mmm a / lib / ansible / plugins / connections / docker . py <nl> ppp b / lib / ansible / plugins / connections / docker . py <nl> def normalize ( v ) : <nl> <nl> def _connect ( self , port = None ) : <nl> \" \" \" Connect to the container . Nothing to do \" \" \" <nl> + if not self . _connected : <nl> + self . _display . vvv ( \" ESTABLISH LOCAL CONNECTION FOR USER : { 0 } \" . format ( <nl> + self . _play_context . remote_user , host = self . _play_context . remote_addr ) <nl> + ) <nl> + self . _connected = True <nl> + <nl> return self <nl> <nl> def exec_command ( self , cmd , tmp_path , sudo_user = None , sudoable = False , <nl> def fetch_file ( self , in_path , out_path ) : <nl> <nl> def close ( self ) : <nl> \" \" \" Terminate the connection . Nothing to do for Docker \" \" \" <nl> - pass <nl> + self . _connected = False <nl>\n", "msg": "fake being connected for logging purposes\n"}
{"diff_id": 9297, "repo": "zulip/zulip\n", "sha": "bff503feb4a1d07ad994bc13d061010278233999\n", "time": "2020-10-30T18:42:40Z\n", "diff": "new file mode 100644 <nl> index 000000000000 . . edc49e7e709f <nl> mmm / dev / null <nl> ppp b / zerver / management / commands / delete_realm . py <nl> <nl> + import os <nl> + import subprocess <nl> + from argparse import ArgumentParser <nl> + from typing import Any <nl> + <nl> + from django . conf import settings <nl> + from django . core . management . base import CommandError <nl> + <nl> + from zerver . lib . management import ZulipBaseCommand <nl> + from zerver . models import Message , UserProfile <nl> + <nl> + <nl> + class Command ( ZulipBaseCommand ) : <nl> + help = \" \" \" Script to permanently delete a realm . Recommended only for removing <nl> + realms used for testing ; consider using deactivate_realm instead . \" \" \" <nl> + <nl> + def add_arguments ( self , parser : ArgumentParser ) - > None : <nl> + self . add_realm_args ( parser , True ) <nl> + <nl> + def handle ( self , * args : Any , * * options : str ) - > None : <nl> + realm = self . get_realm ( options ) <nl> + assert realm is not None # Should be ensured by parser <nl> + <nl> + user_count = UserProfile . objects . filter ( <nl> + realm_id = realm . id , <nl> + is_active = True , <nl> + is_bot = False , <nl> + ) . count ( ) <nl> + <nl> + message_count = Message . objects . filter ( <nl> + sender__realm = realm <nl> + ) . count ( ) <nl> + <nl> + print ( f \" This realm has { user_count } users and { message_count } messages . \\ n \" ) <nl> + <nl> + print ( \" This command will \\ 033 [ 91mPERMANENTLY DELETE \\ 033 [ 0m all data for this realm . \" <nl> + \" Most use cases will be better served by scrub_realm and / or deactivate_realm . \" ) <nl> + <nl> + confirmation = input ( \" Type the name of the realm to confirm : \" ) <nl> + if confirmation ! = realm . string_id : <nl> + raise CommandError ( \" Aborting ! \" ) <nl> + <nl> + # TODO : This approach leaks Recipient and Huddle objects , <nl> + # because those don ' t have a foreign key to the Realm or any <nl> + # other model it cascades to ( Realm / Stream / UserProfile / etc . ) . <nl> + realm . delete ( ) <nl> + subprocess . check_call ( [ os . path . join ( settings . DEPLOY_ROOT , \" scripts \" , \" setup \" , \" flush - memcached \" ) ] ) <nl> + <nl> + print ( \" Realm has been successfully permanently deleted . \" ) <nl>\n", "msg": "delete_realm : Add command to completely remove realms .\n"}
{"diff_id": 9335, "repo": "ipython/ipython\n", "sha": "20aabaa37f8ce4b29cae40b9c9bd1328682a2239\n", "time": "2012-06-02T06:51:20Z\n", "diff": "mmm a / IPython / extensions / rmagic . py <nl> ppp b / IPython / extensions / rmagic . py <nl> <nl> class RMagicError ( ri . RRuntimeError ) : <nl> pass <nl> <nl> + def Rconverter ( Robj ) : <nl> + \" \" \" <nl> + Convert an object in R ' s namespace to one suitable <nl> + for ipython ' s namespace . <nl> + <nl> + For a data . frame , it tries to return a structured array . <nl> + <nl> + Parameters <nl> + mmmmmmmmm - <nl> + <nl> + Robj : an R object returned from rpy2 <nl> + \" \" \" <nl> + if is_data_frame ( Robj ) : <nl> + names = np . array ( Robj . do_slot ( ' names ' ) ) <nl> + Robj = np . rec . fromarrays ( Robj , names = tuple ( names ) ) <nl> + return np . asarray ( Robj ) <nl> + <nl> + is_data_frame = None <nl> + <nl> @ magics_class <nl> class RMagics ( Magics ) : <nl> \" \" \" A set of magics useful for interactive work with R via rpy2 . <nl> \" \" \" <nl> <nl> - def __init__ ( self , shell , Rconverter = np . asarray , <nl> + def __init__ ( self , shell , Rconverter = Rconverter , <nl> pyconverter = np . asarray , <nl> cache_display_data = False ) : <nl> \" \" \" <nl> def __init__ ( self , shell , Rconverter = np . asarray , <nl> <nl> shell : IPython shell <nl> <nl> - Rconverter : callable <nl> - To be called on return values from R before returning <nl> - to ipython . <nl> - <nl> pyconverter : callable <nl> To be called on values in ipython namespace before <nl> assigning to variables in rpy2 . <nl> def __init__ ( self , shell , Rconverter = np . asarray , <nl> self . cache_display_data = cache_display_data <nl> <nl> self . r = ro . R ( ) <nl> + global is_data_frame <nl> + is_data_frame = self . r ( ' is . data . frame ' ) <nl> + <nl> self . Rstdout_cache = [ ] <nl> - self . Rconverter = Rconverter <nl> self . pyconverter = pyconverter <nl> + self . Rconverter = Rconverter <nl> <nl> def eval ( self , line ) : <nl> ' ' ' <nl> def Rpull ( self , line ) : <nl> ' ' ' <nl> outputs = line . split ( ' ' ) <nl> for output in outputs : <nl> - if self . r ( \" is . data . frame ( { 0 } ) \" . format ( output ) ) [ 0 ] : <nl> - o = np . rec . fromarrays ( self . r ( output ) , names = tuple ( self . r ( output ) . names ) ) <nl> - self . shell . push ( { output : self . Rconverter ( o ) } ) <nl> - else : <nl> - self . shell . push ( { output : self . Rconverter ( self . r ( output ) ) } ) <nl> + self . shell . push ( { output : self . Rconverter ( self . r ( output ) ) } ) <nl> <nl> <nl> @ skip_doctest <nl> def R ( self , line , cell = None ) : <nl> <nl> if args . output : <nl> for output in ' , ' . join ( args . output ) . split ( ' , ' ) : <nl> - if self . r ( \" is . data . frame ( { 0 } ) \" . format ( output ) ) [ 0 ] : <nl> - o = np . rec . fromarrays ( self . r ( output ) , names = tuple ( self . r ( output ) . names ) ) <nl> - self . shell . push ( { output : self . Rconverter ( o ) } ) <nl> - else : <nl> - self . shell . push ( { output : self . Rconverter ( self . r ( output ) ) } ) <nl> + self . shell . push ( { output : self . Rconverter ( self . r ( output ) ) } ) <nl> <nl> for tag , disp_d in display_data : <nl> publish_display_data ( tag , disp_d ) <nl>\n", "msg": "fixed Rconverter a little to catch return values from % R\n"}
{"diff_id": 9348, "repo": "numpy/numpy\n", "sha": "377ade5f3a2d2b780743b8992d29b0e6ba037f53\n", "time": "2009-07-23T00:37:08Z\n", "diff": "mmm a / numpy / doc / subclassing . py <nl> ppp b / numpy / doc / subclassing . py <nl> <nl> <nl> Introduction <nl> mmmmmmmmmmmm <nl> - Subclassing ndarray is relatively simple , but you will need to <nl> - understand some behavior of ndarrays to understand some minor <nl> - complications to subclassing . There are examples at the bottom of the <nl> - page , but you will probably want to read the background to understand <nl> - why subclassing works as it does . <nl> + <nl> + Subclassing ndarray is relatively simple , but it has some complications <nl> + compared to other Python objects . On this page we explain the machinery <nl> + that allows you to subclass ndarray , and the implications for <nl> + implementing a subclass . <nl> <nl> ndarrays and object creation <nl> = = = = = = = = = = = = = = = = = = = = = = = = = = = = <nl> <nl> - The creation of ndarrays is complicated by the need to return views of <nl> - ndarrays , that are also ndarrays . Views can come about in two ways . <nl> - First , they can be created directly with a call to the ` ` view ` ` method : <nl> - <nl> - . . testcode : : <nl> + Subclassing ndarray is complicated by the fact that new instances of <nl> + ndarray classes can come about in three different ways . These are : <nl> <nl> - import numpy as np <nl> - # create a completely useless ndarray subclass <nl> - class C ( np . ndarray ) : pass <nl> - # create a standard ndarray <nl> - arr = np . zeros ( ( 3 , ) ) <nl> - # take a view of it , as our useless subclass <nl> - c_arr = arr . view ( C ) <nl> - print type ( c_arr ) <nl> + # . Explicit constructor call - as in ` ` MySubClass ( params ) ` ` . This is <nl> + the usual route to Python instance creation . <nl> + # . View casting - casting an existing ndarray as a given subclass <nl> + # . Slicing an ndarray instance <nl> <nl> - giving the following output <nl> + The last two are particular features of ndarray , and the complications <nl> + of subclassing ndarray are due to the need to support these latter two <nl> + routes of instance creation . <nl> <nl> - . . testoutput : : <nl> + . . _view - casting : <nl> <nl> - < class ' C ' > <nl> + View casting <nl> + mmmmmmmmmmmm <nl> <nl> - Views can also come about by taking slices of subclassed arrays . For example : <nl> + * View casting * is the standard ndarray mechanism by which you take an <nl> + ndarray of any subclass , and return a view of the array as another <nl> + ( specified ) subclass : <nl> <nl> - . . testcode : : <nl> + > > > import numpy as np <nl> + > > > # create a completely useless ndarray subclass <nl> + > > > class C ( np . ndarray ) : pass <nl> + > > > # create a standard ndarray <nl> + > > > arr = np . zeros ( ( 3 , ) ) <nl> + > > > # take a view of it , as our useless subclass <nl> + > > > c_arr = arr . view ( C ) <nl> + > > > type ( c_arr ) <nl> + < class ' C ' > <nl> <nl> - v = c_arr [ 1 : ] <nl> - print type ( v ) <nl> - print v is c_arr <nl> + . . _instance - slicing : <nl> <nl> - giving : <nl> + Array slicing <nl> + mmmmmmmmmmmm - <nl> <nl> - . . testoutput : : <nl> + New instances of an ndarray subclass can also come about by taking <nl> + slices of subclassed arrays . For example : <nl> <nl> - < class ' C ' > <nl> - False <nl> + > > > v = c_arr [ 1 : ] <nl> + > > > type ( v ) # the view is of type ' C ' <nl> + < class ' C ' > <nl> + > > > v is c_arr # but it ' s a new instance <nl> + False <nl> <nl> So , when we take a view from the ndarray , we return a new ndarray , that <nl> - points to the data in the original . If we subclass ndarray , we need to <nl> - make sure that taking a view of our subclassed instance needs to return <nl> - another instance of our own class . Numpy has the machinery to do this , <nl> - but it is this view - creating machinery that makes subclassing slightly <nl> - non - standard . <nl> + points to the data in the original . <nl> <nl> - To allow subclassing , and views of subclasses , ndarray uses the <nl> - ndarray ` ` __new__ ` ` method for the main work of object initialization , <nl> - rather then the more usual ` ` __init__ ` ` method . <nl> + Implications for subclassing <nl> + mmmmmmmmmmmmmmmmmmmmmmmmmmm - <nl> <nl> - ` ` __new__ ` ` and ` ` __init__ ` ` <nl> - = = = = = = = = = = = = = = = = = = = = = = = = = = = = <nl> + If we subclass ndarray , we need to make sure that taking a view cast or <nl> + a slice of our subclassed instance returns another instance of our own <nl> + class . Numpy has the machinery to do this , but it is this view - creating <nl> + machinery that makes subclassing slightly non - standard . <nl> + <nl> + There are two aspects to the machinery that ndarray uses to support <nl> + views and slices in subclasses . <nl> + <nl> + The first is the use of the ` ` ndarray . __new__ ` ` method for the main work <nl> + of object initialization , rather then the more usual ` ` __init__ ` ` <nl> + method . The second is the use of the ` ` __array_finalize__ ` ` method to <nl> + allow subclasses to clean up after the creation of views and slices . <nl> + <nl> + A brief Python primer on ` ` __new__ ` ` and ` ` __init__ ` ` <nl> + = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = <nl> <nl> ` ` __new__ ` ` is a standard python method , and , if present , is called <nl> before ` ` __init__ ` ` when we create a class instance . Consider the <nl> - following : <nl> + following pure Python code : <nl> <nl> . . testcode : : <nl> <nl> class C ( object ) : <nl> def __new__ ( cls , * args ) : <nl> + print ' Cls in __new__ : ' , cls <nl> print ' Args in __new__ : ' , args <nl> return object . __new__ ( cls , * args ) <nl> + <nl> def __init__ ( self , * args ) : <nl> + print ' type ( self ) in __init__ : ' , type ( self ) <nl> print ' Args in __init__ : ' , args <nl> <nl> - c = C ( ' hello ' ) <nl> - <nl> - The code gives the following output : <nl> - <nl> - . . testoutput : : <nl> + meaning that we get : <nl> <nl> - Args in __new__ : ( ' hello ' , ) <nl> - Args in __init__ : ( ' hello ' , ) <nl> + > > > c = C ( ' hello ' ) <nl> + Cls in __new__ : < class ' C ' > <nl> + Args in __new__ : ( ' hello ' , ) <nl> + type ( self ) in __init__ : < class ' C ' > <nl> + Args in __init__ : ( ' hello ' , ) <nl> <nl> When we call ` ` C ( ' hello ' ) ` ` , the ` ` __new__ ` ` method gets its own class <nl> as first argument , and the passed argument , which is the string <nl> def __init__ ( self , * args ) : <nl> <nl> . . testcode : : <nl> <nl> - class C ( object ) : <nl> - def __new__ ( cls , * args ) : <nl> - print ' cls is : ' , cls <nl> - print ' Args in __new__ : ' , args <nl> - return object . __new__ ( cls , * args ) <nl> - def __init__ ( self , * args ) : <nl> - print ' self is : ' , self <nl> - print ' Args in __init__ : ' , args <nl> - <nl> class D ( C ) : <nl> def __new__ ( cls , * args ) : <nl> print ' D cls is : ' , cls <nl> print ' D args in __new__ : ' , args <nl> return C . __new__ ( C , * args ) <nl> - def __init__ ( self , * args ) : <nl> - print ' D self is : ' , self <nl> - print ' D args in __init__ : ' , args <nl> - <nl> - D ( ' hello ' ) <nl> <nl> - which gives : <nl> + def __init__ ( self , * args ) : <nl> + # we never get here <nl> + print ' In D __init__ ' <nl> <nl> - . . testoutput : : <nl> + meaning that : <nl> <nl> - D cls is : < class ' D ' > <nl> - D args in __new__ : ( ' hello ' , ) <nl> - cls is : < class ' C ' > <nl> - Args in __new__ : ( ' hello ' , ) <nl> + > > > obj = D ( ' hello ' ) <nl> + D cls is : < class ' D ' > <nl> + D args in __new__ : ( ' hello ' , ) <nl> + Cls in __new__ : < class ' C ' > <nl> + Args in __new__ : ( ' hello ' , ) <nl> + > > > type ( obj ) <nl> + < class ' C ' > <nl> <nl> The definition of ` ` C ` ` is the same as before , but for ` ` D ` ` , the <nl> ` ` __new__ ` ` method returns an instance of class ` ` C ` ` rather than <nl> class other than the class in which it is defined , the ` ` __init__ ` ` <nl> ` ` __array_finalize__ ` ` is the mechanism that numpy provides to allow <nl> subclasses to handle the various ways that new instances get created . <nl> <nl> - We already know that new subclass instances can come about in these <nl> - three ways : <nl> + Renenber that subclass instances can come about in these three ways : <nl> <nl> - explicit constructor call <nl> - as in ` ` obj = MySubClass ( params ) ` ` . This will call the usual <nl> + # . explicit constructor call ( ` ` obj = MySubClass ( params ) ` ` . This will call the usual <nl> sequence of ` ` MySubClass . __new__ ` ` then ` ` MySubClass . __init__ ` ` . <nl> - <nl> - view casting call <nl> - We can create an instance of our subclass from any other type of <nl> - numpy array , via a view casting call , like : ` ` obj = <nl> - arr . view ( MySubClass ) ` ` . <nl> - <nl> - instance slicing <nl> - by taking a slice from an instance of our own class , as in ` ` v_obj = <nl> - obj [ : 3 ] ` ` or similar . <nl> + # . : ref : ` view - casting ` <nl> + # . : ref : ` instance - slicing ` <nl> <nl> Our ` ` MySubClass . __new__ ` ` method only gets called in the case of the <nl> explicit constructor call , so we can ' t rely on ` ` __new__ ` ` or <nl> def __array_finalize__ ( self , obj ) : <nl> print ' In array_finalize with instance type % s ' % type ( obj ) <nl> <nl> <nl> - Now : : <nl> + Now : <nl> <nl> - > > > # Explicit constructor <nl> - > > > c = C ( ( 10 , ) ) <nl> - In __new__ with class < class ' C ' > <nl> - In array_finalize with instance type < type ' NoneType ' > <nl> - In __init__ with class < class ' C ' > <nl> - > > > # View casting <nl> - > > > a = np . arange ( 10 ) <nl> - In array_finalize with instance type < type ' numpy . ndarray ' > <nl> - > > > cast_a = a . view ( C ) <nl> - > > > # Slicing <nl> - > > > cv = c [ : 1 ] <nl> - In array_finalize with instance type < class ' C ' > <nl> + > > > # Explicit constructor <nl> + > > > c = C ( ( 10 , ) ) <nl> + In __new__ with class < class ' C ' > <nl> + In array_finalize with instance type < type ' NoneType ' > <nl> + In __init__ with class < class ' C ' > <nl> + > > > # View casting <nl> + > > > a = np . arange ( 10 ) <nl> + > > > cast_a = a . view ( C ) <nl> + In array_finalize with instance type < type ' numpy . ndarray ' > <nl> + > > > # Slicing <nl> + > > > cv = c [ : 1 ] <nl> + In array_finalize with instance type < class ' C ' > <nl> <nl> The signature of ` ` __array_finalize__ ` ` is : : <nl> <nl> def __array_finalize__ ( self , obj ) : <nl> # We do not need to return anything <nl> <nl> <nl> - So : : <nl> + So : <nl> <nl> > > > arr = np . arange ( 5 ) <nl> > > > obj = RealisticInfoArray ( arr , info = ' information ' ) <nl>\n", "msg": "Moving more to doctests , more explanation of views , slices\n"}
{"diff_id": 9400, "repo": "keras-team/keras\n", "sha": "2143046261e91e8b893bc9cbeedf08b732555e36\n", "time": "2016-01-21T01:59:26Z\n", "diff": "mmm a / keras / layers / recurrent . py <nl> ppp b / keras / layers / recurrent . py <nl> def get_output ( self , train = False ) : <nl> if not self . input_shape [ 1 ] : <nl> raise Exception ( ' When using TensorFlow , you should define ' + <nl> ' explicitly the number of timesteps of ' + <nl> - ' your sequences . Make sure the first layer ' + <nl> - ' has a \" batch_input_shape \" argument ' + <nl> - ' including the samples axis . ' ) <nl> + ' your sequences . \\ n ' + <nl> + ' If your first layer is an Embedding , ' + <nl> + ' make sure to pass it an \" input_length \" ' + <nl> + ' argument . Otherwise , make sure ' + <nl> + ' the first layer has ' + <nl> + ' an \" input_shape \" or \" batch_input_shape \" ' + <nl> + ' argument , including the time axis . ' ) <nl> if self . stateful : <nl> initial_states = self . states <nl> else : <nl>\n", "msg": "Improve error message in recurrent . py\n"}
{"diff_id": 9401, "repo": "ansible/ansible\n", "sha": "98546444316f8cbfe7e51f57a2026a58e786d814\n", "time": "2016-09-28T18:36:53Z\n", "diff": "mmm a / lib / ansible / module_utils / junos . py <nl> ppp b / lib / ansible / module_utils / junos . py <nl> def get_config ( self , config_format = \" text \" ) : <nl> return ele <nl> <nl> def load_config ( self , config , commit = False , replace = False , confirm = None , <nl> - comment = None , config_format = ' text ' ) : <nl> + comment = None , config_format = ' text ' , overwrite = False ) : <nl> + <nl> + if all ( [ replace , overwrite ] ) : <nl> + self . raise_exc ( ' setting both replace and overwrite to True is invalid ' ) <nl> <nl> if replace : <nl> merge = False <nl> - overwrite = True <nl> + overwrite = False <nl> + elif overwrite : <nl> + merge = True <nl> + overwrite = False <nl> else : <nl> merge = True <nl> overwrite = False <nl>\n", "msg": "adds overwrite kwarg to load_config in junos ( )\n"}
{"diff_id": 9655, "repo": "ansible/ansible\n", "sha": "3129cbb9e180586be256c39b1ed4b56828d5be81\n", "time": "2017-11-23T02:29:46Z\n", "diff": "mmm a / lib / ansible / modules / identity / ipa / ipa_user . py <nl> ppp b / lib / ansible / modules / identity / ipa / ipa_user . py <nl> <nl> description : Display name <nl> givenname : <nl> description : First name <nl> + krbpasswordexpiration : <nl> + description : <nl> + - Date at which the user password will expire <nl> + - In the format YYYYMMddHHmmss <nl> + - e . g . 20180121182022 will expire on 21 January 2018 at 18 : 20 : 22 <nl> + version_added : 2 . 5 <nl> loginshell : <nl> description : Login shell <nl> mail : <nl> <nl> - ipa_user : <nl> name : pinky <nl> state : present <nl> + krbpasswordexpiration : 20200119235959 <nl> givenname : Pinky <nl> sn : Acme <nl> mail : <nl> def user_enable ( self , name ) : <nl> return self . _post_json ( method = ' user_enable ' , name = name ) <nl> <nl> <nl> - def get_user_dict ( displayname = None , givenname = None , loginshell = None , mail = None , nsaccountlock = False , sn = None , <nl> - sshpubkey = None , telephonenumber = None , title = None , userpassword = None , gidnumber = None , uidnumber = None ) : <nl> + def get_user_dict ( displayname = None , givenname = None , krbpasswordexpiration = None , loginshell = None , <nl> + mail = None , nsaccountlock = False , sn = None , sshpubkey = None , telephonenumber = None , <nl> + title = None , userpassword = None , gidnumber = None , uidnumber = None ) : <nl> user = { } <nl> if displayname is not None : <nl> user [ ' displayname ' ] = displayname <nl> + if krbpasswordexpiration is not None : <nl> + user [ ' krbpasswordexpiration ' ] = krbpasswordexpiration + \" Z \" <nl> if givenname is not None : <nl> user [ ' givenname ' ] = givenname <nl> if loginshell is not None : <nl> def ensure ( module , client ) : <nl> nsaccountlock = state = = ' disabled ' <nl> <nl> module_user = get_user_dict ( displayname = module . params . get ( ' displayname ' ) , <nl> + krbpasswordexpiration = module . params . get ( ' krbpasswordexpiration ' ) , <nl> givenname = module . params . get ( ' givenname ' ) , <nl> loginshell = module . params [ ' loginshell ' ] , <nl> mail = module . params [ ' mail ' ] , sn = module . params [ ' sn ' ] , <nl> def main ( ) : <nl> argument_spec = ipa_argument_spec ( ) <nl> argument_spec . update ( displayname = dict ( type = ' str ' ) , <nl> givenname = dict ( type = ' str ' ) , <nl> + krbpasswordexpiration = dict ( type = ' str ' ) , <nl> loginshell = dict ( type = ' str ' ) , <nl> mail = dict ( type = ' list ' ) , <nl> sn = dict ( type = ' str ' ) , <nl>\n", "msg": "[ WIP ] Add option to ` ipa_user ` for setting expiration date ( )\n"}
{"diff_id": 9731, "repo": "getredash/redash\n", "sha": "ae2927d7d39d54d6fc9a12b39f2ab0d92f7b906c\n", "time": "2017-01-26T20:08:05Z\n", "diff": "mmm a / redash / models . py <nl> ppp b / redash / models . py <nl> def should_schedule_next ( previous_iteration , now , schedule ) : <nl> <nl> class Query ( ChangeTrackingMixin , TimestampMixin , BelongsToOrgMixin , db . Model ) : <nl> id = Column ( db . Integer , primary_key = True ) <nl> - version = Column ( db . Integer ) <nl> + version = Column ( db . Integer , default = 1 ) <nl> org_id = Column ( db . Integer , db . ForeignKey ( ' organizations . id ' ) ) <nl> org = db . relationship ( Organization , backref = \" queries \" ) <nl> data_source_id = Column ( db . Integer , db . ForeignKey ( \" data_sources . id \" ) , nullable = True ) <nl>\n", "msg": "Set default value for query version\n"}
{"diff_id": 9820, "repo": "ansible/ansible\n", "sha": "a326d2444671a8f9f1999d94d89b971fd97ca153\n", "time": "2014-02-09T16:33:34Z\n", "diff": "mmm a / plugins / inventory / linode . py <nl> ppp b / plugins / inventory / linode . py <nl> def is_cache_valid ( self ) : <nl> if os . path . isfile ( self . cache_path_index ) : <nl> return True <nl> return False <nl> - <nl> + <nl> def read_settings ( self ) : <nl> \" \" \" Reads the settings from the . ini file . \" \" \" <nl> config = ConfigParser . SafeConfigParser ( ) <nl> def add_node ( self , node ) : <nl> dest = node . label <nl> <nl> # Add to index <nl> - self . index [ dest ] = [ node . api_id ] <nl> + self . index [ dest ] = node . api_id <nl> <nl> # Inventory : Group by node ID ( always a group of 1 ) <nl> self . inventory [ node . api_id ] = [ dest ] <nl>\n", "msg": "Save api id to index as an int instead of a list\n"}
{"diff_id": 10020, "repo": "python/cpython\n", "sha": "0fd2dd6db0a279546747a43c777522b266e7734d\n", "time": "2000-08-07T00:45:51Z\n", "diff": "mmm a / Lib / distutils / fancy_getopt . py <nl> ppp b / Lib / distutils / fancy_getopt . py <nl> def __init__ ( self , option_table = None ) : <nl> <nl> <nl> def _build_index ( self ) : <nl> + self . option_index . clear ( ) <nl> for option in self . option_table : <nl> self . option_index [ option [ 0 ] ] = option <nl> <nl> def _grok_option_table ( self ) : <nl> the option table . Called by ' getopt ( ) ' before it can do <nl> anything worthwhile . \" \" \" <nl> <nl> + self . long_opts = [ ] <nl> + self . short_opts = [ ] <nl> + self . short2long . clear ( ) <nl> + <nl> for option in self . option_table : <nl> try : <nl> ( long , short , help ) = option <nl>\n", "msg": "Fix so we clear or reinitialize various data structures before populating\n"}
{"diff_id": 10135, "repo": "ansible/ansible\n", "sha": "b649a15a9737baa1cc037a7c4ec2e659e5f8177a\n", "time": "2016-12-08T16:33:47Z\n", "diff": "mmm a / lib / ansible / modules / extras / monitoring / sensu_check . py <nl> ppp b / lib / ansible / modules / extras / monitoring / sensu_check . py <nl> <nl> default : null <nl> custom : <nl> description : <nl> - - JSON mixin to add to the configuration <nl> + - A hash / dictionary of custom parameters for mixing to the configuration . <nl> + - You can ' t rewrite others module parameters using this <nl> required : false <nl> - default : \" { } \" <nl> + default : { } <nl> requirements : [ ] <nl> author : \" Anders Ingemann ( @ andsens ) \" <nl> ' ' ' <nl> <nl> # Let snippet from module_utils / basic . py return a proper error in this case <nl> pass <nl> <nl> - import ast <nl> - <nl> def sensu_check ( module , path , name , state = ' present ' , backup = False ) : <nl> changed = False <nl> reasons = [ ] <nl> def sensu_check ( module , path , name , state = ' present ' , backup = False ) : <nl> <nl> if module . params [ ' custom ' ] : <nl> # Convert to json <nl> - try : <nl> - custom_params = ast . literal_eval ( module . params [ ' custom ' ] ) <nl> - except : <nl> - msg = ' Module parameter \" custom \" contains invalid JSON . Example : custom = \\ ' { \" JSON \" : \" here \" } \\ ' ' <nl> - module . fail_json ( msg = msg ) <nl> - <nl> - overwrited_fields = set ( custom_params . keys ( ) ) & set ( simple_opts + [ ' type ' , ' subdue ' ] ) <nl> + custom_params = module . params [ ' custom ' ] <nl> + overwrited_fields = set ( custom_params . keys ( ) ) & set ( simple_opts + [ ' type ' , ' subdue ' , ' subdue_begin ' , ' subdue_end ' ] ) <nl> if overwrited_fields : <nl> - msg = ' You can \\ ' t overwriting standard module parameters via \" custom \" . You are trying overwrite : { of } ' . format ( of = list ( overwrited_fields ) ) <nl> + msg = ' You can \\ ' t overwriting standard module parameters via \" custom \" . You are trying overwrite : { opt } ' . format ( opt = list ( overwrited_fields ) ) <nl> module . fail_json ( msg = msg ) <nl> <nl> for k , v in custom_params . items ( ) : <nl> - if k in config [ ' checks ' ] [ name ] . keys ( ) : <nl> + if k in config [ ' checks ' ] [ name ] : <nl> if not config [ ' checks ' ] [ name ] [ k ] = = v : <nl> changed = True <nl> - reasons . append ( ' ` custom param { k } \\ ' was changed ' . format ( k = k ) ) <nl> + reasons . append ( ' ` custom param { opt } \\ ' was changed ' . format ( opt = k ) ) <nl> else : <nl> changed = True <nl> - reasons . append ( ' ` custom param { k } \\ ' was added ' . format ( k = k ) ) <nl> + reasons . append ( ' ` custom param { opt } \\ ' was added ' . format ( opt = k ) ) <nl> check [ k ] = v <nl> simple_opts + = custom_params . keys ( ) <nl> <nl> # Remove obsolete custom params <nl> - for opt in set ( config [ ' checks ' ] [ name ] . keys ( ) ) - set ( simple_opts + [ ' type ' , ' subdue ' ] ) : <nl> + for opt in set ( config [ ' checks ' ] [ name ] . keys ( ) ) - set ( simple_opts + [ ' type ' , ' subdue ' , ' subdue_begin ' , ' subdue_end ' ] ) : <nl> changed = True <nl> reasons . append ( ' ` custom param { opt } \\ ' was deleted ' . format ( opt = opt ) ) <nl> del check [ opt ] <nl> def main ( ) : <nl> ' aggregate ' : { ' type ' : ' bool ' } , <nl> ' low_flap_threshold ' : { ' type ' : ' int ' } , <nl> ' high_flap_threshold ' : { ' type ' : ' int ' } , <nl> - ' custom ' : { ' type ' : ' str ' } , <nl> + ' custom ' : { ' type ' : ' dict ' } , <nl> } <nl> <nl> required_together = [ [ ' subdue_begin ' , ' subdue_end ' ] ] <nl>\n", "msg": "Add custom parameter for a sensu_check . Fixes .\n"}
{"diff_id": 10138, "repo": "ansible/ansible\n", "sha": "f01885130357ea5c54ef62d3f4d5df3e3027c204\n", "time": "2017-07-10T19:07:25Z\n", "diff": "mmm a / lib / ansible / modules / monitoring / sensu_subscription . py <nl> ppp b / lib / ansible / modules / monitoring / sensu_subscription . py <nl> def sensu_subscription ( module , path , name , state = ' present ' , backup = False ) : <nl> if ' subscriptions ' not in config [ ' client ' ] : <nl> if state = = ' absent ' : <nl> reasons . append ( ' ` client . subscriptions \\ ' did not exist and state is ` absent \\ ' ' ) <nl> - return changed <nl> + return changed , reasons <nl> config [ ' client ' ] [ ' subscriptions ' ] = [ ] <nl> changed = True <nl> reasons . append ( ' ` client . subscriptions \\ ' did not exist ' ) <nl> def sensu_subscription ( module , path , name , state = ' present ' , backup = False ) : <nl> if name not in config [ ' client ' ] [ ' subscriptions ' ] : <nl> if state = = ' absent ' : <nl> reasons . append ( ' channel subscription was absent ' ) <nl> - return changed <nl> + return changed , reasons <nl> config [ ' client ' ] [ ' subscriptions ' ] . append ( name ) <nl> changed = True <nl> reasons . append ( ' channel subscription was absent and state is ` present \\ ' ' ) <nl>\n", "msg": "sensu_subscription : Fix return type for 2 cases ( )\n"}
{"diff_id": 10190, "repo": "zulip/zulip\n", "sha": "e249326510b4bb00d87c3d107153d8f52cc57e86\n", "time": "2013-01-28T21:59:25Z\n", "diff": "mmm a / zephyr / tests . py <nl> ppp b / zephyr / tests . py <nl> <nl> from zephyr . models import Message , UserProfile , Stream , Recipient , Subscription , \\ <nl> filter_by_subscriptions , get_display_recipient , Realm , Client <nl> from zephyr . tornadoviews import json_get_updates , api_get_messages <nl> - from zephyr . views import gather_subscriptions , api_get_profile , \\ <nl> - api_get_public_streams , api_add_subscriptions , api_get_subscribers <nl> + from zephyr . views import gather_subscriptions <nl> from zephyr . decorator import RespondAsynchronously , RequestVariableConversionError <nl> from zephyr . lib . initial_password import initial_password , initial_api_key <nl> from zephyr . lib . actions import do_send_message <nl> def common_get_profile ( self , email ) : <nl> user = User . objects . get ( email = email ) <nl> <nl> api_key = self . get_api_key ( email ) <nl> - request = POSTRequestMock ( { ' email ' : email , ' api - key ' : api_key } , user , None ) <nl> - result = api_get_profile ( request ) <nl> + result = self . client . post ( \" / api / v1 / get_profile \" , { ' email ' : email , ' api - key ' : api_key } ) <nl> <nl> stream = self . message_stream ( user ) <nl> max_id = - 1 <nl> def test_public_streams ( self ) : <nl> Ensure that get_public_streams successfully returns a list of streams <nl> \" \" \" <nl> email = ' hamlet @ humbughq . com ' <nl> - user = User . objects . get ( email = email ) <nl> + self . login ( email ) <nl> <nl> api_key = self . get_api_key ( email ) <nl> - request = POSTRequestMock ( { ' email ' : email , ' api - key ' : api_key } , user , None ) <nl> - result = api_get_public_streams ( request ) <nl> + result = self . client . post ( \" / json / get_public_streams \" , { ' email ' : email , ' api - key ' : api_key } ) <nl> <nl> self . assert_json_success ( result ) <nl> json = simplejson . loads ( result . content ) <nl> class InviteOnlyStreamTest ( AuthedTestCase ) : <nl> fixtures = [ ' messages . json ' ] <nl> <nl> def common_subscribe_to_stream ( self , email , streams , extra_post_data = { } , invite_only = False ) : <nl> - user = User . objects . get ( email = email ) <nl> api_key = self . get_api_key ( email ) <nl> <nl> post_data = { ' email ' : email , <nl> def common_subscribe_to_stream ( self , email , streams , extra_post_data = { } , invit <nl> ' subscriptions ' : streams , <nl> ' invite_only ' : invite_only } <nl> post_data . update ( extra_post_data ) <nl> - request = POSTRequestMock ( post_data , user , None ) <nl> - result = api_add_subscriptions ( request ) <nl> + <nl> + result = self . client . post ( \" / json / subscriptions / add \" , post_data ) <nl> return result <nl> <nl> def test_inviteonly ( self ) : <nl> # Creating an invite - only stream is allowed <nl> email = ' hamlet @ humbughq . com ' <nl> + self . login ( email ) <nl> + <nl> result = self . common_subscribe_to_stream ( email , ' [ \" Saxony \" ] ' , invite_only = True ) <nl> self . assert_json_success ( result ) <nl> <nl> def test_inviteonly ( self ) : <nl> self . assertEquals ( json [ \" already_subscribed \" ] , [ ] ) <nl> <nl> # Make sure both users are subscribed to this stream <nl> - user = User . objects . get ( email = email ) <nl> - request = POSTRequestMock ( { ' email ' : email , <nl> - ' api - key ' : self . get_api_key ( email ) , <nl> - ' stream ' : ' Saxony ' } , <nl> - user , None ) <nl> - result = api_get_subscribers ( request ) <nl> + result = self . client . post ( \" / json / get_subscribers \" , { ' email ' : email , <nl> + ' api - key ' : self . get_api_key ( email ) , <nl> + ' stream ' : ' Saxony ' } ) <nl> self . assert_json_success ( result ) <nl> json = simplejson . loads ( result . content ) <nl> <nl>\n", "msg": "Convert tests to using json api rather than views\n"}
{"diff_id": 10320, "repo": "ansible/ansible\n", "sha": "42a5b1977f0a86beeb9467dfcf8bb0f61a1f2d08\n", "time": "2016-12-08T16:25:35Z\n", "diff": "mmm a / lib / ansible / modules / system / mount . py <nl> ppp b / lib / ansible / modules / system / mount . py <nl> <nl> choices : [ \" present \" , \" absent \" , \" mounted \" , \" unmounted \" ] <nl> fstab : <nl> description : <nl> - - File to use instead of C ( / etc / fstab ) . You shouldn ' t use that option <nl> + - File to use instead of C ( / etc / fstab ) . You shouldn ' t use this option <nl> unless you really know what you are doing . This might be useful if <nl> - you need to configure mountpoints in a chroot environment . <nl> + you need to configure mountpoints in a chroot environment . OpenBSD <nl> + does not allow specifying alternate fstab files with mount so do not <nl> + use this on OpenBSD with any state that operates on the live filesystem . <nl> required : false <nl> default : / etc / fstab ( / etc / vfstab on Solaris ) <nl> boot : <nl> def unset_mount ( module , args ) : <nl> <nl> return ( args [ ' name ' ] , changed ) <nl> <nl> - def _set_fstab_args ( args ) : <nl> + def _set_fstab_args ( fstab_file ) : <nl> result = [ ] <nl> - if ' fstab ' in args and args [ ' fstab ' ] ! = ' / etc / fstab ' : <nl> + if fstab_file and fstab_file ! = ' / etc / fstab ' : <nl> if get_platform ( ) . lower ( ) . endswith ( ' bsd ' ) : <nl> result . append ( ' - F ' ) <nl> else : <nl> result . append ( ' - T ' ) <nl> - result . append ( args [ ' fstab ' ] ) <nl> + result . append ( fstab_file ) <nl> return result <nl> <nl> def mount ( module , args ) : <nl> def mount ( module , args ) : <nl> if ismount ( name ) : <nl> return remount ( module , mount_bin , args ) <nl> <nl> - cmd + = _set_fstab_args ( args ) <nl> + if get_platform ( ) . lower ( ) = = ' openbsd ' : <nl> + # Use module . params [ ' fstab ' ] here as args [ ' fstab ' ] has been set to the <nl> + # default value . <nl> + if module . params [ ' fstab ' ] is not None : <nl> + module . fail_json ( msg = ' OpenBSD does not support alternate fstab files . Do not specify the fstab parameter for OpenBSD hosts ' ) <nl> + else : <nl> + cmd + = _set_fstab_args ( args [ ' fstab ' ] ) <nl> <nl> cmd + = [ name ] <nl> <nl> def remount ( module , mount_bin , args ) : <nl> else : <nl> cmd + = [ ' - o ' , ' remount ' ] <nl> <nl> - cmd + = _set_fstab_args ( args ) <nl> + if get_platform ( ) . lower ( ) = = ' openbsd ' : <nl> + # Use module . params [ ' fstab ' ] here as args [ ' fstab ' ] has been set to the <nl> + # default value . <nl> + if module . params [ ' fstab ' ] is not None : <nl> + module . fail_json ( msg = ' OpenBSD does not support alternate fstab files . Do not specify the fstab parameter for OpenBSD hosts ' ) <nl> + else : <nl> + cmd + = _set_fstab_args ( args [ ' fstab ' ] ) <nl> cmd + = [ args [ ' name ' ] , ] <nl> out = err = ' ' <nl> try : <nl> def main ( ) : <nl> argument_spec = dict ( <nl> boot = dict ( default = ' yes ' , choices = [ ' yes ' , ' no ' ] ) , <nl> dump = dict ( ) , <nl> - fstab = dict ( default = ' / etc / fstab ' ) , <nl> + fstab = dict ( default = None ) , <nl> fstype = dict ( ) , <nl> name = dict ( required = True , type = ' path ' ) , <nl> opts = dict ( ) , <nl> def main ( ) : <nl> # name , src , fstype , opts , boot , passno , state , fstab = / etc / vfstab <nl> # linux args : <nl> # name , src , fstype , opts , dump , passno , state , fstab = / etc / fstab <nl> - if get_platform ( ) = = ' SunOS ' : <nl> + # Note : Do not modify module . params [ ' fstab ' ] as we need to know if the user <nl> + # explicitly specified it in mount ( ) and remount ( ) <nl> + if get_platform ( ) . lower ( ) = = ' sunos ' : <nl> args = dict ( <nl> name = module . params [ ' name ' ] , <nl> opts = ' - ' , <nl> passno = ' - ' , <nl> - fstab = ' / etc / vfstab ' , <nl> + fstab = module . params [ ' fstab ' ] , <nl> boot = ' yes ' <nl> ) <nl> + if args [ ' fstab ' ] is None : <nl> + args [ ' fstab ' ] = ' / etc / vfstab ' <nl> else : <nl> args = dict ( <nl> name = module . params [ ' name ' ] , <nl> opts = ' defaults ' , <nl> dump = ' 0 ' , <nl> passno = ' 0 ' , <nl> - fstab = ' / etc / fstab ' <nl> + fstab = module . params [ ' fstab ' ] <nl> ) <nl> + if args [ ' fstab ' ] is None : <nl> + args [ ' fstab ' ] = ' / etc / fstab ' <nl> <nl> - # FreeBSD doesn ' t have any ' default ' so set ' rw ' instead <nl> - if get_platform ( ) = = ' FreeBSD ' : <nl> - args [ ' opts ' ] = ' rw ' <nl> + # FreeBSD doesn ' t have any ' default ' so set ' rw ' instead <nl> + if get_platform ( ) = = ' FreeBSD ' : <nl> + args [ ' opts ' ] = ' rw ' <nl> <nl> linux_mounts = [ ] <nl> <nl> def main ( ) : <nl> if module . params [ key ] is not None : <nl> args [ key ] = module . params [ key ] <nl> <nl> - if get_platform ( ) = = ' SunOS ' and args [ ' fstab ' ] = = ' / etc / fstab ' : <nl> - args [ ' fstab ' ] = ' / etc / vfstab ' <nl> - <nl> # If fstab file does not exist , we first need to create it . This mainly <nl> # happens when fstab option is passed to the module . <nl> if not os . path . exists ( args [ ' fstab ' ] ) : <nl>\n", "msg": "Do not use the fstab parameter on openbsd for mounting ( )\n"}
{"diff_id": 10352, "repo": "ansible/ansible\n", "sha": "33aa50eae7845940950c6713fa0a22aa08f73442\n", "time": "2012-03-18T21:04:07Z\n", "diff": "mmm a / lib / ansible / runner . py <nl> ppp b / lib / ansible / runner . py <nl> <nl> import tempfile <nl> import subprocess <nl> <nl> - # FIXME : stop importing * , use as utils / errors <nl> - from ansible . utils import * <nl> + from ansible import utils <nl> from ansible . errors import * <nl> <nl> # should be True except in debug <nl> def _executor_hook ( job_queue , result_queue ) : <nl> result_queue . put ( runner . _executor ( host ) ) <nl> except Queue . Empty : <nl> pass <nl> - except AnsibleError , ae : <nl> + except errors . AnsibleError , ae : <nl> result_queue . put ( [ host , False , str ( ae ) ] ) <nl> except Exception , ee : <nl> # probably should include the full trace <nl> def parse_hosts ( cls , host_list ) : <nl> try : <nl> groups = json . loads ( out ) <nl> except : <nl> - raise AnsibleError ( \" invalid JSON response from script : % s \" % host_list ) <nl> + raise errors . AnsibleError ( \" invalid JSON response from script : % s \" % host_list ) <nl> for ( groupname , hostlist ) in groups . iteritems ( ) : <nl> for host in hostlist : <nl> if host not in results : <nl> def _return_from_module ( self , conn , host , result ) : <nl> <nl> try : <nl> # try to parse the JSON response <nl> - return [ host , True , parse_json ( result ) ] <nl> + return [ host , True , utils . parse_json ( result ) ] <nl> except Exception , e : <nl> # it failed , say so , but return the string anyway <nl> return [ host , False , \" % s / % s \" % ( str ( e ) , result ) ] <nl> def _execute_module ( self , conn , tmp , remote_module_path , module_args , <nl> try : <nl> inject2 = json . loads ( out ) <nl> except : <nl> - raise AnsibleError ( \" % s returned invalid result when called with hostname % s \" % ( <nl> + raise errors . AnsibleError ( \" % s returned invalid result when called with hostname % s \" % ( <nl> Runner . _external_variable_script , <nl> host <nl> ) ) <nl> def _execute_normal_module ( self , conn , host , tmp ) : <nl> if self . module_name = = ' setup ' : <nl> host = conn . host <nl> try : <nl> - var_result = parse_json ( result ) <nl> + var_result = utils . parse_json ( result ) <nl> except : <nl> var_result = { } <nl> <nl> def _execute_copy ( self , conn , host , tmp ) : <nl> # transfer the file to a remote tmp location <nl> tmp_path = tmp <nl> tmp_src = tmp_path + source . split ( ' / ' ) [ - 1 ] <nl> - self . _transfer_file ( conn , path_dwim ( self . basedir , source ) , tmp_src ) <nl> + self . _transfer_file ( conn , utils . path_dwim ( self . basedir , source ) , tmp_src ) <nl> <nl> # install the copy module <nl> self . module_name = ' copy ' <nl> def _execute_template ( self , conn , host , tmp ) : <nl> tpath = tmp <nl> tempname = os . path . split ( source ) [ - 1 ] <nl> temppath = tpath + tempname <nl> - self . _transfer_file ( conn , path_dwim ( self . basedir , source ) , temppath ) <nl> + self . _transfer_file ( conn , utils . path_dwim ( self . basedir , source ) , temppath ) <nl> <nl> # install the template module <nl> template_module = self . _transfer_module ( conn , tmp , ' template ' ) <nl>\n", "msg": "Inside of runner , do not ' import * ' from utils , so we can more easily tell where functions come from\n"}
{"diff_id": 10392, "repo": "ipython/ipython\n", "sha": "c168107a7fdc3862367a535dfdd8effb2cac6dbd\n", "time": "2016-11-18T13:57:45Z\n", "diff": "mmm a / IPython / core / display . py <nl> ppp b / IPython / core / display . py <nl> <nl> except ImportError : <nl> from base64 import encodestring as base64_encode <nl> <nl> + from binascii import b2a_hex <nl> import json <nl> import mimetypes <nl> import os <nl> <nl> ' display_javascript ' , ' display_pdf ' , ' DisplayObject ' , ' TextDisplayObject ' , <nl> ' Pretty ' , ' HTML ' , ' Markdown ' , ' Math ' , ' Latex ' , ' SVG ' , ' JSON ' , ' Javascript ' , <nl> ' Image ' , ' clear_output ' , ' set_matplotlib_formats ' , ' set_matplotlib_close ' , <nl> - ' publish_display_data ' , ' update_display ' ] <nl> + ' publish_display_data ' , ' update_display ' , ' DisplayHandle ' ] <nl> <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> # utility functions <nl> def publish_display_data ( data , metadata = None , source = None , * , transient = None , * * <nl> * * kwargs <nl> ) <nl> <nl> + <nl> + def _new_id ( ) : <nl> + \" \" \" Generate a new random text id with urandom \" \" \" <nl> + return b2a_hex ( os . urandom ( 16 ) ) . decode ( ' ascii ' ) <nl> + <nl> + <nl> def display ( * objs , include = None , exclude = None , metadata = None , transient = None , display_id = None , * * kwargs ) : <nl> \" \" \" Display a Python object in all frontends . <nl> <nl> def display ( * objs , include = None , exclude = None , metadata = None , transient = None , di <nl> display_id : str , optional <nl> Set an id for the display . <nl> This id can be used for updating this display area later via update_display . <nl> + If given as True , generate a new display_id <nl> kwargs : additional keyword - args , optional <nl> Additional keyword - arguments are passed through to the display publisher . <nl> + <nl> + Returns <nl> + mmmmmm - <nl> + <nl> + handle : DisplayHandle <nl> + Returns a handle on updatable displays , if display_id is given . <nl> + Returns None if no display_id is given ( default ) . <nl> \" \" \" <nl> raw = kwargs . pop ( ' raw ' , False ) <nl> if transient is None : <nl> transient = { } <nl> if display_id : <nl> + if display_id = = True : <nl> + display_id = _new_id ( ) <nl> transient [ ' display_id ' ] = display_id <nl> if kwargs . get ( ' update ' ) and ' display_id ' not in transient : <nl> raise TypeError ( ' display_id required for update_display ' ) <nl> def display ( * objs , include = None , exclude = None , metadata = None , transient = None , di <nl> if metadata : <nl> # kwarg - specified metadata gets precedence <nl> _merge ( md_dict , metadata ) <nl> - publish_display_data ( data = format_dict , metadata = md_dict , <nl> - * * kwargs ) <nl> + publish_display_data ( data = format_dict , metadata = md_dict , * * kwargs ) <nl> + if display_id : <nl> + return DisplayHandle ( display_id ) <nl> <nl> <nl> - def update_display ( obj , * , display_id = None , * * kwargs ) : <nl> - \" \" \" Update an existing display . <nl> + def update_display ( obj , * , display_id , * * kwargs ) : <nl> + \" \" \" Update an existing display by id <nl> <nl> Parameters <nl> mmmmmmmmm - <nl> def update_display ( obj , * , display_id = None , * * kwargs ) : <nl> The id of the display to update <nl> \" \" \" <nl> kwargs [ ' update ' ] = True <nl> - return display ( obj , * * kwargs ) <nl> + display ( obj , display_id = display_id , * * kwargs ) <nl> + <nl> + <nl> + class DisplayHandle ( object ) : <nl> + \" \" \" A handle on an updatable display <nl> + <nl> + Call . update ( obj ) to display a new object . <nl> + <nl> + Call . display ( obj ) to add a new instance of this display , <nl> + and update existing instances . <nl> + \" \" \" <nl> + <nl> + def __init__ ( self , display_id = None ) : <nl> + if display_id is None : <nl> + display_id = _new_id ( ) <nl> + self . display_id = display_id <nl> <nl> + def __repr__ ( self ) : <nl> + return \" < % s display_id = % s > \" % ( self . __class__ . __name__ , self . display_id ) <nl> + <nl> + def display ( self , obj , * * kwargs ) : <nl> + \" \" \" Make a new display with my id , updating existing instances . <nl> + <nl> + Parameters <nl> + mmmmmmmmm - <nl> + <nl> + obj : <nl> + object to display <nl> + * * kwargs : <nl> + additional keyword arguments passed to display <nl> + \" \" \" <nl> + display ( obj , display_id = self . display_id , * * kwargs ) <nl> + <nl> + def update ( self , obj , * * kwargs ) : <nl> + \" \" \" Update existing displays with my id <nl> + <nl> + Parameters <nl> + mmmmmmmmm - <nl> + <nl> + obj : <nl> + object to display <nl> + * * kwargs : <nl> + additional keyword arguments passed to update_display <nl> + \" \" \" <nl> + update_display ( obj , display_id = self . display_id , * * kwargs ) <nl> <nl> <nl> def display_pretty ( * objs , * * kwargs ) : <nl>\n", "msg": "add DisplayHandle for updating displays\n"}
{"diff_id": 10450, "repo": "ansible/ansible\n", "sha": "7092dd7fbf81c6959dd22d99e2a6438c1f1e9759\n", "time": "2018-09-17T20:12:46Z\n", "diff": "mmm a / lib / ansible / modules / network / asa / asa_command . py <nl> ppp b / lib / ansible / modules / network / asa / asa_command . py <nl> <nl> \" \" \" <nl> <nl> EXAMPLES = \" \" \" <nl> - # Note : examples below use the following provider dict to handle <nl> - # transport and authentication to the node . <nl> mmm - <nl> - vars : <nl> - cli : <nl> - host : \" { { inventory_hostname } } \" <nl> - username : cisco <nl> - password : cisco <nl> - authorize : yes <nl> - auth_pass : cisco <nl> - transport : cli <nl> <nl> mmm <nl> - - asa_command : <nl> + - name : \" Show the ASA version \" <nl> + asa_command : <nl> commands : <nl> - show version <nl> - provider : \" { { cli } } \" <nl> <nl> - - asa_command : <nl> + - name : \" Show ASA drops and memory \" <nl> + asa_command : <nl> commands : <nl> - show asp drop <nl> - show memory <nl> - provider : \" { { cli } } \" <nl> <nl> - - asa_command : <nl> + - name : \" Send repeat pings and wait for the result to pass 100 % \" <nl> + asa_command : <nl> commands : <nl> - - show version <nl> - provider : \" { { cli } } \" <nl> - context : system <nl> + - ping 8 . 8 . 8 . 8 repeat 20 size 350 <nl> + wait_for : <nl> + - result [ 0 ] contains 100 <nl> + retries : 2 <nl> \" \" \" <nl> <nl> RETURN = \" \" \" <nl>\n", "msg": "Add ' Wait for ' example to asa_command module ( )\n"}
{"diff_id": 10490, "repo": "ansible/ansible\n", "sha": "ef8c1124d0c05048dc8b654ad779d17c3be36ee6\n", "time": "2016-03-26T07:46:50Z\n", "diff": "mmm a / lib / ansible / module_utils / ec2 . py <nl> ppp b / lib / ansible / module_utils / ec2 . py <nl> def page ( * args , * * kwargs ) : <nl> return page <nl> return wrapper <nl> <nl> + <nl> + def camel_dict_to_snake_dict ( camel_dict ) : <nl> + <nl> + def camel_to_snake ( name ) : <nl> + <nl> + import re <nl> + <nl> + first_cap_re = re . compile ( ' ( . ) ( [ A - Z ] [ a - z ] + ) ' ) <nl> + all_cap_re = re . compile ( ' ( [ a - z0 - 9 ] ) ( [ A - Z ] ) ' ) <nl> + s1 = first_cap_re . sub ( r ' \\ 1_ \\ 2 ' , name ) <nl> + <nl> + return all_cap_re . sub ( r ' \\ 1_ \\ 2 ' , s1 ) . lower ( ) <nl> + <nl> + <nl> + snake_dict = { } <nl> + for k , v in camel_dict . iteritems ( ) : <nl> + if isinstance ( v , dict ) : <nl> + v = camel_dict_to_snake_dict ( v ) <nl> + snake_dict [ camel_to_snake ( k ) ] = v <nl> + <nl> + return snake_dict <nl>\n", "msg": "Add function to convert CamelCased key names to snake_names\n"}
{"diff_id": 10559, "repo": "matplotlib/matplotlib\n", "sha": "5309a225d972bd577e8fdc89cdfc80386a106abb\n", "time": "2017-06-29T17:21:10Z\n", "diff": "mmm a / doc / make . py <nl> ppp b / doc / make . py <nl> def texinfo ( ) : <nl> def clean ( ) : <nl> \" \" \" Remove generated files . \" \" \" <nl> shutil . rmtree ( \" build \" , ignore_errors = True ) <nl> - shutil . rmtree ( \" examples \" , ignore_errors = True ) <nl> + shutil . rmtree ( \" tutorials \" , ignore_errors = True ) <nl> shutil . rmtree ( \" api / _as_gen \" , ignore_errors = True ) <nl> for pattern in [ ' _static / matplotlibrc ' , <nl> ' _templates / gallery . html ' , <nl>\n", "msg": "Update make . py clean for tutorials\n"}
{"diff_id": 10737, "repo": "matplotlib/matplotlib\n", "sha": "1deb3176ee138e34d6c49cfc734912e72a893edf\n", "time": "2012-03-09T16:57:50Z\n", "diff": "mmm a / lib / mpl_toolkits / mplot3d / axes3d . py <nl> ppp b / lib / mpl_toolkits / mplot3d / axes3d . py <nl> def can_pan ( self ) : <nl> def cla ( self ) : <nl> \" \" \" Clear axes and disable mouse button callbacks . <nl> \" \" \" <nl> - self . disable_mouse_rotation ( ) <nl> + # Disabling mouse interaction might have been needed a long <nl> + # time ago , but I can ' t find a reason for it now - BVR ( 2012 - 03 ) <nl> + # self . disable_mouse_rotation ( ) <nl> self . zaxis . cla ( ) <nl> <nl> # TODO : Support sharez <nl>\n", "msg": "Don ' t disable mouse interaction upon call to cla ( ) in mplot3d\n"}
{"diff_id": 10825, "repo": "quantopian/zipline\n", "sha": "6a8654342add82ea6e6b69fe3c1d8ba8fae1d02f\n", "time": "2012-06-07T21:40:26Z\n", "diff": "mmm a / zipline / finance / performance . py <nl> ppp b / zipline / finance / performance . py <nl> <nl> + mmmmmmmmmmmmmmm - - + mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - + <nl> | last_sale_price | price at last sale of the security on the exchange | <nl> + mmmmmmmmmmmmmmm - - + mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - + <nl> + | cost_basis | the volume weighted average price paid per share | <nl> + + mmmmmmmmmmmmmmm - - + mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - + <nl> + <nl> <nl> <nl> Performance Period <nl> = = = = = = = = = = = = = = = = = = <nl> <nl> + Performance Periods are updated with every trade . When calling <nl> + code needs <nl> + <nl> + mmmmmmmmmmmmmmm + mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm + <nl> | key | value | <nl> + = = = = = = = = = = = = = = = + = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = + <nl> def __init__ ( self , trading_environment ) : <nl> ) <nl> <nl> def get_portfolio ( self ) : <nl> - return self . cumulative_performance . to_ndict ( ) <nl> + return self . cumulative_performance . as_portfolio ( ) <nl> <nl> def open ( self , context ) : <nl> if self . results_addr : <nl> def update_last_sale ( self , event ) : <nl> self . positions [ event . sid ] . last_sale_price = event . price <nl> self . positions [ event . sid ] . last_sale_date = event . dt <nl> <nl> - def to_dict ( self ) : <nl> - \" \" \" <nl> - Creates a dictionary representing the state of this performance <nl> - period . See header comments for a detailed description . <nl> - \" \" \" <nl> - positions = self . get_positions_list ( ) <nl> - transactions = [ x . as_dict ( ) for x in self . processed_transactions ] <nl> - <nl> + def __core_dict ( self ) : <nl> rval = { <nl> ' ending_value ' : self . ending_value , <nl> ' capital_used ' : self . period_capital_used , <nl> ' starting_value ' : self . starting_value , <nl> ' starting_cash ' : self . starting_cash , <nl> - ' cash ' : self . ending_cash , <nl> + ' ending_cash ' : self . ending_cash , <nl> ' portfolio_value ' : self . ending_cash + self . ending_value , <nl> ' cumulative_capital_used ' : self . cumulative_capital_used , <nl> ' max_capital_used ' : self . max_capital_used , <nl> ' max_leverage ' : self . max_leverage , <nl> - ' positions ' : positions , <nl> ' pnl ' : self . pnl , <nl> ' returns ' : self . returns , <nl> - ' transactions ' : transactions , <nl> ' period_open ' : self . period_open , <nl> ' period_close ' : self . period_close <nl> } <nl> <nl> - # we want the key to be absent , not just empty <nl> - if not self . keep_transactions : <nl> - del rval [ ' transactions ' ] <nl> - <nl> return rval <nl> <nl> - def to_ndict ( self ) : <nl> - \" \" \" <nl> - Creates a ndict representing the state of this perfomance period . <nl> - Properties are the same as the results of to_dict . See header comments <nl> - for a detailed description . <nl> <nl> + def to_dict ( self ) : <nl> + \" \" \" <nl> + Creates a dictionary representing the state of this performance <nl> + period . See header comments for a detailed description . <nl> \" \" \" <nl> - positions = self . get_positions ( ndicted = True ) <nl> + rval = self . __core_dict ( ) <nl> + positions = self . get_positions_list ( ) <nl> + rval [ ' positions ' ] = positions <nl> <nl> - positions = zp . ndict ( positions ) <nl> + # we want the key to be absent , not just empty <nl> + if self . keep_transactions : <nl> + transactions = [ x . as_dict ( ) for x in self . processed_transactions ] <nl> + rval [ ' transactions ' ] = transactions <nl> <nl> - return zp . ndict ( { <nl> - ' ending_value ' : self . ending_value , <nl> - ' capital_used ' : self . period_capital_used , <nl> - ' starting_value ' : self . starting_value , <nl> - ' starting_cash ' : self . starting_cash , <nl> - ' ending_cash ' : self . ending_cash , <nl> - ' cumulative_capital_used ' : self . cumulative_capital_used , <nl> - ' max_capital_used ' : self . max_capital_used , <nl> - ' max_leverage ' : self . max_leverage , <nl> - ' positions ' : positions , <nl> - ' transactions ' : self . processed_transactions <nl> - } ) <nl> + return rval <nl> + <nl> + def as_portfolio ( self ) : <nl> + \" \" \" <nl> + The purpose of this method is to provide a portfolio <nl> + object to algorithms running inside the same trading <nl> + client . The data needed is captured raw in a <nl> + PerformancePeriod , and in this method we rename some <nl> + fields for usability and remove extraneous fields . <nl> + \" \" \" <nl> + portfolio = self . __core_dict ( ) <nl> + # rename : <nl> + # ending_cash - > cash <nl> + # period_open - > backtest_start <nl> + # <nl> + # remove : <nl> + # period_close , starting_value , <nl> + # cumulative_capital_used , max_leverage , max_capital_used <nl> + portfolio [ ' cash ' ] = portfolio [ ' ending_cash ' ] <nl> + portfolio [ ' start_date ' ] = portfolio [ ' period_open ' ] <nl> + portfolio [ ' position_value ' ] = portfolio [ ' ending_value ' ] <nl> + <nl> + del ( portfolio [ ' ending_cash ' ] ) <nl> + del ( portfolio [ ' period_open ' ] ) <nl> + del ( portfolio [ ' period_close ' ] ) <nl> + del ( portfolio [ ' starting_value ' ] ) <nl> + del ( portfolio [ ' ending_value ' ] ) <nl> + del ( portfolio [ ' cumulative_capital_used ' ] ) <nl> + del ( portfolio [ ' max_leverage ' ] ) <nl> + del ( portfolio [ ' max_capital_used ' ] ) <nl> + <nl> + portfolio [ ' positions ' ] = self . get_positions ( ndicted = True ) <nl> + return zp . ndict ( portfolio ) <nl> <nl> def get_positions ( self , ndicted = False ) : <nl> - positions = { } <nl> + if ndicted : <nl> + positions = zp . ndict ( { } ) <nl> + else : <nl> + positions = { } <nl> + <nl> for sid , pos in self . positions . iteritems ( ) : <nl> cur = pos . to_dict ( ) <nl> if ndicted : <nl>\n", "msg": "added an as_portfolio method to the PerformancePeriod .\n"}
{"diff_id": 10889, "repo": "cookiecutter/cookiecutter\n", "sha": "8df7a212dfe557b814617a1861b5b928bba3cdd9\n", "time": "2016-07-31T14:31:52Z\n", "diff": "mmm a / cookiecutter / log . py <nl> ppp b / cookiecutter / log . py <nl> def filter ( self , record ) : <nl> return record <nl> <nl> <nl> - def create_logger ( template , level = ' DEBUG ' , log_file = None ) : <nl> - # Get settings for given log level <nl> - log_level = LOG_LEVELS [ level ] <nl> - log_format = LOG_FORMATS [ level ] <nl> - <nl> + def create_logger ( template , stream_level = ' DEBUG ' , debug_file = None ) : <nl> # Set up ' cookiecutter ' logger <nl> logger = logging . getLogger ( ' cookiecutter ' ) <nl> - logger . setLevel ( log_level ) <nl> + logger . setLevel ( logging . DEBUG ) <nl> <nl> # Remove all attached handlers , in case there was <nl> # a logger with using the name ' cookiecutter ' <nl> def create_logger ( template , level = ' DEBUG ' , log_file = None ) : <nl> # Add additional information to the logger <nl> context_filter = ContextFilter ( template ) <nl> <nl> - # Create formatter based on the given level <nl> - log_formatter = logging . Formatter ( log_format ) <nl> - <nl> # Create a file handler if a log file is provided <nl> - if log_file is not None : <nl> - file_handler = logging . FileHandler ( log_file ) <nl> - file_handler . setLevel ( log_level ) <nl> - file_handler . setFormatter ( log_formatter ) <nl> + if debug_file is not None : <nl> + debug_formatter = logging . Formatter ( LOG_FORMATS [ ' DEBUG ' ] ) <nl> + file_handler = logging . FileHandler ( debug_file ) <nl> + file_handler . setLevel ( LOG_LEVELS [ ' DEBUG ' ] ) <nl> + file_handler . setFormatter ( debug_formatter ) <nl> file_handler . addFilter ( context_filter ) <nl> logger . addHandler ( file_handler ) <nl> <nl> + # Get settings based on the given stream_level <nl> + log_formatter = logging . Formatter ( LOG_FORMATS [ stream_level ] ) <nl> + log_level = LOG_LEVELS [ stream_level ] <nl> + <nl> # Create a stream handler <nl> stream_handler = logging . StreamHandler ( ) <nl> stream_handler . setLevel ( log_level ) <nl>\n", "msg": "Change logging to always log on DEBUG to file\n"}
{"diff_id": 10925, "repo": "python/cpython\n", "sha": "3b4849a21d691bf95b9d4d4171b60c496e109ae6\n", "time": "2010-06-02T10:05:31Z\n", "diff": "mmm a / Lib / logging / config . py <nl> ppp b / Lib / logging / config . py <nl> def resolve ( self , s ) : <nl> \" \" \" <nl> name = s . split ( ' . ' ) <nl> used = name . pop ( 0 ) <nl> - found = self . importer ( used ) <nl> - for frag in name : <nl> - used + = ' . ' + frag <nl> - try : <nl> - found = getattr ( found , frag ) <nl> - except AttributeError : <nl> - self . importer ( used ) <nl> - found = getattr ( found , frag ) <nl> - return found <nl> + try : <nl> + found = self . importer ( used ) <nl> + for frag in name : <nl> + used + = ' . ' + frag <nl> + try : <nl> + found = getattr ( found , frag ) <nl> + except AttributeError : <nl> + self . importer ( used ) <nl> + found = getattr ( found , frag ) <nl> + return found <nl> + except ImportError : <nl> + e , tb = sys . exc_info ( ) [ 1 : ] <nl> + v = ValueError ( ' Cannot resolve % r : % s ' % ( s , e ) ) <nl> + v . __cause__ , v . __traceback__ = e , tb <nl> + raise v <nl> <nl> def ext_convert ( self , value ) : <nl> \" \" \" Default converter for the ext : / / protocol . \" \" \" <nl>\n", "msg": "Logging : improved error reporting for BaseConfigurator . resolve ( ) .\n"}
{"diff_id": 10938, "repo": "python/cpython\n", "sha": "f1725296cc81f20b5aad19009da0efda48a87fc9\n", "time": "2010-09-12T18:16:01Z\n", "diff": "mmm a / Lib / collections . py <nl> ppp b / Lib / collections . py <nl> def move_to_end ( self , key , last = True ) : <nl> <nl> def __repr__ ( self ) : <nl> ' od . __repr__ ( ) < = = > repr ( od ) ' <nl> - if self . __in_repr : <nl> - return ' . . . ' <nl> if not self : <nl> return ' % s ( ) ' % ( self . __class__ . __name__ , ) <nl> + if self . __in_repr : <nl> + return ' . . . ' <nl> self . __in_repr = True <nl> try : <nl> result = ' % s ( % r ) ' % ( self . __class__ . __name__ , list ( self . items ( ) ) ) <nl>\n", "msg": "Put tests in more logical order .\n"}
{"diff_id": 11069, "repo": "ansible/ansible\n", "sha": "276ad4f8f5cfb2e0a86a93c0f47837a20fd3799e\n", "time": "2020-12-01T15:25:15Z\n", "diff": "mmm a / lib / ansible / modules / systemd . py <nl> ppp b / lib / ansible / modules / systemd . py <nl> <nl> description : <nl> - run systemctl within a given service manager scope , either as the default system scope ( system ) , <nl> the current user ' s scope ( user ) , or the scope of all users ( global ) . <nl> - - \" For systemd to work with ' user ' , the executing user must have its own instance of dbus started ( systemd requirement ) . <nl> - The user dbus process is normally started during normal login , but not during the run of Ansible tasks . <nl> + - \" For systemd to work with ' user ' , the executing user must have its own instance of dbus started and accessible ( systemd requirement ) . \" <nl> + - \" The user dbus process is normally started during normal login , but not during the run of Ansible tasks . <nl> Otherwise you will probably get a ' Failed to connect to bus : no such file or directory ' error . \" <nl> + - The user must have access , normally given via setting the ` ` XDG_RUNTIME_DIR ` ` variable , see example below . <nl> + <nl> type : str <nl> choices : [ system , user , global ] <nl> default : system <nl> <nl> - name : Just force systemd to re - execute itself ( 2 . 8 and above ) <nl> systemd : <nl> daemon_reexec : yes <nl> + <nl> + - name : run a user service when XDG_RUNTIME_DIR is not set on remote login . <nl> + systemd : <nl> + name : myservice <nl> + state : started <nl> + scope : user <nl> + environment : <nl> + XDG_RUNTIME_DIR : \" / run / user / { { myuid } } \" <nl> ' ' ' <nl> <nl> RETURN = ' ' ' <nl>\n", "msg": "added more specific info about user scope ( )\n"}
{"diff_id": 11547, "repo": "home-assistant/core\n", "sha": "d516bc44fa77a3b827972f8446d94a25d64fede9\n", "time": "2019-10-12T05:40:44Z\n", "diff": "mmm a / homeassistant / components / trend / binary_sensor . py <nl> ppp b / homeassistant / components / trend / binary_sensor . py <nl> <nl> import logging <nl> import math <nl> <nl> + import numpy as np <nl> import voluptuous as vol <nl> <nl> from homeassistant . components . binary_sensor import ( <nl> <nl> CONF_DEVICE_CLASS , <nl> CONF_ENTITY_ID , <nl> CONF_FRIENDLY_NAME , <nl> - STATE_UNKNOWN , <nl> - STATE_UNAVAILABLE , <nl> CONF_SENSORS , <nl> + STATE_UNAVAILABLE , <nl> + STATE_UNKNOWN , <nl> ) <nl> from homeassistant . core import callback <nl> import homeassistant . helpers . config_validation as cv <nl> def _calculate_gradient ( self ) : <nl> <nl> This need run inside executor . <nl> \" \" \" <nl> - import numpy as np <nl> - <nl> timestamps = np . array ( [ t for t , _ in self . samples ] ) <nl> values = np . array ( [ s for _ , s in self . samples ] ) <nl> coeffs = np . polyfit ( timestamps , values , 1 ) <nl>\n", "msg": "Move trend imports to top level ( )\n"}
{"diff_id": 11615, "repo": "zulip/zulip\n", "sha": "75bbda1dad3dd1e12dfee32354ba669fa47ac055\n", "time": "2013-04-24T15:30:24Z\n", "diff": "mmm a / zephyr / views . py <nl> ppp b / zephyr / views . py <nl> def update_pointer_backend ( request , user_profile , <nl> if pointer < = user_profile . pointer : <nl> return json_success ( ) <nl> <nl> + prev_pointer = user_profile . pointer <nl> user_profile . pointer = pointer <nl> user_profile . save ( update_fields = [ \" pointer \" ] ) <nl> <nl> def update_pointer_backend ( request , user_profile , <nl> # this is a shim that will mark as read any messages up until the <nl> # pointer move <nl> UserMessage . objects . filter ( user_profile = user_profile , <nl> + message__id__gt = prev_pointer , <nl> message__id__lte = pointer , <nl> flags = ~ UserMessage . flags . read ) \\ <nl> . update ( flags = F ( ' flags ' ) . bitor ( UserMessage . flags . read ) ) <nl>\n", "msg": "Add lower message id bound when marking messages as read for the mobile unread count hack\n"}
{"diff_id": 11781, "repo": "bokeh/bokeh\n", "sha": "88dec0bd00e811b12c2fee9c2923c115ab9b7bca\n", "time": "2015-01-28T05:11:18Z\n", "diff": "mmm a / examples / app / stock_applet / stock_data . py <nl> ppp b / examples / app / stock_applet / stock_data . py <nl> <nl> import os <nl> - import urllib <nl> + from six . moves import urllib <nl> import zipfile <nl> <nl> <nl> - def download_data ( url , save_dir , exclude_term ) : <nl> - name = os . path . join ( save_dir , ' temp . zip ' ) <nl> + def extract_hosted_zip ( data_url , save_dir , exclude_term = None ) : <nl> + \" \" \" Downloads , then extracts a zip file . \" \" \" <nl> <nl> - # get the zip file <nl> + zip_name = os . path . join ( save_dir , ' temp . zip ' ) <nl> + <nl> + # get the zip file <nl> try : <nl> - name , hdrs = urllib . urlretrieve ( url , name ) <nl> + zip_name , hdrs = urllib . request . urlretrieve ( url = data_url , filename = zip_name ) <nl> except IOError , e : <nl> - print \" Can ' t retrieve % r to % r : % s \" % ( url , save_dir , e ) <nl> + print \" Can ' t retrieve % r to % r : % s \" % ( data_url , save_dir , e ) <nl> return <nl> <nl> - # open the zip file <nl> + # extract , then remove temp file <nl> + extract_zip ( zip_name = zip_name , exclude_term = exclude_term ) <nl> + os . unlink ( zip_name ) <nl> + <nl> + <nl> + def extract_zip ( zip_name , exclude_term = None ) : <nl> + \" \" \" Extracts a zip file to its containing directory . \" \" \" <nl> + <nl> + zip_dir = os . path . dirname ( os . path . abspath ( zip_name ) ) <nl> + <nl> try : <nl> - z = zipfile . ZipFile ( name ) <nl> - except zipfile . error , e : <nl> - print \" Bad zipfile ( from % r ) : % s \" % ( url , e ) <nl> - return <nl> + with zipfile . ZipFile ( zip_name ) as z : <nl> + <nl> + # write each zipped file out if it isn ' t a directory <nl> + files = [ file for file in z . namelist ( ) if not file . endswith ( ' / ' ) ] <nl> + for file in files : <nl> + <nl> + # remove any provided extra directory term from zip file <nl> + if exclude_term : <nl> + dest_file = file . replace ( exclude_term , ' ' ) <nl> + else : <nl> + dest_file = file <nl> <nl> - # loop over files in list and extract each , if it isn ' t a directory <nl> - for n in z . namelist ( ) : <nl> + dest_file = os . path . normpath ( os . path . join ( zip_dir , dest_file ) ) <nl> + dest_dir = os . path . dirname ( dest_file ) <nl> <nl> - if not n . endswith ( ' / ' ) : <nl> - rel_file = n . replace ( exclude_term , ' ' ) <nl> + # make directory if it does not exist <nl> + if not os . path . isdir ( dest_dir ) : <nl> + os . makedirs ( dest_dir ) <nl> <nl> - dest = os . path . normpath ( os . path . join ( save_dir , rel_file ) ) <nl> - destdir = os . path . dirname ( dest ) <nl> - if not os . path . isdir ( destdir ) : <nl> - os . makedirs ( destdir ) <nl> - data = z . read ( n ) <nl> - f = open ( dest , ' w ' ) <nl> - f . write ( data ) <nl> - f . close ( ) <nl> + # read file from zip , then write to <nl> + data = z . read ( file ) <nl> + with open ( dest_file , ' w ' ) as f : <nl> + f . write ( data ) <nl> <nl> - z . close ( ) <nl> - os . unlink ( name ) <nl> + except zipfile . error , e : <nl> + print \" Bad zipfile ( % r ) : % s \" % ( zip_name , e ) <nl> + return <nl> <nl> <nl> if __name__ = = ' __main__ ' : <nl> def download_data ( url , save_dir , exclude_term ) : <nl> zip_file = ' http : / / quantquote . com / files / quantquote_daily_sp500_83986 . zip ' <nl> zip_dir = ' quantquote_daily_sp500_83986 / ' <nl> <nl> - download_data ( url = zip_file , save_dir = this_dir , exclude_term = zip_dir ) <nl> \\ No newline at end of file <nl> + extract_hosted_zip ( data_url = zip_file , save_dir = this_dir , exclude_term = zip_dir ) <nl> \\ No newline at end of file <nl>\n", "msg": "Added python3 support for data extraction script . Factored out the zip extraction portion into its own function . Used context managers where possible .\n"}
{"diff_id": 11861, "repo": "ansible/ansible\n", "sha": "679e75189bc7635c7a2980a2b23ab96b9b71845c\n", "time": "2019-03-08T22:19:12Z\n", "diff": "mmm a / lib / ansible / modules / storage / purestorage / purefa_ds . py <nl> ppp b / lib / ansible / modules / storage / purestorage / purefa_ds . py <nl> <nl> specify OU = for each OU and multiple OUs should be separated by commas . <nl> The order of OUs is important and should get larger in scope from left <nl> to right . Each OU should not exceed 64 characters in length . <nl> + - Not Supported from Purity 5 . 2 . 0 or higher . Use I ( purefa_dsrole ) module . <nl> ro_group : <nl> description : <nl> - Sets the common Name ( CN ) of the configured directory service group <nl> containing users with read - only privileges on the FlashArray . This <nl> name should be just the Common Name of the group without the CN = <nl> specifier . Common Names should not exceed 64 characters in length . <nl> + - Not Supported from Purity 5 . 2 . 0 or higher . Use I ( purefa_dsrole ) module . <nl> sa_group : <nl> description : <nl> - Sets the common Name ( CN ) of the configured directory service group <nl> <nl> FlashArray . This name should be just the Common Name of the group <nl> without the CN = specifier . Common Names should not exceed 64 <nl> characters in length . <nl> + - Not Supported from Purity 5 . 2 . 0 or higher . Use I ( purefa_dsrole ) module . <nl> aa_group : <nl> description : <nl> - Sets the common Name ( CN ) of the directory service group containing <nl> administrators with full privileges when managing the FlashArray . <nl> The name should be just the Common Name of the group without the <nl> CN = specifier . Common Names should not exceed 64 characters in length . <nl> + - Not Supported from Purity 5 . 2 . 0 or higher . Use I ( purefa_dsrole ) module . <nl> extends_documentation_fragment : <nl> - purestorage . fa <nl> ' ' ' <nl> <nl> EXAMPLES = r ' ' ' <nl> - - name : Delete exisitng directory service <nl> + - name : Delete existing directory service <nl> purefa_ds : <nl> state : absent <nl> fa_url : 10 . 10 . 10 . 2 <nl> api_token : e31060a7 - 21fc - e277 - 6240 - 25983c6c4592 <nl> <nl> - - name : Create directory service ( disabled ) <nl> + - name : Create directory service ( disabled ) - Pre - 5 . 2 . 0 <nl> purefa_ds : <nl> uri : \" ldap : / / lab . purestorage . com \" <nl> base_dn : \" DC = lab , DC = purestorage , DC = com \" <nl> <nl> fa_url : 10 . 10 . 10 . 2 <nl> api_token : e31060a7 - 21fc - e277 - 6240 - 25983c6c4592 <nl> <nl> - - name : Enable exisitng directory service <nl> + - name : Create directory service ( disabled ) - 5 . 2 . 0 or higher <nl> + purefa_ds : <nl> + uri : \" ldap : / / lab . purestorage . com \" <nl> + base_dn : \" DC = lab , DC = purestorage , DC = com \" <nl> + bind_user : Administrator <nl> + bind_password : password <nl> + fa_url : 10 . 10 . 10 . 2 <nl> + api_token : e31060a7 - 21fc - e277 - 6240 - 25983c6c4592 <nl> + <nl> + - name : Enable existing directory service <nl> purefa_ds : <nl> enable : true <nl> fa_url : 10 . 10 . 10 . 2 <nl> api_token : e31060a7 - 21fc - e277 - 6240 - 25983c6c4592 <nl> <nl> - - name : Disable exisitng directory service <nl> + - name : Disable existing directory service <nl> purefa_ds : <nl> enable : false <nl> fa_url : 10 . 10 . 10 . 2 <nl> api_token : e31060a7 - 21fc - e277 - 6240 - 25983c6c4592 <nl> <nl> - - name : Create directory service ( enabled ) <nl> + - name : Create directory service ( enabled ) - Pre - 5 . 2 . 0 <nl> purefa_ds : <nl> enable : true <nl> uri : \" ldap : / / lab . purestorage . com \" <nl> <nl> aa_group : PureAdmin <nl> fa_url : 10 . 10 . 10 . 2 <nl> api_token : e31060a7 - 21fc - e277 - 6240 - 25983c6c4592 <nl> + <nl> + - name : Create directory service ( enabled ) - 5 . 2 . 0 or higher <nl> + purefa_ds : <nl> + enable : true <nl> + uri : \" ldap : / / lab . purestorage . com \" <nl> + base_dn : \" DC = lab , DC = purestorage , DC = com \" <nl> + bind_user : Administrator <nl> + bind_password : password <nl> + fa_url : 10 . 10 . 10 . 2 <nl> + api_token : e31060a7 - 21fc - e277 - 6240 - 25983c6c4592 <nl> ' ' ' <nl> <nl> RETURN = r ' ' ' <nl> <nl> from ansible . module_utils . pure import get_system , purefa_argument_spec <nl> <nl> <nl> + DS_ROLE_REQUIRED_API_VERSION = ' 1 . 16 ' <nl> + <nl> + <nl> def update_ds ( module , array ) : <nl> \" \" \" Update Directory Service \" \" \" <nl> changed = False <nl> def delete_ds ( module , array ) : <nl> \" \" \" Delete Directory Service \" \" \" <nl> changed = False <nl> try : <nl> + api_version = array . _list_available_rest_versions ( ) <nl> array . set_directory_service ( enabled = False ) <nl> - array . set_directory_service ( uri = [ ' ' ] , <nl> - base_dn = \" \" , <nl> - group_base = \" \" , <nl> - bind_user = \" \" , <nl> - bind_password = \" \" , <nl> - readonly_group = \" \" , <nl> - storage_admin_group = \" \" , <nl> - array_admin_group = \" \" , <nl> - certificate = \" \" ) <nl> - changed = True <nl> + if DS_ROLE_REQUIRED_API_VERSION in api_version : <nl> + array . set_directory_service ( uri = [ ' ' ] , <nl> + base_dn = \" \" , <nl> + bind_user = \" \" , <nl> + bind_password = \" \" , <nl> + certificate = \" \" ) <nl> + changed = True <nl> + else : <nl> + array . set_directory_service ( uri = [ ' ' ] , <nl> + base_dn = \" \" , <nl> + group_base = \" \" , <nl> + bind_user = \" \" , <nl> + bind_password = \" \" , <nl> + readonly_group = \" \" , <nl> + storage_admin_group = \" \" , <nl> + array_admin_group = \" \" , <nl> + certificate = \" \" ) <nl> + changed = True <nl> except Exception : <nl> module . fail_json ( msg = ' Delete Directory Service failed ' ) <nl> module . exit_json ( changed = changed ) <nl> def delete_ds ( module , array ) : <nl> def create_ds ( module , array ) : <nl> \" \" \" Create Directory Service \" \" \" <nl> changed = False <nl> - groups_rule = [ not module . params [ ' ro_group ' ] , <nl> - not module . params [ ' sa_group ' ] , <nl> - not module . params [ ' aa_group ' ] ] <nl> + api_version = array . _list_available_rest_versions ( ) <nl> + if DS_ROLE_REQUIRED_API_VERSION in api_version : <nl> + if not module . params [ ' role ' ] : <nl> + module . fail_json ( msg = ' At least one role must be configured ' ) <nl> + try : <nl> + array . set_directory_service ( uri = module . params [ ' uri ' ] , <nl> + base_dn = module . params [ ' base_dn ' ] , <nl> + bind_user = module . params [ ' bind_user ' ] , <nl> + bind_password = module . params [ ' bind_password ' ] ) <nl> + array . set_directory_service ( enabled = module . params [ ' enable ' ] ) <nl> + changed = True <nl> + except Exception : <nl> + module . fail_json ( msg = ' Create Directory Service failed : Check configuration ' ) <nl> + else : <nl> + groups_rule = [ not module . params [ ' ro_group ' ] , <nl> + not module . params [ ' sa_group ' ] , <nl> + not module . params [ ' aa_group ' ] ] <nl> <nl> - if all ( groups_rule ) : <nl> - module . fail_json ( msg = ' At least one group must be configured ' ) <nl> - try : <nl> - array . set_directory_service ( uri = module . params [ ' uri ' ] , <nl> - base_dn = module . params [ ' base_dn ' ] , <nl> - group_base = module . params [ ' group_base ' ] , <nl> - bind_user = module . params [ ' bind_user ' ] , <nl> - bind_password = module . params [ ' bind_password ' ] , <nl> - readonly_group = module . params [ ' ro_group ' ] , <nl> - storage_admin_group = module . params [ ' sa_group ' ] , <nl> - array_admin_group = module . params [ ' aa_group ' ] ) <nl> - array . set_directory_service ( enabled = module . params [ ' enable ' ] ) <nl> - changed = True <nl> - except Exception : <nl> - module . fail_json ( msg = ' Create Directory Service failed : Check configuration ' ) <nl> + if all ( groups_rule ) : <nl> + module . fail_json ( msg = ' At least one group must be configured ' ) <nl> + try : <nl> + array . set_directory_service ( uri = module . params [ ' uri ' ] , <nl> + base_dn = module . params [ ' base_dn ' ] , <nl> + group_base = module . params [ ' group_base ' ] , <nl> + bind_user = module . params [ ' bind_user ' ] , <nl> + bind_password = module . params [ ' bind_password ' ] , <nl> + readonly_group = module . params [ ' ro_group ' ] , <nl> + storage_admin_group = module . params [ ' sa_group ' ] , <nl> + array_admin_group = module . params [ ' aa_group ' ] ) <nl> + array . set_directory_service ( enabled = module . params [ ' enable ' ] ) <nl> + changed = True <nl> + except Exception : <nl> + module . fail_json ( msg = ' Create Directory Service failed : Check configuration ' ) <nl> module . exit_json ( changed = changed ) <nl> <nl> <nl>\n", "msg": "Update purefa_ds to support new directory services format ( )\n"}
{"diff_id": 12098, "repo": "scikit-learn/scikit-learn\n", "sha": "b301464fb38007a92c86497cd61a07c64838c51a\n", "time": "2013-10-20T13:58:00Z\n", "diff": "mmm a / sklearn / linear_model / ransac . py <nl> ppp b / sklearn / linear_model / ransac . py <nl> class RANSAC ( BaseEstimator ) : <nl> from a subset of inliers from the complete data set . More information can <nl> be found in the general documentation of linear models . <nl> <nl> + A detailed description of the algorithm can be found in the documentation <nl> + of the ` ` linear_model ` ` sub - package . <nl> + <nl> Parameters <nl> mmmmmmmmm - <nl> base_estimator : object , optional <nl>\n", "msg": "Add reference to narrative documentation for detailed description of RANSAC\n"}
{"diff_id": 12145, "repo": "matplotlib/matplotlib\n", "sha": "bf7ca4c2380278175f804d1b2f5f02f38a0c8586\n", "time": "2011-01-03T19:00:11Z\n", "diff": "new file mode 100644 <nl> index 00000000000 . . 759fea87a80 <nl> mmm / dev / null <nl> ppp b / examples / misc / developer_commit_history . py <nl> <nl> + \" \" \" <nl> + report how many days it has been since each developer committed . You <nl> + must do an <nl> + <nl> + svn log > log . txt <nl> + <nl> + and place the output next to this file before running <nl> + <nl> + \" \" \" <nl> + import os , datetime <nl> + <nl> + import matplotlib . cbook as cbook <nl> + <nl> + todate = cbook . todate ( ' % Y - % m - % d ' ) <nl> + today = datetime . date . today ( ) <nl> + if not os . path . exists ( ' log . txt ' ) : <nl> + print ( ' You must place the \" svn log \" output into a file \" log . txt \" ' ) <nl> + raise SystemExit <nl> + <nl> + parse = False <nl> + <nl> + lastd = dict ( ) <nl> + for line in file ( ' log . txt ' ) : <nl> + if line . startswith ( ' mmmmmm - - ' ) : <nl> + parse = True <nl> + continue <nl> + <nl> + if parse : <nl> + parts = [ part . strip ( ) for part in line . split ( ' | ' ) ] <nl> + developer = parts [ 1 ] <nl> + dateparts = parts [ 2 ] . split ( ' ' ) <nl> + ymd = todate ( dateparts [ 0 ] ) <nl> + <nl> + <nl> + if developer not in lastd : <nl> + lastd [ developer ] = ymd <nl> + <nl> + parse = False <nl> + <nl> + dsu = [ ( ( today - lastdate ) . days , developer ) for developer , lastdate in lastd . items ( ) ] <nl> + <nl> + dsu . sort ( ) <nl> + for timedelta , developer in dsu : <nl> + print ( ' % s : % d ' % ( developer , timedelta ) ) <nl>\n", "msg": "added developer commit history example for annual purge\n"}
{"diff_id": 12148, "repo": "matplotlib/matplotlib\n", "sha": "a145aff2b6d3eb7b179365013d06c20479e66aea\n", "time": "2014-01-27T06:38:20Z\n", "diff": "mmm a / lib / matplotlib / axes / _axes . py <nl> ppp b / lib / matplotlib / axes / _axes . py <nl> def boxplot ( self , x , notch = False , sym = ' b + ' , vert = True , whis = 1 . 5 , <nl> Examples <nl> mmmmmm - - <nl> <nl> - . . plot : : examples / statistics / boxplot_demo . py <nl> + . . plot : : ~ / examples / statistics / boxplot_demo . py <nl> \" \" \" <nl> bxpstats = cbook . boxplot_stats ( x , whis = whis , bootstrap = bootstrap , <nl> labels = labels ) <nl> def bxp ( self , bxpstats , positions = None , widths = None , vert = True , <nl> Examples <nl> mmmmmm - - <nl> <nl> - . . plot : : examples / statistics / bxp_demo . py <nl> + . . plot : : ~ / examples / statistics / bxp_demo . py <nl> \" \" \" <nl> # lists of artists to be output <nl> whiskers = [ ] <nl>\n", "msg": "fixed references to demos in boxplot and bxp docstrings\n"}
{"diff_id": 12379, "repo": "zulip/zulip\n", "sha": "9be1333c6b6490d1bd1c6cf57204289078e60d0f\n", "time": "2013-10-29T21:49:27Z\n", "diff": "mmm a / zerver / lib / socket . py <nl> ppp b / zerver / lib / socket . py <nl> <nl> from zerver . lib . queue import queue_json_publish <nl> from zerver . lib . actions import check_send_message , extract_recipients <nl> from zerver . decorator import JsonableError <nl> + from zerver . lib . utils import statsd <nl> <nl> djsession_engine = import_module ( settings . SESSION_ENGINE ) <nl> def get_user_profile ( session_id ) : <nl> def respond_send_message ( chan , method , props , data ) : <nl> connection = get_connection ( data [ ' server_meta ' ] [ ' connection_id ' ] ) <nl> if connection is not None : <nl> connection . session . send_message ( { ' client_meta ' : data [ ' client_meta ' ] , ' response ' : data [ ' response ' ] } ) <nl> + <nl> + time_elapsed = time . time ( ) - data [ ' server_meta ' ] [ ' start_time ' ] <nl> fake_log_line ( connection . session . conn_info , <nl> - time . time ( ) - data [ ' server_meta ' ] [ ' start_time ' ] , <nl> + time_elapsed , <nl> 200 , ' send_message ' , connection . session . user_profile . email ) <nl> + # Fake the old JSON send_message endpoint <nl> + statsd_prefix = \" webreq . json . send_message . total \" <nl> + statsd . timing ( statsd_prefix , time_elapsed * 1000 ) <nl> <nl> sockjs_router = sockjs . tornado . SockJSRouter ( SocketConnection , \" / sockjs \" , <nl> { ' sockjs_url ' : ' https : / / % s / static / third / sockjs / sockjs - 0 . 3 . 4 . js ' % ( settings . EXTERNAL_HOST , ) , <nl>\n", "msg": "Log send_message times through our new socket infrastructure as well\n"}
{"diff_id": 12462, "repo": "python/cpython\n", "sha": "4f1974496a2c718dfd8f5f5f332d24856dbce4ea\n", "time": "2013-03-07T00:12:03Z\n", "diff": "mmm a / setup . py <nl> ppp b / setup . py <nl> def _decimal_ext ( self ) : <nl> <nl> # Increase warning level for gcc : <nl> if ' gcc ' in cc : <nl> - cmd = ( \" echo ' ' | gcc - Wextra - Wno - missing - field - initializers - E - \" <nl> - \" > / dev / null 2 > & 1 \" ) <nl> + cmd = ( \" echo ' ' | % s - Wextra - Wno - missing - field - initializers - E - \" <nl> + \" > / dev / null 2 > & 1 \" % cc ) <nl> ret = os . system ( cmd ) <nl> if ret > > 8 = = 0 : <nl> extra_compile_args . extend ( [ ' - Wextra ' , <nl>\n", "msg": "Issue : Use cc from sysconfig for testing flags .\n"}
{"diff_id": 12566, "repo": "mitmproxy/mitmproxy\n", "sha": "4cf3392e50da9b5d45027e707c0db5844b93d628\n", "time": "2011-02-10T01:59:51Z\n", "diff": "mmm a / libmproxy / utils . py <nl> ppp b / libmproxy / utils . py <nl> def __repr__ ( self ) : <nl> Returns a string containing a formatted header string . <nl> \" \" \" <nl> headerElements = [ ] <nl> - for key in self . keys ( ) : <nl> + for key in sorted ( self . keys ( ) ) : <nl> for val in self [ key ] : <nl> headerElements . append ( key + \" : \" + val ) <nl> headerElements . append ( \" \" ) <nl>\n", "msg": "Sort header names for a predictable result\n"}
{"diff_id": 12601, "repo": "zulip/zulip\n", "sha": "85c0da67a4e977f901e7b105eaa6a0067bd0a5e5\n", "time": "2016-06-03T16:17:04Z\n", "diff": "mmm a / api / zulip / __init__ . py <nl> ppp b / api / zulip / __init__ . py <nl> def _register ( cls , name , url = None , make_request = None , <nl> if url is None : <nl> url = name <nl> if make_request is None : <nl> - make_request = lambda request = None : { } if request is None else request <nl> + def make_request ( request = None ) : <nl> + if request is None : <nl> + request = { } <nl> + return request <nl> def call ( self , * args , * * kwargs ) : <nl> request = make_request ( * args , * * kwargs ) <nl> if computed_url is not None : <nl>\n", "msg": "Changed make_request lambda to more readable function .\n"}
{"diff_id": 12742, "repo": "open-mmlab/mmdetection\n", "sha": "1a35fb68f8f268a46e9b845273e5ffdd736400de\n", "time": "2020-06-17T06:14:29Z\n", "diff": "mmm a / mmdet / apis / test . py <nl> ppp b / mmdet / apis / test . py <nl> def single_gpu_test ( model , <nl> result = bbox_results , encoded_mask_results <nl> results . append ( result ) <nl> <nl> - batch_size = data [ ' img ' ] [ 0 ] . size ( 0 ) <nl> + batch_size = len ( data [ ' img_metas ' ] [ 0 ] . data ) <nl> for _ in range ( batch_size ) : <nl> prog_bar . update ( ) <nl> return results <nl> def multi_gpu_test ( model , data_loader , tmpdir = None , gpu_collect = False ) : <nl> results . append ( result ) <nl> <nl> if rank = = 0 : <nl> - batch_size = ( <nl> - len ( data [ ' img_meta ' ] . _data ) <nl> - if ' img_meta ' in data else data [ ' img ' ] [ 0 ] . size ( 0 ) ) <nl> + batch_size = len ( data [ ' img_metas ' ] [ 0 ] . data ) <nl> for _ in range ( batch_size * world_size ) : <nl> prog_bar . update ( ) <nl> <nl>\n", "msg": "Use img_metas to indicate batch ( )\n"}
{"diff_id": 12878, "repo": "ansible/ansible\n", "sha": "39824f50b10f7deb6be2a31eac01cc6393d7248e\n", "time": "2019-01-10T01:59:40Z\n", "diff": "mmm a / test / runner / lib / ansible_util . py <nl> ppp b / test / runner / lib / ansible_util . py <nl> def ansible_environment ( args , color = True ) : <nl> ANSIBLE_FORCE_COLOR = ' % s ' % ' true ' if args . color and color else ' false ' , <nl> ANSIBLE_DEPRECATION_WARNINGS = ' false ' , <nl> ANSIBLE_HOST_KEY_CHECKING = ' false ' , <nl> + ANSIBLE_RETRY_FILES_ENABLED = ' false ' , <nl> ANSIBLE_CONFIG = os . path . abspath ( ansible_config ) , <nl> ANSIBLE_LIBRARY = ' / dev / null ' , <nl> PYTHONPATH = os . path . abspath ( ' lib ' ) , <nl>\n", "msg": "Disable retry files for integration tests .\n"}
{"diff_id": 13042, "repo": "matplotlib/matplotlib\n", "sha": "c4c23adfadd528d89fe7443a2e8f59a92c663811\n", "time": "2008-05-07T16:35:29Z\n", "diff": "mmm a / lib / matplotlib / widgets . py <nl> ppp b / lib / matplotlib / widgets . py <nl> def __init__ ( self , ax , labels , active = 0 , activecolor = ' blue ' ) : <nl> def _clicked ( self , event ) : <nl> if event . button ! = 1 : return <nl> if event . inaxes ! = self . ax : return <nl> - xy = self . ax . transAxes . inverse_xy_tup ( ( event . x , event . y ) ) <nl> + xy = self . ax . transAxes . inverted ( ) . transform_point ( ( event . x , event . y ) ) <nl> pclicked = np . array ( [ xy [ 0 ] , xy [ 1 ] ] ) <nl> def inside ( p ) : <nl> pcirc = np . array ( [ p . center [ 0 ] , p . center [ 1 ] ] ) <nl>\n", "msg": "Update radio buttons to new transformation framework ( thanks Matthias Michler )\n"}
{"diff_id": 13127, "repo": "ipython/ipython\n", "sha": "ead8b8f877216e2570f8a3ba85538c3ad48682d2\n", "time": "2011-12-11T23:59:39Z\n", "diff": "mmm a / IPython / zmq / completer . py <nl> ppp b / IPython / zmq / completer . py <nl> <nl> from __future__ import print_function <nl> <nl> import itertools <nl> - import readline <nl> + try : <nl> + import readline <nl> + except ImportError : <nl> + readline = None <nl> import rlcompleter <nl> import time <nl> <nl> class ClientCompleter ( object ) : <nl> and then return them for each value of state . \" \" \" <nl> <nl> def __init__ ( self , client , session , socket ) : <nl> - # ugly , but we get called asynchronously and need access to some <nl> - # client state , like backgrounded code <nl> + # ugly , but we get called asynchronously and need access to some <nl> + # client state , like backgrounded code <nl> + assert readline is not None , \" ClientCompleter depends on readline \" <nl> self . client = client <nl> self . session = session <nl> self . socket = socket <nl>\n", "msg": "allow IPython . zmq . completer to be imported without readline\n"}
{"diff_id": 13368, "repo": "ytdl-org/youtube-dl\n", "sha": "0b26ba3fc8eee0bb047c5e78e6f1a8ba59fa9457\n", "time": "2016-01-16T12:45:36Z\n", "diff": "mmm a / youtube_dl / extractor / common . py <nl> ppp b / youtube_dl / extractor / common . py <nl> def _twitter_search_player ( self , html ) : <nl> return self . _html_search_meta ( ' twitter : player ' , html , <nl> ' twitter card player ' ) <nl> <nl> - def _search_json_ld ( self , html , video_id , fatal = True ) : <nl> + def _search_json_ld ( self , html , video_id , * * kwargs ) : <nl> json_ld = self . _search_regex ( <nl> r ' ( ? s ) < script [ ^ > ] + type = ( [ \" \\ ' ] ) application / ld \\ + json \\ 1 [ ^ > ] * > ( ? P < json_ld > . + ? ) < / script > ' , <nl> - html , ' JSON - LD ' , fatal = fatal , group = ' json_ld ' ) <nl> + html , ' JSON - LD ' , group = ' json_ld ' , * * kwargs ) <nl> if not json_ld : <nl> return { } <nl> - return self . _json_ld ( json_ld , video_id , fatal = fatal ) <nl> + return self . _json_ld ( json_ld , video_id , fatal = kwargs . get ( ' fatal ' , True ) ) <nl> <nl> def _json_ld ( self , json_ld , video_id , fatal = True ) : <nl> if isinstance ( json_ld , compat_str ) : <nl>\n", "msg": "[ extractor / common ] Allow passing more parameters to _search_json_ld\n"}
{"diff_id": 13515, "repo": "scikit-learn/scikit-learn\n", "sha": "71fd662f632165111ec5098835f3c39ee829fd83\n", "time": "2012-09-28T17:16:06Z\n", "diff": "mmm a / sklearn / linear_model / ridge . py <nl> ppp b / sklearn / linear_model / ridge . py <nl> def ridge_regression ( X , y , alpha , sample_weight = 1 . 0 , solver = ' auto ' , <nl> Individual weights for each sample <nl> <nl> solver : { ' auto ' , ' dense_cholesky ' , ' lsqr ' , ' sparse_cg ' } <nl> - Solver to use in the computational <nl> - routines . ' dense_cholesky ' will use the standard <nl> - scipy . linalg . solve function , ' sparse_cg ' will use the <nl> - conjugate gradient solver as found in <nl> - scipy . sparse . linalg . cg while ' auto ' will chose the most <nl> - appropriate depending on the matrix X . ' lsqr ' uses <nl> - a direct regularized least - squares routine provided by scipy . <nl> + Solver to use in the computational routines : <nl> + <nl> + - ' auto ' chooses the solver automatically based on the type of data . <nl> + <nl> + - ' dense_cholesky ' uses the standard scipy . linalg . solve function to <nl> + obtain a closed - form solution . <nl> + <nl> + - ' sparse_cg ' uses the conjugate gradient solver as found in <nl> + scipy . sparse . linalg . cg . As an iterative algorithm , this solver is <nl> + more appropriate than ' dense_cholesky ' for large - scale data <nl> + ( possibility to set ` tol ` and ` max_iter ` ) . <nl> + <nl> + - ' lsqr ' uses the dedicated regularized least - squares routine <nl> + scipy . sparse . linalg . lsqr . It is the fatest but may not be available in <nl> + old scipy versions . It also uses an iterative procedure . <nl> + <nl> + All three solvers support both dense and sparse data . <nl> <nl> tol : float <nl> Precision of the solution . <nl> class Ridge ( _BaseRidge , RegressorMixin ) : <nl> If True , the regressors X are normalized <nl> <nl> solver : { ' auto ' , ' dense_cholesky ' , ' lsqr ' , ' sparse_cg ' } <nl> - Solver to use in the computational <nl> - routines . ' dense_cholesky ' will use the standard <nl> - scipy . linalg . solve function , ' sparse_cg ' will use the <nl> - conjugate gradient solver as found in <nl> - scipy . sparse . linalg . cg while ' auto ' will chose the most <nl> - appropriate depending on the matrix X . ' lsqr ' uses <nl> - a direct regularized least - squares routine provided by scipy . <nl> + Solver to use in the computational routines : <nl> + <nl> + - ' auto ' chooses the solver automatically based on the type of data . <nl> + <nl> + - ' dense_cholesky ' uses the standard scipy . linalg . solve function to <nl> + obtain a closed - form solution . <nl> + <nl> + - ' sparse_cg ' uses the conjugate gradient solver as found in <nl> + scipy . sparse . linalg . cg . As an iterative algorithm , this solver is <nl> + more appropriate than ' dense_cholesky ' for large - scale data <nl> + ( possibility to set ` tol ` and ` max_iter ` ) . <nl> + <nl> + - ' lsqr ' uses the dedicated regularized least - squares routine <nl> + scipy . sparse . linalg . lsqr . It is the fatest but may not be available in <nl> + old scipy versions . It also uses an iterative procedure . <nl> + <nl> + All three solvers support both dense and sparse data . <nl> <nl> tol : float <nl> Precision of the solution . <nl>\n", "msg": "Better documentation on the choice of solver .\n"}
{"diff_id": 13578, "repo": "matplotlib/matplotlib\n", "sha": "14a47c40c1b94bba7d143076160cc503a3aa48af\n", "time": "2014-04-17T00:47:47Z\n", "diff": "mmm a / lib / matplotlib / sphinxext / plot_directive . py <nl> ppp b / lib / matplotlib / sphinxext / plot_directive . py <nl> <nl> The encoding will not be inferred using the ` ` - * - coding - * - ` ` <nl> metacomment . <nl> <nl> - context : bool <nl> + context : bool or str <nl> If provided , the code will be run in the context of all <nl> previous plot directives for which the ` : context : ` option was <nl> specified . This only applies to inline code plot directives , <nl> - not those run from files . <nl> + not those run from files . If the ` ` : context : reset ` ` is specified , <nl> + the context is reset for this and future plots . <nl> <nl> nofigs : bool <nl> If specified , the code block will be run , but no figures will <nl> def _option_boolean ( arg ) : <nl> else : <nl> raise ValueError ( ' \" % s \" unknown boolean ' % arg ) <nl> <nl> + <nl> + def _option_context ( arg ) : <nl> + if arg in [ None , ' reset ' ] : <nl> + return arg <nl> + else : <nl> + raise ValueError ( \" argument should be None or ' reset ' \" ) <nl> + return directives . choice ( arg , ( ' None ' , ' reset ' ) ) <nl> + <nl> + <nl> def _option_format ( arg ) : <nl> return directives . choice ( arg , ( ' python ' , ' doctest ' ) ) <nl> <nl> + <nl> def _option_align ( arg ) : <nl> return directives . choice ( arg , ( \" top \" , \" middle \" , \" bottom \" , \" left \" , \" center \" , <nl> \" right \" ) ) <nl> def setup ( app ) : <nl> ' class ' : directives . class_option , <nl> ' include - source ' : _option_boolean , <nl> ' format ' : _option_format , <nl> - ' context ' : directives . flag , <nl> + ' context ' : _option_context , <nl> ' nofigs ' : directives . flag , <nl> ' encoding ' : directives . encoding <nl> } <nl> def clear_state ( plot_rcparams , close = True ) : <nl> matplotlib . rcParams . update ( plot_rcparams ) <nl> <nl> def render_figures ( code , code_path , output_dir , output_base , context , <nl> - function_name , config ) : <nl> + function_name , config , context_reset = False ) : <nl> \" \" \" <nl> Run a pyplot script and save the low and high res PNGs and a PDF <nl> in outdir . <nl> def render_figures ( code , code_path , output_dir , output_base , context , <nl> else : <nl> ns = { } <nl> <nl> + if context_reset : <nl> + clear_state ( config . plot_rcparams ) <nl> <nl> for i , code_piece in enumerate ( code_pieces ) : <nl> <nl> def run ( arguments , content , options , state_machine , state , lineno ) : <nl> <nl> options . setdefault ( ' include - source ' , config . plot_include_source ) <nl> context = ' context ' in options <nl> + context_reset = True if ( context and options [ ' context ' ] = = ' reset ' ) else False <nl> <nl> rst_file = document . attributes [ ' source ' ] <nl> rst_dir = os . path . dirname ( rst_file ) <nl> def run ( arguments , content , options , state_machine , state , lineno ) : <nl> # make figures <nl> try : <nl> results = render_figures ( code , source_file_name , build_dir , output_base , <nl> - context , function_name , config ) <nl> + context , function_name , config , <nl> + context_reset = context_reset ) <nl> errors = [ ] <nl> except PlotError as err : <nl> reporter = state . memo . reporter <nl>\n", "msg": "Allow : context : directive to take ' reset ' option . Fixes .\n"}
{"diff_id": 13804, "repo": "home-assistant/core\n", "sha": "454bea4237e79357948b5ef5f106f3e3eef4ec9f\n", "time": "2015-01-20T02:57:50Z\n", "diff": "mmm a / homeassistant / __init__ . py <nl> ppp b / homeassistant / __init__ . py <nl> <nl> DOMAIN = \" homeassistant \" <nl> <nl> # How often time_changed event should fire <nl> - TIMER_INTERVAL = 10 # seconds <nl> + TIMER_INTERVAL = 3 # seconds <nl> <nl> # How long we wait for the result of a service call <nl> SERVICE_CALL_LIMIT = 10 # seconds <nl>\n", "msg": "Reduce the timer interval to make sensors useful\n"}
{"diff_id": 13968, "repo": "python/cpython\n", "sha": "b7a0bfe912f203468e67f0541a365a4cc41a7cb2\n", "time": "2012-09-30T21:58:01Z\n", "diff": "mmm a / Lib / bz2 . py <nl> ppp b / Lib / bz2 . py <nl> def _check_not_closed ( self ) : <nl> raise ValueError ( \" I / O operation on closed file \" ) <nl> <nl> def _check_can_read ( self ) : <nl> - if not self . readable ( ) : <nl> + if self . closed : <nl> + raise ValueError ( \" I / O operation on closed file \" ) <nl> + if self . _mode not in ( _MODE_READ , _MODE_READ_EOF ) : <nl> raise io . UnsupportedOperation ( \" File not open for reading \" ) <nl> <nl> def _check_can_write ( self ) : <nl> - if not self . writable ( ) : <nl> + if self . closed : <nl> + raise ValueError ( \" I / O operation on closed file \" ) <nl> + if self . _mode ! = _MODE_WRITE : <nl> raise io . UnsupportedOperation ( \" File not open for writing \" ) <nl> <nl> def _check_can_seek ( self ) : <nl> - if not self . readable ( ) : <nl> + if self . closed : <nl> + raise ValueError ( \" I / O operation on closed file \" ) <nl> + if self . _mode not in ( _MODE_READ , _MODE_READ_EOF ) : <nl> raise io . UnsupportedOperation ( \" Seeking is only supported \" <nl> \" on files open for reading \" ) <nl> if not self . _fp . seekable ( ) : <nl>\n", "msg": "Issue : Further performance improvements for BZ2File .\n"}
{"diff_id": 14244, "repo": "python/cpython\n", "sha": "816a1b75b76b9b6cb74c3ea43508e3507491638e\n", "time": "2001-09-19T11:52:07Z\n", "diff": "mmm a / Lib / encodings / __init__ . py <nl> ppp b / Lib / encodings / __init__ . py <nl> <nl> <nl> \" \" \" # \" <nl> <nl> - import codecs , aliases <nl> + import codecs , aliases , exceptions <nl> <nl> _cache = { } <nl> _unknown = ' - - unknown - - ' <nl> <nl> + class CodecRegistryError ( exceptions . LookupError , <nl> + exceptions . SystemError ) : <nl> + pass <nl> + <nl> def search_function ( encoding ) : <nl> <nl> # Cache lookup <nl> def search_function ( encoding ) : <nl> except AttributeError : <nl> entry = ( ) <nl> if len ( entry ) ! = 4 : <nl> - raise SystemError , \\ <nl> - ' module \" % s . % s \" failed to register ' % \\ <nl> - ( __name__ , modname ) <nl> + raise CodecRegistryError , \\ <nl> + ' module \" % s \" ( % s ) failed to register ' % \\ <nl> + ( mod . __name__ , mod . __file__ ) <nl> for obj in entry : <nl> if not callable ( obj ) : <nl> - raise SystemError , \\ <nl> - ' incompatible codecs in module \" % s . % s \" ' % \\ <nl> - ( __name__ , modname ) <nl> + raise CodecRegistryError , \\ <nl> + ' incompatible codecs in module \" % s \" ( % s ) ' % \\ <nl> + ( mod . __name__ , mod . __file__ ) <nl> <nl> # Cache the codec registry entry <nl> _cache [ encoding ] = entry <nl>\n", "msg": "Fixed search function error reporting in the encodings package\n"}
{"diff_id": 14280, "repo": "zulip/zulip\n", "sha": "1ca2f6fa7f07bd043b2d27f5313fa7218700a502\n", "time": "2013-03-18T19:49:39Z\n", "diff": "new file mode 100644 <nl> index 000000000000 . . 0729be43585b <nl> mmm / dev / null <nl> ppp b / zephyr / management / commands / reset_colors . py <nl> <nl> + from django . core . management . base import BaseCommand <nl> + from zephyr . models import StreamColor , UserProfile , Subscription , Recipient <nl> + <nl> + class Command ( BaseCommand ) : <nl> + help = \" \" \" Reset all colors for a person to the default grey \" \" \" <nl> + <nl> + def handle ( self , * args , * * options ) : <nl> + if not args : <nl> + self . print_help ( \" python manage . py \" , \" reset_colors \" ) <nl> + exit ( 1 ) <nl> + <nl> + for email in args : <nl> + user_profile = UserProfile . objects . get ( user__email__iexact = email ) <nl> + subs = Subscription . objects . filter ( user_profile = user_profile , <nl> + active = True , <nl> + recipient__type = Recipient . STREAM ) <nl> + <nl> + for sub in subs : <nl> + stream_color , _ = StreamColor . objects . get_or_create ( subscription = sub ) <nl> + stream_color . color = StreamColor . DEFAULT_STREAM_COLOR <nl> + stream_color . save ( ) <nl>\n", "msg": "Add a management command to reset your stream colors to the default .\n"}
{"diff_id": 14301, "repo": "python/cpython\n", "sha": "96cf271be9c7046eececc31aac5d65bc904a554d\n", "time": "1999-06-01T18:17:02Z\n", "diff": "mmm a / Tools / idle / ZoomHeight . py <nl> ppp b / Tools / idle / ZoomHeight . py <nl> def __init__ ( self , editwin ) : <nl> <nl> def zoom_height_event ( self , event ) : <nl> top = self . editwin . top <nl> - geom = top . wm_geometry ( ) <nl> - m = re . match ( r \" ( \\ d + ) x ( \\ d + ) \\ + ( - ? \\ d + ) \\ + ( - ? \\ d + ) \" , geom ) <nl> - if not m : <nl> - top . bell ( ) <nl> - return <nl> - width , height , x , y = map ( int , m . groups ( ) ) <nl> - newheight = top . winfo_screenheight ( ) <nl> - if sys . platform = = ' win32 ' : <nl> - newy = 0 <nl> - newheight = newheight - 72 <nl> - else : <nl> - newy = 24 <nl> - newheight = newheight - 96 <nl> - if height > = newheight : <nl> - newgeom = \" \" <nl> - else : <nl> - newgeom = \" % dx % d + % d + % d \" % ( width , newheight , x , newy ) <nl> - top . wm_geometry ( newgeom ) <nl> + zoom_height ( top ) <nl> + <nl> + def zoom_height ( top ) : <nl> + geom = top . wm_geometry ( ) <nl> + m = re . match ( r \" ( \\ d + ) x ( \\ d + ) \\ + ( - ? \\ d + ) \\ + ( - ? \\ d + ) \" , geom ) <nl> + if not m : <nl> + top . bell ( ) <nl> + return <nl> + width , height , x , y = map ( int , m . groups ( ) ) <nl> + newheight = top . winfo_screenheight ( ) <nl> + if sys . platform = = ' win32 ' : <nl> + newy = 0 <nl> + newheight = newheight - 72 <nl> + else : <nl> + newy = 24 <nl> + newheight = newheight - 96 <nl> + if height > = newheight : <nl> + newgeom = \" \" <nl> + else : <nl> + newgeom = \" % dx % d + % d + % d \" % ( width , newheight , x , newy ) <nl> + top . wm_geometry ( newgeom ) <nl>\n", "msg": "Move zoom height functionality to separate function .\n"}
{"diff_id": 14316, "repo": "celery/celery\n", "sha": "a516749fd8ae396b8d27d9e5144889aa68c0d8f0\n", "time": "2010-02-18T00:35:08Z\n", "diff": "mmm a / celery / execute / __init__ . py <nl> ppp b / celery / execute / __init__ . py <nl> def apply_async ( task , args = None , kwargs = None , countdown = None , eta = None , <nl> <nl> \" \" \" <nl> if conf . ALWAYS_EAGER : <nl> - return apply ( task , args , kwargs ) <nl> + return apply ( task , args , kwargs , task_id = task_id ) <nl> return _apply_async ( task , args = args , kwargs = kwargs , countdown = countdown , <nl> eta = eta , task_id = task_id , publisher = publisher , <nl> connection = connection , <nl> def apply ( task , args , kwargs , * * options ) : <nl> \" \" \" <nl> args = args or [ ] <nl> kwargs = kwargs or { } <nl> - task_id = gen_unique_id ( ) <nl> + task_id = options . get ( \" task_id \" , gen_unique_id ( ) ) <nl> retries = options . get ( \" retries \" , 0 ) <nl> <nl> task = tasks [ task . name ] # Make sure we get the instance , not class . <nl>\n", "msg": "Added task_id to apply ( ) call from apply_async . Made apply ( ) respect task_id argument if it exists .\n"}
{"diff_id": 14322, "repo": "python/cpython\n", "sha": "909e334e8a525e8430f1532c0ecf133f19d3d185\n", "time": "2008-01-24T23:50:26Z\n", "diff": "mmm a / Lib / rational . py <nl> ppp b / Lib / rational . py <nl> def _div ( a , b ) : <nl> __truediv__ , __rtruediv__ = _operator_fallbacks ( _div , operator . truediv ) <nl> __div__ , __rdiv__ = _operator_fallbacks ( _div , operator . div ) <nl> <nl> - @ classmethod <nl> - def _floordiv ( cls , a , b ) : <nl> + def __floordiv__ ( a , b ) : <nl> + \" \" \" a / / b \" \" \" <nl> + # Will be math . floor ( a / b ) in 3 . 0 . <nl> div = a / b <nl> if isinstance ( div , RationalAbc ) : <nl> # trunc ( math . floor ( div ) ) doesn ' t work if the rational is <nl> def _floordiv ( cls , a , b ) : <nl> else : <nl> return math . floor ( div ) <nl> <nl> - def __floordiv__ ( a , b ) : <nl> - \" \" \" a / / b \" \" \" <nl> - # Will be math . floor ( a / b ) in 3 . 0 . <nl> - return a . _floordiv ( a , b ) <nl> - <nl> def __rfloordiv__ ( b , a ) : <nl> \" \" \" a / / b \" \" \" <nl> # Will be math . floor ( a / b ) in 3 . 0 . <nl> - return b . _floordiv ( a , b ) <nl> - <nl> - @ classmethod <nl> - def _mod ( cls , a , b ) : <nl> - div = a / / b <nl> - return a - b * div <nl> + div = a / b <nl> + if isinstance ( div , RationalAbc ) : <nl> + # trunc ( math . floor ( div ) ) doesn ' t work if the rational is <nl> + # more precise than a float because the intermediate <nl> + # rounding may cross an integer boundary . <nl> + return div . numerator / / div . denominator <nl> + else : <nl> + return math . floor ( div ) <nl> <nl> def __mod__ ( a , b ) : <nl> \" \" \" a % b \" \" \" <nl> - return a . _mod ( a , b ) <nl> + div = a / / b <nl> + return a - b * div <nl> <nl> def __rmod__ ( b , a ) : <nl> \" \" \" a % b \" \" \" <nl> - return b . _mod ( a , b ) <nl> + div = a / / b <nl> + return a - b * div <nl> <nl> def __pow__ ( a , b ) : <nl> \" \" \" a * * b <nl>\n", "msg": "More code cleanup . Remove unnecessary indirection to useless class methods .\n"}
{"diff_id": 14324, "repo": "python/cpython\n", "sha": "3e533c229003395e2a842e8b893d119c5210f458\n", "time": "2007-08-27T01:03:18Z\n", "diff": "mmm a / Lib / test / test_ssl . py <nl> ppp b / Lib / test / test_ssl . py <nl> <nl> CERTFILE = None <nl> GMAIL_POP_CERTFILE = None <nl> <nl> + <nl> + def handle_error ( prefix ) : <nl> + exc_format = ' ' . join ( traceback . format_exception ( * sys . exc_info ( ) ) ) <nl> + sys . stdout . write ( prefix + exc_format ) <nl> + <nl> + <nl> class BasicTests ( unittest . TestCase ) : <nl> <nl> def testRudeShutdown ( self ) : <nl> def testTLSecho ( self ) : <nl> try : <nl> s1 . connect ( ( ' 127 . 0 . 0 . 1 ' , 10024 ) ) <nl> except : <nl> - sys . stdout . write ( \" connection failure : \\ n \" + ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( \" connection failure : \\ n \" ) <nl> raise test_support . TestFailed ( \" Can ' t connect to test server \" ) <nl> else : <nl> try : <nl> c1 = ssl . sslsocket ( s1 , ssl_version = ssl . PROTOCOL_TLSv1 ) <nl> except : <nl> - sys . stdout . write ( \" SSL handshake failure : \\ n \" + ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( \" SSL handshake failure : \\ n \" ) <nl> raise test_support . TestFailed ( \" Can ' t SSL - handshake with test server \" ) <nl> else : <nl> if not c1 : <nl> def testReadCert ( self ) : <nl> try : <nl> s2 . connect ( ( ' 127 . 0 . 0 . 1 ' , 10024 ) ) <nl> except : <nl> - sys . stdout . write ( \" connection failure : \\ n \" + ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( \" connection failure : \\ n \" ) <nl> raise test_support . TestFailed ( \" Can ' t connect to test server \" ) <nl> else : <nl> try : <nl> c2 = ssl . sslsocket ( s2 , ssl_version = ssl . PROTOCOL_TLSv1 , <nl> cert_reqs = ssl . CERT_REQUIRED , ca_certs = CERTFILE ) <nl> except : <nl> - sys . stdout . write ( \" SSL handshake failure : \\ n \" + ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( \" SSL handshake failure : \\ n \" ) <nl> raise test_support . TestFailed ( \" Can ' t SSL - handshake with test server \" ) <nl> else : <nl> if not c2 : <nl> def run ( self ) : <nl> except : <nl> # here , we want to stop the server , because this shouldn ' t <nl> # happen in the context of our test case <nl> - sys . stdout . write ( \" Test server failure : \\ n \" + ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( \" Test server failure : \\ n \" ) <nl> self . running = False <nl> # normally , we ' d just stop here , but for the test <nl> # harness , we want to stop the server <nl> def run ( self ) : <nl> # sys . stdout . write ( \" \\ nserver : % s \\ n \" % msg . strip ( ) . lower ( ) ) <nl> sslconn . write ( msg . lower ( ) ) <nl> except ssl . sslerror : <nl> - sys . stdout . write ( \" Test server failure : \\ n \" + ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( \" Test server failure : \\ n \" ) <nl> sslconn . close ( ) <nl> self . running = False <nl> # normally , we ' d just stop here , but for the test <nl> # harness , we want to stop the server <nl> self . server . stop ( ) <nl> except : <nl> - sys . stdout . write ( ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( ' ' ) <nl> <nl> def __init__ ( self , port , certificate , ssl_version = None , <nl> certreqs = None , cacerts = None ) : <nl> def run ( self ) : <nl> except KeyboardInterrupt : <nl> self . stop ( ) <nl> except : <nl> - sys . stdout . write ( \" Test server failure : \\ n \" + ' ' . join ( <nl> - traceback . format_exception ( * sys . exc_info ( ) ) ) ) <nl> + handle_error ( \" Test server failure : \\ n \" ) <nl> <nl> def stop ( self ) : <nl> self . active = False <nl>\n", "msg": "Make a utility function for handling ( printing ) an error\n"}
{"diff_id": 14336, "repo": "numpy/numpy\n", "sha": "f2871d19ba37424270ccb2d0d5b937776de5542b\n", "time": "2002-01-09T06:45:56Z\n", "diff": "new file mode 100644 <nl> index 00000000000 . . 2f499e1e2e3 <nl> mmm / dev / null <nl> ppp b / scipy_distutils / extension . py <nl> <nl> + \" \" \" distutils . extension <nl> + <nl> + Provides the Extension class , used to describe C / C + + extension <nl> + modules in setup scripts . <nl> + <nl> + Overridden to support f2py . <nl> + \" \" \" <nl> + <nl> + # created 2000 / 05 / 30 , Greg Ward <nl> + <nl> + __revision__ = \" $ Id $ \" <nl> + <nl> + from distutils . extension import Extension as old_Extension <nl> + <nl> + class Extension ( old_Extension ) : <nl> + def __init__ ( self , name , sources , <nl> + include_dirs = None , <nl> + define_macros = None , <nl> + undef_macros = None , <nl> + library_dirs = None , <nl> + libraries = None , <nl> + runtime_library_dirs = None , <nl> + extra_objects = None , <nl> + extra_compile_args = None , <nl> + extra_link_args = None , <nl> + export_symbols = None , <nl> + f2py_options = None <nl> + ) : <nl> + old_Extension . __init__ ( self , name , sources , <nl> + include_dirs , <nl> + define_macros , <nl> + undef_macros , <nl> + library_dirs , <nl> + libraries , <nl> + runtime_library_dirs , <nl> + extra_objects , <nl> + extra_compile_args , <nl> + extra_link_args , <nl> + export_symbols ) <nl> + self . f2py_options = f2py_options or [ ] <nl> + <nl> + # class Extension <nl>\n", "msg": "extension needed for adding f2py_options keyword to Extension\n"}
{"diff_id": 14368, "repo": "scrapy/scrapy\n", "sha": "bcf81cd3c40c9d756a48e9398fb6388f8d0ea623\n", "time": "2009-08-31T10:36:09Z\n", "diff": "mmm a / scrapy / conf / __init__ . py <nl> ppp b / scrapy / conf / __init__ . py <nl> <nl> import os <nl> import cPickle as pickle <nl> <nl> + from scrapy . conf import default_settings <nl> + <nl> import_ = lambda x : __import__ ( x , { } , { } , [ ' ' ] ) <nl> <nl> class Settings ( object ) : <nl> <nl> - default_settings_module = ' scrapy . conf . default_settings ' <nl> - <nl> def __init__ ( self ) : <nl> self . defaults = { } <nl> - self . global_defaults = import_ ( self . default_settings_module ) <nl> + self . global_defaults = default_settings <nl> self . disabled = os . environ . get ( ' SCRAPY_SETTINGS_DISABLED ' , False ) <nl> settings_module_path = os . environ . get ( ' SCRAPY_SETTINGS_MODULE ' , \\ <nl> os . environ . get ( ' SCRAPYSETTINGS_MODULE ' , ' scrapy_settings ' ) ) <nl>\n", "msg": "minor simplification to how default settings are loaded\n"}
{"diff_id": 14444, "repo": "scikit-learn/scikit-learn\n", "sha": "031e5e7881e3d07eef2aa9dcf245c99f59682676\n", "time": "2014-05-18T14:32:54Z\n", "diff": "mmm a / sklearn / tests / test_isotonic . py <nl> ppp b / sklearn / tests / test_isotonic . py <nl> <nl> from sklearn . isotonic import isotonic_regression , IsotonicRegression <nl> <nl> from sklearn . utils . testing import assert_raises <nl> + from sklearn . utils . testing import assert_equal <nl> from sklearn . utils . testing import assert_array_equal <nl> <nl> <nl> def test_isotonic_regression_reversed ( ) : <nl> np . arange ( len ( y ) ) , y ) <nl> assert_array_equal ( np . ones ( y_ [ : - 1 ] . shape ) , ( ( y_ [ : - 1 ] - y_ [ 1 : ] ) > = 0 ) ) <nl> <nl> + def test_isotonic_regression_pearson_decreasing ( ) : <nl> + # Set y and x for decreasing <nl> + y = np . array ( [ 10 , 9 , 10 , 7 , 6 , 6 . 1 , 5 ] ) <nl> + x = np . arange ( len ( y ) ) <nl> + <nl> + y_ = IsotonicRegression ( increasing = ' pearson ' ) . fit_transform ( <nl> + x , y ) <nl> + <nl> + is_increasing = y_ [ 0 ] < y_ [ - 1 ] <nl> + assert_equal ( is_increasing , False ) <nl> + <nl> + def test_isotonic_regression_pearson_increasing ( ) : <nl> + # Set y and x for decreasing <nl> + y = np . array ( [ 5 , 6 . 1 , 6 , 7 , 10 , 9 , 10 ] ) <nl> + x = np . arange ( len ( y ) ) <nl> + <nl> + y_ = IsotonicRegression ( increasing = ' pearson ' ) . fit_transform ( <nl> + x , y ) <nl> + <nl> + is_increasing = y_ [ 0 ] < y_ [ - 1 ] <nl> + assert_equal ( is_increasing , True ) <nl> + <nl> + def test_isotonic_regression_spearman_decreasing ( ) : <nl> + # Set y and x for decreasing <nl> + y = np . array ( [ 10 , 9 , 10 , 7 , 6 , 6 . 1 , 5 ] ) <nl> + x = np . arange ( len ( y ) ) <nl> + <nl> + y_ = IsotonicRegression ( increasing = ' spearman ' ) . fit_transform ( <nl> + x , y ) <nl> + <nl> + is_increasing = y_ [ 0 ] < y_ [ - 1 ] <nl> + assert_equal ( is_increasing , False ) <nl> + <nl> + def test_isotonic_regression_spearman_increasing ( ) : <nl> + # Set y and x for decreasing <nl> + y = np . array ( [ 5 , 6 . 1 , 6 , 7 , 10 , 9 , 10 ] ) <nl> + x = np . arange ( len ( y ) ) <nl> + <nl> + y_ = IsotonicRegression ( increasing = ' spearman ' ) . fit_transform ( <nl> + x , y ) <nl> + <nl> + is_increasing = y_ [ 0 ] < y_ [ - 1 ] <nl> + assert_equal ( is_increasing , True ) <nl> + <nl> <nl> def test_assert_raises_exceptions ( ) : <nl> ir = IsotonicRegression ( ) <nl>\n", "msg": "Adding increasing and decreasing tests for both Pearson and Spearman increasing argument options\n"}
{"diff_id": 14546, "repo": "matplotlib/matplotlib\n", "sha": "7db54c0ab554d64adba1542c0e2548e5b9497757\n", "time": "2019-06-05T04:11:27Z\n", "diff": "mmm a / lib / matplotlib / backends / backend_qt5agg . py <nl> ppp b / lib / matplotlib / backends / backend_qt5agg . py <nl> def paintEvent ( self , event ) : <nl> <nl> painter = QtGui . QPainter ( self ) <nl> <nl> - # get bounding box scaled to the figure <nl> + # See documentation of QRect : bottom ( ) and right ( ) are off by 1 , so use <nl> + # left ( ) + width ( ) and top ( ) + height ( ) . <nl> rect = event . rect ( ) <nl> - left , top = self . mouseEventCoords ( rect . bottomLeft ( ) ) <nl> width = rect . width ( ) * self . _dpi_ratio <nl> height = rect . height ( ) * self . _dpi_ratio <nl> - # See documentation of QRect : bottom ( ) and right ( ) are off by 1 , so use <nl> - # left ( ) + width ( ) and top ( ) + height ( ) . <nl> + left , top = self . mouseEventCoords ( rect . topLeft ( ) ) <nl> + # shift the \" top \" by the height of the image to get the <nl> + # correct corner for our coordinate system <nl> + top - = height <nl> bbox = Bbox ( [ [ left , top ] , [ left + width , top + height ] ] ) <nl> # create a buffer using this bounding box <nl> reg = self . copy_from_bbox ( bbox ) <nl> def paintEvent ( self , event ) : <nl> if hasattr ( qimage , ' setDevicePixelRatio ' ) : <nl> # Not available on Qt4 or some older Qt5 . <nl> qimage . setDevicePixelRatio ( self . _dpi_ratio ) <nl> + # set origin using original QT coordinates <nl> origin = QtCore . QPoint ( rect . left ( ) , rect . top ( ) ) <nl> painter . drawImage ( origin , qimage ) <nl> # Adjust the buf reference count to work around a memory <nl>\n", "msg": "Using different corner to avoid off - by - one issue .\n"}
{"diff_id": 14610, "repo": "home-assistant/core\n", "sha": "e6445a602b3a3486581102eadf870656a1e96955\n", "time": "2019-07-23T17:05:53Z\n", "diff": "mmm a / homeassistant / components / cover / __init__ . py <nl> ppp b / homeassistant / components / cover / __init__ . py <nl> <nl> from homeassistant . helpers . entity import Entity <nl> from homeassistant . helpers . config_validation import ( # noqa <nl> PLATFORM_SCHEMA , PLATFORM_SCHEMA_BASE ) <nl> - import homeassistant . helpers . config_validation as cv <nl> + from homeassistant . helpers . config_validation import ENTITY_SERVICE_SCHEMA <nl> from homeassistant . components import group <nl> from homeassistant . helpers import intent <nl> from homeassistant . const import ( <nl> <nl> SERVICE_STOP_COVER , SERVICE_TOGGLE , SERVICE_OPEN_COVER_TILT , <nl> SERVICE_CLOSE_COVER_TILT , SERVICE_STOP_COVER_TILT , <nl> SERVICE_SET_COVER_TILT_POSITION , SERVICE_TOGGLE_COVER_TILT , <nl> - STATE_OPEN , STATE_CLOSED , STATE_OPENING , STATE_CLOSING , ATTR_ENTITY_ID ) <nl> + STATE_OPEN , STATE_CLOSED , STATE_OPENING , STATE_CLOSING ) <nl> <nl> _LOGGER = logging . getLogger ( __name__ ) <nl> <nl> <nl> INTENT_OPEN_COVER = ' HassOpenCover ' <nl> INTENT_CLOSE_COVER = ' HassCloseCover ' <nl> <nl> - COVER_SERVICE_SCHEMA = vol . Schema ( { <nl> - vol . Optional ( ATTR_ENTITY_ID ) : cv . comp_entity_ids , <nl> - } ) <nl> - <nl> - COVER_SET_COVER_POSITION_SCHEMA = COVER_SERVICE_SCHEMA . extend ( { <nl> + COVER_SET_COVER_POSITION_SCHEMA = ENTITY_SERVICE_SCHEMA . extend ( { <nl> vol . Required ( ATTR_POSITION ) : <nl> vol . All ( vol . Coerce ( int ) , vol . Range ( min = 0 , max = 100 ) ) , <nl> } ) <nl> <nl> - COVER_SET_COVER_TILT_POSITION_SCHEMA = COVER_SERVICE_SCHEMA . extend ( { <nl> + COVER_SET_COVER_TILT_POSITION_SCHEMA = ENTITY_SERVICE_SCHEMA . extend ( { <nl> vol . Required ( ATTR_TILT_POSITION ) : <nl> vol . All ( vol . Coerce ( int ) , vol . Range ( min = 0 , max = 100 ) ) , <nl> } ) <nl> async def async_setup ( hass , config ) : <nl> await component . async_setup ( config ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_OPEN_COVER , COVER_SERVICE_SCHEMA , <nl> + SERVICE_OPEN_COVER , ENTITY_SERVICE_SCHEMA , <nl> ' async_open_cover ' <nl> ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_CLOSE_COVER , COVER_SERVICE_SCHEMA , <nl> + SERVICE_CLOSE_COVER , ENTITY_SERVICE_SCHEMA , <nl> ' async_close_cover ' <nl> ) <nl> <nl> async def async_setup ( hass , config ) : <nl> ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_STOP_COVER , COVER_SERVICE_SCHEMA , <nl> + SERVICE_STOP_COVER , ENTITY_SERVICE_SCHEMA , <nl> ' async_stop_cover ' <nl> ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_TOGGLE , COVER_SERVICE_SCHEMA , <nl> + SERVICE_TOGGLE , ENTITY_SERVICE_SCHEMA , <nl> ' async_toggle ' <nl> ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_OPEN_COVER_TILT , COVER_SERVICE_SCHEMA , <nl> + SERVICE_OPEN_COVER_TILT , ENTITY_SERVICE_SCHEMA , <nl> ' async_open_cover_tilt ' <nl> ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_CLOSE_COVER_TILT , COVER_SERVICE_SCHEMA , <nl> + SERVICE_CLOSE_COVER_TILT , ENTITY_SERVICE_SCHEMA , <nl> ' async_close_cover_tilt ' <nl> ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_STOP_COVER_TILT , COVER_SERVICE_SCHEMA , <nl> + SERVICE_STOP_COVER_TILT , ENTITY_SERVICE_SCHEMA , <nl> ' async_stop_cover_tilt ' <nl> ) <nl> <nl> async def async_setup ( hass , config ) : <nl> ) <nl> <nl> component . async_register_entity_service ( <nl> - SERVICE_TOGGLE_COVER_TILT , COVER_SERVICE_SCHEMA , <nl> + SERVICE_TOGGLE_COVER_TILT , ENTITY_SERVICE_SCHEMA , <nl> ' async_toggle_tilt ' <nl> ) <nl> <nl>\n", "msg": "Add area support to cover service schemas ( )\n"}
{"diff_id": 14660, "repo": "openai/gym\n", "sha": "9dea81b48a2e1d8f7e7a81211c0f09f627ee61a9\n", "time": "2019-07-23T19:34:39Z\n", "diff": "mmm a / gym / vector / __init__ . py <nl> ppp b / gym / vector / __init__ . py <nl> <nl> + try : <nl> + from collections . abc import Iterable <nl> + except ImportError : <nl> + Iterable = ( tuple , list ) <nl> + <nl> from gym . vector . async_vector_env import AsyncVectorEnv <nl> from gym . vector . sync_vector_env import SyncVectorEnv <nl> from gym . vector . vector_env import VectorEnv <nl> <nl> __all__ = [ ' AsyncVectorEnv ' , ' SyncVectorEnv ' , ' VectorEnv ' , ' make ' ] <nl> <nl> - def make ( id , num_envs = 1 , asynchronous = True , * * kwargs ) : <nl> + def make ( id , num_envs = 1 , asynchronous = True , wrappers = None , * * kwargs ) : <nl> \" \" \" Create a vectorized environment from multiple copies of an environment , <nl> from its id <nl> <nl> def make ( id , num_envs = 1 , asynchronous = True , * * kwargs ) : <nl> If ` True ` , wraps the environments in an ` AsyncVectorEnv ` ( which uses <nl> ` multiprocessing ` to run the environments in parallel ) . If ` False ` , <nl> wraps the environments in a ` SyncVectorEnv ` . <nl> + <nl> + wrappers : Callable or Iterable of Callables ( default : ` None ` ) <nl> + If not ` None ` , then apply the wrappers to each internal <nl> + environment during creation . <nl> <nl> Returns <nl> mmmmmm - <nl> def make ( id , num_envs = 1 , asynchronous = True , * * kwargs ) : <nl> \" \" \" <nl> from gym . envs import make as make_ <nl> def _make_env ( ) : <nl> - return make_ ( id , * * kwargs ) <nl> + env = make_ ( id , * * kwargs ) <nl> + if wrappers is not None : <nl> + if callable ( wrappers ) : <nl> + env = wrappers ( env ) <nl> + elif isinstance ( wrappers , Iterable ) and all ( [ callable ( w ) for w in wrappers ] ) : <nl> + for wrapper in wrappers : <nl> + env = wrapper ( env ) <nl> + else : <nl> + raise NotImplementedError <nl> + return env <nl> env_fns = [ _make_env for _ in range ( num_envs ) ] <nl> return AsyncVectorEnv ( env_fns ) if asynchronous else SyncVectorEnv ( env_fns ) <nl>\n", "msg": "Allow atomic transformation ( sequence of wrapping ) for vectorized environment ( )\n"}
{"diff_id": 14666, "repo": "tornadoweb/tornado\n", "sha": "8e7effdaefaddc5b61ba0aa6ff8e94f1422b6092\n", "time": "2013-08-19T03:54:59Z\n", "diff": "mmm a / tornado / ioloop . py <nl> ppp b / tornado / ioloop . py <nl> def add_callback ( self , callback , * args , * * kwargs ) : <nl> list_empty = not self . _callbacks <nl> self . _callbacks . append ( functools . partial ( <nl> stack_context . wrap ( callback ) , * args , * * kwargs ) ) <nl> - if list_empty and thread . get_ident ( ) ! = self . _thread_ident : <nl> - # If we ' re in the IOLoop ' s thread , we know it ' s not currently <nl> - # polling . If we ' re not , and we added the first callback to an <nl> - # empty list , we may need to wake it up ( it may wake up on its <nl> - # own , but an occasional extra wake is harmless ) . Waking <nl> - # up a polling IOLoop is relatively expensive , so we try to <nl> - # avoid it when we can . <nl> - self . _waker . wake ( ) <nl> + if list_empty and thread . get_ident ( ) ! = self . _thread_ident : <nl> + # If we ' re in the IOLoop ' s thread , we know it ' s not currently <nl> + # polling . If we ' re not , and we added the first callback to an <nl> + # empty list , we may need to wake it up ( it may wake up on its <nl> + # own , but an occasional extra wake is harmless ) . Waking <nl> + # up a polling IOLoop is relatively expensive , so we try to <nl> + # avoid it when we can . <nl> + self . _waker . wake ( ) <nl> <nl> def add_callback_from_signal ( self , callback , * args , * * kwargs ) : <nl> with stack_context . NullContext ( ) : <nl>\n", "msg": "In add_callback , hold the lock while writing to the waker pipe .\n"}
{"diff_id": 14688, "repo": "numpy/numpy\n", "sha": "d8bf05c235e55f08324f1b7e156ef9277f25634c\n", "time": "2017-08-30T20:39:50Z\n", "diff": "mmm a / numpy / lib / npyio . py <nl> ppp b / numpy / lib / npyio . py <nl> def save ( file , arr , allow_pickle = True , fix_imports = True ) : <nl> then the filename is unchanged . If file is a string or Path , a ` ` . npy ` ` <nl> extension will be appended to the file name if it does not already <nl> have one . <nl> + arr : array_like <nl> + Array data to be saved . <nl> allow_pickle : bool , optional <nl> Allow saving object arrays using Python pickles . Reasons for disallowing <nl> pickles include security ( loading pickled data can execute arbitrary <nl> def save ( file , arr , allow_pickle = True , fix_imports = True ) : <nl> pickled in a Python 2 compatible way . If ` fix_imports ` is True , pickle <nl> will try to map the new Python 3 names to the old module names used in <nl> Python 2 , so that the pickle data stream is readable with Python 2 . <nl> - arr : array_like <nl> - Array data to be saved . <nl> <nl> See Also <nl> mmmmmm - - <nl>\n", "msg": "Updates order of parameters in save docstring to match function parameter order\n"}
{"diff_id": 14756, "repo": "zulip/zulip\n", "sha": "7d63928fe1b4d36e5941eb3a149a9a3ac7a3bc40\n", "time": "2017-10-02T06:52:08Z\n", "diff": "mmm a / zerver / tests / test_signup . py <nl> ppp b / zerver / tests / test_signup . py <nl> def test_register ( self ) : <nl> self . assertEqual ( get_session_dict_user ( self . client . session ) , user_profile . id ) <nl> self . assertFalse ( user_profile . enable_stream_desktop_notifications ) <nl> <nl> + @ override_settings ( REALMS_HAVE_SUBDOMAINS = True ) <nl> def test_register_deactivated ( self ) : <nl> # type : ( ) - > None <nl> \" \" \" <nl> def test_register_deactivated ( self ) : <nl> realm . deactivated = True <nl> realm . save ( update_fields = [ \" deactivated \" ] ) <nl> <nl> - result = self . register ( self . nonreg_email ( ' test ' ) , \" test \" ) <nl> + result = self . client_post ( ' / accounts / home / ' , { ' email ' : self . nonreg_email ( ' test ' ) } , <nl> + subdomain = \" zulip \" ) <nl> self . assertEqual ( result . status_code , 302 ) <nl> - self . assertIn ( ' deactivated ' , result . url ) <nl> + self . assertEqual ( ' / accounts / deactivated / ' , result . url ) <nl> <nl> with self . assertRaises ( UserProfile . DoesNotExist ) : <nl> self . nonreg_user ( ' test ' ) <nl> <nl> + @ override_settings ( REALMS_HAVE_SUBDOMAINS = True ) <nl> + def test_register_deactivated_partway_through ( self ) : <nl> + # type : ( ) - > None <nl> + \" \" \" <nl> + If you try to register for a deactivated realm , you get a clear error <nl> + page . <nl> + \" \" \" <nl> + email = self . nonreg_email ( ' test ' ) <nl> + result = self . client_post ( ' / accounts / home / ' , { ' email ' : email } , <nl> + subdomain = \" zulip \" ) <nl> + self . assertEqual ( result . status_code , 302 ) <nl> + print ( result . url ) <nl> + self . assertNotIn ( ' deactivated ' , result . url ) <nl> + <nl> + realm = get_realm ( \" zulip \" ) <nl> + realm . deactivated = True <nl> + realm . save ( update_fields = [ \" deactivated \" ] ) <nl> + <nl> + result = self . submit_reg_form_for_user ( email , \" abcd1234 \" , subdomain = \" zulip \" ) <nl> + self . assertEqual ( result . status_code , 302 ) <nl> + self . assertEqual ( ' / accounts / deactivated / ' , result . url ) <nl> + <nl> + with self . assertRaises ( UserProfile . DoesNotExist ) : <nl> + self . nonreg_user ( ' test ' ) <nl> + <nl> + @ override_settings ( REALMS_HAVE_SUBDOMAINS = True ) <nl> def test_login_deactivated ( self ) : <nl> # type : ( ) - > None <nl> \" \" \" <nl> def test_login_deactivated ( self ) : <nl> realm . deactivated = True <nl> realm . save ( update_fields = [ \" deactivated \" ] ) <nl> <nl> - result = self . login_with_return ( self . example_email ( \" hamlet \" ) ) <nl> - self . assert_in_response ( \" has been deactivated \" , result ) <nl> + result = self . login_with_return ( self . example_email ( \" hamlet \" ) , subdomain = \" zulip \" ) <nl> + self . assertEqual ( result . status_code , 302 ) <nl> + self . assertEqual ( ' / accounts / deactivated / ' , result . url ) <nl> <nl> def test_logout ( self ) : <nl> # type : ( ) - > None <nl>\n", "msg": "test_signup : Update deactivate realm tests for subdomains .\n"}
{"diff_id": 14847, "repo": "zulip/zulip\n", "sha": "beb80219a870b4d3a53d9f49bc268b8901d47ade\n", "time": "2018-11-27T17:18:35Z\n", "diff": "mmm a / zerver / lib / push_notifications . py <nl> ppp b / zerver / lib / push_notifications . py <nl> def handle_push_notification ( user_profile_id : int , missed_message : Dict [ str , Any <nl> user_profile = get_user_profile_by_id ( user_profile_id ) <nl> ( message , user_message ) = access_message ( user_profile , missed_message [ ' message_id ' ] ) <nl> if user_message is not None : <nl> - # If ther user has read the message already , don ' t push - notify . <nl> + # If the user has read the message already , don ' t push - notify . <nl> # <nl> # TODO : It feels like this is already handled when things are <nl> # put in the queue ; maybe we should centralize this logic with <nl>\n", "msg": "push notifications : Fix a comment typo .\n"}
{"diff_id": 14963, "repo": "python/cpython\n", "sha": "671dc20efccf7fb75fa4aa7ac2d94593c58c4abf\n", "time": "1996-12-27T15:26:15Z\n", "diff": "mmm a / Lib / urlparse . py <nl> ppp b / Lib / urlparse . py <nl> <nl> # Characters valid in scheme names <nl> scheme_chars = string . letters + string . digits + ' + - . ' <nl> <nl> + MAX_CACHE_SIZE = 2000 <nl> _parse_cache = { } <nl> <nl> def clear_cache ( ) : <nl> + \" \" \" Clear the parse cache . \" \" \" <nl> global _parse_cache <nl> _parse_cache = { } <nl> <nl> def urlparse ( url , scheme = ' ' , allow_framents = 1 ) : <nl> key = url , scheme , allow_framents <nl> if _parse_cache . has_key ( key ) : <nl> return _parse_cache [ key ] <nl> + if len ( _parse_cache ) > = MAX_CACHE_SIZE : # avoid runaway growth <nl> + clear_cache ( ) <nl> netloc = path = params = query = fragment = ' ' <nl> i = string . find ( url , ' : ' ) <nl> if i > 0 : <nl>\n", "msg": "Crude but effective hack to clear the parser cache every so often .\n"}
{"diff_id": 15176, "repo": "quantopian/zipline\n", "sha": "5cf472c5b2bd650f0312a8d213f2e536ec455c9a\n", "time": "2012-05-15T21:28:13Z\n", "diff": "mmm a / zipline / test / test_optimize . py <nl> ppp b / zipline / test / test_optimize . py <nl> <nl> <nl> allocator = AddressAllocator ( 1000 ) <nl> <nl> - class FinanceTestCase ( TestCase ) : <nl> + class TestUpDown ( TestCase ) : <nl> + \" \" \" This unittest establishes that the BuySellAlgorithm in <nl> + combination with the UpDownSource are suitable for usage in an <nl> + optimization framework . <nl> <nl> + \" \" \" <nl> leased_sockets = defaultdict ( list ) <nl> <nl> def setUp ( self ) : <nl> def setUp ( self ) : <nl> } <nl> <nl> @ timed ( DEFAULT_TIMEOUT ) <nl> - def test_buysell ( self ) : <nl> + def test_source_and_orders ( self ) : <nl> + \" \" \" Establishes that the UpDownSource is having the correct <nl> + behavior and that the BuySellAlgorithm places the buy / sell <nl> + orders at the right time . Moreover , establishes that <nl> + UpDownSource and BuySellAlgorithm interact correctly . \" <nl> + <nl> + \" \" \" <nl> # generate events <nl> trade_count = 50 <nl> sid = 133 <nl> def test_buysell ( self ) : <nl> SIMULATION_STYLE . FIXED_SLIPPAGE <nl> <nl> trading_environment = factory . create_trading_environment ( ) <nl> - source = factory . create_updown_trade_source ( sid , <nl> + source = create_updown_trade_source ( sid , <nl> trade_count , <nl> trading_environment , <nl> base_price , <nl> def test_buysell ( self ) : <nl> \" Algorithm did not sell when price was going to increase . \" <nl> ) <nl> <nl> - def test_buysell_concave ( self ) : <nl> + def test_concavity_of_returns ( self ) : <nl> + \" \" \" Establishes that the free parameter of the BuySellAlgorithm <nl> + and the returns have a ( strictly ) concave relationship in a <nl> + certain region around the max . Moreover , establishes that the <nl> + max returns is at the correct value ( i . e . 0 ) . <nl> + <nl> + \" \" \" <nl> # generate events <nl> trade_count = 6 <nl> sid = 133 <nl> def test_buysell_concave ( self ) : <nl> ziplines = [ ] <nl> for i , test_offset in enumerate ( test_offsets ) : <nl> trading_environment = factory . create_trading_environment ( ) <nl> - source = factory . create_updown_trade_source ( sid , <nl> + source = create_updown_trade_source ( sid , <nl> trade_count , <nl> trading_environment , <nl> base_price , <nl> def test_buysell_concave ( self ) : <nl> <nl> <nl> def test_optimize ( self ) : <nl> + \" \" \" Establishes that a simple gradient descent algorithm <nl> + ( Powell ' s method ) can find the free parameter of the <nl> + BuySellAlgorithm producing maximum returns . <nl> + <nl> + \" \" \" <nl> def simulate ( offset ) : <nl> # generate events <nl> trade_count = 3 <nl> def simulate ( offset ) : <nl> zipline = SimulatedTrading . create_test_zipline ( * * self . zipline_test_config ) <nl> zipline . simulate ( blocking = True ) <nl> zipline . shutdown ( ) <nl> - # function is getting minimized , so have return negative . <nl> + # function is getting minimized , so have to return negative cum returns . <nl> return - zipline . get_cumulative_performance ( ) [ ' returns ' ] <nl> <nl> from scipy import optimize <nl>\n", "msg": "Fixed wrong path for UpDownTradeSource . Added docs .\n"}
{"diff_id": 15206, "repo": "encode/django-rest-framework\n", "sha": "36119cad31ea8a0d5a248fd0a1929350d291ef92\n", "time": "2018-03-26T11:22:45Z\n", "diff": "mmm a / rest_framework / filters . py <nl> ppp b / rest_framework / filters . py <nl> def filter_queryset ( self , request , queryset , view ) : <nl> <nl> def get_template_context ( self , request , queryset , view ) : <nl> current = self . get_ordering ( request , queryset , view ) <nl> - current = None if current is None else current [ 0 ] <nl> + current = None if not current else current [ 0 ] <nl> options = [ ] <nl> context = { <nl> ' request ' : request , <nl>\n", "msg": "Enable OrderingFilter to handle an empty tuple ( or list ) for the ' ordering ' field . ( )\n"}
{"diff_id": 15330, "repo": "zulip/zulip\n", "sha": "c9b801efded50751af8f6efcac821ef880a8db98\n", "time": "2018-12-06T00:15:01Z\n", "diff": "mmm a / zerver / lib / import_realm . py <nl> ppp b / zerver / lib / import_realm . py <nl> def import_uploads_local ( import_dir : Path , records : List [ Dict [ str , Any ] ] , <nl> random_name ( 18 ) , <nl> sanitize_name ( os . path . basename ( record [ ' path ' ] ) ) <nl> ] ) <nl> - path_maps [ ' attachment_path ' ] [ record [ ' path ' ] ] = relative_path <nl> + path_maps [ ' attachment_path ' ] [ record [ ' s3_path ' ] ] = relative_path <nl> <nl> if processing_avatars or processing_emojis : <nl> file_path = os . path . join ( settings . LOCAL_UPLOADS_DIR , \" avatars \" , relative_path ) <nl>\n", "msg": "import : Use the s3_path attribute for path_maps unconditionally .\n"}
{"diff_id": 15412, "repo": "tornadoweb/tornado\n", "sha": "d577bc518d1dc73edc3b4863cf59002c823e5881\n", "time": "2013-03-16T02:24:13Z\n", "diff": "mmm a / tornado / auth . py <nl> ppp b / tornado / auth . py <nl> <nl> <nl> Example usage for Google OpenID : : <nl> <nl> - class GoogleHandler ( tornado . web . RequestHandler , tornado . auth . GoogleMixin ) : <nl> + class GoogleLoginHandler ( tornado . web . RequestHandler , <nl> + tornado . auth . GoogleMixin ) : <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> if self . get_argument ( \" openid . mode \" , None ) : <nl> - self . get_authenticated_user ( self . async_callback ( self . _on_auth ) ) <nl> - return <nl> - self . authenticate_redirect ( ) <nl> - <nl> - def _on_auth ( self , user ) : <nl> - if not user : <nl> - raise tornado . web . HTTPError ( 500 , \" Google auth failed \" ) <nl> - # Save the user with , e . g . , set_secure_cookie ( ) <nl> + user = yield self . get_authenticated_user ( ) <nl> + # Save the user with e . g . set_secure_cookie ( ) <nl> + else : <nl> + self . authenticate_redirect ( ) <nl> \" \" \" <nl> <nl> from __future__ import absolute_import , division , print_function , with_statement <nl> class TwitterMixin ( OAuthMixin ) : <nl> class TwitterLoginHandler ( tornado . web . RequestHandler , <nl> tornado . auth . TwitterMixin ) : <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> if self . get_argument ( \" oauth_token \" , None ) : <nl> - self . get_authenticated_user ( self . async_callback ( self . _on_auth ) ) <nl> - return <nl> - self . authorize_redirect ( ) <nl> - <nl> - def _on_auth ( self , user ) : <nl> - if not user : <nl> - raise tornado . web . HTTPError ( 500 , \" Twitter auth failed \" ) <nl> - # Save the user using , e . g . , set_secure_cookie ( ) <nl> + user = yield self . get_authenticated_user ( ) <nl> + # Save the user using e . g . set_secure_cookie ( ) <nl> + else : <nl> + self . authorize_redirect ( ) <nl> <nl> The user object returned by ` get_authenticated_user ( ) ` includes the <nl> attributes ` ` username ` ` , ` ` name ` ` , ` ` access_token ` ` , and all of the <nl> class MainHandler ( tornado . web . RequestHandler , <nl> tornado . auth . TwitterMixin ) : <nl> @ tornado . web . authenticated <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> - self . twitter_request ( <nl> + new_entry = yield self . twitter_request ( <nl> \" / statuses / update \" , <nl> post_args = { \" status \" : \" Testing Tornado Web Server \" } , <nl> - access_token = user [ \" access_token \" ] , <nl> - callback = self . async_callback ( self . _on_post ) ) <nl> - <nl> - def _on_post ( self , new_entry ) : <nl> + access_token = self . current_user [ \" access_token \" ] ) <nl> if not new_entry : <nl> # Call failed ; perhaps missing permission ? <nl> self . authorize_redirect ( ) <nl> class FriendFeedMixin ( OAuthMixin ) : <nl> When your application is set up , you can use this mixin like this <nl> to authenticate the user with FriendFeed and get access to their feed : : <nl> <nl> - class FriendFeedHandler ( tornado . web . RequestHandler , <nl> - tornado . auth . FriendFeedMixin ) : <nl> + class FriendFeedLoginHandler ( tornado . web . RequestHandler , <nl> + tornado . auth . FriendFeedMixin ) : <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> if self . get_argument ( \" oauth_token \" , None ) : <nl> - self . get_authenticated_user ( self . async_callback ( self . _on_auth ) ) <nl> - return <nl> - self . authorize_redirect ( ) <nl> - <nl> - def _on_auth ( self , user ) : <nl> - if not user : <nl> - raise tornado . web . HTTPError ( 500 , \" FriendFeed auth failed \" ) <nl> - # Save the user using , e . g . , set_secure_cookie ( ) <nl> + user = yield self . get_authenticated_user ( ) <nl> + # Save the user using e . g . set_secure_cookie ( ) <nl> + else : <nl> + self . authorize_redirect ( ) <nl> <nl> The user object returned by ` ~ OAuthMixin . get_authenticated_user ( ) ` includes the <nl> attributes ` ` username ` ` , ` ` name ` ` , and ` ` description ` ` in addition to <nl> class MainHandler ( tornado . web . RequestHandler , <nl> tornado . auth . FriendFeedMixin ) : <nl> @ tornado . web . authenticated <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> - self . friendfeed_request ( <nl> + new_entry = yield self . friendfeed_request ( <nl> \" / entry \" , <nl> post_args = { \" body \" : \" Testing Tornado Web Server \" } , <nl> - access_token = self . current_user [ \" access_token \" ] , <nl> - callback = self . async_callback ( self . _on_post ) ) <nl> + access_token = self . current_user [ \" access_token \" ] ) <nl> <nl> - def _on_post ( self , new_entry ) : <nl> if not new_entry : <nl> # Call failed ; perhaps missing permission ? <nl> self . authorize_redirect ( ) <nl> class GoogleMixin ( OpenIdMixin , OAuthMixin ) : <nl> <nl> Example usage : : <nl> <nl> - class GoogleHandler ( tornado . web . RequestHandler , tornado . auth . GoogleMixin ) : <nl> + class GoogleLoginHandler ( tornado . web . RequestHandler , <nl> + tornado . auth . GoogleMixin ) : <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> if self . get_argument ( \" openid . mode \" , None ) : <nl> - self . get_authenticated_user ( self . async_callback ( self . _on_auth ) ) <nl> - return <nl> - self . authenticate_redirect ( ) <nl> - <nl> - def _on_auth ( self , user ) : <nl> - if not user : <nl> - raise tornado . web . HTTPError ( 500 , \" Google auth failed \" ) <nl> - # Save the user with , e . g . , set_secure_cookie ( ) <nl> - <nl> + user = yield self . get_authenticated_user ( ) <nl> + # Save the user with e . g . set_secure_cookie ( ) <nl> + else : <nl> + self . authenticate_redirect ( ) <nl> \" \" \" <nl> _OPENID_ENDPOINT = \" https : / / www . google . com / accounts / o8 / ud \" <nl> _OAUTH_ACCESS_TOKEN_URL = \" https : / / www . google . com / accounts / OAuthGetAccessToken \" <nl> def get_authenticated_user ( self , redirect_uri , client_id , client_secret , <nl> <nl> class FacebookGraphLoginHandler ( LoginHandler , tornado . auth . FacebookGraphMixin ) : <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> if self . get_argument ( \" code \" , False ) : <nl> - self . get_authenticated_user ( <nl> - redirect_uri = ' / auth / facebookgraph / ' , <nl> - client_id = self . settings [ \" facebook_api_key \" ] , <nl> - client_secret = self . settings [ \" facebook_secret \" ] , <nl> - code = self . get_argument ( \" code \" ) , <nl> - callback = self . async_callback ( <nl> - self . _on_login ) ) <nl> - return <nl> - self . authorize_redirect ( redirect_uri = ' / auth / facebookgraph / ' , <nl> - client_id = self . settings [ \" facebook_api_key \" ] , <nl> - extra_params = { \" scope \" : \" read_stream , offline_access \" } ) <nl> - <nl> - def _on_login ( self , user ) : <nl> - logging . error ( user ) <nl> - self . finish ( ) <nl> - <nl> + user = yield self . get_authenticated_user ( <nl> + redirect_uri = ' / auth / facebookgraph / ' , <nl> + client_id = self . settings [ \" facebook_api_key \" ] , <nl> + client_secret = self . settings [ \" facebook_secret \" ] , <nl> + code = self . get_argument ( \" code \" ) ) <nl> + # Save the user with e . g . set_secure_cookie <nl> + else : <nl> + self . authorize_redirect ( <nl> + redirect_uri = ' / auth / facebookgraph / ' , <nl> + client_id = self . settings [ \" facebook_api_key \" ] , <nl> + extra_params = { \" scope \" : \" read_stream , offline_access \" } ) <nl> \" \" \" <nl> http = self . get_auth_http_client ( ) <nl> args = { <nl> class MainHandler ( tornado . web . RequestHandler , <nl> tornado . auth . FacebookGraphMixin ) : <nl> @ tornado . web . authenticated <nl> @ tornado . web . asynchronous <nl> + @ tornado . gen . coroutine <nl> def get ( self ) : <nl> - self . facebook_request ( <nl> + new_entry = yield self . facebook_request ( <nl> \" / me / feed \" , <nl> post_args = { \" message \" : \" I am posting from my Tornado application ! \" } , <nl> - access_token = self . current_user [ \" access_token \" ] , <nl> - callback = self . async_callback ( self . _on_post ) ) <nl> + access_token = self . current_user [ \" access_token \" ] ) <nl> <nl> - def _on_post ( self , new_entry ) : <nl> if not new_entry : <nl> # Call failed ; perhaps missing permission ? <nl> self . authorize_redirect ( ) <nl> return <nl> self . finish ( \" Posted a message ! \" ) <nl> - <nl> \" \" \" <nl> url = \" https : / / graph . facebook . com \" + path <nl> all_args = { } <nl>\n", "msg": "Convert auth module code samples to use gen . coroutine .\n"}
{"diff_id": 15579, "repo": "python/cpython\n", "sha": "1790dd4b66844bec4dfc27a128d968eda36036cc\n", "time": "2000-07-24T06:55:00Z\n", "diff": "mmm a / Lib / test / test_support . py <nl> ppp b / Lib / test / test_support . py <nl> <nl> - # Python test set - - supporting definitions . <nl> + \" \" \" Supporting definitions for the Python regression test . \" \" \" <nl> + <nl> + <nl> + class Error ( Exception ) : <nl> + \" \" \" Base class for regression test exceptions . \" \" \" <nl> + <nl> + class TestFailed ( Error ) : <nl> + \" \" \" Test failed . \" \" \" <nl> + <nl> + class TestSkipped ( Error ) : <nl> + \" \" \" Test skipped . <nl> + <nl> + This can be raised to indicate that a test was deliberatly <nl> + skipped , but not because a feature wasn ' t available . For <nl> + example , if some resource can ' t be used , such as the network <nl> + appears to be unavailable , this should be raised instead of <nl> + TestFailed . <nl> + <nl> + \" \" \" <nl> <nl> - class TestFailed ( Exception ) : <nl> - pass <nl> <nl> verbose = 1 # Flag set to 0 by regrtest . py <nl> + use_large_resources = 1 # Flag set to 0 by regrtest . py <nl> <nl> def unload ( name ) : <nl> import sys <nl>\n", "msg": "Restore TestSkipped exception ; appears to have disappeared in last checkin .\n"}
{"diff_id": 15962, "repo": "home-assistant/core\n", "sha": "edff53609fbad5e2601bd805657dbe4055837c88\n", "time": "2015-12-22T19:50:59Z\n", "diff": "mmm a / homeassistant / components / logger . py <nl> ppp b / homeassistant / components / logger . py <nl> def setup ( hass , config = None ) : <nl> <nl> # Set log filter for all log handler <nl> for handler in logging . root . handlers : <nl> + handler . setLevel ( logging . NOTSET ) <nl> handler . addFilter ( HomeAssistantLogFilter ( logfilter ) ) <nl> <nl> return True <nl>\n", "msg": "Reset log handlers to lowest level .\n"}
{"diff_id": 16076, "repo": "locustio/locust\n", "sha": "a16d657a41cb5305f2a13ad2acbbe2f4069121f3\n", "time": "2011-05-13T14:15:36Z\n", "diff": "mmm a / locust / web . py <nl> ppp b / locust / web . py <nl> def request_stats ( ) : <nl> try : <nl> report [ \" fail_ratio \" ] = float ( stats [ len ( stats ) - 1 ] [ \" num_failures \" ] ) / stats [ len ( stats ) - 1 ] [ \" num_reqs \" ] <nl> except ZeroDivisionError : <nl> - report [ \" fail_ratio \" ] = 0 <nl> + if stats [ len ( stats ) - 1 ] [ \" num_failures \" ] > 0 : <nl> + report [ \" fail_ratio \" ] = 100 <nl> + else : <nl> + report [ \" fail_ratio \" ] = 0 <nl> _request_stats_context_cache = { \" time \" : time ( ) , \" report \" : report } <nl> else : <nl> report = _request_stats_context_cache [ \" report \" ] <nl>\n", "msg": "Small error ratio UI fix if all requests has failed .\n"}
{"diff_id": 16201, "repo": "ansible/ansible\n", "sha": "6ff0b079b4688df943af2da71c7533a0bf9ba0cb\n", "time": "2016-03-02T22:30:16Z\n", "diff": "mmm a / lib / ansible / module_utils / urls . py <nl> ppp b / lib / ansible / module_utils / urls . py <nl> def get_method ( self ) : <nl> return urllib2 . Request . get_method ( self ) <nl> <nl> <nl> - def RedirectHandlerFactory ( follow_redirects = None ) : <nl> + def RedirectHandlerFactory ( follow_redirects = None , validate_certs = True ) : <nl> \" \" \" This is a class factory that closes over the value of <nl> ` ` follow_redirects ` ` so that the RedirectHandler class has access to <nl> that value without having to use globals , and potentially cause problems <nl> class RedirectHandler ( urllib2 . HTTPRedirectHandler ) : <nl> \" \" \" <nl> <nl> def redirect_request ( self , req , fp , code , msg , hdrs , newurl ) : <nl> - if follow_redirects = = ' urllib2 ' : <nl> - return urllib2 . HTTPRedirectHandler . redirect_request ( self , req , <nl> - fp , code , <nl> - msg , hdrs , <nl> - newurl ) <nl> + handler = maybe_add_ssl_handler ( newurl , validate_certs ) <nl> + if handler : <nl> + urllib2 . _opener . add_handler ( handler ) <nl> <nl> - if follow_redirects in [ None , ' no ' , ' none ' ] : <nl> + if follow_redirects = = ' urllib2 ' : <nl> + return urllib2 . HTTPRedirectHandler . redirect_request ( self , req , fp , code , msg , hdrs , newurl ) <nl> + elif follow_redirects in [ ' no ' , ' none ' , False ] : <nl> raise urllib2 . HTTPError ( newurl , code , msg , hdrs , fp ) <nl> <nl> do_redirect = False <nl> - if follow_redirects in [ ' all ' , ' yes ' ] : <nl> + if follow_redirects in [ ' all ' , ' yes ' , True ] : <nl> do_redirect = ( code > = 300 and code < 400 ) <nl> <nl> elif follow_redirects = = ' safe ' : <nl> def http_request ( self , req ) : <nl> <nl> https_request = http_request <nl> <nl> - # Rewrite of fetch_url to not require the module environment <nl> - def open_url ( url , data = None , headers = None , method = None , use_proxy = True , <nl> - force = False , last_mod_time = None , timeout = 10 , validate_certs = True , <nl> - url_username = None , url_password = None , http_agent = None , <nl> - force_basic_auth = False , follow_redirects = ' urllib2 ' ) : <nl> - ' ' ' <nl> - Fetches a file from an HTTP / FTP server using urllib2 <nl> - ' ' ' <nl> - handlers = [ ] <nl> + def maybe_add_ssl_handler ( url , validate_certs ) : <nl> # FIXME : change the following to use the generic_urlparse function <nl> # to remove the indexed references for ' parsed ' <nl> parsed = urlparse . urlparse ( url ) <nl> def open_url ( url , data = None , headers = None , method = None , use_proxy = True , <nl> port = 443 <nl> # create the SSL validation handler and <nl> # add it to the list of handlers <nl> - ssl_handler = SSLValidationHandler ( hostname , port ) <nl> + return SSLValidationHandler ( hostname , port ) <nl> + <nl> + # Rewrite of fetch_url to not require the module environment <nl> + def open_url ( url , data = None , headers = None , method = None , use_proxy = True , <nl> + force = False , last_mod_time = None , timeout = 10 , validate_certs = True , <nl> + url_username = None , url_password = None , http_agent = None , <nl> + force_basic_auth = False , follow_redirects = False ) : <nl> + ' ' ' <nl> + Fetches a file from an HTTP / FTP server using urllib2 <nl> + ' ' ' <nl> + handlers = [ ] <nl> + ssl_handler = maybe_add_ssl_handler ( url , validate_certs ) <nl> + if ssl_handler : <nl> handlers . append ( ssl_handler ) <nl> <nl> + # FIXME : change the following to use the generic_urlparse function <nl> + # to remove the indexed references for ' parsed ' <nl> + parsed = urlparse . urlparse ( url ) <nl> if parsed [ 0 ] ! = ' ftp ' : <nl> username = url_username <nl> <nl> def open_url ( url , data = None , headers = None , method = None , use_proxy = True , <nl> if hasattr ( socket , ' create_connection ' ) and CustomHTTPSHandler : <nl> handlers . append ( CustomHTTPSHandler ) <nl> <nl> - if follow_redirects ! = ' urllib2 ' : <nl> - handlers . append ( RedirectHandlerFactory ( follow_redirects ) ) <nl> + handlers . append ( RedirectHandlerFactory ( follow_redirects , validate_certs ) ) <nl> <nl> opener = urllib2 . build_opener ( * handlers ) <nl> urllib2 . install_opener ( opener ) <nl> def fetch_url ( module , url , data = None , headers = None , method = None , <nl> password = module . params . get ( ' url_password ' , ' ' ) <nl> http_agent = module . params . get ( ' http_agent ' , None ) <nl> force_basic_auth = module . params . get ( ' force_basic_auth ' , ' ' ) <nl> - follow_redirects = follow_redirects or module . params . get ( ' follow_redirects ' , ' urllib2 ' ) <nl> + <nl> + if not follow_redirects : <nl> + follow_redirects = module . params . get ( ' follow_redirects ' , False ) <nl> <nl> r = None <nl> info = dict ( url = url ) <nl>\n", "msg": "Support SSL validation with redirect control for python versions without ssl context\n"}
{"diff_id": 16314, "repo": "faif/python-patterns\n", "sha": "69b507b87edd49896ff8877f15010e4f8d30d9e9\n", "time": "2015-11-08T17:33:41Z\n", "diff": "mmm a / 3 - tier . py <nl> ppp b / 3 - tier . py <nl> def __get__ ( self , obj , klas ) : <nl> class BusinessLogic ( object ) : <nl> \" \" \" Business logic holding data store instances \" \" \" <nl> <nl> - data = Data ( ) <nl> + def __init__ ( self , data ) : <nl> + self . data = data <nl> <nl> def product_list ( self ) : <nl> return self . data [ ' products ' ] . keys ( ) <nl> def product_information ( self , product ) : <nl> class Ui ( object ) : <nl> \" \" \" UI interaction class \" \" \" <nl> <nl> - def __init__ ( self ) : <nl> - self . business_logic = BusinessLogic ( ) <nl> + def __init__ ( self , logic ) : <nl> + self . business_logic = logic <nl> <nl> def get_product_list ( self ) : <nl> print ( ' PRODUCT LIST : ' ) <nl> def get_product_information ( self , product ) : <nl> <nl> <nl> def main ( ) : <nl> - ui = Ui ( ) <nl> + data = Data ( ) <nl> + logic = BusinessLogic ( data ) <nl> + ui = Ui ( logic ) <nl> ui . get_product_list ( ) <nl> ui . get_product_information ( ' cheese ' ) <nl> ui . get_product_information ( ' eggs ' ) <nl> def main ( ) : <nl> # cheese <nl> # eggs <nl> # milk <nl> - # <nl> + # <nl> # ( Fetching from Data Store ) <nl> # PRODUCT INFORMATION : <nl> # Name : Cheese , Price : 2 . 00 , Quantity : 10 <nl>\n", "msg": "Adjustment to UI and Business logic for injecting dependencies\n"}
{"diff_id": 16386, "repo": "zulip/zulip\n", "sha": "84bbed6c49c840c41c8f6a3bbcb7b6e2636a4696\n", "time": "2017-06-07T04:40:28Z\n", "diff": "mmm a / zerver / templatetags / app_filters . py <nl> ppp b / zerver / templatetags / app_filters . py <nl> def render_markdown_path ( markdown_file_path , context = None ) : <nl> if context is None : <nl> context = { } <nl> <nl> - if markdown_file_path . endswith ( ' doc . md ' ) : <nl> - integration_dir = markdown_file_path . split ( ' / ' ) [ 0 ] <nl> - integration = context [ ' integrations_dict ' ] [ integration_dir ] <nl> - if integration . name = = ' bitbucket2 ' : <nl> - context [ ' integration_name ' ] = ' bitbucket ' <nl> + if markdown_file_path . endswith ( ' . md ' ) : <nl> + if markdown_file_path . endswith ( ' doc . md ' ) : <nl> + integration_dir = markdown_file_path . split ( ' / ' ) [ 0 ] <nl> + integration = context [ ' integrations_dict ' ] [ integration_dir ] <nl> + if integration . name = = ' bitbucket2 ' : <nl> + context [ ' integration_name ' ] = ' bitbucket ' <nl> + else : <nl> + context [ ' integration_name ' ] = integration . name <nl> + context [ ' integration_display_name ' ] = integration . display_name <nl> + context [ ' integration_url ' ] = integration . url [ 3 : ] <nl> else : <nl> - context [ ' integration_name ' ] = integration . name <nl> - context [ ' integration_display_name ' ] = integration . display_name <nl> - context [ ' integration_url ' ] = integration . url [ 3 : ] <nl> + integration_dir = markdown_file_path . split ( ' / ' ) [ - 1 ] . split ( ' . ' ) [ 0 ] <nl> + try : <nl> + if integration_dir in list ( context [ ' integrations_dict ' ] . keys ( ) ) : <nl> + integration = context [ ' integrations_dict ' ] [ integration_dir ] <nl> + context [ ' integration_name ' ] = integration . name <nl> + context [ ' integration_display_name ' ] = integration . display_name <nl> + except KeyError : <nl> + pass <nl> <nl> jinja = engines [ ' Jinja2 ' ] <nl> markdown_string = jinja . env . loader . get_source ( jinja . env , markdown_file_path ) [ 0 ] <nl>\n", "msg": "Add integration name and display_name to context for rendering .\n"}
{"diff_id": 16550, "repo": "celery/celery\n", "sha": "e29aa4fd8ea4a72747c12b0fb3a03e71347a4e24\n", "time": "2009-04-28T11:02:27Z\n", "diff": "mmm a / celery / worker . py <nl> ppp b / celery / worker . py <nl> def fetch_next_task ( self ) : <nl> except Exception , error : <nl> self . logger . critical ( \" Worker got exception % s : % s \\ n % s \" % ( <nl> error . __class__ , error , traceback . format_exc ( ) ) ) <nl> - else : <nl> - message . ack ( ) <nl> + return <nl> <nl> + message . ack ( ) <nl> return result , task_name , task_id <nl> <nl> def run_periodic_tasks ( self ) : <nl> def run ( self ) : <nl> self . run_periodic_tasks ( ) <nl> try : <nl> result , task_name , task_id = self . fetch_next_task ( ) <nl> + except ValueError : <nl> + # fetch_next_task didn ' t return a r / name / id tuple , <nl> + # probably because it got an exception . <nl> + continue <nl> except EmptyQueue : <nl> if not last_empty_emit or \\ <nl> time . time ( ) > last_empty_emit + EMPTY_MSG_EMIT_EVERY : <nl>\n", "msg": "Handle the case where fetch_next_task ( ) returns None ( because of an exception )\n"}
{"diff_id": 16580, "repo": "matplotlib/matplotlib\n", "sha": "26a422640e6b99a01ad53dd687e8abfcf579e3a5\n", "time": "2020-05-06T13:42:49Z\n", "diff": "mmm a / lib / mpl_toolkits / mplot3d / axes3d . py <nl> ppp b / lib / mpl_toolkits / mplot3d / axes3d . py <nl> <nl> import matplotlib . colors as mcolors <nl> import matplotlib . docstring as docstring <nl> import matplotlib . scale as mscale <nl> + import matplotlib . container as mcontainer <nl> from matplotlib . axes import Axes , rcParams <nl> from matplotlib . axes . _base import _axis_method_wrapper , _process_plot_format <nl> from matplotlib . transforms import Bbox <nl> def errorbar ( self , x , y , z , zerr = None , yerr = None , xerr = None , fmt = ' ' , <nl> else : <nl> base_style = next ( self . _get_lines . prop_cycler ) <nl> <nl> + base_style [ ' label ' ] = ' _nolegend_ ' <nl> base_style . update ( fmt_style_kwargs ) <nl> if ' color ' not in base_style : <nl> base_style [ ' color ' ] = ' C0 ' <nl> def _unpack_errs ( err , data , lomask , himask ) : <nl> limmarks . append ( lims_up ) <nl> <nl> errline = art3d . Line3DCollection ( np . array ( coorderr ) . T , <nl> - label = label , <nl> * * eb_lines_style ) <nl> self . add_collection ( errline ) <nl> errlines . append ( errline ) <nl> def _digout_minmax ( err_arr , coord_label ) : <nl> minz , maxz = _digout_minmax ( coorderrs , ' z ' ) <nl> self . auto_scale_xyz ( ( minx , maxx ) , ( miny , maxy ) , ( minz , maxz ) , had_data ) <nl> <nl> + # Adapting errorbar containers for 3d case - assuming z - axis points \" up \" <nl> + errorbar_container = mcontainer . ErrorbarContainer ( <nl> + ( data_line , tuple ( caplines ) , tuple ( errlines ) ) , <nl> + has_xerr = ( xerr is not None or yerr is not None ) , <nl> + has_yerr = ( zerr is not None ) , <nl> + label = label ) <nl> + self . containers . append ( errorbar_container ) <nl> + <nl> return errlines , caplines , limmarks <nl> <nl> <nl>\n", "msg": "adapt 2d errorbar containers for legend handling\n"}
{"diff_id": 16728, "repo": "matplotlib/matplotlib\n", "sha": "17e8f9c527e55673cd2683a416e1c3fefe3b5184\n", "time": "2014-11-25T04:11:29Z\n", "diff": "mmm a / lib / matplotlib / style / core . py <nl> ppp b / lib / matplotlib / style / core . py <nl> def is_style_file ( filename ) : <nl> return STYLE_FILE_PATTERN . match ( filename ) is not None <nl> <nl> <nl> - def use ( name ) : <nl> - \" \" \" Use matplotlib style settings from a known style sheet or from a file . <nl> + def use ( style ) : <nl> + \" \" \" Use matplotlib style settings from a style specification . <nl> <nl> Parameters <nl> mmmmmmmmm - <nl> - name : str , dict , list of str and / or dict <nl> - If ` name ` is a str , then it is the name of a style or a path / URL to a <nl> - style file . For a list of available style names , see ` style . available ` . <nl> - If ` name ` is a dict , then it contains valid rc key / value pairs . If <nl> - ` name ` is a list of styles , then each style ( str or dict ) is applied <nl> - from first to last in the list . <nl> + style : str , dict , or list <nl> + str : The name of a style or a path / URL to a style file . For a list of <nl> + available style names , see ` style . available ` . <nl> + dict : Dictionary with valid key / value pairs in ` matplotlib . rcParams ` . <nl> + list : List of style specifiers ( str or dict ) applied from first to <nl> + last in the list . <nl> + <nl> \" \" \" <nl> - # If name is a single str or dict , make it a single element list . <nl> - if cbook . is_string_like ( name ) or hasattr ( name , ' keys ' ) : <nl> - name = [ name ] <nl> + if cbook . is_string_like ( style ) or hasattr ( style , ' keys ' ) : <nl> + # If name is a single str or dict , make it a single element list . <nl> + styles = [ style ] <nl> + else : <nl> + styles = style <nl> <nl> - for style in name : <nl> + for style in styles : <nl> if not cbook . is_string_like ( style ) : <nl> mpl . rcParams . update ( style ) <nl> continue <nl> def use ( name ) : <nl> <nl> <nl> @ contextlib . contextmanager <nl> - def context ( name , after_reset = False ) : <nl> + def context ( style , after_reset = False ) : <nl> \" \" \" Context manager for using style settings temporarily . <nl> <nl> Parameters <nl> mmmmmmmmm - <nl> - name : str , dict , list of str and / or dict <nl> - If ` name ` is a str , then it is the name of a style or a path / URL to a <nl> - style file . For a list of available style names , see ` style . available ` . <nl> - If ` name ` is a dict , then it contains valid rc key / value pairs . If <nl> - ` name ` is a list of styles , then each style ( str or dict ) is applied <nl> - from first to last in the list . <nl> + style : str , dict , or list <nl> + str : The name of a style or a path / URL to a style file . For a list of <nl> + available style names , see ` style . available ` . <nl> + dict : Dictionary with valid key / value pairs in ` matplotlib . rcParams ` . <nl> + list : List of style specifiers ( str or dict ) applied from first to <nl> + last in the list . <nl> after_reset : bool <nl> If True , apply style after resetting settings to their defaults ; <nl> otherwise , apply style on top of the current settings . <nl> def context ( name , after_reset = False ) : <nl> initial_settings = mpl . rcParams . copy ( ) <nl> if after_reset : <nl> mpl . rcdefaults ( ) <nl> - use ( name ) <nl> + use ( style ) <nl> yield <nl> mpl . rcParams . update ( initial_settings ) <nl> <nl>\n", "msg": "Update docstring for style . use ( ) and style . context ( ) .\n"}
{"diff_id": 16729, "repo": "bokeh/bokeh\n", "sha": "5d73a75149ea4ae7ef4e358dce364ffb2c3e453a\n", "time": "2015-11-28T03:06:28Z\n", "diff": "mmm a / bokeh / command / subcommand . py <nl> ppp b / bokeh / command / subcommand . py <nl> <nl> line application . <nl> <nl> ' ' ' <nl> + from abc import ABCMeta , abstractmethod <nl> <nl> + # TODO ( bev ) change this after bokeh . util . future is merged <nl> + from six import add_metaclass <nl> + <nl> + @ add_metaclass ( ABCMeta ) <nl> class Subcommand ( object ) : <nl> ' ' ' Abstract base class for subcommands ' ' ' <nl> <nl> def __init__ ( self , parser ) : <nl> ' ' ' <nl> self . parser = parser <nl> <nl> - def func ( self , args ) : <nl> + @ abstractmethod <nl> + def invoke ( self , args ) : <nl> ' ' ' Takes over main program flow to perform the subcommand . <nl> <nl> Args : <nl> args ( seq ) : command line arguments for the subcommand to parse <nl> <nl> ' ' ' <nl> - raise NotImplementedError ( \" Implement func ( args ) \" ) <nl> \\ No newline at end of file <nl> + pass <nl> \\ No newline at end of file <nl>\n", "msg": "use abc module to make Subcommand a real abstract base class\n"}
{"diff_id": 16861, "repo": "scrapy/scrapy\n", "sha": "85b52b06542ee5905c174763de874bc9c376dfa5\n", "time": "2014-07-16T14:34:42Z\n", "diff": "mmm a / scrapy / contrib / spiderstate . py <nl> ppp b / scrapy / contrib / spiderstate . py <nl> <nl> from six . moves import cPickle as pickle <nl> <nl> from scrapy import signals <nl> + from scrapy . utils . job import job_dir <nl> <nl> class SpiderState ( object ) : <nl> \" \" \" Store and load spider state during a scraping job \" \" \" <nl> def __init__ ( self , jobdir = None ) : <nl> <nl> @ classmethod <nl> def from_crawler ( cls , crawler ) : <nl> - obj = cls ( crawler . settings . get ( ' JOBDIR ' ) ) <nl> + obj = cls ( job_dir ( crawler . settings ) ) <nl> crawler . signals . connect ( obj . spider_closed , signal = signals . spider_closed ) <nl> crawler . signals . connect ( obj . spider_opened , signal = signals . spider_opened ) <nl> return obj <nl>\n", "msg": "For consistency , use ` job_dir ` helper in ` SpiderState ` extension .\n"}
{"diff_id": 17042, "repo": "ansible/ansible\n", "sha": "7416e0054183ae6335d13087eb98015f99239a2c\n", "time": "2015-06-30T00:45:50Z\n", "diff": "mmm a / lib / ansible / executor / task_queue_manager . py <nl> ppp b / lib / ansible / executor / task_queue_manager . py <nl> def _load_callbacks ( self , stdout_callback ) : <nl> if callback_name ! = stdout_callback or stdout_callback_loaded : <nl> continue <nl> stdout_callback_loaded = True <nl> - elif C . DEFAULT_CALLBACK_WHITELIST is not None and callback_name not in C . DEFAULT_CALLBACK_WHITELIST : <nl> + elif C . DEFAULT_CALLBACK_WHITELIST is None or callback_name not in C . DEFAULT_CALLBACK_WHITELIST : <nl> continue <nl> <nl> loaded_plugins . append ( callback_plugin ( self . _display ) ) <nl>\n", "msg": "fixed condition for loading whitelisted callbacks\n"}
{"diff_id": 17246, "repo": "ansible/ansible\n", "sha": "36534668f0eaed60be087ba1dd0197b522a6a18a\n", "time": "2015-07-26T17:03:56Z\n", "diff": "mmm a / lib / ansible / plugins / filter / core . py <nl> ppp b / lib / ansible / plugins / filter / core . py <nl> def version_compare ( value , version , operator = ' eq ' , strict = False ) : <nl> except Exception , e : <nl> raise errors . AnsibleFilterError ( ' Version comparison : % s ' % e ) <nl> <nl> - def re_escape ( string ) : <nl> + def regex_escape ( string ) : <nl> ' ' ' Escape all regular expressions special characters from STRING . ' ' ' <nl> return re . escape ( string ) <nl> <nl> def filters ( self ) : <nl> ' search ' : search , <nl> ' regex ' : regex , <nl> ' regex_replace ' : regex_replace , <nl> - ' re_escape ' : re_escape , <nl> + ' regex_escape ' : regex_escape , <nl> <nl> # ? : ; <nl> ' ternary ' : ternary , <nl>\n", "msg": "Change name from re_escape to regex_escape to fit existing function names .\n"}
{"diff_id": 17410, "repo": "huggingface/transformers\n", "sha": "bf8d4bc674a6e8c371f7e242803528fe8ae3fa87\n", "time": "2020-01-06T13:54:36Z\n", "diff": "mmm a / src / transformers / data / processors / glue . py <nl> ppp b / src / transformers / data / processors / glue . py <nl> def glue_convert_examples_to_features ( <nl> features = [ ] <nl> for ( ex_index , example ) in enumerate ( examples ) : <nl> if ex_index % 10000 = = 0 : <nl> - logger . info ( \" Writing example % d \" % ( ex_index ) ) <nl> + logger . info ( \" Writing example % d / % d \" % ( ex_index , len ( examples ) ) ) <nl> if is_tf_dataset : <nl> example = processor . get_example_from_tensor_dict ( example ) <nl> example = processor . tfds_map ( example ) <nl>\n", "msg": "Improve logging message in glue feature conversion\n"}
{"diff_id": 17624, "repo": "numpy/numpy\n", "sha": "494fa219b050cb3b1564e78499c8306ea514aa35\n", "time": "2013-06-06T19:15:45Z\n", "diff": "mmm a / numpy / core / numeric . py <nl> ppp b / numpy / core / numeric . py <nl> def ones ( shape , dtype = None , order = ' C ' ) : <nl> \" \" \" <nl> Return a new array of given shape and type , filled with ones . <nl> <nl> - Please refer to the documentation for ` zeros ` for further details . <nl> + Parameters <nl> + mmmmmmmmm - <nl> + shape : int or sequence of ints <nl> + Shape of the new array , e . g . , ` ` ( 2 , 3 ) ` ` or ` ` 2 ` ` . <nl> + dtype : data - type , optional <nl> + The desired data - type for the array , e . g . , ` numpy . int8 ` . Default is <nl> + ` numpy . float64 ` . <nl> + order : { ' C ' , ' F ' } , optional <nl> + Whether to store multidimensional data in C - or Fortran - contiguous <nl> + ( row - or column - wise ) order in memory . <nl> + <nl> + Returns <nl> + mmmmmm - <nl> + out : ndarray <nl> + Array of ones with the given shape , dtype , and order . <nl> <nl> See Also <nl> mmmmmm - - <nl> def filled ( shape , val , dtype = None , order = ' C ' ) : <nl> \" \" \" <nl> Return a new array of given shape and type , filled with ` val ` . <nl> <nl> - Please refer to the documentation for ` zeros ` for further details . <nl> - <nl> - Other parameters <nl> - mmmmmmmmmmmmmmm - <nl> + Parameters <nl> + mmmmmmmmm - <nl> + shape : int or sequence of ints <nl> + Shape of the new array , e . g . , ` ` ( 2 , 3 ) ` ` or ` ` 2 ` ` . <nl> val : scalar <nl> Fill value . <nl> + dtype : data - type , optional <nl> + The desired data - type for the array , e . g . , ` numpy . int8 ` . Default is <nl> + ` numpy . float64 ` . <nl> + order : { ' C ' , ' F ' } , optional <nl> + Whether to store multidimensional data in C - or Fortran - contiguous <nl> + ( row - or column - wise ) order in memory . <nl> + <nl> + Returns <nl> + mmmmmm - <nl> + out : ndarray <nl> + Array of ` val ` with the given shape , dtype , and order . <nl> <nl> See Also <nl> mmmmmm - - <nl> def filled_like ( a , val , dtype = None , order = ' K ' , subok = True ) : <nl> ' F ' means F - order , ' A ' means ' F ' if ` a ` is Fortran contiguous , <nl> ' C ' otherwise . ' K ' means match the layout of ` a ` as closely <nl> as possible . <nl> + subok : bool , optional . <nl> + If True , then the newly created array will use the sub - class <nl> + type of ' a ' , otherwise it will be a base - class array . Defaults <nl> + to True . <nl> <nl> Returns <nl> mmmmmm - <nl> out : ndarray <nl> - Array of nans with the same shape and type as ` a ` . <nl> + Array of ` val ` with the same shape and type as ` a ` . <nl> <nl> See Also <nl> mmmmmm - - <nl>\n", "msg": "Add separate parameter description to filled , filled_like and ones\n"}
{"diff_id": 17670, "repo": "ipython/ipython\n", "sha": "ac706d2d5080772e9992c20c483bd8e227a01836\n", "time": "2015-03-02T04:41:01Z\n", "diff": "mmm a / IPython / kernel / zmq / tests / test_session . py <nl> ppp b / IPython / kernel / zmq / tests / test_session . py <nl> def test_send ( self ) : <nl> <nl> content = msg [ ' content ' ] <nl> header = msg [ ' header ' ] <nl> - header [ ' date ' ] = datetime . now ( ) <nl> + header [ ' msg_id ' ] = self . session . msg_id <nl> parent = msg [ ' parent_header ' ] <nl> metadata = msg [ ' metadata ' ] <nl> msg_type = header [ ' msg_type ' ] <nl> def test_send ( self ) : <nl> ident , msg_list = self . session . feed_identities ( B . recv_multipart ( ) ) <nl> new_msg = self . session . deserialize ( msg_list ) <nl> self . assertEqual ( ident [ 0 ] , b ' foo ' ) <nl> - self . assertEqual ( new_msg [ ' msg_id ' ] , msg [ ' msg_id ' ] ) <nl> + self . assertEqual ( new_msg [ ' msg_id ' ] , header [ ' msg_id ' ] ) <nl> self . assertEqual ( new_msg [ ' msg_type ' ] , msg [ ' msg_type ' ] ) <nl> self . assertEqual ( new_msg [ ' header ' ] , msg [ ' header ' ] ) <nl> self . assertEqual ( new_msg [ ' content ' ] , msg [ ' content ' ] ) <nl> def test_send ( self ) : <nl> self . assertEqual ( new_msg [ ' parent_header ' ] , msg [ ' parent_header ' ] ) <nl> self . assertEqual ( new_msg [ ' buffers ' ] , [ b ' bar ' ] ) <nl> <nl> - header [ ' date ' ] = datetime . now ( ) <nl> + header [ ' msg_id ' ] = self . session . msg_id <nl> <nl> self . session . send ( A , msg , ident = b ' foo ' , buffers = [ b ' bar ' ] ) <nl> ident , new_msg = self . session . recv ( B ) <nl> self . assertEqual ( ident [ 0 ] , b ' foo ' ) <nl> - self . assertEqual ( new_msg [ ' msg_id ' ] , msg [ ' msg_id ' ] ) <nl> + self . assertEqual ( new_msg [ ' msg_id ' ] , header [ ' msg_id ' ] ) <nl> self . assertEqual ( new_msg [ ' msg_type ' ] , msg [ ' msg_type ' ] ) <nl> self . assertEqual ( new_msg [ ' header ' ] , msg [ ' header ' ] ) <nl> self . assertEqual ( new_msg [ ' content ' ] , msg [ ' content ' ] ) <nl>\n", "msg": "rev msg_id to avoid signature collisions\n"}
{"diff_id": 17969, "repo": "zulip/zulip\n", "sha": "1eeb808a79d9788004760a0aaa16b5c1d7d247df\n", "time": "2013-10-02T20:15:24Z\n", "diff": "mmm a / zerver / lib / actions . py <nl> ppp b / zerver / lib / actions . py <nl> def bulk_get_subscriber_emails ( streams , user_profile ) : <nl> recipient__type = Recipient . STREAM , <nl> recipient__type_id__in = [ stream . id for stream in target_streams ] , <nl> user_profile__is_active = True , <nl> - active = True ) . only ( \" user_profile__email \" , \" recipient__type_id \" ) <nl> + active = True ) . values ( \" user_profile__email \" , \" recipient__type_id \" ) <nl> <nl> result = dict ( ( stream . id , [ ] ) for stream in streams ) <nl> for sub in subscriptions : <nl> - result [ sub . recipient . type_id ] . append ( sub . user_profile . email ) <nl> + result [ sub [ \" recipient__type_id \" ] ] . append ( sub [ \" user_profile__email \" ] ) <nl> <nl> return result <nl> <nl>\n", "msg": "bulk_get_subscriber_emails : Use . values ( ) to substantially improve performance .\n"}
{"diff_id": 18020, "repo": "scrapy/scrapy\n", "sha": "2108517ce02038556b3629482275dbd791e64a65\n", "time": "2011-08-08T18:51:22Z\n", "diff": "mmm a / scrapy / commands / crawl . py <nl> ppp b / scrapy / commands / crawl . py <nl> def process_options ( self , args , opts ) : <nl> def run ( self , args , opts ) : <nl> if len ( args ) < 1 : <nl> raise UsageError ( ) <nl> + elif len ( args ) > 1 : <nl> + raise UsageError ( \" running ' scrapy crawl ' with more than one spider is no longer supported \" ) <nl> for spname in args : <nl> spider = self . crawler . spiders . create ( spname , * * opts . spargs ) <nl> self . crawler . crawl ( spider ) <nl>\n", "msg": "removed support for passing more than a single spider on ' scrapy crawl ' command\n"}
{"diff_id": 18025, "repo": "ansible/ansible\n", "sha": "5bda9cbebf083ef7c93f578e1cc8ec7237b8f941\n", "time": "2019-04-21T16:46:42Z\n", "diff": "mmm a / lib / ansible / modules / cloud / amazon / rds_instance . py <nl> ppp b / lib / ansible / modules / cloud / amazon / rds_instance . py <nl> def main ( ) : <nl> new_db_instance_identifier = dict ( aliases = [ ' new_instance_id ' , ' new_id ' ] ) , <nl> option_group_name = dict ( ) , <nl> performance_insights_kms_key_id = dict ( ) , <nl> - performance_insights_retention_period = dict ( ) , <nl> + performance_insights_retention_period = dict ( type = ' int ' ) , <nl> port = dict ( type = ' int ' ) , <nl> preferred_backup_window = dict ( aliases = [ ' backup_window ' ] ) , <nl> preferred_maintenance_window = dict ( aliases = [ ' maintenance_window ' ] ) , <nl>\n", "msg": "Add variable type for performance_insights_retention_period ( )\n"}
{"diff_id": 18068, "repo": "huggingface/transformers\n", "sha": "dcc9bb3252fdad12897851f22f57c3130ba38a7a\n", "time": "2019-09-19T08:55:06Z\n", "diff": "mmm a / pytorch_transformers / tokenization_utils . py <nl> ppp b / pytorch_transformers / tokenization_utils . py <nl> def num_added_tokens ( self , pair = False ) : <nl> \" \" \" <nl> <nl> if pair : <nl> - initial_tokens_len = sum ( [ len ( encoded ) for encoded in self . encode ( \" This is a sequence \" , \" This is another \" ) ] ) <nl> + initial_tokens_len = len ( self . encode ( \" This is a sequence \" ) + self . encode ( \" This is another \" ) ) <nl> final_tokens = self . encode ( \" This is a sequence \" , \" This is another \" , add_special_tokens = True ) <nl> <nl> # In some models ( e . g . GPT - 2 ) , there is no sequence pair encoding . <nl> def _convert_token_to_id_with_added_voc ( self , token ) : <nl> def _convert_token_to_id ( self , token ) : <nl> raise NotImplementedError <nl> <nl> - def encode ( self , text , text_pair = None , add_special_tokens = False , output_mask = False , max_length = None , * * kwargs ) : <nl> + def encode ( self , text , text_pair = None , add_special_tokens = False , * * kwargs ) : <nl> \" \" \" <nl> Converts a string in a sequence of ids ( integer ) , using the tokenizer and vocabulary . <nl> - <nl> + <nl> + Same as doing ` ` self . convert_tokens_to_ids ( self . tokenize ( text ) ) ` ` . <nl> + <nl> + Args : <nl> + text : The first sequence to be encoded . <nl> + text_pair : Optional second sequence to be encoded . <nl> + add_special_tokens : if set to ` ` True ` ` , the sequences will be encoded with the special tokens relative <nl> + to their model . <nl> + \" \" \" <nl> + if text_pair is None : <nl> + if add_special_tokens : <nl> + sequence_tokens = self . convert_tokens_to_ids ( self . tokenize ( text , * * kwargs ) ) <nl> + return self . add_special_tokens_single_sentence ( sequence_tokens ) <nl> + else : <nl> + ids = self . convert_tokens_to_ids ( self . tokenize ( text , * * kwargs ) ) <nl> + return ids <nl> + <nl> + first_sentence_tokens = [ self . _convert_token_to_id ( token ) for token in self . tokenize ( text , * * kwargs ) ] <nl> + second_sentence_tokens = [ self . _convert_token_to_id ( token ) for token in self . tokenize ( text_pair , * * kwargs ) ] <nl> + <nl> + if add_special_tokens : <nl> + return self . add_special_tokens_sentences_pair ( first_sentence_tokens , second_sentence_tokens ) <nl> + else : <nl> + logger . warning ( \" No special tokens were added . The two sequences have been concatenated . \" ) <nl> + return first_sentence_tokens + second_sentence_tokens <nl> + <nl> + def encode_plus ( self , text , text_pair = None , add_special_tokens = False , output_mask = False , max_length = None , * * kwargs ) : <nl> + \" \" \" <nl> + Converts a string in a sequence of ids ( integer ) , using the tokenizer and vocabulary . <nl> + <nl> Same as doing ` ` self . convert_tokens_to_ids ( self . tokenize ( text ) ) ` ` . <nl> <nl> Args : <nl> def encode ( self , text , text_pair = None , add_special_tokens = False , output_mask = Fal <nl> max_length : if set to a number , will limit the total sequence returned so that it has a maximum length . <nl> * * kwargs : passed to the ` self . tokenize ( ) ` method <nl> \" \" \" <nl> + <nl> + information = { } <nl> + <nl> + if text_pair is None : <nl> + n_added_tokens = self . num_added_tokens ( ) <nl> + if add_special_tokens : <nl> + sequence_tokens = self . convert_tokens_to_ids ( self . tokenize ( text , * * kwargs ) ) <nl> + if max_length : <nl> + information [ \" overflowing_tokens \" ] = sequence_tokens [ max_length - n_added_tokens : ] <nl> + sequence_tokens = sequence_tokens [ : max_length - n_added_tokens ] <nl> + sequence = self . add_special_tokens_single_sentence ( sequence_tokens ) <nl> + else : <nl> + sequence_tokens = self . convert_tokens_to_ids ( self . tokenize ( text , * * kwargs ) ) <nl> + if max_length : <nl> + information [ \" overflowing_tokens \" ] = sequence_tokens [ max_length : ] <nl> + sequence_tokens = sequence_tokens [ : max_length ] <nl> + sequence = sequence_tokens <nl> + <nl> + if output_mask : <nl> + information [ \" mask \" ] = [ 0 ] * len ( sequence ) <nl> + <nl> + information [ \" sequence \" ] = sequence <nl> + else : <nl> + first_sentence_tokens = [ self . _convert_token_to_id ( token ) for token in self . tokenize ( text , * * kwargs ) ] <nl> + second_sentence_tokens = [ self . _convert_token_to_id ( token ) for token in self . tokenize ( text_pair , * * kwargs ) ] <nl> + f_len , s_len = len ( first_sentence_tokens ) , len ( second_sentence_tokens ) <nl> + n_added_tokens = self . num_added_tokens ( pair = True ) <nl> + <nl> + if add_special_tokens : <nl> + if max_length : <nl> + if len ( first_sentence_tokens ) + n_added_tokens > = max_length : <nl> + logger . warning ( \" The first sequence is longer than the maximum specified length . This sequence will not be truncated . \" ) <nl> + else : <nl> + if f_len + s_len + self . num_added_tokens ( pair = True ) > max_length : <nl> + information [ \" overflowing_tokens \" ] = second_sentence_tokens [ max_length - f_len - n_added_tokens : ] <nl> + second_sentence_tokens = second_sentence_tokens [ : max_length - f_len - n_added_tokens ] <nl> + <nl> + encoded_sequence = self . add_special_tokens_sentences_pair ( <nl> + first_sentence_tokens , <nl> + second_sentence_tokens , <nl> + output_mask <nl> + ) <nl> + <nl> + if output_mask : <nl> + sequence , information [ \" mask \" ] = encoded_sequence <nl> + else : <nl> + sequence = encoded_sequence <nl> + <nl> + information [ \" sequence \" ] = sequence <nl> + else : <nl> + logger . warning ( \" No special tokens were added . The two sequences have been concatenated . \" ) <nl> + sequence = first_sentence_tokens + second_sentence_tokens <nl> + <nl> + if max_length : <nl> + information [ \" overflowing_tokens \" ] = sequence [ max_length : ] <nl> + sequence = sequence [ : max_length ] <nl> + if output_mask : <nl> + information [ \" mask \" ] = [ 0 ] * len ( sequence ) <nl> + <nl> + information [ \" sequence \" ] = sequence <nl> + <nl> + return information <nl> + <nl> if text_pair is None : <nl> if add_special_tokens : <nl> sequence_tokens = self . convert_tokens_to_ids ( self . tokenize ( text , * * kwargs ) ) <nl> def encode ( self , text , text_pair = None , add_special_tokens = False , output_mask = Fal <nl> if add_special_tokens : <nl> if max_length : <nl> if len ( first_sentence_tokens ) + self . num_added_tokens ( pair = True ) > = max_length : <nl> - logger . warning ( \" The first sequence is longer than the maximum specified length . This sequence will not be truncated . \" ) <nl> + logger . warning ( <nl> + \" The first sequence is longer than the maximum specified length . This sequence will not be truncated . \" ) <nl> else : <nl> - if len ( second_sentence_tokens ) + len ( first_sentence_tokens ) + self . num_added_tokens ( pair = True ) > max_length : <nl> - second_sentence_tokens = second_sentence_tokens [ : max_length - len ( first_sentence_tokens ) - self . num_added_tokens ( pair = True ) ] <nl> - <nl> - return self . add_special_tokens_sentences_pair ( first_sentence_tokens , second_sentence_tokens , output_mask ) <nl> + if len ( second_sentence_tokens ) + len ( first_sentence_tokens ) + self . num_added_tokens ( <nl> + pair = True ) > max_length : <nl> + second_sentence_tokens = second_sentence_tokens [ <nl> + : max_length - len ( first_sentence_tokens ) - self . num_added_tokens ( <nl> + pair = True ) ] <nl> + <nl> + return self . add_special_tokens_sentences_pair ( first_sentence_tokens , second_sentence_tokens , <nl> + output_mask ) <nl> else : <nl> if max_length : <nl> first_sentence_tokens = first_sentence_tokens [ : max_length ] <nl>\n", "msg": "Modified encode to return only lists . Added a more complete encode_plus method\n"}
{"diff_id": 18087, "repo": "spotify/luigi\n", "sha": "94c5df1827ca62e4d30dca6aa74529cc1c794098\n", "time": "2019-06-26T10:53:31Z\n", "diff": "mmm a / luigi / contrib / kubernetes . py <nl> ppp b / luigi / contrib / kubernetes . py <nl> class kubernetes ( luigi . Config ) : <nl> <nl> <nl> class KubernetesJobTask ( luigi . Task ) : <nl> - __POLL_TIME = 5 # see __track_job <nl> + __DEFAULT_POLL_INTERVAL = 5 # see __track_job <nl> _kubernetes_config = None # Needs to be loaded at runtime <nl> <nl> def _init_kubernetes ( self ) : <nl> def kubernetes_config ( self ) : <nl> self . _kubernetes_config = kubernetes ( ) <nl> return self . _kubernetes_config <nl> <nl> + @ property <nl> + def poll_interval ( self ) : <nl> + \" \" \" How often to poll Kubernetes for job status , in seconds . \" \" \" <nl> + return self . __DEFAULT_POLL_INTERVAL <nl> + <nl> def __track_job ( self ) : <nl> \" \" \" Poll job status while active \" \" \" <nl> while not self . __verify_job_has_started ( ) : <nl> - time . sleep ( self . __POLL_TIME ) <nl> + time . sleep ( self . __DEFAULT_POLL_INTERVAL ) <nl> self . __logger . debug ( \" Waiting for Kubernetes job \" + self . uu_name + \" to start \" ) <nl> self . __print_kubectl_hints ( ) <nl> <nl> status = self . __get_job_status ( ) <nl> while status = = \" RUNNING \" : <nl> self . __logger . debug ( \" Kubernetes job \" + self . uu_name + \" is running \" ) <nl> - time . sleep ( self . __POLL_TIME ) <nl> + time . sleep ( self . poll_interval ) <nl> status = self . __get_job_status ( ) <nl> <nl> assert status ! = \" FAILED \" , \" Kubernetes job \" + self . uu_name + \" failed \" <nl>\n", "msg": "Enable overriding how often Luigi polls kubernetes for job completion . ( )\n"}
{"diff_id": 18103, "repo": "explosion/spaCy\n", "sha": "a8b592783b166253c4097ab4cf6e4d298693225e\n", "time": "2017-11-08T10:24:35Z\n", "diff": "mmm a / spacy / _ml . py <nl> ppp b / spacy / _ml . py <nl> def _zero_init_impl ( self , X , y ) : <nl> def _preprocess_doc ( docs , drop = 0 . ) : <nl> keys = [ doc . to_array ( [ LOWER ] ) for doc in docs ] <nl> ops = Model . ops <nl> - lengths = ops . asarray ( [ arr . shape [ 0 ] for arr in keys ] ) <nl> + lengths = ops . asarray ( [ arr . shape [ 0 ] for arr in keys ] , dtype = ' int32 ' ) <nl> keys = ops . xp . concatenate ( keys ) <nl> vals = ops . allocate ( keys . shape [ 0 ] ) + 1 <nl> return ( keys , vals , lengths ) , None <nl>\n", "msg": "Make a dtype more specific , to fix a windows build\n"}
{"diff_id": 18242, "repo": "zulip/zulip\n", "sha": "e0cf6494edc1cf741594b13fda04736ad16f8caa\n", "time": "2013-03-28T14:59:31Z\n", "diff": "mmm a / api / humbug / __init__ . py <nl> ppp b / api / humbug / __init__ . py <nl> def init_from_options ( options ) : <nl> class Client ( object ) : <nl> def __init__ ( self , email = None , api_key = None , config_file = None , <nl> verbose = False , retry_on_errors = True , <nl> - site = None , client = \" Python API \" ) : <nl> + site = None , client = \" API : Python \" ) : <nl> if None in ( api_key , email ) : <nl> if config_file is None : <nl> config_file = os . path . join ( os . environ [ \" HOME \" ] , \" . humbugrc \" ) <nl> def __init__ ( self , email = None , api_key = None , config_file = None , <nl> self . retry_on_errors = retry_on_errors <nl> self . client_name = client <nl> <nl> - def do_api_query ( self , orig_request , url , longpolling = False ) : <nl> + def do_api_query ( self , orig_request , url , method = \" POST \" , longpolling = False ) : <nl> request = { } <nl> - request [ \" email \" ] = self . email <nl> - request [ \" api - key \" ] = self . api_key <nl> request [ \" client \" ] = self . client_name <nl> <nl> for ( key , val ) in orig_request . iteritems ( ) : <nl> def end_error_retry ( succeeded ) : <nl> <nl> while True : <nl> try : <nl> - res = requests . post ( urlparse . urljoin ( self . base_url , url ) , <nl> - data = query_state [ \" request \" ] , <nl> - verify = True , timeout = 55 ) <nl> + res = requests . request ( <nl> + method , <nl> + urlparse . urljoin ( self . base_url , url ) , <nl> + auth = requests . auth . HTTPBasicAuth ( self . email , <nl> + self . api_key ) , <nl> + data = query_state [ \" request \" ] , <nl> + verify = True , timeout = 55 ) <nl> <nl> # On 50x errors , try again after a short sleep <nl> if str ( res . status_code ) . startswith ( ' 5 ' ) : <nl> def end_error_retry ( succeeded ) : <nl> \" status_code \" : res . status_code } <nl> <nl> @ classmethod <nl> - def _register ( cls , name , url = None , make_request = ( lambda request = { } : request ) , * * query_kwargs ) : <nl> + def _register ( cls , name , url = None , make_request = ( lambda request = { } : request ) , <nl> + method = \" POST \" , * * query_kwargs ) : <nl> if url is None : <nl> url = name <nl> def call ( self , * args , * * kwargs ) : <nl> request = make_request ( * args , * * kwargs ) <nl> - return self . do_api_query ( request , API_VERSTRING + url , * * query_kwargs ) <nl> + return self . do_api_query ( request , API_VERSTRING + url , method = method , * * query_kwargs ) <nl> call . func_name = name <nl> setattr ( cls , name , call ) <nl> <nl> def call_on_each_message ( self , callback , options = { } ) : <nl> def _mk_subs ( streams ) : <nl> return { ' subscriptions ' : streams } <nl> <nl> - Client . _register ( ' send_message ' , make_request = ( lambda request : request ) ) <nl> - Client . _register ( ' get_messages ' , longpolling = True ) <nl> - Client . _register ( ' get_profile ' ) <nl> - Client . _register ( ' get_public_streams ' ) <nl> - Client . _register ( ' get_members ' ) <nl> - Client . _register ( ' list_subscriptions ' , url = ' subscriptions / list ' ) <nl> - Client . _register ( ' add_subscriptions ' , url = ' subscriptions / add ' , make_request = _mk_subs ) <nl> - Client . _register ( ' remove_subscriptions ' , url = ' subscriptions / remove ' , make_request = _mk_subs ) <nl> + def _mk_del_subs ( streams ) : <nl> + return { ' delete ' : streams } <nl> + <nl> + Client . _register ( ' send_message ' , url = ' messages ' , make_request = ( lambda request : request ) ) <nl> + Client . _register ( ' get_messages ' , method = ' GET ' , url = ' messages / latest ' , longpolling = True ) <nl> + Client . _register ( ' get_profile ' , method = ' GET ' , url = ' users / me ' ) <nl> + Client . _register ( ' get_public_streams ' , method = ' GET ' , url = ' streams ' ) <nl> + Client . _register ( ' get_members ' , method = ' GET ' , url = ' users ' ) <nl> + Client . _register ( ' list_subscriptions ' , method = ' GET ' , url = ' users / me / subscriptions ' ) <nl> + Client . _register ( ' add_subscriptions ' , url = ' users / me / subscriptions ' , make_request = _mk_subs ) <nl> + Client . _register ( ' delete_subscriptions ' , method = ' PATCH ' , url = ' users / me / subscriptions ' , make_request = _mk_del_subs ) <nl>\n", "msg": "Update API bindings to support new API URIs .\n"}
{"diff_id": 18352, "repo": "matplotlib/matplotlib\n", "sha": "2e7b542ac612852e173cfe8cb59760dbb6fd0768\n", "time": "2008-01-17T04:12:58Z\n", "diff": "mmm a / lib / matplotlib / backends / backend_agg . py <nl> ppp b / lib / matplotlib / backends / backend_agg . py <nl> def print_png ( self , filename , * args , * * kwargs ) : <nl> renderer = self . get_renderer ( ) <nl> original_dpi = renderer . dpi <nl> renderer . dpi = self . figure . dpi <nl> + filename = str ( filename ) # until we figure out unicode handling <nl> renderer . _renderer . write_png ( filename , self . figure . dpi ) <nl> renderer . dpi = original_dpi <nl>\n", "msg": "forced nonunicode fname for save in agg\n"}
{"diff_id": 18561, "repo": "scrapy/scrapy\n", "sha": "31a375bde738a5c9c0a4930075be4535ffff6baf\n", "time": "2011-07-10T07:18:50Z\n", "diff": "mmm a / scrapy / core / engine . py <nl> ppp b / scrapy / core / engine . py <nl> def close_spider ( self , spider , reason = ' cancelled ' ) : <nl> <nl> dfd = slot . close ( ) <nl> <nl> - dfd . addBoth ( lambda _ : self . scheduler . close_spider ( spider ) ) <nl> - dfd . addErrback ( log . err , spider = spider ) <nl> - <nl> dfd . addBoth ( lambda _ : self . downloader . close_spider ( spider ) ) <nl> dfd . addErrback ( log . err , spider = spider ) <nl> <nl> dfd . addBoth ( lambda _ : self . scraper . close_spider ( spider ) ) <nl> dfd . addErrback ( log . err , spider = spider ) <nl> <nl> + dfd . addBoth ( lambda _ : self . scheduler . close_spider ( spider ) ) <nl> + dfd . addErrback ( log . err , spider = spider ) <nl> + <nl> dfd . addBoth ( lambda _ : self . _cancel_next_call ( spider ) ) <nl> dfd . addErrback ( log . err , spider = spider ) <nl> <nl>\n", "msg": "Close the scheduler after closing the scraper and downloader . This shouldn ' t have any real effect in practice , but it feels more appropiate to close the components in this order\n"}
{"diff_id": 18706, "repo": "ansible/ansible\n", "sha": "067bb6ec01398a4148cdecca369810714df57f63\n", "time": "2016-12-08T16:24:44Z\n", "diff": "mmm a / lib / ansible / modules / cloud / google / gce_net . py <nl> ppp b / lib / ansible / modules / cloud / google / gce_net . py <nl> def format_allowed ( allowed ) : <nl> return_value . append ( format_allowed_section ( section ) ) <nl> return return_value <nl> <nl> + def sorted_allowed_list ( allowed_list ) : <nl> + \" \" \" Sort allowed_list ( output of format_allowed ) by protocol and port . \" \" \" <nl> + # sort by protocol <nl> + allowed_by_protocol = sorted ( allowed_list , key = lambda x : x [ ' IPProtocol ' ] ) <nl> + # sort the ports list <nl> + return sorted ( allowed_by_protocol , key = lambda y : y [ ' ports ' ] . sort ( ) ) <nl> + <nl> + <nl> def main ( ) : <nl> module = AnsibleModule ( <nl> argument_spec = dict ( <nl> def main ( ) : <nl> # If old and new attributes are different , we update the firewall rule . <nl> # This implicitly let ' s us clear out attributes as well . <nl> # allowed_list is required and must not be None for firewall rules . <nl> - if allowed_list and ( allowed_list ! = fw . allowed ) : <nl> + if allowed_list and ( sorted_allowed_list ( allowed_list ) ! = sorted_allowed_list ( fw . allowed ) ) : <nl> fw . allowed = allowed_list <nl> fw_changed = True <nl> <nl> - if src_range ! = fw . source_ranges : <nl> - fw . source_ranges = src_range <nl> - fw_changed = True <nl> - <nl> - if src_tags ! = fw . source_tags : <nl> - fw . source_tags = src_tags <nl> - fw_changed = True <nl> - <nl> - if src_tags ! = fw . target_tags : <nl> - fw . target_tags = target_tags <nl> - fw_changed = True <nl> + # If these attributes are lists , we sort them first , then compare . <nl> + # Otherwise , we update if they differ . <nl> + if fw . source_ranges ! = src_range : <nl> + if isinstance ( src_range , list ) : <nl> + if sorted ( fw . source_ranges ) ! = sorted ( src_range ) : <nl> + fw . source_ranges = src_range <nl> + fw_changed = True <nl> + else : <nl> + fw . source_ranges = src_range <nl> + fw_changed = True <nl> + <nl> + if fw . source_tags ! = src_tags : <nl> + if isinstance ( src_range , list ) : <nl> + if sorted ( fw . source_tags ) ! = sorted ( src_tags ) : <nl> + fw . source_tags = src_tags <nl> + fw_changed = True <nl> + else : <nl> + fw . source_tags = src_tags <nl> + fw_changed = True <nl> + <nl> + if fw . target_tags ! = target_tags : <nl> + if isinstance ( target_tags , list ) : <nl> + if sorted ( fw . target_tags ) ! = sorted ( target_tags ) : <nl> + fw . target_tags = target_tags <nl> + fw_changed = True <nl> + else : <nl> + fw . target_tags = target_tags <nl> + fw_changed = True <nl> <nl> if fw_changed is True : <nl> try : <nl>\n", "msg": "Added helper function and logic to sort attributes before comparing .\n"}
{"diff_id": 19003, "repo": "home-assistant/core\n", "sha": "4509caefde4829327fecb45b5aded0938c2f8c0b\n", "time": "2019-02-16T08:29:24Z\n", "diff": "mmm a / homeassistant / components / sensor / scrape . py <nl> ppp b / homeassistant / components / sensor / scrape . py <nl> <nl> <nl> CONF_ATTR = ' attribute ' <nl> CONF_SELECT = ' select ' <nl> + CONF_INDEX = ' index ' <nl> <nl> DEFAULT_NAME = ' Web scrape ' <nl> DEFAULT_VERIFY_SSL = True <nl> <nl> vol . Required ( CONF_RESOURCE ) : cv . string , <nl> vol . Required ( CONF_SELECT ) : cv . string , <nl> vol . Optional ( CONF_ATTR ) : cv . string , <nl> + vol . Optional ( CONF_INDEX , default = 0 ) : cv . positive_int , <nl> vol . Optional ( CONF_AUTHENTICATION ) : <nl> vol . In ( [ HTTP_BASIC_AUTHENTICATION , HTTP_DIGEST_AUTHENTICATION ] ) , <nl> vol . Optional ( CONF_HEADERS ) : vol . Schema ( { cv . string : cv . string } ) , <nl> def setup_platform ( hass , config , add_entities , discovery_info = None ) : <nl> verify_ssl = config . get ( CONF_VERIFY_SSL ) <nl> select = config . get ( CONF_SELECT ) <nl> attr = config . get ( CONF_ATTR ) <nl> + index = config . get ( CONF_INDEX ) <nl> unit = config . get ( CONF_UNIT_OF_MEASUREMENT ) <nl> username = config . get ( CONF_USERNAME ) <nl> password = config . get ( CONF_PASSWORD ) <nl> def setup_platform ( hass , config , add_entities , discovery_info = None ) : <nl> raise PlatformNotReady <nl> <nl> add_entities ( [ <nl> - ScrapeSensor ( rest , name , select , attr , value_template , unit ) ] , True ) <nl> + ScrapeSensor ( rest , name , select , attr , index , value_template , unit ) ] , <nl> + True ) <nl> <nl> <nl> class ScrapeSensor ( Entity ) : <nl> \" \" \" Representation of a web scrape sensor . \" \" \" <nl> <nl> - def __init__ ( self , rest , name , select , attr , value_template , unit ) : <nl> + def __init__ ( self , rest , name , select , attr , index , value_template , unit ) : <nl> \" \" \" Initialize a web scrape sensor . \" \" \" <nl> self . rest = rest <nl> self . _name = name <nl> self . _state = None <nl> self . _select = select <nl> self . _attr = attr <nl> + self . _index = index <nl> self . _value_template = value_template <nl> self . _unit_of_measurement = unit <nl> <nl> def update ( self ) : <nl> <nl> try : <nl> if self . _attr is not None : <nl> - value = raw_data . select ( self . _select ) [ 0 ] [ self . _attr ] <nl> + value = raw_data . select ( self . _select ) [ self . _index ] [ self . _attr ] <nl> else : <nl> - value = raw_data . select ( self . _select ) [ 0 ] . text <nl> + value = raw_data . select ( self . _select ) [ self . _index ] . text <nl> _LOGGER . debug ( value ) <nl> except IndexError : <nl> _LOGGER . error ( \" Unable to extract data from HTML \" ) <nl>\n", "msg": "Add index parameter to scrape sensor ( )\n"}
{"diff_id": 19050, "repo": "python/cpython\n", "sha": "54d489a97f992a4ad25ce7de3e9f4a53dc2bbd0b\n", "time": "2008-04-07T14:53:34Z\n", "diff": "mmm a / Tools / msi / merge . py <nl> ppp b / Tools / msi / merge . py <nl> def merge ( msi , feature , rootdir , modules ) : <nl> msilib . add_stream ( db , stream , cabname ) <nl> os . unlink ( cabname ) <nl> maxmedia + = count <nl> + # The merge module sets ALLUSERS to 1 in the property table . <nl> + # This is undesired ; delete that <nl> + v = db . OpenView ( \" DELETE FROM Property WHERE Property = ' ALLUSERS ' \" ) <nl> + v . Execute ( None ) <nl> + v . Close ( ) <nl> db . Commit ( ) <nl> <nl> merge ( msi , \" SharedCRT \" , \" TARGETDIR \" , modules ) <nl>\n", "msg": "Delete ALLUSERS property merged from CRT merge module , so that per - user installations become possible again .\n"}
{"diff_id": 19121, "repo": "bokeh/bokeh\n", "sha": "31e97fad3cca6deaf240af7327c3f7b0962aba2d\n", "time": "2014-03-04T22:25:55Z\n", "diff": "mmm a / extensions / bokeh_magic . py <nl> ppp b / extensions / bokeh_magic . py <nl> <nl> # Imports <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> <nl> - import collections <nl> + import IPython <nl> from IPython . core . magic import ( Magics , magics_class , line_magic ) <nl> from IPython . testing . skipdoctest import skip_doctest <nl> from IPython . core . magic_arguments import ( argument , magic_arguments , <nl> parse_argstring ) <nl> from IPython . core . error import UsageError <nl> - from bokeh . plotting import ( output_notebook , figure , hold , show ) <nl> + from bokeh . plotting import ( output_notebook , show , hold , figure ) <nl> <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> # Classes and functions <nl> <nl> class BokehMagics ( Magics ) : <nl> \" \" \" Magic to embed Bokeh into the IPython notebook . \" \" \" <nl> <nl> + if IPython . __version__ in [ ' 1 . 2 . 1 ' , ' 1 . 2 . 0 ' , ' 1 . 1 . 2 ' , ' 1 . 1 . 1 ' , ' 1 . 0 . 0 ' ] : <nl> + is_ipytwo = False <nl> + elif IPython . __version__ in [ ' 2 . 0 . 0 ' , ' 2 . 0 . 0 - dev ' ] : <nl> + is_ipytwo = True <nl> + else : <nl> + print \" This version of IPython is not currently supported . \" <nl> + <nl> has_run = False <nl> <nl> @ skip_doctest <nl> def bokeh ( self , arg , line = None ) : <nl> <nl> To enable bokeh for usage with the IPython Notebook : : <nl> <nl> - In [ 3 ] : % bokeh - - notebook <nl> - <nl> - Then you can use several ` modes ` listed below : : <nl> - <nl> - In [ 4 ] : % bokeh - - figure [ - f ] # to enable the autofigure function <nl> + In [ 3 ] : % bokeh - - notebook [ - n ] <nl> <nl> - In [ 5 ] : % bokeh - - figure - off [ - f - off ] to disable the autofigure function <nl> + Then you can use a show ` modes ` : : <nl> <nl> - In [ 6 ] : % bokeh - - hold [ - h ] # to enable the autohold function <nl> + In [ 4 ] : % bokeh - - show [ - s ] # to enable the autoshow function <nl> <nl> - In [ 7 ] : % bokeh - - hold - off [ - h - off ] to disable the autohold function <nl> + In [ 5 ] : % bokeh - - show - off [ - s - off ] to disable the autoshow function <nl> <nl> - In [ 8 ] : % bokeh - - show [ - s ] # to enable the autoshow function <nl> + You can use add the show ` mode ` to the notebook argument : : <nl> <nl> - In [ 9 ] : % bokeh - - show - off [ - s - off ] to disable the autoshow function <nl> + In [ 6 ] : % bokeh - - notebook [ - n ] - - show - off [ - s - off ] <nl> <nl> Note : In order to actually use this magic , you need to have <nl> get_ipython ( ) , so you need to have a running IPython kernel . <nl> def bokeh ( self , arg , line = None ) : <nl> # Activate / deactivate the execution of func accordingly with the args . <nl> if args . notebook : <nl> # Configuring embedded BokehJS mode . <nl> - self . notebook_output ( ) <nl> + if not self . has_run : <nl> + self . notebook_output ( ) <nl> + <nl> + if args . figure : <nl> + if not self . has_run : <nl> + self . notebook_output ( ) <nl> + # Register the figure function . <nl> + if self . is_ipytwo : <nl> + # ip . events . register ( ' post_run_cell ' , self . notebook_show ) <nl> + print \" is_ipytwo figure \" <nl> + else : <nl> + # ip . set_hook ( ' pre_run_code_hook ' , figure ) <nl> + print \" The - - figure mode is not supported with this version of IPython . \" <nl> + elif args . figure_off : <nl> + if not self . has_run : <nl> + self . notebook_output ( ) <nl> + if self . is_ipytwo : <nl> + try : <nl> + # Unregister a function from the _post_execute dict . <nl> + # We do not need to specify the version here . <nl> + # del ip . _post_execute [ self . notebook_show ] <nl> + print \" is_ipytwo figure disable \" <nl> + print \" Automatic figure ( ) is disable . \" <nl> + except KeyError : <nl> + raise UsageError ( \" \" \" You have to enable the - - figure mode before trying to disable it . \" \" \" ) <nl> + else : <nl> + print \" The - - figure mode is not supported with this version of IPython . \" <nl> <nl> if args . hold : <nl> if not self . has_run : <nl> self . notebook_output ( ) <nl> - # Register a function for calling after code execution <nl> - ip . register_post_execute ( hold ) <nl> - print \" Automatic hold ( ) is enable . \" <nl> + # Register the hold function . <nl> + if self . is_ipytwo : <nl> + # ip . events . register ( ' post_run_cell ' , self . notebook_show ) <nl> + print \" is_ipytwo hold \" <nl> + else : <nl> + ip . set_hook ( ' pre_run_code_hook ' , hold ) <nl> + print \" Automatic hold ( ) is irreversible enabled . Just restart your kernel to disable . \" <nl> elif args . hold_off : <nl> - try : <nl> - if not self . has_run : <nl> - self . notebook_output ( ) <nl> - # Unregister a function from the _post_execute dict . <nl> - del ip . _post_execute [ hold ] <nl> - print \" Automatic hold ( ) is disable . \" <nl> - except KeyError : <nl> - raise UsageError ( \" \" \" You have to enable the - - hold mode before trying to disable it . \" \" \" ) <nl> + if not self . has_run : <nl> + self . notebook_output ( ) <nl> + if self . is_ipytwo : <nl> + try : <nl> + # Unregister a function from the _post_execute dict . <nl> + # We do not need to specify the version here . <nl> + # del ip . _post_execute [ self . notebook_show ] <nl> + print \" is_ipytwo hold disable \" <nl> + print \" Automatic hold ( ) is disable . \" <nl> + except KeyError : <nl> + raise UsageError ( \" \" \" You have to enable the - - hold mode before trying to disable it . \" \" \" ) <nl> + else : <nl> + print \" Automatic hold ( ) can not be disable without restarting your kernel . Did you activate it before ? \" <nl> <nl> if args . show : <nl> if not self . has_run : <nl> self . notebook_output ( ) <nl> # Register a function for calling after code execution . <nl> - ip . register_post_execute ( self . notebook_show ) <nl> + if self . is_ipytwo : <nl> + ip . events . register ( ' post_run_cell ' , self . notebook_show ) <nl> + print \" is_ipytwo \" <nl> + else : <nl> + ip . register_post_execute ( self . notebook_show ) <nl> print \" Automatic show ( ) is enable . \" <nl> elif args . show_off : <nl> - try : <nl> - if not self . has_run : <nl> - self . notebook_output ( ) <nl> - # Unregister a function from the _post_execute dict . <nl> - del ip . _post_execute [ self . notebook_show ] <nl> - print \" Automatic show ( ) is disable . \" <nl> - except KeyError : <nl> - raise UsageError ( \" \" \" You have to enable the - - show mode before trying to disable it . \" \" \" ) <nl> - <nl> - if args . figure : <nl> if not self . has_run : <nl> self . notebook_output ( ) <nl> - # Register a function for calling after code execution . <nl> - ip . register_post_execute ( figure ) <nl> - print \" Automatic figure ( ) is enable . \" <nl> - elif args . figure_off : <nl> - try : <nl> - if not self . has_run : <nl> - self . notebook_output ( ) <nl> - # Unregister a function from the _post_execute dict . <nl> - del ip . _post_execute [ figure ] <nl> - print \" Automatic figure ( ) is disable . \" <nl> - except KeyError : <nl> - raise UsageError ( \" You have to enable the - - figure mode before trying to disable it . \" ) <nl> - <nl> - ip . _post_execute = self . ordered_dict ( ip . _post_execute ) <nl> + if self . is_ipytwo : <nl> + try : <nl> + # Unregister a function <nl> + ip . events . unregister ( ' post_run_cell ' , self . notebook_show ) <nl> + print \" Automatic show ( ) is disable . \" <nl> + except ValueError : <nl> + raise UsageError ( \" \" \" You have to enable the - - show mode before trying to disable it . \" \" \" ) <nl> + else : <nl> + try : <nl> + # Unregister a function from the _post_execute dict . <nl> + del ip . _post_execute [ self . notebook_show ] <nl> + print \" Automatic show ( ) is disable . \" <nl> + except KeyError : <nl> + raise UsageError ( \" \" \" You have to enable the - - show mode before trying to disable it . \" \" \" ) <nl> <nl> def notebook_output ( self ) : <nl> \" \" \" Wrapper to execute the open notebook function just once to avoid <nl> def notebook_show ( self ) : <nl> # no plot object in the current cell gives us IndexError <nl> pass <nl> <nl> - def ordered_dict ( self , d ) : <nl> - \" It arrange the dict in show > figure > hold order . \" <nl> - litems = d . items ( ) <nl> - n = len ( litems ) <nl> - new_litems = [ ] <nl> - self . looper ( litems , new_litems , \" hold \" , n ) <nl> - self . looper ( litems , new_litems , \" notebook_show \" , n ) <nl> - self . looper ( litems , new_litems , \" figure \" , n ) <nl> - od = collections . OrderedDict ( new_litems ) <nl> - return od <nl> - <nl> - def looper ( self , old_list , new_list , fname , n ) : <nl> - for x in range ( n ) : <nl> - name = old_list [ x ] [ 0 ] . __name__ <nl> - if name = = fname : <nl> - new_list . append ( old_list [ x ] ) <nl> - <nl> <nl> def load_ipython_extension ( ip ) : <nl> ip . register_magics ( BokehMagics ) <nl>\n", "msg": "Big rewrite because we need to support IPython 2 . 0 with a new API for callbacks .\n"}
{"diff_id": 19283, "repo": "ipython/ipython\n", "sha": "716157fb0d6db653db571988234cb4da2d69dfe1\n", "time": "2012-05-13T22:09:36Z\n", "diff": "mmm a / IPython / parallel / tests / test_view . py <nl> ppp b / IPython / parallel / tests / test_view . py <nl> def test_magic_autopx_blocking ( self ) : <nl> sys . stdout = sio <nl> ip . magic_autopx ( ) <nl> ip . run_cell ( ' \\ n ' . join ( ( ' a = 5 ' , ' b = 10 ' , ' c = 0 ' ) ) ) <nl> - ip . run_cell ( ' print b ' ) <nl> + ip . run_cell ( ' b * = 2 ' ) <nl> + ip . run_cell ( ' print ( b ) ' ) <nl> ip . run_cell ( \" b / c \" ) <nl> - ip . run_code ( compile ( ' b * = 2 ' , ' ' , ' single ' ) ) <nl> ip . magic_autopx ( ) <nl> sys . stdout = savestdout <nl> output = sio . getvalue ( ) . strip ( ) <nl> self . assertTrue ( output . startswith ( ' % autopx enabled ' ) ) <nl> self . assertTrue ( output . endswith ( ' % autopx disabled ' ) ) <nl> self . assertTrue ( ' RemoteError : ZeroDivisionError ' in output ) <nl> - ar = v . get_result ( - 2 ) <nl> + ar = v . get_result ( - 1 ) <nl> self . assertEquals ( v [ ' a ' ] , 5 ) <nl> self . assertEquals ( v [ ' b ' ] , 20 ) <nl> self . assertRaisesRemote ( ZeroDivisionError , ar . get ) <nl> def test_magic_autopx_nonblocking ( self ) : <nl> sys . stdout = sio <nl> ip . magic_autopx ( ) <nl> ip . run_cell ( ' \\ n ' . join ( ( ' a = 5 ' , ' b = 10 ' , ' c = 0 ' ) ) ) <nl> - ip . run_cell ( ' print b ' ) <nl> + ip . run_cell ( ' print ( b ) ' ) <nl> ip . run_cell ( \" b / c \" ) <nl> - ip . run_code ( compile ( ' b * = 2 ' , ' ' , ' single ' ) ) <nl> + ip . run_cell ( ' b * = 2 ' ) <nl> ip . magic_autopx ( ) <nl> sys . stdout = savestdout <nl> output = sio . getvalue ( ) . strip ( ) <nl>\n", "msg": "avoid passing code objects to execute , now that execute uses code - as - text .\n"}
{"diff_id": 19394, "repo": "spotify/luigi\n", "sha": "b14d1de4950f87b2869ad8ed1d0b5b826aeffb06\n", "time": "2013-03-14T18:16:17Z\n", "diff": "mmm a / luigi / hdfs . py <nl> ppp b / luigi / hdfs . py <nl> def use_cdh4_syntax ( ) : <nl> import interface <nl> return interface . get_config ( ) . get ( \" hadoop \" , \" version \" , \" cdh4 \" ) . lower ( ) = = \" cdh4 \" <nl> <nl> - def exists ( path ) : <nl> - cmd = [ ' hadoop ' , ' fs ' , ' - test ' , ' - e ' , path ] <nl> - p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) <nl> - stdout , _ = p . communicate ( ) <nl> - <nl> - if stdout : <nl> - # TODO : Having certain stuff on the classpath might trigger this case . <nl> - # Ideally we should be able to ignore it , because apparently there is <nl> - # also some case where data on stdout signals an error . <nl> - # From @ freider : <nl> - # If I remember correctly there are some cases where <nl> - # ` hadoop fs - test ` exits with exit status 0 ( which would mean <nl> - # the file exists ) although the file doesn ' t exist , if there is some <nl> - # special error . We ran into this , that ' s why we added detection of <nl> - # output of the command . <nl> - # Afaicr this is a known bug but it hadn ' t been fixed in our version <nl> - # of hadoop . <nl> - raise RuntimeError ( \" Command % r failed [ exit code % d ] because it wrote output . \\ nmmmOutputmmm \\ n % s \\ nmmmmmmmmmmmm \" % ( cmd , p . returncode , stdout ) ) <nl> - elif p . returncode not in ( 0 , 1 ) : <nl> - raise RuntimeError ( \" Command % r failed with return code % s \" % ( cmd , p . returncode ) ) <nl> - <nl> - if p . returncode = = 0 : <nl> - return True <nl> - elif p . returncode = = 1 : <nl> - return False <nl> - assert False <nl> - <nl> <nl> def tmppath ( path = None ) : <nl> return tempfile . gettempdir ( ) + ' / ' + ( path + \" - \" if path else \" \" ) + \" luigitemp - % 08d \" % random . randrange ( 1e9 ) <nl> <nl> <nl> - def rename ( path , dest ) : <nl> - parent_dir = os . path . dirname ( dest ) <nl> - if parent_dir ! = ' ' and not exists ( parent_dir ) : <nl> - mkdir ( parent_dir ) <nl> - cmd = [ ' hadoop ' , ' fs ' , ' - mv ' , path , dest ] <nl> - if subprocess . call ( cmd ) : <nl> - raise RuntimeError ( ' Command % s failed ' % repr ( cmd ) ) <nl> + class HdfsClient ( object ) : <nl> + def exists ( self , path ) : <nl> + cmd = [ ' hadoop ' , ' fs ' , ' - test ' , ' - e ' , path ] <nl> + p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) <nl> + stdout , _ = p . communicate ( ) <nl> + if stdout and ' No such file or directory ' not in stdout : <nl> + # TODO2 : Using globs with - test - e will print ' No such file or directory ' <nl> + # when no files exists <nl> + # TODO : Having certain stuff on the classpath might trigger this case . <nl> + # Ideally we should be able to ignore it , because apparently there is <nl> + # also some case where data on stdout signals an error . <nl> + # From @ freider : <nl> + # If I remember correctly there are some cases where <nl> + # ` hadoop fs - test ` exits with exit status 0 ( which would mean <nl> + # the file exists ) although the file doesn ' t exist , if there is some <nl> + # special error . We ran into this , that ' s why we added detection of <nl> + # output of the command . <nl> + # Afaicr this is a known bug but it hadn ' t been fixed in our version <nl> + # of hadoop . <nl> + raise RuntimeError ( \" Command % r failed [ exit code % d ] because it wrote output . \\ nmmmOutputmmm \\ n % s \\ nmmmmmmmmmmmm \" % ( cmd , p . returncode , stdout ) ) <nl> + elif p . returncode not in ( 0 , 1 ) : <nl> + raise RuntimeError ( \" Command % r failed with return code % s \" % ( cmd , p . returncode ) ) <nl> + <nl> + if p . returncode = = 0 : <nl> + return True <nl> + elif p . returncode = = 1 : <nl> + return False <nl> + assert False <nl> <nl> <nl> - def remove ( path , recursive = True ) : <nl> - if recursive : <nl> - if use_cdh4_syntax ( ) : <nl> - cmd = [ ' hadoop ' , ' fs ' , ' - rm ' , ' - r ' , path ] <nl> - else : <nl> - cmd = [ ' hadoop ' , ' fs ' , ' - rmr ' , path ] <nl> - else : <nl> - cmd = [ ' hadoop ' , ' fs ' , ' - rm ' , path ] <nl> - if subprocess . call ( cmd ) : <nl> - raise RuntimeError ( ' Command % s failed ' % repr ( cmd ) ) <nl> - <nl> - <nl> - def mkdir ( path ) : <nl> - cmd = [ ' hadoop ' , ' fs ' , ' - mkdir ' , path ] <nl> - if subprocess . call ( cmd ) : <nl> - raise RuntimeError ( ' Command % s failed ' % repr ( cmd ) ) <nl> - <nl> - <nl> - def listdir ( path , ignore_directories = False , ignore_files = False , <nl> - include_size = False , include_type = False , include_time = False ) : <nl> - if not path : <nl> - path = \" . \" # default to current / home catalog <nl> - <nl> - cmd = [ ' hadoop ' , ' fs ' , ' - ls ' , path ] <nl> - proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) <nl> - lines = proc . stdout <nl> - <nl> - for line in lines : <nl> - if line . startswith ( ' Found ' ) : <nl> - continue # \" hadoop fs - ls \" outputs \" Found % d items \" as its first line <nl> - line = line . rstrip ( \" \\ n \" ) <nl> - if ignore_directories and line [ 0 ] = = ' d ' : <nl> - continue <nl> - if ignore_files and line [ 0 ] = = ' - ' : <nl> - continue <nl> - data = line . split ( ' ' ) <nl> - <nl> - file = data [ - 1 ] <nl> - size = int ( data [ - 4 ] ) <nl> - line_type = line [ 0 ] <nl> - extra_data = ( ) <nl> - <nl> - if include_size : <nl> - extra_data + = ( size , ) <nl> - if include_type : <nl> - extra_data + = ( line_type , ) <nl> - if include_time : <nl> - time_str = ' % sT % s ' % ( data [ - 3 ] , data [ - 2 ] ) <nl> - modification_time = datetime . datetime . strptime ( time_str , <nl> - ' % Y - % m - % dT % H : % M ' ) <nl> - extra_data + = ( modification_time , ) <nl> - <nl> - if len ( extra_data ) > 0 : <nl> - yield ( file , ) + extra_data <nl> - else : <nl> - yield file <nl> + def rename ( self , path , dest ) : <nl> + parent_dir = os . path . dirname ( dest ) <nl> + if parent_dir ! = ' ' and not exists ( parent_dir ) : <nl> + mkdir ( parent_dir ) <nl> + cmd = [ ' hadoop ' , ' fs ' , ' - mv ' , path , dest ] <nl> + if subprocess . call ( cmd ) : <nl> + raise RuntimeError ( ' Command % s failed ' % repr ( cmd ) ) <nl> <nl> <nl> + def remove ( self , path , recursive = True ) : <nl> + if recursive : <nl> + if use_cdh4_syntax ( ) : <nl> + cmd = [ ' hadoop ' , ' fs ' , ' - rm ' , ' - r ' , path ] <nl> + else : <nl> + cmd = [ ' hadoop ' , ' fs ' , ' - rmr ' , path ] <nl> + else : <nl> + cmd = [ ' hadoop ' , ' fs ' , ' - rm ' , path ] <nl> + if subprocess . call ( cmd ) : <nl> + raise RuntimeError ( ' Command % s failed ' % repr ( cmd ) ) <nl> + <nl> + <nl> + def mkdir ( self , path ) : <nl> + cmd = [ ' hadoop ' , ' fs ' , ' - mkdir ' , path ] <nl> + if subprocess . call ( cmd ) : <nl> + raise RuntimeError ( ' Command % s failed ' % repr ( cmd ) ) <nl> + <nl> + <nl> + def listdir ( self , path , ignore_directories = False , ignore_files = False , <nl> + include_size = False , include_type = False , include_time = False ) : <nl> + if not path : <nl> + path = \" . \" # default to current / home catalog <nl> + <nl> + cmd = [ ' hadoop ' , ' fs ' , ' - ls ' , path ] <nl> + proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) <nl> + lines = proc . stdout <nl> + <nl> + for line in lines : <nl> + if line . startswith ( ' Found ' ) : <nl> + continue # \" hadoop fs - ls \" outputs \" Found % d items \" as its first line <nl> + line = line . rstrip ( \" \\ n \" ) <nl> + if ignore_directories and line [ 0 ] = = ' d ' : <nl> + continue <nl> + if ignore_files and line [ 0 ] = = ' - ' : <nl> + continue <nl> + data = line . split ( ' ' ) <nl> + <nl> + file = data [ - 1 ] <nl> + size = int ( data [ - 4 ] ) <nl> + line_type = line [ 0 ] <nl> + extra_data = ( ) <nl> + <nl> + if include_size : <nl> + extra_data + = ( size , ) <nl> + if include_type : <nl> + extra_data + = ( line_type , ) <nl> + if include_time : <nl> + time_str = ' % sT % s ' % ( data [ - 3 ] , data [ - 2 ] ) <nl> + modification_time = datetime . datetime . strptime ( time_str , <nl> + ' % Y - % m - % dT % H : % M ' ) <nl> + extra_data + = ( modification_time , ) <nl> + <nl> + if len ( extra_data ) > 0 : <nl> + yield ( file , ) + extra_data <nl> + else : <nl> + yield file <nl> + <nl> + client = HdfsClient ( ) <nl> + <nl> + exists = client . exists <nl> + rename = client . rename <nl> + remove = client . remove <nl> + mkdir = client . mkdir <nl> + listdir = client . listdir <nl> + <nl> class HdfsReadPipe ( luigi . format . InputPipeProcessWrapper ) : <nl> def __init__ ( self , path ) : <nl> super ( HdfsReadPipe , self ) . __init__ ( [ ' hadoop ' , ' fs ' , ' - cat ' , path ] ) <nl>\n", "msg": "Moved hdfs operations into HdfsClient class , added aliases so they can be used as module methods as before .\n"}
{"diff_id": 19650, "repo": "home-assistant/core\n", "sha": "9a23a1c3365f1711db40e2a73980c2cdf39e51de\n", "time": "2019-11-22T12:54:56Z\n", "diff": "mmm a / homeassistant / components / deconz / device_trigger . py <nl> ppp b / homeassistant / components / deconz / device_trigger . py <nl> <nl> ( CONF_SHAKE , \" \" ) : 1007 , <nl> } <nl> <nl> + AQARA_SQUARE_SWITCH_WXKG11LM_2016_MODEL = \" lumi . sensor_switch . aq2 \" <nl> + AQARA_SQUARE_SWITCH_WXKG11LM_2016 = { <nl> + ( CONF_SHORT_PRESS , CONF_TURN_ON ) : 1002 , <nl> + ( CONF_DOUBLE_PRESS , CONF_TURN_ON ) : 1004 , <nl> + ( CONF_TRIPLE_PRESS , CONF_TURN_ON ) : 1005 , <nl> + ( CONF_QUADRUPLE_PRESS , CONF_TURN_ON ) : 1006 , <nl> + } <nl> + <nl> REMOTES = { <nl> HUE_DIMMER_REMOTE_MODEL_GEN1 : HUE_DIMMER_REMOTE , <nl> HUE_DIMMER_REMOTE_MODEL_GEN2 : HUE_DIMMER_REMOTE , <nl> <nl> AQARA_MINI_SWITCH_MODEL : AQARA_MINI_SWITCH , <nl> AQARA_ROUND_SWITCH_MODEL : AQARA_ROUND_SWITCH , <nl> AQARA_SQUARE_SWITCH_MODEL : AQARA_SQUARE_SWITCH , <nl> + AQARA_SQUARE_SWITCH_WXKG11LM_2016_MODEL : AQARA_SQUARE_SWITCH_WXKG11LM_2016 , <nl> } <nl> <nl> TRIGGER_SCHEMA = TRIGGER_BASE_SCHEMA . extend ( <nl>\n", "msg": "Add device trigger support for Aqara WXKG11LM 2016 switch to Deconz ( )\n"}
{"diff_id": 19845, "repo": "ansible/ansible\n", "sha": "4e6270750a748ae854481e157821f51eda36ded0\n", "time": "2019-09-12T10:38:40Z\n", "diff": "mmm a / lib / ansible / modules / network / junos / junos_user . py <nl> ppp b / lib / ansible / modules / network / junos / junos_user . py <nl> def main ( ) : <nl> name = dict ( ) , <nl> full_name = dict ( ) , <nl> role = dict ( choices = ROLES ) , <nl> - encrypted_password = dict ( ) , <nl> + encrypted_password = dict ( no_log = True ) , <nl> sshkey = dict ( ) , <nl> state = dict ( choices = [ ' present ' , ' absent ' ] , default = ' present ' ) , <nl> active = dict ( type = ' bool ' , default = True ) <nl>\n", "msg": "Set no_log to True for junos_user encrypted_password ( )\n"}
{"diff_id": 20080, "repo": "home-assistant/core\n", "sha": "23290fa6ee2e72ab095265a75d46a1a49d06995f\n", "time": "2018-11-01T18:37:38Z\n", "diff": "mmm a / tests / components / dialogflow / test_init . py <nl> ppp b / tests / components / dialogflow / test_init . py <nl> <nl> REQUEST_TIMESTAMP = \" 2017 - 01 - 21T17 : 54 : 18 . 952Z \" <nl> CONTEXT_NAME = \" 78a5db95 - b7d6 - 4d50 - 9c9b - 2fc73a5e34c3_id_dialog_context \" <nl> <nl> - calls = [ ] <nl> - <nl> <nl> @ pytest . fixture <nl> - async def fixture ( hass , aiohttp_client ) : <nl> - \" \" \" Initialize a Home Assistant server for testing this module . \" \" \" <nl> + async def calls ( hass , fixture ) : <nl> + \" \" \" Return a list of Dialogflow calls triggered . \" \" \" <nl> + calls = [ ] <nl> + <nl> @ callback <nl> def mock_service ( call ) : <nl> \" \" \" Mock action call . \" \" \" <nl> def mock_service ( call ) : <nl> <nl> hass . services . async_register ( ' test ' , ' dialogflow ' , mock_service ) <nl> <nl> + return calls <nl> + <nl> + <nl> + @ pytest . fixture <nl> + async def fixture ( hass , aiohttp_client ) : <nl> + \" \" \" Initialize a Home Assistant server for testing this module . \" \" \" <nl> await async_setup_component ( hass , dialogflow . DOMAIN , { <nl> \" dialogflow \" : { } , <nl> } ) <nl> async def test_intent_request_without_slots ( hass , fixture ) : <nl> assert \" You are both home , you silly \" = = text <nl> <nl> <nl> - async def test_intent_request_calling_service ( fixture ) : <nl> + async def test_intent_request_calling_service ( fixture , calls ) : <nl> \" \" \" Test a request for calling a service . <nl> <nl> If this request is done async the test could finish before the action <nl>\n", "msg": "Use a fixture for dialogflow calls in unit tests ( )\n"}
{"diff_id": 20103, "repo": "ansible/ansible\n", "sha": "2b4ddfb0a2ede5c7500572dc6dac9224a029dee4\n", "time": "2013-02-23T17:28:42Z\n", "diff": "mmm a / lib / ansible / runner / __init__ . py <nl> ppp b / lib / ansible / runner / __init__ . py <nl> def _executor_internal_inner ( self , host , module_name , module_args , inject , port , <nl> actual_port = delegate_info . get ( ' ansible_ssh_port ' , port ) <nl> actual_user = delegate_info . get ( ' ansible_ssh_user ' , actual_user ) <nl> actual_pass = delegate_info . get ( ' ansible_ssh_pass ' , actual_pass ) <nl> + actual_transport = delegate_info . get ( ' ansible_connection ' , self . transport ) <nl> for i in delegate_info : <nl> if i . startswith ( \" ansible_ \" ) and i . endswith ( \" _interpreter \" ) : <nl> inject [ i ] = delegate_info [ i ] <nl>\n", "msg": "Handle delegate_to case for local connections in hosts file\n"}
{"diff_id": 20115, "repo": "zulip/zulip\n", "sha": "438a545477ad2a7f98c072b2c341ae4d03ee00b8\n", "time": "2020-04-19T20:36:58Z\n", "diff": "mmm a / tools / lib / gitlint - rules . py <nl> ppp b / tools / lib / gitlint - rules . py <nl> <nl> from typing import Text , List <nl> <nl> - import gitlint <nl> + from gitlint . git import GitCommit <nl> from gitlint . rules import LineRule , RuleViolation , CommitMessageTitle <nl> from gitlint . options import StrOption <nl> import re <nl> class ImperativeMood ( LineRule ) : <nl> error_msg = ( ' The first word in commit title should be in imperative mood ' <nl> ' ( \" { word } \" - > \" { imperative } \" ) : \" { title } \" ' ) <nl> <nl> - def validate ( self , line : Text , commit : gitlint . commit ) - > List [ RuleViolation ] : <nl> + def validate ( self , line : Text , commit : GitCommit ) - > List [ RuleViolation ] : <nl> violations = [ ] <nl> <nl> # Ignore the section tag ( ie ` < section tag > : < message body > . ` ) <nl> class TitleMatchRegexAllowException ( LineRule ) : <nl> target = CommitMessageTitle <nl> options_spec = [ StrOption ( ' regex ' , \" . * \" , \" Regex the title should match \" ) ] <nl> <nl> - def validate ( self , title : Text , commit : gitlint . commit ) - > List [ RuleViolation ] : <nl> + def validate ( self , title : Text , commit : GitCommit ) - > List [ RuleViolation ] : <nl> <nl> regex = self . options [ ' regex ' ] . value <nl> pattern = re . compile ( regex , re . UNICODE ) <nl>\n", "msg": "gitlint - rules : Fix bogus type annotations .\n"}
{"diff_id": 20170, "repo": "ansible/ansible\n", "sha": "8659082857a5dae32533ad35652d6ba8c5a09a3d\n", "time": "2016-12-08T16:34:26Z\n", "diff": "mmm a / lib / ansible / modules / extras / network / f5 / bigip_facts . py <nl> ppp b / lib / ansible / modules / extras / network / f5 / bigip_facts . py <nl> <nl> # ! / usr / bin / python <nl> # - * - coding : utf - 8 - * - <nl> - <nl> + # <nl> # ( c ) 2013 , Matt Hite < mhite @ hotmail . com > <nl> # <nl> # This file is part of Ansible <nl> <nl> DOCUMENTATION = ' ' ' <nl> mmm <nl> module : bigip_facts <nl> - short_description : \" Collect facts from F5 BIG - IP devices \" <nl> + short_description : Collect facts from F5 BIG - IP devices <nl> description : <nl> - - \" Collect facts from F5 BIG - IP devices via iControl SOAP API \" <nl> + - Collect facts from F5 BIG - IP devices via iControl SOAP API <nl> version_added : \" 1 . 6 \" <nl> author : <nl> - - Matt Hite ( @ mhite ) <nl> - - Tim Rupp ( @ caphrim007 ) <nl> + - Matt Hite ( @ mhite ) <nl> + - Tim Rupp ( @ caphrim007 ) <nl> notes : <nl> - - \" Requires BIG - IP software version > = 11 . 4 \" <nl> - - \" F5 developed module ' bigsuds ' required ( see http : / / devcentral . f5 . com ) \" <nl> - - \" Best run as a local_action in your playbook \" <nl> - - \" Tested with manager and above account privilege level \" <nl> - <nl> + - Requires BIG - IP software version > = 11 . 4 <nl> + - F5 developed module ' bigsuds ' required ( see http : / / devcentral . f5 . com ) <nl> + - Best run as a local_action in your playbook <nl> + - Tested with manager and above account privilege level <nl> requirements : <nl> - - bigsuds <nl> + - bigsuds <nl> options : <nl> - server : <nl> - description : <nl> - - BIG - IP host <nl> - required : true <nl> - default : null <nl> - choices : [ ] <nl> - aliases : [ ] <nl> - server_port : <nl> - description : <nl> - - BIG - IP server port <nl> - required : false <nl> - default : 443 <nl> - version_added : \" 2 . 2 \" <nl> - user : <nl> - description : <nl> - - BIG - IP username <nl> - required : true <nl> - default : null <nl> - choices : [ ] <nl> - aliases : [ ] <nl> - password : <nl> - description : <nl> - - BIG - IP password <nl> - required : true <nl> - default : null <nl> - choices : [ ] <nl> - aliases : [ ] <nl> - validate_certs : <nl> - description : <nl> - - If C ( no ) , SSL certificates will not be validated . This should only be used <nl> - on personally controlled sites . Prior to 2 . 0 , this module would always <nl> - validate on python > = 2 . 7 . 9 and never validate on python < = 2 . 7 . 8 <nl> - required : false <nl> - default : ' yes ' <nl> - choices : [ ' yes ' , ' no ' ] <nl> - version_added : 1 . 9 . 1 <nl> - session : <nl> - description : <nl> - - BIG - IP session support ; may be useful to avoid concurrency <nl> - issues in certain circumstances . <nl> - required : false <nl> - default : true <nl> - choices : [ ] <nl> - aliases : [ ] <nl> - include : <nl> - description : <nl> - - Fact category or list of categories to collect <nl> - required : true <nl> - default : null <nl> - choices : [ ' address_class ' , ' certificate ' , ' client_ssl_profile ' , <nl> - ' device ' , ' device_group ' , ' interface ' , ' key ' , ' node ' , ' pool ' , <nl> - ' rule ' , ' self_ip ' , ' software ' , ' system_info ' , ' traffic_group ' , <nl> - ' trunk ' , ' virtual_address ' , ' virtual_server ' , ' vlan ' ] <nl> - aliases : [ ] <nl> - filter : <nl> - description : <nl> - - Shell - style glob matching string used to filter fact keys . Not <nl> - applicable for software and system_info fact categories . <nl> - required : false <nl> - default : null <nl> - choices : [ ] <nl> - aliases : [ ] <nl> + server : <nl> + description : <nl> + - BIG - IP host <nl> + required : true <nl> + default : null <nl> + choices : [ ] <nl> + aliases : [ ] <nl> + server_port : <nl> + description : <nl> + - BIG - IP server port <nl> + required : false <nl> + default : 443 <nl> + version_added : \" 2 . 2 \" <nl> + user : <nl> + description : <nl> + - BIG - IP username <nl> + required : true <nl> + default : null <nl> + choices : [ ] <nl> + aliases : [ ] <nl> + password : <nl> + description : <nl> + - BIG - IP password <nl> + required : true <nl> + default : null <nl> + choices : [ ] <nl> + aliases : [ ] <nl> + validate_certs : <nl> + description : <nl> + - If C ( no ) , SSL certificates will not be validated . This should only be used <nl> + on personally controlled sites . Prior to 2 . 0 , this module would always <nl> + validate on python > = 2 . 7 . 9 and never validate on python < = 2 . 7 . 8 <nl> + required : false <nl> + default : yes <nl> + choices : <nl> + - yes <nl> + - no <nl> + version_added : 2 . 0 <nl> + session : <nl> + description : <nl> + - BIG - IP session support ; may be useful to avoid concurrency <nl> + issues in certain circumstances . <nl> + required : false <nl> + default : true <nl> + choices : [ ] <nl> + aliases : [ ] <nl> + include : <nl> + description : <nl> + - Fact category or list of categories to collect <nl> + required : true <nl> + default : null <nl> + choices : <nl> + - address_class <nl> + - certificate <nl> + - client_ssl_profile <nl> + - device <nl> + - device_group <nl> + - interface <nl> + - key <nl> + - node <nl> + - pool <nl> + - rule <nl> + - self_ip <nl> + - software <nl> + - system_info <nl> + - traffic_group <nl> + - trunk <nl> + - virtual_address <nl> + - virtual_server <nl> + - vlan <nl> + aliases : [ ] <nl> + filter : <nl> + description : <nl> + - Shell - style glob matching string used to filter fact keys . Not <nl> + applicable for software and system_info fact categories . <nl> + required : false <nl> + default : null <nl> + choices : [ ] <nl> + aliases : [ ] <nl> ' ' ' <nl> <nl> EXAMPLES = ' ' ' <nl> - <nl> - # # playbook task examples : <nl> - <nl> mmm - <nl> - # file bigip - test . yml <nl> - # . . . <nl> - - hosts : bigip - test <nl> - tasks : <nl> - - name : Collect BIG - IP facts <nl> - local_action : > <nl> - bigip_facts <nl> - server = lb . mydomain . com <nl> - user = admin <nl> - password = mysecret <nl> - include = interface , vlan <nl> - <nl> + - name : Collect BIG - IP facts <nl> + bigip_facts : <nl> + server : \" lb . mydomain . com \" <nl> + user : \" admin \" <nl> + password : \" secret \" <nl> + include : \" interface , vlan \" <nl> + delegate_to : localhost <nl> ' ' ' <nl> <nl> try : <nl> - import bigsuds <nl> from suds import MethodNotFound , WebFault <nl> except ImportError : <nl> bigsuds_found = False <nl> <nl> bigsuds_found = True <nl> <nl> import fnmatch <nl> - import traceback <nl> import re <nl> + import traceback <nl> <nl> - # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = <nl> - # bigip_facts module specific support methods . <nl> - # <nl> <nl> class F5 ( object ) : <nl> \" \" \" F5 iControl class . <nl> def get_verification_status ( self ) : <nl> def get_definition ( self ) : <nl> return [ x [ ' rule_definition ' ] for x in self . api . LocalLB . Rule . query_rule ( rule_names = self . rules ) ] <nl> <nl> + <nl> class Nodes ( object ) : <nl> \" \" \" Nodes class . <nl> <nl> def generate_dict ( api_obj , fields ) : <nl> result_dict [ j ] = temp <nl> return result_dict <nl> <nl> + <nl> def generate_simple_dict ( api_obj , fields ) : <nl> result_dict = { } <nl> for field in fields : <nl> def generate_simple_dict ( api_obj , fields ) : <nl> result_dict [ field ] = api_response <nl> return result_dict <nl> <nl> + <nl> def generate_interface_dict ( f5 , regex ) : <nl> interfaces = Interfaces ( f5 . get_api ( ) , regex ) <nl> fields = [ ' active_media ' , ' actual_flow_control ' , ' bundle_state ' , <nl> def generate_interface_dict ( f5 , regex ) : <nl> ' stp_protocol_detection_reset_state ' ] <nl> return generate_dict ( interfaces , fields ) <nl> <nl> + <nl> def generate_self_ip_dict ( f5 , regex ) : <nl> self_ips = SelfIPs ( f5 . get_api ( ) , regex ) <nl> fields = [ ' address ' , ' allow_access_list ' , ' description ' , <nl> def generate_self_ip_dict ( f5 , regex ) : <nl> ' vlan ' , ' is_traffic_group_inherited ' ] <nl> return generate_dict ( self_ips , fields ) <nl> <nl> + <nl> def generate_trunk_dict ( f5 , regex ) : <nl> trunks = Trunks ( f5 . get_api ( ) , regex ) <nl> fields = [ ' active_lacp_state ' , ' configured_member_count ' , ' description ' , <nl> def generate_trunk_dict ( f5 , regex ) : <nl> ' stp_protocol_detection_reset_state ' ] <nl> return generate_dict ( trunks , fields ) <nl> <nl> + <nl> def generate_vlan_dict ( f5 , regex ) : <nl> vlans = Vlans ( f5 . get_api ( ) , regex ) <nl> fields = [ ' auto_lasthop ' , ' cmp_hash_algorithm ' , ' description ' , <nl> def generate_vlan_dict ( f5 , regex ) : <nl> ' source_check_state ' , ' true_mac_address ' , ' vlan_id ' ] <nl> return generate_dict ( vlans , fields ) <nl> <nl> + <nl> def generate_vs_dict ( f5 , regex ) : <nl> virtual_servers = VirtualServers ( f5 . get_api ( ) , regex ) <nl> fields = [ ' actual_hardware_acceleration ' , ' authentication_profile ' , <nl> def generate_vs_dict ( f5 , regex ) : <nl> ' translate_port_state ' , ' type ' , ' vlan ' , ' wildmask ' ] <nl> return generate_dict ( virtual_servers , fields ) <nl> <nl> + <nl> def generate_pool_dict ( f5 , regex ) : <nl> pools = Pools ( f5 . get_api ( ) , regex ) <nl> fields = [ ' action_on_service_down ' , ' active_member_count ' , <nl> def generate_pool_dict ( f5 , regex ) : <nl> ' simple_timeout ' , ' slow_ramp_time ' ] <nl> return generate_dict ( pools , fields ) <nl> <nl> + <nl> def generate_device_dict ( f5 , regex ) : <nl> devices = Devices ( f5 . get_api ( ) , regex ) <nl> fields = [ ' active_modules ' , ' base_mac_address ' , ' blade_addresses ' , <nl> def generate_device_dict ( f5 , regex ) : <nl> ' timelimited_modules ' , ' timezone ' , ' unicast_addresses ' ] <nl> return generate_dict ( devices , fields ) <nl> <nl> + <nl> def generate_device_group_dict ( f5 , regex ) : <nl> device_groups = DeviceGroups ( f5 . get_api ( ) , regex ) <nl> - fields = [ ' all_preferred_active ' , ' autosync_enabled_state ' , ' description ' , <nl> + fields = [ ' all_preferred_active ' , ' autosync_enabled_state ' , ' description ' , <nl> ' device ' , ' full_load_on_sync_state ' , <nl> ' incremental_config_sync_size_maximum ' , <nl> ' network_failover_enabled_state ' , ' sync_status ' , ' type ' ] <nl> return generate_dict ( device_groups , fields ) <nl> <nl> + <nl> def generate_traffic_group_dict ( f5 , regex ) : <nl> traffic_groups = TrafficGroups ( f5 . get_api ( ) , regex ) <nl> fields = [ ' auto_failback_enabled_state ' , ' auto_failback_time ' , <nl> def generate_traffic_group_dict ( f5 , regex ) : <nl> ' unit_id ' ] <nl> return generate_dict ( traffic_groups , fields ) <nl> <nl> + <nl> def generate_rule_dict ( f5 , regex ) : <nl> rules = Rules ( f5 . get_api ( ) , regex ) <nl> fields = [ ' definition ' , ' description ' , ' ignore_vertification ' , <nl> ' verification_status ' ] <nl> return generate_dict ( rules , fields ) <nl> <nl> + <nl> def generate_node_dict ( f5 , regex ) : <nl> nodes = Nodes ( f5 . get_api ( ) , regex ) <nl> fields = [ ' address ' , ' connection_limit ' , ' description ' , ' dynamic_ratio ' , <nl> def generate_node_dict ( f5 , regex ) : <nl> ' object_status ' , ' rate_limit ' , ' ratio ' , ' session_status ' ] <nl> return generate_dict ( nodes , fields ) <nl> <nl> + <nl> def generate_virtual_address_dict ( f5 , regex ) : <nl> virtual_addresses = VirtualAddresses ( f5 . get_api ( ) , regex ) <nl> fields = [ ' address ' , ' arp_state ' , ' auto_delete_state ' , ' connection_limit ' , <nl> def generate_virtual_address_dict ( f5 , regex ) : <nl> ' route_advertisement_state ' , ' traffic_group ' ] <nl> return generate_dict ( virtual_addresses , fields ) <nl> <nl> + <nl> def generate_address_class_dict ( f5 , regex ) : <nl> address_classes = AddressClasses ( f5 . get_api ( ) , regex ) <nl> fields = [ ' address_class ' , ' description ' ] <nl> return generate_dict ( address_classes , fields ) <nl> <nl> + <nl> def generate_certificate_dict ( f5 , regex ) : <nl> certificates = Certificates ( f5 . get_api ( ) , regex ) <nl> return dict ( zip ( certificates . get_list ( ) , certificates . get_certificate_list ( ) ) ) <nl> <nl> + <nl> def generate_key_dict ( f5 , regex ) : <nl> keys = Keys ( f5 . get_api ( ) , regex ) <nl> return dict ( zip ( keys . get_list ( ) , keys . get_key_list ( ) ) ) <nl> <nl> + <nl> def generate_client_ssl_profile_dict ( f5 , regex ) : <nl> profiles = ProfileClientSSL ( f5 . get_api ( ) , regex ) <nl> fields = [ ' alert_timeout ' , ' allow_nonssl_state ' , ' authenticate_depth ' , <nl> def generate_client_ssl_profile_dict ( f5 , regex ) : <nl> ' unclean_shutdown_state ' , ' is_base_profile ' , ' is_system_profile ' ] <nl> return generate_dict ( profiles , fields ) <nl> <nl> + <nl> def generate_system_info_dict ( f5 ) : <nl> system_info = SystemInfo ( f5 . get_api ( ) ) <nl> fields = [ ' base_mac_address ' , <nl> def generate_system_info_dict ( f5 ) : <nl> ' time_zone ' , ' uptime ' ] <nl> return generate_simple_dict ( system_info , fields ) <nl> <nl> + <nl> def generate_software_list ( f5 ) : <nl> software = Software ( f5 . get_api ( ) ) <nl> software_list = software . get_all_software_status ( ) <nl> def generate_software_list ( f5 ) : <nl> <nl> <nl> def main ( ) : <nl> + argument_spec = f5_argument_spec ( ) <nl> + <nl> + meta_args = dict ( <nl> + session = dict ( type = ' bool ' , default = False ) , <nl> + include = dict ( type = ' list ' , required = True ) , <nl> + filter = dict ( type = ' str ' , required = False ) , <nl> + ) <nl> + argument_spec . update ( meta_args ) <nl> + <nl> module = AnsibleModule ( <nl> - argument_spec = dict ( <nl> - server = dict ( type = ' str ' , required = True ) , <nl> - user = dict ( type = ' str ' , required = True ) , <nl> - password = dict ( type = ' str ' , required = True ) , <nl> - validate_certs = dict ( default = ' yes ' , type = ' bool ' ) , <nl> - session = dict ( type = ' bool ' , default = False ) , <nl> - include = dict ( type = ' list ' , required = True ) , <nl> - filter = dict ( type = ' str ' , required = False ) , <nl> - ) <nl> + argument_spec = argument_spec <nl> ) <nl> <nl> if not bigsuds_found : <nl> def main ( ) : <nl> <nl> result = { ' ansible_facts ' : facts } <nl> <nl> - except Exception , e : <nl> + except Exception as e : <nl> module . fail_json ( msg = \" received exception : % s \\ ntraceback : % s \" % ( e , traceback . format_exc ( ) ) ) <nl> <nl> module . exit_json ( * * result ) <nl>\n", "msg": "Adds coding conventions to the bigip_facts module ( )\n"}
{"diff_id": 20194, "repo": "bokeh/bokeh\n", "sha": "0813563bba4171e59286459bbe015d6350e8da6a\n", "time": "2015-05-15T09:06:15Z\n", "diff": "mmm a / examples / plotting / file / color_sliders . py <nl> ppp b / examples / plotting / file / color_sliders . py <nl> <nl> - from bokeh import plotting as bkplt <nl> + from bokeh . plotting import ColumnDataSource , figure , hplot , output_file , show <nl> from bokeh . models import LinearColorMapper , HoverTool , TapTool <nl> from bokeh . models . actions import Callback <nl> from bokeh . models . widgets import Slider <nl> def hex_to_dec ( hex ) : <nl> text_color = ' # 000000 ' <nl> <nl> # create a data source to enable refreshing of fill & text color <nl> - source = bkplt . ColumnDataSource ( data = dict ( x = x , y = y , color = [ hex_color ] , text_color = [ text_color ] ) ) <nl> + source = ColumnDataSource ( data = dict ( x = x , y = y , color = [ hex_color ] , text_color = [ text_color ] ) ) <nl> <nl> tools1 = ' reset , save ' <nl> <nl> # create first plot , as a rect ( ) glyph and centered text label , with fill and text color taken from source <nl> - p1 = bkplt . figure ( x_range = ( - 8 , 8 ) , y_range = ( - 4 , 4 ) , plot_width = 600 , plot_height = 300 , title = None , tools = tools1 ) <nl> + p1 = figure ( x_range = ( - 8 , 8 ) , y_range = ( - 4 , 4 ) , plot_width = 600 , plot_height = 300 , title = None , tools = tools1 ) <nl> color_block = p1 . rect ( x = ' x ' , y = ' y ' , width = 18 , height = 10 , fill_color = ' color ' , line_color = ' black ' , source = source ) <nl> hex_code_text = p1 . text ( ' x ' , ' y ' , text = ' color ' , text_color = ' text_color ' , alpha = 0 . 6667 , text_font_size = ' 36pt ' , text_baseline = ' middle ' , text_align = ' center ' , source = source ) <nl> <nl> def hex_to_dec ( hex ) : <nl> crcolor , crRGBs = generate_color_range ( 1000 , brightness ) # produce spectrum <nl> <nl> # make data source object to allow information to be displayed by hover tool <nl> - crsource = bkplt . ColumnDataSource ( data = dict ( x = crx , y = cry , crcolor = crcolor , RGBs = crRGBs ) ) <nl> + crsource = ColumnDataSource ( data = dict ( x = crx , y = cry , crcolor = crcolor , RGBs = crRGBs ) ) <nl> <nl> tools2 = ' reset , save , hover ' <nl> <nl> # create second plot <nl> - p2 = bkplt . figure ( x_range = ( 0 , 1000 ) , y_range = ( 0 , 10 ) , plot_width = 600 , plot_height = 150 , tools = tools2 , title = ' hover over color ' ) <nl> + p2 = figure ( x_range = ( 0 , 1000 ) , y_range = ( 0 , 10 ) , plot_width = 600 , plot_height = 150 , tools = tools2 , title = ' hover over color ' ) <nl> color_range1 = p2 . rect ( x = ' x ' , y = ' y ' , width = 1 , height = 10 , color = ' crcolor ' , source = crsource ) <nl> # set up hover tool to show color hex code and sample swatch <nl> hover = p2 . select ( dict ( type = HoverTool ) ) <nl> def hex_to_dec ( hex ) : <nl> p2 . axis . major_tick_line_color = None <nl> p2 . axis . minor_tick_line_color = None <nl> <nl> - layout = bkplt . hplot ( <nl> + layout = hplot ( <nl> vform ( red_slider , green_slider , blue_slider ) , <nl> vform ( p1 , p2 ) <nl> ) <nl> <nl> - bkplt . output_file ( \" color_sliders . html \" ) <nl> - bkplt . show ( layout ) <nl> + output_file ( \" color_sliders . html \" ) <nl> + show ( layout ) <nl>\n", "msg": "changed explicit importing to fit with convention\n"}
{"diff_id": 20205, "repo": "OWASP/CheatSheetSeries\n", "sha": "1debd4bcc3b3839c75f76dd770f11256453aa126\n", "time": "2019-08-24T16:27:25Z\n", "diff": "mmm a / scripts / Identify_Old_Issue_And_PR . py <nl> ppp b / scripts / Identify_Old_Issue_And_PR . py <nl> def is_old_pull_request ( issue ) : <nl> print ( \" [ + ] State : \" ) <nl> return_code = len ( old_issues [ \" PR \" ] ) + len ( old_issues [ \" ISSUE \" ] ) <nl> if len ( old_issues [ \" PR \" ] ) > 0 : <nl> - print ( \" \\ tOld pull request identified : % s \" % old_issues [ \" PR \" ] ) <nl> - else : <nl> - print ( \" \\ tAll PR are OK . \" ) <nl> + print ( \" Old pull request identified : % s \" % old_issues [ \" PR \" ] ) <nl> if len ( old_issues [ \" ISSUE \" ] ) > 0 : <nl> - print ( \" \\ tOld issue identified : % s \" % old_issues [ \" ISSUE \" ] ) <nl> - else : <nl> - print ( \" \\ tAll issues are OK . \" ) <nl> + print ( \" Old issue identified : % s \" % old_issues [ \" ISSUE \" ] ) <nl> + if return_code = = 0 : <nl> + print ( \" Nothing identified . \" ) <nl> sys . exit ( return_code ) <nl> \\ No newline at end of file <nl>\n", "msg": "Add a script to identify any old Issue or PR .\n"}
{"diff_id": 20249, "repo": "tornadoweb/tornado\n", "sha": "8ad43811a61cc0e11239bbca62ba350dfe433a22\n", "time": "2016-02-26T15:46:40Z\n", "diff": "old mode 100644 <nl> new mode 100755 <nl> index 8826c62b0 . . 504becad9 <nl> mmm a / tornado / web . py <nl> ppp b / tornado / web . py <nl> def render ( self , template_name , * * kwargs ) : <nl> if body_part : <nl> html_bodies . append ( utf8 ( body_part ) ) <nl> <nl> - def is_absolute ( path ) : <nl> - return any ( path . startswith ( x ) for x in [ \" / \" , \" http : \" , \" https : \" ] ) <nl> if js_files : <nl> # Maintain order of JavaScript files given by modules <nl> - paths = [ ] <nl> - unique_paths = set ( ) <nl> - for path in js_files : <nl> - if not is_absolute ( path ) : <nl> - path = self . static_url ( path ) <nl> - if path not in unique_paths : <nl> - paths . append ( path ) <nl> - unique_paths . add ( path ) <nl> - js = ' ' . join ( ' < script src = \" ' + escape . xhtml_escape ( p ) + <nl> - ' \" type = \" text / javascript \" > < / script > ' <nl> - for p in paths ) <nl> + js = self . render_linked_js ( js_files ) <nl> sloc = html . rindex ( b ' < / body > ' ) <nl> html = html [ : sloc ] + utf8 ( js ) + b ' \\ n ' + html [ sloc : ] <nl> if js_embed : <nl> - js = b ' < script type = \" text / javascript \" > \\ n / / < ! [ CDATA [ \\ n ' + \\ <nl> - b ' \\ n ' . join ( js_embed ) + b ' \\ n / / ] ] > \\ n < / script > ' <nl> + js = self . render_embed_js ( js_embed ) <nl> sloc = html . rindex ( b ' < / body > ' ) <nl> html = html [ : sloc ] + js + b ' \\ n ' + html [ sloc : ] <nl> if css_files : <nl> - paths = [ ] <nl> - unique_paths = set ( ) <nl> - for path in css_files : <nl> - if not is_absolute ( path ) : <nl> - path = self . static_url ( path ) <nl> - if path not in unique_paths : <nl> - paths . append ( path ) <nl> - unique_paths . add ( path ) <nl> - css = ' ' . join ( ' < link href = \" ' + escape . xhtml_escape ( p ) + ' \" ' <nl> - ' type = \" text / css \" rel = \" stylesheet \" / > ' <nl> - for p in paths ) <nl> + css = self . render_linked_css ( css_files ) <nl> hloc = html . index ( b ' < / head > ' ) <nl> html = html [ : hloc ] + utf8 ( css ) + b ' \\ n ' + html [ hloc : ] <nl> if css_embed : <nl> - css = b ' < style type = \" text / css \" > \\ n ' + b ' \\ n ' . join ( css_embed ) + \\ <nl> - b ' \\ n < / style > ' <nl> + css = self . render_embed_css ( css_embed ) <nl> hloc = html . index ( b ' < / head > ' ) <nl> html = html [ : hloc ] + css + b ' \\ n ' + html [ hloc : ] <nl> if html_heads : <nl> def is_absolute ( path ) : <nl> html = html [ : hloc ] + b ' ' . join ( html_bodies ) + b ' \\ n ' + html [ hloc : ] <nl> self . finish ( html ) <nl> <nl> + def render_linked_js ( self , js_files ) : <nl> + \" \" \" Default method used to render the final js links for the <nl> + rendered webpage . <nl> + <nl> + Override this method in a sub - classed controller to change the output . <nl> + \" \" \" <nl> + paths = [ ] <nl> + unique_paths = set ( ) <nl> + <nl> + for path in js_files : <nl> + if not is_absolute ( path ) : <nl> + path = self . static_url ( path ) <nl> + if path not in unique_paths : <nl> + paths . append ( path ) <nl> + unique_paths . add ( path ) <nl> + <nl> + return ' ' . join ( ' < script src = \" ' + escape . xhtml_escape ( p ) + <nl> + ' \" type = \" text / javascript \" > < / script > ' <nl> + for p in paths ) <nl> + <nl> + def render_embed_js ( self , js_embed ) : <nl> + \" \" \" Default method used to render the final embedded js for the <nl> + rendered webpage . <nl> + <nl> + Override this method in a sub - classed controller to change the output . <nl> + \" \" \" <nl> + return b ' < script type = \" text / javascript \" > \\ n / / < ! [ CDATA [ \\ n ' + \\ <nl> + b ' \\ n ' . join ( js_embed ) + b ' \\ n / / ] ] > \\ n < / script > ' <nl> + <nl> + def render_linked_css ( self , css_files ) : <nl> + \" \" \" Default method used to render the final css links for the <nl> + rendered webpage . <nl> + <nl> + Override this method in a sub - classed controller to change the output . <nl> + \" \" \" <nl> + paths = [ ] <nl> + unique_paths = set ( ) <nl> + <nl> + for path in css_files : <nl> + if not is_absolute ( path ) : <nl> + path = self . static_url ( path ) <nl> + if path not in unique_paths : <nl> + paths . append ( path ) <nl> + unique_paths . add ( path ) <nl> + <nl> + return ' ' . join ( ' < link href = \" ' + escape . xhtml_escape ( p ) + ' \" ' <nl> + ' type = \" text / css \" rel = \" stylesheet \" / > ' <nl> + for p in paths ) <nl> + <nl> + def render_embed_css ( self , css_embed ) : <nl> + \" \" \" Default method used to render the final embedded css for the <nl> + rendered webpage . <nl> + <nl> + Override this method in a sub - classed controller to change the output . <nl> + \" \" \" <nl> + return b ' < style type = \" text / css \" > \\ n ' + b ' \\ n ' . join ( css_embed ) + \\ <nl> + b ' \\ n < / style > ' <nl> + <nl> def render_string ( self , template_name , * * kwargs ) : <nl> \" \" \" Generate the given template with the given arguments . <nl> <nl> def _unquote_or_none ( s ) : <nl> if s is None : <nl> return s <nl> return escape . url_unescape ( s , encoding = None , plus = False ) <nl> + <nl> + def is_absolute ( path ) : <nl> + return any ( path . startswith ( x ) for x in [ \" / \" , \" http : \" , \" https : \" ] ) <nl>\n", "msg": "Moved the rendering of CSS and JS assets to their own instance methods\n"}
{"diff_id": 20316, "repo": "home-assistant/core\n", "sha": "9026a78a6671dc11284601bdab96b6db0371f6bf\n", "time": "2013-10-06T20:12:22Z\n", "diff": "mmm a / homeassistant / observers . py <nl> ppp b / homeassistant / observers . py <nl> def get_active_devices ( self ) : <nl> <nl> def _update_tomato_info ( self ) : <nl> \" \" \" Ensures the information from the Tomato router is up to date . <nl> - Returns boolean if successful . \" \" \" <nl> + Returns boolean if scanning successful . \" \" \" <nl> + <nl> + self . lock . acquire ( ) <nl> <nl> # if date_updated is not defined ( update has never ran ) or the date is too old we scan for new data <nl> if self . date_updated is None or datetime . now ( ) - self . date_updated > TOMATO_MIN_TIME_BETWEEN_SCANS : <nl> - self . lock . acquire ( ) <nl> - <nl> self . logger . info ( \" Tomato : Scanning \" ) <nl> <nl> try : <nl> def _update_tomato_info ( self ) : <nl> data = { ' _http_id ' : self . http_id , ' exec ' : ' devlist ' } , <nl> auth = requests . auth . HTTPBasicAuth ( self . username , self . password ) ) <nl> <nl> - \" \" \" <nl> - Tomato API : <nl> - arplist contains a list of lists with items : <nl> - - ip ( string ) <nl> - - mac ( string ) <nl> - - iface ( string ) <nl> - <nl> - wldev contains list of lists with items : <nl> - - iface ( string ) <nl> - - mac ( string ) <nl> - - rssi ( int ) <nl> - - tx ( int ) <nl> - - rx ( int ) <nl> - - quality ( int ) <nl> - - unknown_num ( int ) <nl> - <nl> - dhcpd_lease contains a list of lists with items : <nl> - - name ( string ) <nl> - - ip ( string ) <nl> - - mac ( string ) <nl> - - lease_age ( string ) <nl> - \" \" \" <nl> + # Calling and parsing the Tomato api here . We only need the wldev and dhcpd_lease values . <nl> + # See http : / / paulusschoutsen . nl / blog / 2013 / 10 / tomato - api - documentation / for what ' s going on here . <nl> self . last_results = { param : json . loads ( value . replace ( \" ' \" , ' \" ' ) ) <nl> for param , value in re . findall ( r \" ( ? P < param > \\ w * ) = ( ? P < value > . * ) ; \" , req . text ) <nl> if param in [ \" wldev \" , \" dhcpd_lease \" ] } <nl> def _update_tomato_info ( self ) : <nl> self . known_devices [ mac ] = { ' name ' : name , ' track ' : ' 0 ' } <nl> <nl> except requests . ConnectionError : <nl> - self . logger . exception ( \" Tomato : Scanning failed \" ) <nl> + # If we could not connect to the router <nl> + self . logger . exception ( \" Tomato : Failed to connect to the router \" ) <nl> + <nl> + return False <nl> + <nl> + except ValueError : <nl> + # If json decoder could not parse the response <nl> + self . logger . exception ( \" Tomato : Failed to parse response from router \" ) <nl> <nl> return False <nl> <nl> except IOError : <nl> + # If scanning was successful but we failed to be able to write to the known devices file <nl> self . logger . exception ( \" Tomato : Updating { } failed \" . format ( TOMATO_KNOWN_DEVICES_FILE ) ) <nl> <nl> return True <nl> def _update_tomato_info ( self ) : <nl> finally : <nl> self . lock . release ( ) <nl> <nl> + else : <nl> + # We acquired the lock before the IF check , release it before we return True <nl> + self . lock . release ( ) <nl> + <nl> <nl> return True <nl>\n", "msg": "Better documentation of TomatoDeviceScanner\n"}
{"diff_id": 20405, "repo": "ytdl-org/youtube-dl\n", "sha": "a7a14d958604f5334413e2fc1872a8317d5e4884\n", "time": "2015-01-29T19:15:38Z\n", "diff": "mmm a / youtube_dl / YoutubeDL . py <nl> ppp b / youtube_dl / YoutubeDL . py <nl> def process_video_result ( self , info_dict , download = True ) : <nl> if thumbnails is None : <nl> thumbnail = info_dict . get ( ' thumbnail ' ) <nl> if thumbnail : <nl> - thumbnails = [ { ' url ' : thumbnail } ] <nl> + info_dict [ ' thumbnails ' ] = thumbnails = [ { ' url ' : thumbnail } ] <nl> if thumbnails : <nl> thumbnails . sort ( key = lambda t : ( <nl> t . get ( ' preference ' ) , t . get ( ' width ' ) , t . get ( ' height ' ) , <nl>\n", "msg": "[ YoutubeDL ] set the ' thumbnails ' field if the info_dict has the ' thumbnails ' field\n"}
{"diff_id": 20476, "repo": "scikit-learn/scikit-learn\n", "sha": "cb132cdf99855f81830e9a93604095be3cb8ea5e\n", "time": "2012-05-04T22:17:58Z\n", "diff": "mmm a / sklearn / cluster / dbscan_ . py <nl> ppp b / sklearn / cluster / dbscan_ . py <nl> <nl> # <nl> # License : BSD <nl> <nl> + import warnings <nl> import numpy as np <nl> <nl> from . . base import BaseEstimator <nl> def fit ( self , X , * * params ) : <nl> params : dict <nl> Overwrite keywords from __init__ . <nl> \" \" \" <nl> - <nl> - self . set_params ( * * params ) <nl> + if params : <nl> + warnings . warn ( ' Passing parameters to fit methods is ' <nl> + ' depreciated ' , stacklevel = 2 ) <nl> + self . set_params ( * * params ) <nl> self . core_sample_indices_ , self . labels_ = dbscan ( X , <nl> * * self . get_params ( ) ) <nl> self . components_ = X [ self . core_sample_indices_ ] . copy ( ) <nl>\n", "msg": "Warn : Passing params to fit is depreciated\n"}
{"diff_id": 20637, "repo": "ansible/ansible\n", "sha": "8c8e064828b3e3946770af04d5282335ffd3add0\n", "time": "2016-06-14T19:03:59Z\n", "diff": "mmm a / lib / ansible / plugins / connection / chroot . py <nl> ppp b / lib / ansible / plugins / connection / chroot . py <nl> def __init__ ( self , play_context , new_stdin , * args , * * kwargs ) : <nl> raise AnsibleError ( \" % s is not a directory \" % self . chroot ) <nl> <nl> chrootsh = os . path . join ( self . chroot , ' bin / sh ' ) <nl> - if not is_executable ( chrootsh ) : <nl> + # Want to check for a usable bourne shell inside the chroot . <nl> + # is_executable ( ) = = True is sufficient . For symlinks it <nl> + # gets really complicated really fast . So we punt on finding that <nl> + # out . As long as it ' s a symlink we assume that it will work <nl> + if not ( is_executable ( chrootsh ) or ( os . path . lexists ( chrootsh ) and os . path . islink ( chrootsh ) ) ) : <nl> raise AnsibleError ( \" % s does not look like a chrootable dir ( / bin / sh missing ) \" % self . chroot ) <nl> <nl> self . chroot_cmd = distutils . spawn . find_executable ( ' chroot ' ) <nl>\n", "msg": "Be more lenient of symlinked / bin / sh inside the chroot ( )\n"}
{"diff_id": 20750, "repo": "mitmproxy/mitmproxy\n", "sha": "4339b8e7fa1140b9138a023e7e61d78cefe6bb02\n", "time": "2015-08-19T19:09:48Z\n", "diff": "mmm a / libmproxy / protocol2 / http . py <nl> ppp b / libmproxy / protocol2 / http . py <nl> class Http2Layer ( Layer ) : <nl> def __init__ ( self , ctx , mode ) : <nl> super ( Http2Layer , self ) . __init__ ( ctx ) <nl> self . mode = mode <nl> - self . client_protocol = HTTP2Protocol ( self . client_conn , is_server = True ) <nl> - self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False ) <nl> + self . client_protocol = HTTP2Protocol ( self . client_conn , is_server = True , unhandled_frame_cb = self . handle_unexpected_frame ) <nl> + self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False , unhandled_frame_cb = self . handle_unexpected_frame ) <nl> <nl> def read_from_client ( self ) : <nl> return HTTPRequest . from_protocol ( <nl> def send_to_server ( self , message ) : <nl> <nl> def connect ( self ) : <nl> self . ctx . connect ( ) <nl> - self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False ) <nl> + self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False , unhandled_frame_cb = self . handle_unexpected_frame ) <nl> self . server_protocol . perform_connection_preface ( ) <nl> <nl> def reconnect ( self ) : <nl> self . ctx . reconnect ( ) <nl> - self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False ) <nl> + self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False , unhandled_frame_cb = self . handle_unexpected_frame ) <nl> self . server_protocol . perform_connection_preface ( ) <nl> <nl> def set_server ( self , * args , * * kwargs ) : <nl> self . ctx . set_server ( * args , * * kwargs ) <nl> - self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False ) <nl> + self . server_protocol = HTTP2Protocol ( self . server_conn , is_server = False , unhandled_frame_cb = self . handle_unexpected_frame ) <nl> self . server_protocol . perform_connection_preface ( ) <nl> <nl> def __call__ ( self ) : <nl> def __call__ ( self ) : <nl> layer = HttpLayer ( self , self . mode ) <nl> layer ( ) <nl> <nl> + def handle_unexpected_frame ( self , frm ) : <nl> + print ( frm . human_readable ( ) ) <nl> + <nl> <nl> def make_error_response ( status_code , message , headers = None ) : <nl> response = status_codes . RESPONSES . get ( status_code , \" Unknown \" ) <nl>\n", "msg": "http2 : use callback for handle unexpected frames\n"}
{"diff_id": 20769, "repo": "3b1b/manim\n", "sha": "15e5969eed6429562739d574c473e152ab1ad240\n", "time": "2017-10-17T02:21:42Z\n", "diff": "mmm a / scene / scene . py <nl> ppp b / scene / scene . py <nl> def setup ( self ) : <nl> \" \" \" <nl> pass <nl> <nl> + def setup_bases ( self ) : <nl> + for base in self . __class__ . __bases__ : <nl> + base . setup ( self ) <nl> + <nl> def construct ( self ) : <nl> pass # To be implemented in subclasses <nl> <nl>\n", "msg": "setup_bases for scenes that subclass multiple other scenes\n"}
{"diff_id": 20980, "repo": "matplotlib/matplotlib\n", "sha": "cf37d541758a360d988701b69175cd72cc351ea4\n", "time": "2019-12-02T21:41:01Z\n", "diff": "mmm a / lib / matplotlib / fontconfig_pattern . py <nl> ppp b / lib / matplotlib / fontconfig_pattern . py <nl> <nl> <nl> from functools import lru_cache <nl> import re <nl> - <nl> + import numpy as np <nl> from pyparsing import ( Literal , ZeroOrMore , Optional , Regex , StringEnd , <nl> ParseException , Suppress ) <nl> <nl> def _escape_val ( val , escape_func ) : <nl> the input escape function to make the values into legal font config <nl> strings . The result is returned as a string . <nl> \" \" \" <nl> - if isinstance ( val , list ) : <nl> - val = [ escape_func ( r ' \\ \\ \\ 1 ' , str ( x ) ) for x in val <nl> - if x is not None ] <nl> - val = ' , ' . join ( val ) <nl> - else : <nl> - val = escape_func ( r ' \\ \\ \\ 1 ' , str ( val ) ) <nl> + if not np . iterable ( val ) or isinstance ( val , str ) : <nl> + val = [ val ] <nl> <nl> - return val <nl> + return ' , ' . join ( escape_func ( r ' \\ \\ \\ 1 ' , str ( x ) ) for x in val <nl> + if x is not None ) <nl> <nl> <nl> def generate_fontconfig_pattern ( d ) : <nl> def generate_fontconfig_pattern ( d ) : <nl> # The other keys are added as key = value <nl> for key in [ ' style ' , ' variant ' , ' weight ' , ' stretch ' , ' file ' , ' size ' ] : <nl> val = getattr ( d , ' get_ ' + key ) ( ) <nl> + # Don ' t use ' if not val ' because 0 is a valid input . <nl> if val is not None and val ! = [ ] : <nl> props . append ( \" : % s = % s \" % ( key , _escape_val ( val , value_escape ) ) ) <nl> <nl>\n", "msg": "Simplified escape function to a single path\n"}
{"diff_id": 21128, "repo": "scrapy/scrapy\n", "sha": "8781ef3914d1d6bf28db078ada1c7962e2e3cb86\n", "time": "2010-07-28T14:16:42Z\n", "diff": "mmm a / scrapy / contrib / ibl / extraction / pageparsing . py <nl> ppp b / scrapy / contrib / ibl / extraction / pageparsing . py <nl> def _close_unpaired_tag ( self ) : <nl> def _handle_unpaired_tag ( self , html_tag ) : <nl> if self . _read_bool_template_attribute ( html_tag , \" ignore \" ) and html_tag . tag = = \" img \" : <nl> self . ignored_regions . append ( ( self . next_tag_index , self . next_tag_index + 1 ) ) <nl> - elif self . _read_bool_template_attribute ( html_tag , \" ignore - beneath \" ) and html_tag . tag = = \" img \" : <nl> + elif self . _read_bool_template_attribute ( html_tag , \" ignore - beneath \" ) : <nl> self . ignored_regions . append ( ( self . next_tag_index , None ) ) <nl> jannotation = self . _read_template_annotation ( html_tag ) <nl> if jannotation : <nl>\n", "msg": "Remove restriction of marking ignore - beneath only for img unpaired tags\n"}
{"diff_id": 21156, "repo": "ytdl-org/youtube-dl\n", "sha": "d9dd3584e1051b159a748da32cd7d7a1da0bb787\n", "time": "2014-05-28T12:38:44Z\n", "diff": "mmm a / youtube_dl / extractor / cinemassacre . py <nl> ppp b / youtube_dl / extractor / cinemassacre . py <nl> <nl> # encoding : utf - 8 <nl> from __future__ import unicode_literals <nl> + <nl> import re <nl> <nl> from . common import InfoExtractor <nl> from . . utils import ( <nl> ExtractorError , <nl> + int_or_none , <nl> ) <nl> <nl> <nl> class CinemassacreIE ( InfoExtractor ) : <nl> _TESTS = [ <nl> { <nl> ' url ' : ' http : / / cinemassacre . com / 2012 / 11 / 10 / avgn - the - movie - trailer / ' , <nl> - ' file ' : ' 19911 . mp4 ' , <nl> ' md5 ' : ' fde81fbafaee331785f58cd6c0d46190 ' , <nl> ' info_dict ' : { <nl> + ' id ' : ' 19911 ' , <nl> + ' ext ' : ' mp4 ' , <nl> ' upload_date ' : ' 20121110 ' , <nl> ' title ' : ' \u201c Angry Video Game Nerd : The Movie \u201d \u2013 Trailer ' , <nl> ' description ' : ' md5 : fb87405fcb42a331742a0dce2708560b ' , <nl> class CinemassacreIE ( InfoExtractor ) : <nl> } , <nl> { <nl> ' url ' : ' http : / / cinemassacre . com / 2013 / 10 / 02 / the - mummys - hand - 1940 ' , <nl> - ' file ' : ' 521be8ef82b16 . mp4 ' , <nl> ' md5 ' : ' d72f10cd39eac4215048f62ab477a511 ' , <nl> ' info_dict ' : { <nl> + ' id ' : ' 521be8ef82b16 ' , <nl> + ' ext ' : ' mp4 ' , <nl> ' upload_date ' : ' 20131002 ' , <nl> ' title ' : ' The Mummy \u2019 s Hand ( 1940 ) ' , <nl> } , <nl> def _real_extract ( self , url ) : <nl> r ' < div class = \" entry - content \" > ( ? P < description > . + ? ) < / div > ' , <nl> webpage , ' description ' , flags = re . DOTALL , fatal = False ) <nl> <nl> - playerdata = self . _download_webpage ( playerdata_url , video_id ) <nl> - video_thumbnail = self . _search_regex ( r ' image : \\ ' ( ? P < thumbnail > [ ^ \\ ' ] + ) \\ ' ' , playerdata , ' thumbnail ' , fatal = False ) <nl> + playerdata = self . _download_webpage ( playerdata_url , video_id , ' Downloading player webpage ' ) <nl> + video_thumbnail = self . _search_regex ( <nl> + r ' image : \\ ' ( ? P < thumbnail > [ ^ \\ ' ] + ) \\ ' ' , playerdata , ' thumbnail ' , fatal = False ) <nl> sd_url = self . _search_regex ( r ' file : \\ ' ( [ ^ \\ ' ] + ) \\ ' , label : \\ ' SD \\ ' ' , playerdata , ' sd_file ' ) <nl> videolist_url = self . _search_regex ( r ' file : \\ ' ( [ ^ \\ ' ] + \\ . smil ) \\ ' } ' , playerdata , ' videolist_url ' ) <nl> - <nl> - videolist = self . _download_webpage ( videolist_url , video_id ) <nl> + <nl> + videolist = self . _download_xml ( videolist_url , video_id , ' Downloading videolist XML ' ) <nl> + <nl> formats = [ ] <nl> baseurl = sd_url [ : sd_url . rfind ( ' / ' ) + 1 ] <nl> - for match in re . finditer ( ' < video src = \" mp4 : ( ? P < file > [ ^ \" ] + _ ( ? P < format_id > [ ^ \" ] + ) \\ . [ ^ \" ] + ) \" system - bitrate = \" ( ? P < br > \\ d + ) \" ( ? : width = \" ( ? P < width > \\ d + ) \" height = \" ( ? P < height > \\ d + ) \" ) ? / > ' , videolist ) : <nl> + for video in videolist . findall ( ' . / / video ' ) : <nl> + src = video . get ( ' src ' ) <nl> + if not src : <nl> + continue <nl> + file_ = src . partition ( ' : ' ) [ - 1 ] <nl> + width = int_or_none ( video . get ( ' width ' ) ) <nl> + height = int_or_none ( video . get ( ' height ' ) ) <nl> + bitrate = int_or_none ( video . get ( ' system - bitrate ' ) ) <nl> format = { <nl> - ' url ' : baseurl + match . group ( ' file ' ) , <nl> - ' format_id ' : match . group ( ' format_id ' ) <nl> + ' url ' : baseurl + file_ , <nl> + ' format_id ' : src . rpartition ( ' . ' ) [ 0 ] . rpartition ( ' _ ' ) [ - 1 ] , <nl> } <nl> - if match . group ( ' width ' ) : <nl> + if width or height : <nl> format . update ( { <nl> - ' tbr ' : int ( match . group ( ' br ' ) ) / / 1000 , <nl> - ' width ' : int ( match . group ( ' width ' ) ) , <nl> - ' height ' : int ( match . group ( ' height ' ) ) <nl> + ' tbr ' : bitrate / / 1000 if bitrate else None , <nl> + ' width ' : width , <nl> + ' height ' : height , <nl> } ) <nl> else : <nl> format . update ( { <nl> - ' abr ' : int ( match . group ( ' br ' ) ) / / 1000 , <nl> - ' vcodec ' : ' none ' <nl> + ' abr ' : bitrate / / 1000 if bitrate else None , <nl> + ' vcodec ' : ' none ' , <nl> } ) <nl> formats . append ( format ) <nl> self . _sort_formats ( formats ) <nl>\n", "msg": "[ cinemassacre ] Improve formats extraction and modernize\n"}
{"diff_id": 21235, "repo": "scikit-learn/scikit-learn\n", "sha": "c9b082a00886c0920ac28e9096de498481a89643\n", "time": "2016-04-13T22:20:44Z\n", "diff": "mmm a / sklearn / linear_model / logistic . py <nl> ppp b / sklearn / linear_model / logistic . py <nl> class LogisticRegression ( BaseEstimator , LinearClassifierMixin , <nl> \" \" \" Logistic Regression ( aka logit , MaxEnt ) classifier . <nl> <nl> In the multiclass case , the training algorithm uses the one - vs - rest ( OvR ) <nl> - scheme if the ' multi_class ' option is set to ' ovr ' and uses the cross - <nl> - entropy loss , if the ' multi_class ' option is set to ' multinomial ' . <nl> + scheme if the ' multi_class ' option is set to ' ovr ' , and uses the cross - <nl> + entropy loss if the ' multi_class ' option is set to ' multinomial ' . <nl> ( Currently the ' multinomial ' option is supported only by the ' lbfgs ' , <nl> ' sag ' and ' newton - cg ' solvers . ) <nl> <nl>\n", "msg": "[ DOC ] Moves comma to make sentence more clear\n"}
{"diff_id": 21259, "repo": "keras-team/keras\n", "sha": "ed365e94fd461995d1834f61663cf95615e6996c\n", "time": "2016-04-25T21:46:03Z\n", "diff": "mmm a / keras / engine / training . py <nl> ppp b / keras / engine / training . py <nl> def data_generator_task ( ) : <nl> class Model ( Container ) : <nl> <nl> def compile ( self , optimizer , loss , metrics = [ ] , loss_weights = None , <nl> - sample_weight_mode = None , * * kwargs ) : <nl> + sample_weight_mode = None , multi_target_loss = False , * * kwargs ) : <nl> ' ' ' Configures the model for training . <nl> <nl> # Arguments <nl> def compile ( self , optimizer , loss , metrics = [ ] , loss_weights = None , <nl> If the model has multiple outputs , you can use a different <nl> ` sample_weight_mode ` on each output by passing a <nl> dictionary or a list of modes . <nl> + multi_target_loss : add each target ' s loss function as a metric <nl> kwargs : when using the Theano backend , these arguments <nl> are passed into K . function . Ignored for Tensorflow backend . <nl> ' ' ' <nl> def compile ( self , optimizer , loss , metrics = [ ] , loss_weights = None , <nl> name = self . output_names [ i ] <nl> self . targets . append ( K . placeholder ( ndim = len ( shape ) , name = name + ' _target ' ) ) <nl> <nl> + # prepare metrics <nl> + self . metrics_names = [ ' loss ' ] <nl> + self . metrics = [ ] <nl> + <nl> # compute total loss <nl> total_loss = None <nl> for i in range ( len ( self . outputs ) ) : <nl> def compile ( self , optimizer , loss , metrics = [ ] , loss_weights = None , <nl> loss_weight = loss_weights_list [ i ] <nl> output_loss = loss_weight * weighted_loss ( y_true , y_pred , <nl> sample_weight , mask ) <nl> + if multi_target_loss : <nl> + self . metrics . append ( output_loss ) <nl> + self . metrics_names . append ( ' loss_ ' + self . output_names [ i ] ) <nl> if total_loss is None : <nl> total_loss = output_loss <nl> else : <nl> total_loss + = output_loss <nl> + <nl> # add regularization penalties to the loss <nl> for r in self . regularizers : <nl> total_loss = r ( total_loss ) <nl> <nl> - # prepare metrics <nl> - self . metrics_names = [ ' loss ' ] <nl> - self . metrics = [ ] <nl> # list of same size as output_names . <nl> # contains tuples ( metrics for output , names of metrics ) <nl> nested_metrics = collect_metrics ( metrics , self . output_names ) <nl>\n", "msg": "Added simple support for returning a multitarget loss\n"}
{"diff_id": 21363, "repo": "ipython/ipython\n", "sha": "6d152e27f02de96f8a97f99e584e6846380352fc\n", "time": "2014-07-13T04:32:34Z\n", "diff": "mmm a / IPython / html / widgets / widget . py <nl> ppp b / IPython / html / widgets / widget . py <nl> def get_state ( self , key = None ) : <nl> A single property ' s name to get . <nl> \" \" \" <nl> keys = self . keys if key is None else [ key ] <nl> - return { k : self . _pack_widgets ( getattr ( self , k ) ) for k in keys } <nl> - <nl> + state = { } <nl> + for k in keys : <nl> + f = self . trait_metadata ( k , ' to_json ' ) <nl> + value = getattr ( self , k ) <nl> + if f is not None : <nl> + state [ k ] = f ( value ) <nl> + else : <nl> + state [ k ] = self . _serialize_trait ( value ) <nl> + return state <nl> + <nl> def send ( self , content ) : <nl> \" \" \" Sends a custom msg to the widget model in the front - end . <nl> <nl> def _handle_receive_state ( self , sync_data ) : <nl> \" \" \" Called when a state is received from the front - end . \" \" \" <nl> for name in self . keys : <nl> if name in sync_data : <nl> - value = self . _unpack_widgets ( sync_data [ name ] ) <nl> + f = self . trait_metadata ( name , ' from_json ' ) <nl> + if f is not None : <nl> + value = f ( sync_data [ name ] ) <nl> + else : <nl> + value = self . _unserialize_trait ( sync_data [ name ] ) <nl> with self . _lock_property ( name , value ) : <nl> setattr ( self , name , value ) <nl> <nl> def _handle_displayed ( self , * * kwargs ) : <nl> \" \" \" Called when a view has been displayed for this widget instance \" \" \" <nl> self . _display_callbacks ( self , * * kwargs ) <nl> <nl> - def _pack_widgets ( self , x ) : <nl> - \" \" \" Recursively converts all widget instances to model id strings . <nl> + def _serialize_trait ( self , x ) : <nl> + \" \" \" Serialize a trait value to json <nl> <nl> - Children widgets will be stored and transmitted to the front - end by <nl> - their model ids . Return value must be JSON - able . \" \" \" <nl> + Traverse lists / tuples and dicts and serialize their values as well . <nl> + Replace any widgets with their model_id <nl> + \" \" \" <nl> if isinstance ( x , dict ) : <nl> - return { k : self . _pack_widgets ( v ) for k , v in x . items ( ) } <nl> + return { k : self . _serialize_trait ( v ) for k , v in x . items ( ) } <nl> elif isinstance ( x , ( list , tuple ) ) : <nl> - return [ self . _pack_widgets ( v ) for v in x ] <nl> + return [ self . _serialize_trait ( v ) for v in x ] <nl> elif isinstance ( x , Widget ) : <nl> return x . model_id <nl> else : <nl> return x # Value must be JSON - able <nl> <nl> - def _unpack_widgets ( self , x ) : <nl> - \" \" \" Recursively converts all model id strings to widget instances . <nl> + def _unserialize_trait ( self , x ) : <nl> + \" \" \" Convert json values to objects <nl> <nl> - Children widgets will be stored and transmitted to the front - end by <nl> - their model ids . \" \" \" <nl> + We explicitly support converting valid string widget UUIDs to Widget references . <nl> + \" \" \" <nl> if isinstance ( x , dict ) : <nl> - return { k : self . _unpack_widgets ( v ) for k , v in x . items ( ) } <nl> + return { k : self . _unserialize_trait ( v ) for k , v in x . items ( ) } <nl> elif isinstance ( x , ( list , tuple ) ) : <nl> - return [ self . _unpack_widgets ( v ) for v in x ] <nl> - elif isinstance ( x , string_types ) : <nl> - return x if x not in Widget . widgets else Widget . widgets [ x ] <nl> + return [ self . _unserialize_trait ( v ) for v in x ] <nl> + elif isinstance ( x , string_types ) and x in Widget . widgets : <nl> + # we want to support having child widgets at any level in a hierarchy <nl> + # trusting that a widget UUID will not appear out in the wild <nl> + return Widget . widgets [ x ] <nl> else : <nl> return x <nl> <nl>\n", "msg": "Update widget serializer / unserializer to allow custom serialization on a trait - by - trait basis .\n"}
{"diff_id": 21435, "repo": "matplotlib/matplotlib\n", "sha": "fcaa2fbde4b845af0f5c2619dabb40dc30f977c4\n", "time": "2005-12-15T05:06:37Z\n", "diff": "mmm a / setup . py <nl> ppp b / setup . py <nl> <nl> if os . path . exists ( ' MANIFEST ' ) : os . remove ( ' MANIFEST ' ) <nl> <nl> try : <nl> - from setuptools . command import bdist_egg <nl> - # from setuptools import setup # use setuptools if possible <nl> + # check if we have a reasonably recent copy of setuptools <nl> + from setuptools . command import bdist_egg <nl> has_setuptools = True <nl> except ImportError : <nl> from distutils . core import setup <nl> has_setuptools = False <nl> + <nl> + if has_setuptools : <nl> + from setuptools import setup <nl> <nl> import sys , os <nl> import glob <nl>\n", "msg": "fixed ' python setup . py foo ' if setuptools installed\n"}
{"diff_id": 21490, "repo": "ansible/ansible\n", "sha": "da5de725d767a227f57a485627e34b3c7d687dcf\n", "time": "2014-09-18T21:05:30Z\n", "diff": "mmm a / lib / ansible / runner / lookup_plugins / first_found . py <nl> ppp b / lib / ansible / runner / lookup_plugins / first_found . py <nl> def run ( self , terms , inject = None , * * kwargs ) : <nl> else : <nl> total_search = terms <nl> <nl> - result = None <nl> for fn in total_search : <nl> + if inject and ' _original_file ' in inject : <nl> + # check the templates and vars directories too , <nl> + # if they exist <nl> + for roledir in ( ' templates ' , ' vars ' ) : <nl> + path = utils . path_dwim ( os . path . join ( self . basedir , ' . . ' , roledir ) , fn ) <nl> + if os . path . exists ( path ) : <nl> + return [ path ] <nl> + # if none of the above were found , just check the <nl> + # current filename against the basedir ( this will already <nl> + # have . . / files from runner , if it ' s a role task <nl> path = utils . path_dwim ( self . basedir , fn ) <nl> if os . path . exists ( path ) : <nl> return [ path ] <nl> - <nl> - <nl> - if not result : <nl> + else : <nl> if skip : <nl> return [ ] <nl> else : <nl>\n", "msg": "Allow with_first_files to search relative to templates and vars in roles\n"}
{"diff_id": 21575, "repo": "python/cpython\n", "sha": "9a6e855a2752945addb01bd77390ab7051e5b6e9\n", "time": "1997-08-10T16:47:17Z\n", "diff": "mmm a / Tools / freeze / freeze . py <nl> ppp b / Tools / freeze / freeze . py <nl> <nl> <nl> Options : <nl> <nl> - - p prefix : This is the prefix used when you ran ` ` name install ' ' <nl> + - p prefix : This is the prefix used when you ran ` ` make install ' ' <nl> in the Python build directory . <nl> ( If you never ran this , freeze won ' t work . ) <nl> The default is whatever sys . prefix evaluates to . <nl> + It can also be the top directory of the Python source <nl> + tree ; then - P must point to the build tree . <nl> <nl> - P exec_prefix : Like - p but this is the ' exec_prefix ' , used to <nl> install objects etc . The default is whatever sys . exec_prefix <nl> evaluates to , or the - p argument if given . <nl> + If - p points to the Python source tree , - P must point <nl> + to the build tree , if different . <nl> <nl> - e extension : A directory containing additional . o files that <nl> may be used to resolve modules . This directory <nl> <nl> <nl> - o dir : Directory where the output files are created ; default ' . ' . <nl> <nl> + - h : Print this help message . <nl> + <nl> Arguments : <nl> <nl> script . py : The Python script to be executed by the resulting binary . <nl> def main ( ) : <nl> <nl> # parse command line <nl> try : <nl> - opts , args = getopt . getopt ( sys . argv [ 1 : ] , ' e : o : p : P : ' ) <nl> + opts , args = getopt . getopt ( sys . argv [ 1 : ] , ' he : o : p : P : ' ) <nl> except getopt . error , msg : <nl> usage ( ' getopt error : ' + str ( msg ) ) <nl> <nl> # proces option arguments <nl> for o , a in opts : <nl> + if o = = ' - h ' : <nl> + print __doc__ <nl> + return <nl> if o = = ' - e ' : <nl> extensions . append ( a ) <nl> if o = = ' - o ' : <nl> def main ( ) : <nl> if not prefix : <nl> prefix = sys . prefix <nl> <nl> + # determine whether - p points to the Python source tree <nl> + ishome = os . path . exists ( os . path . join ( prefix , ' Include ' , ' pythonrun . h ' ) ) <nl> + <nl> # locations derived from options <nl> version = sys . version [ : 3 ] <nl> - binlib = os . path . join ( exec_prefix , ' lib / python % s / config ' % version ) <nl> - incldir = os . path . join ( prefix , ' include / python % s ' % version ) <nl> - config_c_in = os . path . join ( binlib , ' config . c . in ' ) <nl> - frozenmain_c = os . path . join ( binlib , ' frozenmain . c ' ) <nl> + if ishome : <nl> + print \" ( Using Python source directory ) \" <nl> + binlib = exec_prefix <nl> + incldir = os . path . join ( prefix , ' Include ' ) <nl> + config_c_in = os . path . join ( prefix , ' Modules ' , ' config . c . in ' ) <nl> + frozenmain_c = os . path . join ( prefix , ' Modules ' , ' frozenmain . c ' ) <nl> + makefile_in = os . path . join ( exec_prefix , ' Modules ' , ' Makefile ' ) <nl> + else : <nl> + binlib = os . path . join ( exec_prefix , <nl> + ' lib ' , ' python % s ' % version , ' config ' ) <nl> + incldir = os . path . join ( prefix , ' include ' , ' python % s ' % version ) <nl> + config_c_in = os . path . join ( binlib , ' config . c . in ' ) <nl> + frozenmain_c = os . path . join ( binlib , ' frozenmain . c ' ) <nl> + makefile_in = os . path . join ( binlib , ' Makefile ' ) <nl> supp_sources = [ ] <nl> - makefile_in = os . path . join ( binlib , ' Makefile ' ) <nl> defines = [ ] <nl> includes = [ ' - I ' + incldir , ' - I ' + binlib ] <nl> <nl> def main ( ) : <nl> <nl> # Print usage message and exit <nl> <nl> - def usage ( msg = None ) : <nl> - sys . stderr . write ( __doc__ ) <nl> - # Put the error last since the usage message scrolls off the screen <nl> - if msg : <nl> - sys . stderr . write ( ' \\ nError : ' + str ( msg ) + ' \\ n ' ) <nl> + def usage ( msg ) : <nl> + sys . stdout = sys . stderr <nl> + print \" Error : \" , msg <nl> + print \" Use ` ` % s - h ' ' for help \" % sys . argv [ 0 ] <nl> sys . exit ( 2 ) <nl> <nl> <nl>\n", "msg": "Support using - p / - P to point to the source / build directory instead of\n"}
{"diff_id": 21741, "repo": "numpy/numpy\n", "sha": "8fb4d0c4b687b514416c280c5c738469e67854b2\n", "time": "2016-12-19T18:47:17Z\n", "diff": "mmm a / numpy / lib / function_base . py <nl> ppp b / numpy / lib / function_base . py <nl> def cov ( m , y = None , rowvar = True , bias = False , ddof = None , fweights = None , <nl> return c . squeeze ( ) <nl> <nl> <nl> - def corrcoef ( x , y = None , rowvar = 1 , bias = np . _NoValue , ddof = np . _NoValue ) : <nl> + def corrcoef ( x , y = None , rowvar = True , bias = np . _NoValue , ddof = np . _NoValue ) : <nl> \" \" \" <nl> Return Pearson product - moment correlation coefficients . <nl> <nl> def corrcoef ( x , y = None , rowvar = 1 , bias = np . _NoValue , ddof = np . _NoValue ) : <nl> y : array_like , optional <nl> An additional set of variables and observations . ` y ` has the same <nl> shape as ` x ` . <nl> - rowvar : int , optional <nl> - If ` rowvar ` is non - zero ( default ) , then each row represents a <nl> + rowvar : bool , optional <nl> + If ` rowvar ` is True ( default ) , then each row represents a <nl> variable , with observations in the columns . Otherwise , the relationship <nl> is transposed : each column represents a variable , while the rows <nl> contain observations . <nl>\n", "msg": "change rowvar param to boolean from int in corrcoef\n"}
{"diff_id": 22058, "repo": "python/cpython\n", "sha": "eee44f28a93adbf8ff0b9391ddd0e77e99bbc55f\n", "time": "2014-02-06T21:46:38Z\n", "diff": "mmm a / Lib / doctest . py <nl> ppp b / Lib / doctest . py <nl> def _from_module ( self , module , object ) : <nl> elif inspect . isfunction ( object ) : <nl> return module . __dict__ is object . __globals__ <nl> elif inspect . ismethoddescriptor ( object ) : <nl> - return module . __name__ = = object . __objclass__ . __module__ <nl> + if hasattr ( object , ' __objclass__ ' ) : <nl> + obj_mod = object . __objclass__ . __module__ <nl> + elif hasattr ( object , ' __module__ ' ) : <nl> + obj_mod = object . __module__ <nl> + else : <nl> + return True # [ XX ] no easy way to tell otherwise <nl> + return module . __name__ = = obj_mod <nl> elif inspect . isclass ( object ) : <nl> return module . __name__ = = object . __module__ <nl> elif hasattr ( object , ' __module__ ' ) : <nl>\n", "msg": "Issue : Provide a couple of fallbacks for in case a method_descriptor\n"}
{"diff_id": 22135, "repo": "bokeh/bokeh\n", "sha": "0cffe4932a3b94275fe24ef6a2f1bb4a2b8f4d77\n", "time": "2015-02-03T17:43:18Z\n", "diff": "mmm a / bokeh / charts / horizon . py <nl> ppp b / bokeh / charts / horizon . py <nl> <nl> from collections import OrderedDict , defaultdict <nl> import math <nl> <nl> - from . _chartobject import ChartObject <nl> - from . . models import ColumnDataSource , Range1d , DataRange1d , FactorRange , HoverTool , CategoricalAxis <nl> + from . _builder import Builder , create_and_build <nl> + from . . models import ColumnDataSource , Range1d , DataRange1d , FactorRange , GlyphRenderer , CategoricalAxis <nl> from . . models . glyphs import Patches <nl> <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> # Classes and functions <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> <nl> + def Horizon ( values , index = None , nb_folds = 3 , pos_color = ' # 006400 ' , <nl> + neg_color = ' # 6495ed ' , xscale = ' datetime ' , * * kws ) : <nl> + tools = kws . get ( ' tools ' , True ) <nl> <nl> - class Horizon ( ChartObject ) : <nl> + if tools = = True : <nl> + tools = \" save , resize , reset \" <nl> + elif isinstance ( tools , string_types ) : <nl> + tools = tools . replace ( ' pan ' , ' ' ) <nl> + tools = tools . replace ( ' wheel_zoom ' , ' ' ) <nl> + tools = tools . replace ( ' box_zoom ' , ' ' ) <nl> + tools = tools . replace ( ' , , ' , ' , ' ) <nl> + kws [ ' tools ' ] = tools <nl> + <nl> + chart = create_and_build ( <nl> + HorizonBuilder , values , index = index , nb_folds = nb_folds , pos_color = pos_color , <nl> + neg_color = neg_color , xscale = xscale , * * kws <nl> + ) <nl> + <nl> + return chart <nl> + <nl> + class HorizonBuilder ( Builder ) : <nl> <nl> \" \" \" This is the Horizon class and it is in charge of plotting <nl> Horizon charts in an easy and intuitive way . <nl> class Horizon ( ChartObject ) : <nl> y_pypy = xyvalues [ ' pypy ' ] = [ 12 , 33 , 47 , 15 , 126 ] <nl> y_jython = xyvalues [ ' jython ' ] = [ 22 , 43 , 10 , 25 , 26 ] <nl> <nl> - ts = Horizon ( xyvalues , index = ' Date ' , title = \" horizon \" , <nl> + hz = Horizon ( xyvalues , index = ' Date ' , title = \" horizon \" , legend = \" top_left \" , <nl> ylabel = ' Stock Prices ' , filename = \" stocks_ts . html \" ) <nl> - ts . legend ( \" top_left \" ) . show ( ) <nl> + hz . show ( ) <nl> <nl> \" \" \" <nl> - <nl> - def __init__ ( self , values , index = None , title = None , xlabel = None , ylabel = None , <nl> - legend = False , xscale = \" datetime \" , yscale = \" linear \" , width = 800 , <nl> - height = 600 , tools = True , filename = False , server = False , <nl> - notebook = False , facet = False , xgrid = False , ygrid = False , <nl> - nb_folds = 3 , pos_color = ' # 006400 ' , neg_color = ' # 6495ed ' ) : <nl> + def __init__ ( self , values , index = None , legend = False , palette = None , <nl> + nb_folds = 3 , pos_color = ' # 006400 ' , neg_color = ' # 6495ed ' , * * kws ) : <nl> + # <nl> + # def __init__ ( self , values , index = None , title = None , xlabel = None , ylabel = None , <nl> + # legend = False , xscale = \" datetime \" , yscale = \" linear \" , width = 800 , <nl> + # height = 600 , tools = True , filename = False , server = False , <nl> + # notebook = False , facet = False , xgrid = False , ygrid = False , <nl> + # nb_folds = 3 , pos_color = ' # 006400 ' , neg_color = ' # 6495ed ' ) : <nl> \" \" \" <nl> Args : <nl> values ( iterable ) : iterable 2d representing the data series <nl> def __init__ ( self , values , index = None , title = None , xlabel = None , ylabel = None , <nl> loading the data dict . <nl> Needed for _set_And_get method . <nl> \" \" \" <nl> - self . values = values <nl> - self . source = None <nl> - self . xdr = None <nl> - self . ydr = None <nl> - self . groups = [ ] <nl> - self . series = [ ] <nl> - self . fold_height = { } <nl> - self . max_y = 0 <nl> - self . data = { } <nl> - self . attr = [ ] <nl> self . index = index <nl> self . nb_folds = nb_folds <nl> self . pos_color = pos_color <nl> self . neg_color = neg_color <nl> self . fold_names = [ ] <nl> <nl> - super ( Horizon , self ) . __init__ ( <nl> - title , xlabel , ylabel , legend , xscale , yscale , width , height , <nl> - tools , filename , server , notebook , facet , xgrid , ygrid <nl> - ) <nl> + # - self . source = None <nl> + # - self . xdr = None <nl> + # - self . ydr = None <nl> + # - self . groups = [ ] <nl> + self . series = [ ] <nl> + # - self . fold_height = { } <nl> + self . max_y = 0 <nl> <nl> - def check_attr ( self ) : <nl> - \" \" \" Disable zoom and pan tools since horizon plots display a predetermined <nl> - data range . Also , secondary axis is broken during zooming . <nl> - \" \" \" <nl> - super ( Horizon , self ) . check_attr ( ) <nl> - if self . _tools = = True : <nl> - self . _tools = \" save , resize , reset \" <nl> - elif isinstance ( self . _tools , string_types ) : <nl> - self . _tools = self . _tools . replace ( ' pan ' , ' ' ) <nl> - self . _tools = self . _tools . replace ( ' wheel_zoom ' , ' ' ) <nl> - self . _tools = self . _tools . replace ( ' box_zoom ' , ' ' ) <nl> - self . _tools = self . _tools . replace ( ' , , ' , ' , ' ) <nl> + super ( HorizonBuilder , self ) . __init__ ( <nl> + values , legend = legend , palette = palette <nl> + ) <nl> + # super ( Horizon , self ) . __init__ ( <nl> + # title , xlabel , ylabel , legend , xscale , yscale , width , height , <nl> + # tools , filename , server , notebook , facet , xgrid , ygrid <nl> + # ) <nl> + <nl> + # def check_attr ( self ) : <nl> + # \" \" \" Disable zoom and pan tools since horizon plots display a predetermined <nl> + # data range . Also , secondary axis is broken during zooming . <nl> + # \" \" \" <nl> + # super ( Horizon , self ) . check_attr ( ) <nl> + # if self . _tools = = True : <nl> + # self . _tools = \" save , resize , reset \" <nl> + # elif isinstance ( self . _tools , string_types ) : <nl> + # self . _tools = self . _tools . replace ( ' pan ' , ' ' ) <nl> + # self . _tools = self . _tools . replace ( ' wheel_zoom ' , ' ' ) <nl> + # self . _tools = self . _tools . replace ( ' box_zoom ' , ' ' ) <nl> + # self . _tools = self . _tools . replace ( ' , , ' , ' , ' ) <nl> <nl> def fold_coordinates ( self , y , fold_no , fold_height , y_origin = 0 , graph_ratio = 1 ) : <nl> \" \" \" Function that calculate the coordinates for a value given a fold <nl> def get_source ( self ) : <nl> calculate the proper ranges . <nl> \" \" \" <nl> self . source = ColumnDataSource ( self . data ) <nl> - self . xdr = DataRange1d ( sources = [ self . source . columns ( self . attr [ 0 ] ) ] ) <nl> - self . ydr = Range1d ( start = 0 , end = self . max_y ) <nl> + self . x_range = DataRange1d ( sources = [ self . source . columns ( self . attr [ 0 ] ) ] ) <nl> + self . y_range = Range1d ( start = 0 , end = self . max_y ) <nl> <nl> def draw ( self ) : <nl> \" \" \" Use the patch glyphs to connect the xy points in the time series . <nl> def draw ( self ) : <nl> \" \" \" <nl> patches = Patches ( <nl> fill_color = ' fill_color ' , fill_alpha = ' fill_alpha ' , xs = ' x_all ' , ys = ' y_all ' ) <nl> - self . chart . plot . add_glyph ( self . source , patches ) <nl> + # self . chart . plot . add_glyph ( self . source , patches ) <nl> + <nl> + renderer = GlyphRenderer ( data_source = self . source , glyph = patches ) <nl> + # self . _legends . append ( ( self . groups [ i - 1 ] , [ renderer ] ) ) <nl> + yield renderer <nl> <nl> def _show_teardown ( self ) : <nl> \" \" \" Add the serie names to the y axis , the hover tooltips and legend \" \" \" <nl>\n", "msg": "change Horizon chart to builder pattern\n"}
{"diff_id": 22281, "repo": "chubin/cheat.sh\n", "sha": "99399c1d8abc205664ca10acbfb2cca62ed33e7c\n", "time": "2018-07-19T08:58:59Z\n", "diff": "mmm a / lib / get_answer . py <nl> ppp b / lib / get_answer . py <nl> def get_topic_type ( topic ) : # pylint : disable = too - many - locals , too - many - branches , t <nl> if ' + ' in topic_name : <nl> result = ' question ' <nl> else : <nl> - if topic_type in _get_topics_dirs ( ) and topic_name in [ ' : list ' ] : <nl> + # if topic_type in _get_topics_dirs ( ) and topic_name in [ ' : list ' ] : <nl> + if topic_name in [ ' : list ' ] : <nl> result = \" internal \" <nl> elif is_valid_learnxy ( topic ) : <nl> result = ' learnxiny ' <nl> + elif topic_name in [ ' : learn ' ] : <nl> + result = \" internal \" <nl> else : <nl> # let us activate the ' question ' feature for all subsections <nl> result = ' question ' <nl> def _get_cheat_sheets ( topic ) : <nl> filename = PATH_CHEAT_SHEETS + \" % s \" % topic <nl> if not os . path . exists ( filename ) : <nl> filename = PATH_CHEAT_SHEETS + \" _ % s \" % topic <nl> - return open ( filename , \" r \" ) . read ( ) . decode ( ' utf - 8 ' ) <nl> + if os . path . isdir ( filename ) : <nl> + return \" \" <nl> + else : <nl> + return open ( filename , \" r \" ) . read ( ) . decode ( ' utf - 8 ' ) <nl> <nl> def _get_cheat_sheets_dir ( topic ) : <nl> answer = [ ] <nl>\n", "msg": "show empty list for empty sections\n"}
{"diff_id": 22291, "repo": "ipython/ipython\n", "sha": "44370f2cd31723a4a9e83f412cfe1c9e53d226a4\n", "time": "2013-09-29T22:18:38Z\n", "diff": "mmm a / IPython / lib / display . py <nl> ppp b / IPython / lib / display . py <nl> <nl> \" \" \" <nl> from os . path import exists , isfile , splitext , abspath , join , isdir <nl> from os import walk , sep <nl> - import base64 <nl> - import struct <nl> - from io import BytesIO <nl> - import wave <nl> - import mimetypes <nl> <nl> from IPython . core . display import DisplayObject <nl> <nl> class Audio ( DisplayObject ) : <nl> data : numpy array , unicode , str or bytes <nl> Can be a <nl> * Numpy array containing the desired waveform , <nl> - * String containingvthe filename <nl> + * String containing the filename <nl> * Bytestring containing raw PCM data or <nl> * URL pointing to a file on the web . <nl> <nl> def __init__ ( self , data = None , filename = None , url = None , embed = None , rate = None , au <nl> <nl> def reload ( self ) : <nl> \" \" \" Reload the raw data from file or URL . \" \" \" <nl> + import mimetypes <nl> if self . embed : <nl> super ( Audio , self ) . reload ( ) <nl> <nl> def reload ( self ) : <nl> <nl> def _make_wav ( self , data , rate ) : <nl> \" \" \" Transform a numpy array to a PCM bytestring \" \" \" <nl> + import struct <nl> + from io import BytesIO <nl> + import wave <nl> maxabsvalue = max ( map ( abs , data ) ) <nl> scaled = map ( lambda x : int ( x / maxabsvalue * 32767 ) , data ) <nl> fp = BytesIO ( ) <nl> def _repr_html_ ( self ) : <nl> return src . format ( src = self . src_attr ( ) , type = self . mimetype , autoplay = self . autoplay_attr ( ) ) <nl> <nl> def src_attr ( self ) : <nl> + import base64 <nl> if self . embed and ( self . data is not None ) : <nl> return \" \" \" data : { type } ; base64 , { base64 } \" \" \" . format ( type = self . mimetype , base64 = base64 . encodestring ( self . data ) ) <nl> elif self . url is not None : <nl>\n", "msg": "Move imports into methods and fix docstring typo\n"}
{"diff_id": 22425, "repo": "python/cpython\n", "sha": "3e1b85ead189a33dc20c190bfe6428c991a8ee04\n", "time": "2007-05-30T02:07:00Z\n", "diff": "mmm a / Lib / dis . py <nl> ppp b / Lib / dis . py <nl> def distb ( tb = None ) : <nl> while tb . tb_next : tb = tb . tb_next <nl> disassemble ( tb . tb_frame . f_code , tb . tb_lasti ) <nl> <nl> + # XXX This duplicates information from code . h , also duplicated in inspect . py . <nl> + # XXX Maybe this ought to be put in a central location , like opcode . py ? <nl> + flag2name = { <nl> + 1 : \" OPTIMIZED \" , <nl> + 2 : \" NEWLOCALS \" , <nl> + 4 : \" VARARGS \" , <nl> + 8 : \" VARKEYWORDS \" , <nl> + 16 : \" NESTED \" , <nl> + 32 : \" GENERATOR \" , <nl> + 64 : \" NOFREE \" , <nl> + } <nl> + <nl> + def pretty_flags ( flags ) : <nl> + \" \" \" Return pretty representation of code flags . \" \" \" <nl> + names = [ ] <nl> + for i in range ( 32 ) : <nl> + flag = 1 < < i <nl> + if flags & flag : <nl> + names . append ( flag2name . get ( flag , hex ( flag ) ) ) <nl> + flags ^ = flag <nl> + if not flags : <nl> + break <nl> + else : <nl> + names . append ( hex ( flags ) ) <nl> + return \" , \" . join ( names ) <nl> + <nl> + def show_code ( co ) : <nl> + \" \" \" Show details about a code object . \" \" \" <nl> + print ( \" Name : \" , co . co_name ) <nl> + print ( \" Filename : \" , co . co_filename ) <nl> + print ( \" Argument count : \" , co . co_argcount ) <nl> + print ( \" Kw - only arguments : \" , co . co_kwonlyargcount ) <nl> + print ( \" Number of locals : \" , co . co_nlocals ) <nl> + print ( \" Stack size : \" , co . co_stacksize ) <nl> + print ( \" Flags : \" , pretty_flags ( co . co_flags ) ) <nl> + if co . co_consts : <nl> + print ( \" Constants : \" ) <nl> + for i_c in enumerate ( co . co_consts ) : <nl> + print ( \" % 4d : % r \" % i_c ) <nl> + if co . co_names : <nl> + print ( \" Names : \" ) <nl> + for i_n in enumerate ( co . co_names ) : <nl> + print ( \" % 4d : % s \" % i_n ) <nl> + if co . co_varnames : <nl> + print ( \" Variable names : \" ) <nl> + for i_n in enumerate ( co . co_varnames ) : <nl> + print ( \" % 4d : % s \" % i_n ) <nl> + if co . co_freevars : <nl> + print ( \" Free variables : \" ) <nl> + for i_n in enumerate ( co . co_freevars ) : <nl> + print ( \" % 4d : % s \" % i_n ) <nl> + if co . co_cellvars : <nl> + print ( \" Cell variables : \" ) <nl> + for i_n in enumerate ( co . co_cellvars ) : <nl> + print ( \" % 4d : % s \" % i_n ) <nl> + <nl> def disassemble ( co , lasti = - 1 ) : <nl> \" \" \" Disassemble a code object . \" \" \" <nl> code = co . co_code <nl>\n", "msg": "Add a helper to display the various flags and components of code objects\n"}
{"diff_id": 22454, "repo": "celery/celery\n", "sha": "6b2ef3597b1b227a3b659d78ada18d4625732e96\n", "time": "2013-08-13T20:33:26Z\n", "diff": "mmm a / celery / worker / consumer . py <nl> ppp b / celery / worker / consumer . py <nl> class Blueprint ( bootsteps . Blueprint ) : <nl> ' celery . worker . consumer : Events ' , <nl> ' celery . worker . consumer : Gossip ' , <nl> ' celery . worker . consumer : Heart ' , <nl> + ' celery . worker . consumer : Control ' , <nl> ' celery . worker . consumer : Tasks ' , <nl> ' celery . worker . consumer : Evloop ' , <nl> ' celery . worker . consumer : Agent ' , <nl> def __init__ ( self , on_task , <nl> # so if the connection timeout is exceeded once , it can NEVER <nl> # connect again . <nl> self . app . conf . BROKER_CONNECTION_TIMEOUT = None <nl> - <nl> - <nl> - additional_steps = None <nl> - if self . app . conf . CELERY_ENABLE_REMOTE_CONTROL : <nl> - additional_steps = [ ' celery . worker . consumer : Control ' ] <nl> <nl> self . steps = [ ] <nl> self . blueprint = self . Blueprint ( <nl> - app = self . app , on_close = self . on_close , steps = additional_steps , <nl> + app = self . app , on_close = self . on_close , <nl> ) <nl> self . blueprint . apply ( self , * * dict ( worker_options or { } , * * kwargs ) ) <nl> <nl> def __init__ ( self , c , * * kwargs ) : <nl> self . start = self . box . start <nl> self . stop = self . box . stop <nl> self . shutdown = self . box . shutdown <nl> + <nl> + def include_if ( self , c ) : <nl> + return c . app . conf . CELERY_ENABLE_REMOTE_CONTROL <nl> <nl> <nl> class Tasks ( bootsteps . StartStopStep ) : <nl>\n", "msg": "use Bootstep . include_if to control if Remote Control is enabled\n"}
{"diff_id": 22463, "repo": "ansible/ansible\n", "sha": "6352e67ab208619e764515dfb05ade21f0f9471f\n", "time": "2018-03-21T18:05:59Z\n", "diff": "mmm a / test / runner / lib / util . py <nl> ppp b / test / runner / lib / util . py <nl> def is_binary_file ( path ) : <nl> : type path : str <nl> : rtype : bool <nl> \" \" \" <nl> + assume_text = set ( [ <nl> + ' . cfg ' , <nl> + ' . conf ' , <nl> + ' . crt ' , <nl> + ' . css ' , <nl> + ' . html ' , <nl> + ' . ini ' , <nl> + ' . j2 ' , <nl> + ' . js ' , <nl> + ' . json ' , <nl> + ' . md ' , <nl> + ' . pem ' , <nl> + ' . ps1 ' , <nl> + ' . psm1 ' , <nl> + ' . py ' , <nl> + ' . rst ' , <nl> + ' . sh ' , <nl> + ' . txt ' , <nl> + ' . xml ' , <nl> + ' . yaml ' , <nl> + ' . yml ' , <nl> + ] ) <nl> + <nl> + assume_binary = set ( [ <nl> + ' . bin ' , <nl> + ' . eot ' , <nl> + ' . gz ' , <nl> + ' . ico ' , <nl> + ' . iso ' , <nl> + ' . jpg ' , <nl> + ' . otf ' , <nl> + ' . p12 ' , <nl> + ' . png ' , <nl> + ' . pyc ' , <nl> + ' . rpm ' , <nl> + ' . ttf ' , <nl> + ' . woff ' , <nl> + ' . woff2 ' , <nl> + ' . zip ' , <nl> + ] ) <nl> + <nl> + ext = os . path . splitext ( path ) [ 1 ] <nl> + <nl> + if ext in assume_text : <nl> + return False <nl> + <nl> + if ext in assume_binary : <nl> + return True <nl> + <nl> with open ( path , ' rb ' ) as path_fd : <nl> return b ' \\ 0 ' in path_fd . read ( 1024 ) <nl> <nl>\n", "msg": "Update ansible - test is_binary_file test .\n"}
{"diff_id": 22556, "repo": "celery/celery\n", "sha": "925ca9c468aca361b3d74051af0264508f97bff1\n", "time": "2011-01-18T13:14:55Z\n", "diff": "mmm a / celery / contrib / rdb . py <nl> ppp b / celery / contrib / rdb . py <nl> def add ( x , y ) : <nl> return result <nl> <nl> <nl> + * * Environment Variables * * <nl> + <nl> + . . envvar : : CELERY_RDB_HOST <nl> + <nl> + Hostname to bind to . Default is ' 127 . 0 . 01 ' , which means the socket <nl> + will only be accessible from the local host . <nl> + <nl> + . . envvar : : CELERY_RDB_PORt <nl> + <nl> + Base port to bind to . Default is 6899 . <nl> + The debugger will try to find an available port starting from the <nl> + base port . The selected port will be logged by celeryd . <nl> <nl> \" \" \" <nl> import bdb <nl> def add ( x , y ) : <nl> <nl> default_port = 6899 <nl> <nl> - CELERY_RDB_HOST = os . environ . get ( \" CELERY_RDB_HOST \" ) or socket . gethostname ( ) <nl> + CELERY_RDB_HOST = os . environ . get ( \" CELERY_RDB_HOST \" ) or \" 127 . 0 . 0 . 1 \" <nl> CELERY_RDB_PORT = int ( os . environ . get ( \" CELERY_RDB_PORT \" ) or default_port ) <nl> <nl> # : Holds the currently active debugger . <nl>\n", "msg": "Remote Debugger : Only bind to localhost by default ( not accessible by other hosts )\n"}
{"diff_id": 22633, "repo": "home-assistant/core\n", "sha": "229000b8345878e3024c635ff44189ca5c2eab44\n", "time": "2017-07-12T06:26:29Z\n", "diff": "mmm a / homeassistant / components / media_player / plex . py <nl> ppp b / homeassistant / components / media_player / plex . py <nl> def setup_platform ( hass , config , add_devices_callback , discovery_info = None ) : <nl> <nl> if file_config : <nl> # Setup a configured PlexServer <nl> - host , token = file_config . popitem ( ) <nl> - token = token [ ' token ' ] <nl> + host , host_config = file_config . popitem ( ) <nl> + token = host_config [ ' token ' ] <nl> + try : <nl> + has_ssl = host_config [ ' ssl ' ] <nl> + except KeyError : <nl> + has_ssl = False <nl> + try : <nl> + verify_ssl = host_config [ ' verify ' ] <nl> + except KeyError : <nl> + verify_ssl = True <nl> + <nl> # Via discovery <nl> elif discovery_info is not None : <nl> # Parse discovery data <nl> def setup_platform ( hass , config , add_devices_callback , discovery_info = None ) : <nl> if host in _CONFIGURING : <nl> return <nl> token = None <nl> + has_ssl = False <nl> + verify_ssl = True <nl> else : <nl> return <nl> <nl> - setup_plexserver ( host , token , hass , config , add_devices_callback ) <nl> + setup_plexserver ( <nl> + host , token , has_ssl , verify_ssl , <nl> + hass , config , add_devices_callback <nl> + ) <nl> <nl> <nl> - def setup_plexserver ( host , token , hass , config , add_devices_callback ) : <nl> + def setup_plexserver ( <nl> + host , token , has_ssl , verify_ssl , hass , config , add_devices_callback ) : <nl> \" \" \" Set up a plexserver based on host parameter . \" \" \" <nl> import plexapi . server <nl> import plexapi . exceptions <nl> <nl> + cert_session = None <nl> + http_prefix = ' https ' if has_ssl else ' http ' <nl> + if has_ssl and ( verify_ssl is False ) : <nl> + _LOGGER . info ( \" Ignoring SSL verification \" ) <nl> + cert_session = requests . Session ( ) <nl> + cert_session . verify = False <nl> try : <nl> - plexserver = plexapi . server . PlexServer ( ' http : / / % s ' % host , token ) <nl> + plexserver = plexapi . server . PlexServer ( <nl> + ' % s : / / % s ' % ( http_prefix , host ) , <nl> + token , cert_session <nl> + ) <nl> _LOGGER . info ( \" Discovery configuration done ( no token needed ) \" ) <nl> except ( plexapi . exceptions . BadRequest , plexapi . exceptions . Unauthorized , <nl> plexapi . exceptions . NotFound ) as error : <nl> def setup_plexserver ( host , token , hass , config , add_devices_callback ) : <nl> # Save config <nl> if not config_from_file ( <nl> hass . config . path ( PLEX_CONFIG_FILE ) , { host : { <nl> - ' token ' : token <nl> + ' token ' : token , <nl> + ' ssl ' : has_ssl , <nl> + ' verify ' : verify_ssl , <nl> } } ) : <nl> _LOGGER . error ( \" Failed to save configuration file \" ) <nl> <nl> - _LOGGER . info ( ' Connected to : http : / / % s ' , host ) <nl> + _LOGGER . info ( ' Connected to : % s : / / % s ' , http_prefix , host ) <nl> <nl> plex_clients = { } <nl> plex_sessions = { } <nl> def request_configuration ( host , hass , config , add_devices_callback ) : <nl> def plex_configuration_callback ( data ) : <nl> \" \" \" Handle configuration changes . \" \" \" <nl> setup_plexserver ( <nl> - host , data . get ( ' token ' ) , hass , config , add_devices_callback ) <nl> + host , data . get ( ' token ' ) , <nl> + cv . boolean ( data . get ( ' has_ssl ' ) ) , <nl> + cv . boolean ( data . get ( ' do_not_verify ' ) ) , <nl> + hass , config , add_devices_callback <nl> + ) <nl> <nl> _CONFIGURING [ host ] = configurator . request_config ( <nl> hass , <nl> def plex_configuration_callback ( data ) : <nl> ' id ' : ' token ' , <nl> ' name ' : ' X - Plex - Token ' , <nl> ' type ' : ' ' <nl> + } , { <nl> + ' id ' : ' has_ssl ' , <nl> + ' name ' : ' Use SSL ' , <nl> + ' type ' : ' ' <nl> + } , { <nl> + ' id ' : ' do_not_verify_ssl ' , <nl> + ' name ' : ' Do not verify SSL ' , <nl> + ' type ' : ' ' <nl> } ] ) <nl> <nl> <nl>\n", "msg": "Support for Plex servers with enforced SSL ( )\n"}
{"diff_id": 22680, "repo": "zulip/zulip\n", "sha": "7961d4f6b37c8cb40d46793b8016706340779825\n", "time": "2013-10-24T18:54:30Z\n", "diff": "new file mode 100644 <nl> index 000000000000 . . bf434e2db973 <nl> mmm / dev / null <nl> ppp b / zerver / lib / push_notifications . py <nl> <nl> + from __future__ import absolute_import <nl> + <nl> + from zerver . models import UserProfile , AppleDeviceToken <nl> + from zerver . lib . timestamp import timestamp_to_datetime <nl> + from zerver . decorator import statsd_increment <nl> + <nl> + from apnsclient import Session , Connection , Message , APNs <nl> + <nl> + from django . conf import settings <nl> + <nl> + import base64 , binascii , logging <nl> + <nl> + # Maintain a long - lived Session object to avoid having to re - SSL - handshake <nl> + # for each request <nl> + session = Session ( ) <nl> + connection = session . get_connection ( settings . APNS_SANDBOX , cert_file = settings . APNS_CERT_FILE ) <nl> + <nl> + <nl> + def num_push_devices_for_user ( user_profile ) : <nl> + return AppleDeviceToken . objects . filter ( user = user_profile ) . count ( ) <nl> + <nl> + # We store the token as b64 , but apns - client wants hex strings <nl> + def b64_to_hex ( data ) : <nl> + return binascii . hexlify ( base64 . b64decode ( data ) ) <nl> + <nl> + def hex_to_b64 ( data ) : <nl> + return base64 . b64encode ( binascii . unhexlify ( data ) ) <nl> + <nl> + # Send a push notification to the desired clients <nl> + # extra_data is a dict that will be passed to the <nl> + # mobile app <nl> + @ statsd_increment ( \" apple_push_notification \" ) <nl> + def send_apple_push_notification ( user , alert , * * extra_data ) : <nl> + # Sends a push notifications to all the PushClients <nl> + # Only Apple Push Notifications clients are supported at the moment <nl> + tokens = [ b64_to_hex ( device . token ) for device in AppleDeviceToken . objects . filter ( user = user ) ] <nl> + <nl> + logging . info ( \" Sending apple push notification to devices : % s \" % ( tokens , ) ) <nl> + message = Message ( tokens , alert = alert , * * extra_data ) <nl> + <nl> + apns_client = APNs ( connection ) <nl> + ret = apns_client . send ( message ) <nl> + if not ret : <nl> + logging . warning ( \" Failed to send push notification for clients % s \" % ( tokens , ) ) <nl> + return <nl> + <nl> + for token , reason in ret . failed . items ( ) : <nl> + code , errmsg = reason <nl> + logging . warning ( \" Failed to deliver APNS notification to % s , reason : % s \" % ( token , errmsg ) ) <nl> + <nl> + # Check failures not related to devices . <nl> + for code , errmsg in ret . errors : <nl> + logging . warning ( \" Unknown error when delivering APNS : % s \" % ( errmsg , ) ) <nl> + <nl> + if ret . needs_retry ( ) : <nl> + # TODO handle retrying by potentially scheduling a background job <nl> + # or re - queueing <nl> + logging . warning ( \" APNS delivery needs a retry but ignoring \" ) <nl> + <nl> + # NOTE : This is used by the check_apns_tokens manage . py command . Do not call it otherwise , as the <nl> + # feedback ( ) call can take up to 15s <nl> + def check_apns_feedback ( ) : <nl> + apns_client = APNs ( connection , tail_timeout = 20 ) <nl> + <nl> + for token , since in apns_client . feedback ( ) : <nl> + since_date = timestamp_to_datetime ( since ) <nl> + logging . info ( \" Found unavailable token % s , unavailable since % s \" % ( token , since_date ) ) <nl> + <nl> + AppleDeviceToken . objects . filter ( token = hex_to_b64 ( token ) , last_updates__lt = since_date ) . delete ( ) <nl> + logging . info ( \" Finished checking feedback for stale tokens \" ) <nl>\n", "msg": "Add a push notification module to handle mobile client notifications\n"}
{"diff_id": 22720, "repo": "zulip/zulip\n", "sha": "1197ff96552476f8d5846e6a4cc51dad17b4a655\n", "time": "2017-11-05T02:47:45Z\n", "diff": "mmm a / zproject / backends . py <nl> ppp b / zproject / backends . py <nl> def has_module_perms ( self , user , app_label ) : <nl> return False <nl> <nl> def get_all_permissions ( self , user , obj = None ) : <nl> - # type : ( Optional [ UserProfile ] , Any ) - > Set <nl> + # type : ( Optional [ UserProfile ] , Any ) - > Set [ Any ] <nl> # Using Any type is safe because we are not doing anything with <nl> - # the arguments . <nl> + # the arguments and always return empty set . <nl> return set ( ) <nl> <nl> def get_group_permissions ( self , user , obj = None ) : <nl> - # type : ( Optional [ UserProfile ] , Any ) - > Set <nl> + # type : ( Optional [ UserProfile ] , Any ) - > Set [ Any ] <nl> # Using Any type is safe because we are not doing anything with <nl> - # the arguments . <nl> + # the arguments and always return empty set . <nl> return set ( ) <nl> <nl> def django_to_ldap_username ( self , username ) : <nl>\n", "msg": "mypy : Explicitly return Set [ Any ] for empty set in backends . py .\n"}
{"diff_id": 22749, "repo": "matplotlib/matplotlib\n", "sha": "14154065afcdb920008c85f4b9807c8e07808430\n", "time": "2020-06-05T09:42:11Z\n", "diff": "mmm a / lib / matplotlib / pyplot . py <nl> ppp b / lib / matplotlib / pyplot . py <nl> def subplot2grid ( shape , loc , rowspan = 1 , colspan = 1 , fig = None , * * kwargs ) : <nl> * * kwargs <nl> Additional keyword arguments are handed to ` add_subplot ` . <nl> <nl> + Returns <nl> + mmmmmm - <nl> + an ` . axes . SubplotBase ` subclass of ` ~ . axes . Axes ` ( or a subclass of \\ <nl> + ` ~ . axes . Axes ` ) <nl> + <nl> + The axes of the subplot . The returned axes base class depends on the <nl> + projection used . It is ` ~ . axes . Axes ` if rectilinear projection is used <nl> + and ` . projections . polar . PolarAxes ` if polar projection is used . The <nl> + returned axes is then a subplot subclass of the base class . <nl> + <nl> Notes <nl> mmm - - <nl> The following call : : <nl> <nl> - subplot2grid ( shape , loc , rowspan = 1 , colspan = 1 ) <nl> + ax = subplot2grid ( ( nrows , ncols ) , ( row , col ) , rowspan , colspan ) <nl> <nl> is identical to : : <nl> <nl> - gridspec = GridSpec ( shape [ 0 ] , shape [ 1 ] ) <nl> - subplotspec = gridspec . new_subplotspec ( loc , rowspan , colspan ) <nl> - subplot ( subplotspec ) <nl> + fig = gcf ( ) <nl> + gs = fig . add_gridspec ( nrows , ncols ) <nl> + ax = fig . add_subplot ( gs [ row : row + rowspan , col : col + colspan ] ) <nl> \" \" \" <nl> <nl> if fig is None : <nl>\n", "msg": "Update subplot2grid doc to use Figure . add_gridspec , not GridSpec .\n"}
{"diff_id": 22823, "repo": "zulip/zulip\n", "sha": "df7a569a0c480b24371c48b49c4cad6a21898895\n", "time": "2017-05-08T06:21:50Z\n", "diff": "mmm a / bots / jabber_mirror_backend . py <nl> ppp b / bots / jabber_mirror_backend . py <nl> def __init__ ( self , jid , password , rooms ) : <nl> self . nick = jid . username <nl> jid . resource = \" zulip \" <nl> ClientXMPP . __init__ ( self , jid , password ) <nl> - self . rooms = set ( ) # type : Set [ str ] <nl> + self . rooms = set ( ) # type : Set [ str ] <nl> self . rooms_to_join = rooms <nl> self . add_event_handler ( \" session_start \" , self . session_start ) <nl> self . add_event_handler ( \" message \" , self . message ) <nl> - self . zulip = None # type : Client <nl> + self . zulip = None # type : Client <nl> self . use_ipv6 = False <nl> <nl> - self . register_plugin ( ' xep_0045 ' ) # Jabber chatrooms <nl> - self . register_plugin ( ' xep_0199 ' ) # XMPP Ping <nl> + self . register_plugin ( ' xep_0045 ' ) # Jabber chatrooms <nl> + self . register_plugin ( ' xep_0199 ' ) # XMPP Ping <nl> <nl> def set_zulip_client ( self , zulipToJabberClient ) : <nl> # type : ( ZulipToJabberBot ) - > None <nl> class ZulipToJabberBot ( object ) : <nl> def __init__ ( self , zulip_client ) : <nl> # type : ( Client ) - > None <nl> self . client = zulip_client <nl> - self . jabber = None # type : JabberToZulipBot <nl> + self . jabber = None # type : JabberToZulipBot <nl> <nl> def set_jabber_client ( self , client ) : <nl> # type : ( JabberToZulipBot ) - > None <nl> def get_stream_infos ( key , method ) : <nl> else : <nl> stream_infos = get_stream_infos ( \" subscriptions \" , zulipToJabber . client . list_subscriptions ) <nl> <nl> - rooms = [ ] # type : List [ str ] <nl> + rooms = [ ] # type : List [ str ] <nl> for stream_info in stream_infos : <nl> stream = stream_info [ ' name ' ] <nl> if stream . endswith ( \" / xmpp \" ) : <nl>\n", "msg": "pep8 : Add compliance with rule E261 to jabber_mirror_backend . py .\n"}
{"diff_id": 23059, "repo": "scikit-learn/scikit-learn\n", "sha": "728b413990b0ced2a8e8fb5b076550eb2f5a5028\n", "time": "2020-06-08T22:47:50Z\n", "diff": "mmm a / sklearn / preprocessing / _data . py <nl> ppp b / sklearn / preprocessing / _data . py <nl> def minmax_scale ( X , feature_range = ( 0 , 1 ) , * , axis = 0 , copy = True ) : <nl> X_tr : ndarray of shape ( n_samples , n_features ) <nl> The transformed data . <nl> <nl> + . . warning : : Risk of data leak <nl> + <nl> + Do not use : func : ` ~ sklearn . preprocessing . minmax_scale ` unless you know <nl> + what you are doing . A common mistake is to apply it to the entire data <nl> + * before * splitting into training and test sets . This will bias the <nl> + model evaluation because information would have leaked from the test <nl> + set to the training set . <nl> + In general , we recommend using <nl> + : class : ` ~ sklearn . preprocessing . MinMaxScaler ` within a <nl> + : ref : ` Pipeline < pipeline > ` in order to prevent most risks of data <nl> + leaking : ` pipe = make_pipeline ( MinMaxScaler ( ) , LogisticRegression ( ) ) ` . <nl> + <nl> See also <nl> mmmmmm - - <nl> MinMaxScaler : Performs scaling to a given range using the ` ` Transformer ` ` API <nl>\n", "msg": "DOC added a warning to minmax_scale # dataumbrella ( )\n"}
{"diff_id": 23097, "repo": "ansible/ansible\n", "sha": "1ec518543e679348740fb3fb817a3b8eab9116a9\n", "time": "2012-06-18T19:48:19Z\n", "diff": "mmm a / lib / ansible / runner / __init__ . py <nl> ppp b / lib / ansible / runner / __init__ . py <nl> def __init__ ( self , <nl> <nl> euid = pwd . getpwuid ( os . geteuid ( ) ) [ 0 ] <nl> if self . transport = = ' local ' and self . remote_user ! = euid : <nl> - raise Exception ( \" User mismatch : expected % s , but is % s \" % ( self . remote_user , euid ) ) <nl> + raise errors . AnsibleError ( \" User mismatch : expected % s , but is % s \" % ( self . remote_user , euid ) ) <nl> if type ( self . module_args ) not in [ str , unicode , dict ] : <nl> - raise Exception ( \" module_args must be a string or dict : % s \" % self . module_args ) <nl> + raise errors . AnsibleError ( \" module_args must be a string or dict : % s \" % self . module_args ) <nl> + if self . transport = = ' ssh ' and self . remote_pass : <nl> + raise errors . AnsibleError ( \" SSH transport does not support remote passwords , only keys or agents \" ) <nl> <nl> self . _tmp_paths = { } <nl> random . seed ( ) <nl>\n", "msg": "make Runner options conflict errors raise AnsibleErrors not traceback in general\n"}
{"diff_id": 23349, "repo": "python/cpython\n", "sha": "393da3240a29852c0e1188c6ccd007e89426a887\n", "time": "2012-05-27T18:05:36Z\n", "diff": "mmm a / Lib / test / test_venv . py <nl> ppp b / Lib / test / test_venv . py <nl> def setUp ( self ) : <nl> self . ps3name = ' pysetup3 - script . py ' <nl> self . lib = ( ' Lib ' , ) <nl> self . include = ' Include ' <nl> - self . exe = ' python . exe ' <nl> else : <nl> self . bindir = ' bin ' <nl> self . ps3name = ' pysetup3 ' <nl> self . lib = ( ' lib ' , ' python % s ' % sys . version [ : 3 ] ) <nl> self . include = ' include ' <nl> - self . exe = ' python ' <nl> + self . exe = os . path . split ( sys . executable ) [ - 1 ] <nl> <nl> def tearDown ( self ) : <nl> shutil . rmtree ( self . env_dir ) <nl>\n", "msg": "Changed executable name computation in test_venv to allow for debug executables .\n"}
{"diff_id": 23377, "repo": "matplotlib/matplotlib\n", "sha": "82ecfcff4bcfe6a6d5c091f4f2d6193c9d02458a\n", "time": "2013-02-27T00:49:00Z\n", "diff": "mmm a / lib / matplotlib / testing / compare . py <nl> ppp b / lib / matplotlib / testing / compare . py <nl> def crop_to_same ( actual_path , actual_image , expected_path , expected_image ) : <nl> return actual_image , expected_image <nl> <nl> def calculate_rms ( expectedImage , actualImage ) : <nl> - # compare the resulting image histogram functions <nl> - expected_version = version . LooseVersion ( \" 1 . 6 \" ) <nl> - found_version = version . LooseVersion ( np . __version__ ) <nl> + # calculate the per - pixel errors , then compute the root mean square error <nl> + num_values = reduce ( operator . mul , expectedImage . shape ) <nl> + absDiffImage = abs ( expectedImage - actualImage ) <nl> <nl> # On Numpy 1 . 6 , we can use bincount with minlength , which is much faster than <nl> # using histogram <nl> + expected_version = version . LooseVersion ( \" 1 . 6 \" ) <nl> + found_version = version . LooseVersion ( np . __version__ ) <nl> if found_version > = expected_version : <nl> - rms = 0 <nl> - <nl> - for i in xrange ( 0 , 3 ) : <nl> - h1p = expectedImage [ : , : , i ] <nl> - h2p = actualImage [ : , : , i ] <nl> - <nl> - h1h = np . bincount ( h1p . ravel ( ) , minlength = 256 ) <nl> - h2h = np . bincount ( h2p . ravel ( ) , minlength = 256 ) <nl> - <nl> - rms + = np . sum ( np . power ( ( h1h - h2h ) , 2 ) ) <nl> + histogram = np . bincount ( absDiffImage . ravel ( ) , minlength = 256 ) <nl> else : <nl> - rms = 0 <nl> - bins = np . arange ( 257 ) <nl> - <nl> - for i in xrange ( 0 , 3 ) : <nl> - h1p = expectedImage [ : , : , i ] <nl> - h2p = actualImage [ : , : , i ] <nl> - <nl> - h1h = np . histogram ( h1p , bins = bins ) [ 0 ] <nl> - h2h = np . histogram ( h2p , bins = bins ) [ 0 ] <nl> + histogram = np . histogram ( absDiffImage , bins = np . arange ( 257 ) ) [ 0 ] <nl> <nl> - rms + = np . sum ( np . power ( ( h1h - h2h ) , 2 ) ) <nl> - <nl> - rms = np . sqrt ( rms / ( 256 * 3 ) ) <nl> + sumOfSquares = sum ( count * ( i * * 2 ) for i , count in enumerate ( histogram ) ) <nl> + rms = np . sqrt ( float ( sumOfSquares ) / num_values ) <nl> <nl> return rms <nl> <nl> def compare_images ( expected , actual , tol , in_decorator = False ) : <nl> <nl> actualImage , expectedImage = crop_to_same ( actual , actualImage , expected , expectedImage ) <nl> <nl> - # compare the resulting image histogram functions <nl> - expected_version = version . LooseVersion ( \" 1 . 6 \" ) <nl> - found_version = version . LooseVersion ( np . __version__ ) <nl> + # convert to signed integers , so that the images can be subtracted without <nl> + # overflow <nl> + expectedImage = expectedImage . astype ( np . int32 ) <nl> + actualImage = actualImage . astype ( np . int32 ) <nl> <nl> rms = calculate_rms ( expectedImage , actualImage ) <nl> <nl>\n", "msg": "testing / compare : Fix image comparison RMS calculation .\n"}
{"diff_id": 23451, "repo": "bokeh/bokeh\n", "sha": "88c9be7231b9487bc1f06ba753e8edb1917b512a\n", "time": "2016-01-13T17:11:21Z\n", "diff": "mmm a / bokeh / charts / chart . py <nl> ppp b / bokeh / charts / chart . py <nl> def apply ( self , chart ) : <nl> raise ValueError ( \" ChartsDefaults should be only used on Chart \\ <nl> objects but it ' s being used on % s instead . \" % chart ) <nl> <nl> - for k in chart . properties_with_values ( include_defaults = True ) : <nl> + for k in list ( chart . properties_with_values ( include_defaults = True ) . keys ( ) ) + \\ <nl> + list ( chart . __deprecated_attributes__ ) : <nl> if k = = ' tools ' : <nl> value = getattr ( self , k , True ) <nl> if getattr ( chart , ' _tools ' , None ) is None : <nl> class Chart ( Plot ) : <nl> What kind of scale to use for the y - axis . <nl> \" \" \" ) <nl> <nl> - width = Int ( 600 , help = \" \" \" <nl> - Width of the rendered chart , in pixels . <nl> - \" \" \" ) <nl> - <nl> - height = Int ( 400 , help = \" \" \" <nl> - Height of the rendered chart , in pixels . <nl> - \" \" \" ) <nl> - <nl> title_text_font_size = Override ( default = { ' value ' : ' 14pt ' } ) <nl> <nl> responsive = Override ( default = False ) <nl> <nl> _defaults = defaults <nl> <nl> - __deprecated_attributes__ = ( ' filename ' , ' server ' , ' notebook ' ) <nl> + __deprecated_attributes__ = ( ' filename ' , ' server ' , ' notebook ' , ' width ' , ' height ' ) <nl> <nl> def __init__ ( self , * args , * * kwargs ) : <nl> # pop tools as it is also a property that doesn ' t match the argument <nl> def __init__ ( self , * args , * * kwargs ) : <nl> if tools is not None : <nl> self . _tools = tools <nl> <nl> + # TODO ( fpliger ) : we do this to still support deprecated document but <nl> + # should go away when __deprecated_attributes__ is empty <nl> + for k in self . __deprecated_attributes__ : <nl> + if k in kwargs : <nl> + setattr ( self , k , kwargs [ k ] ) <nl> + <nl> # TODO ( bev ) have to force serialization of overriden defaults on subtypes for now <nl> self . title_text_font_size = \" 10pt \" <nl> self . title_text_font_size = \" 14pt \" <nl> def notebook ( self , flag ) : <nl> def show ( self ) : <nl> import bokeh . io <nl> bokeh . io . show ( self ) <nl> + <nl> + @ property <nl> + def width ( self ) : <nl> + warnings . warn ( \" Chart property ' width ' was deprecated in 0 . 11 \\ <nl> + and will be removed in the future . \" ) <nl> + return self . plot_width <nl> + <nl> + @ width . setter <nl> + def width ( self , width ) : <nl> + self . plot_width = width <nl> + <nl> + @ property <nl> + def height ( self ) : <nl> + warnings . warn ( \" Chart property ' height ' was deprecated in 0 . 11 \\ <nl> + and will be removed in the future . \" ) <nl> + return self . plot_height <nl> + <nl> + @ height . setter <nl> + def height ( self , height ) : <nl> + self . plot_height = height <nl>\n", "msg": "add height and width to list of deprecated Chart attributes\n"}
{"diff_id": 23519, "repo": "sherlock-project/sherlock\n", "sha": "bd70be289bdf4eef68a5e56c51ac81748f986288\n", "time": "2019-05-28T12:26:30Z\n", "diff": "mmm a / site_list . py <nl> ppp b / site_list . py <nl> <nl> import sys <nl> import requests <nl> import threading <nl> - from bs4 import BeautifulSoup as bs <nl> + import xml . etree . ElementTree as ET <nl> from datetime import datetime <nl> from argparse import ArgumentParser , RawDescriptionHelpFormatter <nl> <nl> <nl> <nl> def get_rank ( domain_to_query , dest ) : <nl> result = - 1 <nl> - url = \" http : / / www . alexa . com / siteinfo / \" + domain_to_query <nl> - page = requests . get ( url ) . text <nl> - soup = bs ( page , features = \" lxml \" ) <nl> - for span in soup . find_all ( ' span ' ) : <nl> - if span . has_attr ( \" class \" ) : <nl> - if \" globleRank \" in span [ \" class \" ] : <nl> - for strong in span . find_all ( \" strong \" ) : <nl> - if strong . has_attr ( \" class \" ) : <nl> - if \" metrics - data \" in strong [ \" class \" ] : <nl> - result = int ( strong . text . strip ( ) . replace ( ' , ' , ' ' ) ) <nl> - dest [ ' rank ' ] = result <nl> + <nl> + # Retrieve ranking data via alexa API <nl> + url = f \" http : / / data . alexa . com / data ? cli = 10 & url = { domain_to_query } \" <nl> + xml_data = requests . get ( url ) . text <nl> + root = ET . fromstring ( xml_data ) <nl> + try : <nl> + # Get ranking for this site . <nl> + dest [ ' rank ' ] = int ( root . find ( \" . / / REACH \" ) . attrib [ \" RANK \" ] ) <nl> + except : <nl> + # We did not find the rank for some reason . <nl> + print ( f \" Error retrieving rank information for ' { domain_to_query } ' \" ) <nl> + print ( f \" Returned XML is | { xml_data } | \" ) <nl> + <nl> + return <nl> <nl> parser = ArgumentParser ( formatter_class = RawDescriptionHelpFormatter <nl> ) <nl>\n", "msg": "Change method used to get site ranking . Not only has alexa . com changed the format of their web site , they also seem to have gotten even more picky about people scraping it . So , use their API to query the site data .\n"}
{"diff_id": 23621, "repo": "ansible/ansible\n", "sha": "c845181dc1177ec61d10cf051e1f6ff24074b79b\n", "time": "2015-10-21T14:59:46Z\n", "diff": "mmm a / lib / ansible / plugins / connection / winrm . py <nl> ppp b / lib / ansible / plugins / connection / winrm . py <nl> <nl> from ansible . plugins . connection import ConnectionBase <nl> from ansible . plugins import shell_loader <nl> from ansible . utils . path import makedirs_safe <nl> - from ansible . utils . unicode import to_bytes , to_unicode <nl> + from ansible . utils . unicode import to_bytes , to_unicode , to_str <nl> <nl> class Connection ( ConnectionBase ) : <nl> ' ' ' WinRM connections over HTTP / HTTPS . ' ' ' <nl> def exec_command ( self , cmd , in_data = None , sudoable = True ) : <nl> except Exception as e : <nl> traceback . print_exc ( ) <nl> raise AnsibleError ( \" failed to exec cmd % s \" % cmd ) <nl> - result . std_out = to_unicode ( result . std_out ) <nl> - result . std_err = to_unicode ( result . std_err ) <nl> + result . std_out = to_bytes ( result . std_out ) <nl> + result . std_err = to_bytes ( result . std_err ) <nl> return ( result . status_code , result . std_out , result . std_err ) <nl> <nl> def put_file ( self , in_path , out_path ) : <nl> def put_file ( self , in_path , out_path ) : <nl> cmd_parts = self . _shell . _encode_script ( script , as_list = True ) <nl> result = self . _winrm_exec ( cmd_parts [ 0 ] , cmd_parts [ 1 : ] ) <nl> if result . status_code ! = 0 : <nl> - raise IOError ( to_unicode ( result . std_err ) ) <nl> + raise IOError ( to_str ( result . std_err ) ) <nl> except Exception : <nl> traceback . print_exc ( ) <nl> raise AnsibleError ( ' failed to transfer file to \" % s \" ' % out_path ) <nl> def fetch_file ( self , in_path , out_path ) : <nl> cmd_parts = self . _shell . _encode_script ( script , as_list = True ) <nl> result = self . _winrm_exec ( cmd_parts [ 0 ] , cmd_parts [ 1 : ] ) <nl> if result . status_code ! = 0 : <nl> - raise IOError ( to_unicode ( result . std_err ) ) <nl> + raise IOError ( to_str ( result . std_err ) ) <nl> if result . std_out . strip ( ) = = ' [ DIR ] ' : <nl> data = None <nl> else : <nl>\n", "msg": "In v2 , exec_command should return bytes and the caller will take responsibility for converting to unicode\n"}
{"diff_id": 23743, "repo": "ipython/ipython\n", "sha": "927b3c956f80ca3b4c8dd0d4a1dd9c9d6dd180c6\n", "time": "2010-09-07T15:36:04Z\n", "diff": "mmm a / IPython / zmq / entry_point . py <nl> ppp b / IPython / zmq / entry_point . py <nl> def make_argument_parser ( ) : <nl> <nl> def make_kernel ( namespace , kernel_factory , <nl> out_stream_factory = None , display_hook_factory = None ) : <nl> - \" \" \" Creates a kernel . <nl> + \" \" \" Creates a kernel , redirects stdout / stderr , and installs a display hook <nl> + and exception handler . <nl> \" \" \" <nl> + # If running under pythonw . exe , the interpreter will crash if more than 4KB <nl> + # of data is written to stdout or stderr . This is a bug that has been with <nl> + # Python for a very long time ; see http : / / bugs . python . org / issue706263 . <nl> + if sys . executable . endswith ( ' pythonw . exe ' ) : <nl> + blackhole = file ( os . devnull , ' w ' ) <nl> + sys . stdout = sys . stderr = blackhole <nl> + sys . __stdout__ = sys . __stderr__ = blackhole <nl> + <nl> # Install minimal exception handling <nl> sys . excepthook = FormattedTB ( mode = ' Verbose ' , color_scheme = ' NoColor ' , <nl> ostream = sys . __stdout__ ) <nl> def base_launch_kernel ( code , xrep_port = 0 , pub_port = 0 , req_port = 0 , hb_port = 0 , <nl> <nl> # Spawn a kernel . <nl> if sys . platform = = ' win32 ' : <nl> + <nl> # If using pythonw , stdin , stdout , and stderr are invalid . Popen will <nl> # fail unless they are suitably redirected . We don ' t read from the <nl> # pipes , but they must exist . <nl> redirect = PIPE if sys . executable . endswith ( ' pythonw . exe ' ) else None <nl> + <nl> if independent : <nl> proc = Popen ( [ ' start ' , ' / b ' ] + arguments , shell = True , <nl> stdout = redirect , stderr = redirect , stdin = redirect ) <nl> def base_launch_kernel ( code , xrep_port = 0 , pub_port = 0 , req_port = 0 , hb_port = 0 , <nl> DUPLICATE_SAME_ACCESS ) <nl> proc = Popen ( arguments + [ ' - - parent ' , str ( int ( handle ) ) ] , <nl> stdout = redirect , stderr = redirect , stdin = redirect ) <nl> + <nl> + # Clean up pipes created to work around Popen bug . <nl> + if redirect is not None : <nl> + proc . stdout . close ( ) <nl> + proc . stderr . close ( ) <nl> + proc . stdin . close ( ) <nl> + <nl> else : <nl> if independent : <nl> proc = Popen ( arguments , preexec_fn = lambda : os . setsid ( ) ) <nl>\n", "msg": "More pythonw . exe - specific fixes to the kernel launching pipeline .\n"}
{"diff_id": 23762, "repo": "ansible/ansible\n", "sha": "fa368934bd26600104f72f9d8f0591c0fd68e958\n", "time": "2016-06-07T14:42:39Z\n", "diff": "mmm a / lib / ansible / executor / play_iterator . py <nl> ppp b / lib / ansible / executor / play_iterator . py <nl> def __init__ ( self , inventory , play , play_context , variable_manager , all_vars , st <nl> self . _play . handlers . extend ( play . compile_roles_handlers ( ) ) <nl> <nl> def get_host_state ( self , host ) : <nl> - try : <nl> - return self . _host_states [ host . name ] . copy ( ) <nl> - except KeyError : <nl> - raise AnsibleError ( \" invalid host ( % s ) specified for playbook iteration \" % host ) <nl> + # Since we ' re using the PlayIterator to carry forward failed hosts , <nl> + # in the event that a previous host was not in the current inventory <nl> + # we create a stub state for it now <nl> + if host . name not in self . _host_states : <nl> + self . _host_states [ host . name ] = HostState ( blocks = [ ] ) <nl> + <nl> + return self . _host_states [ host . name ] . copy ( ) <nl> <nl> def get_next_task_for_host ( self , host , peek = False ) : <nl> <nl>\n", "msg": "Create state in PlayIterator for unknown hosts rather than raise errors\n"}
{"diff_id": 23864, "repo": "ipython/ipython\n", "sha": "b2876b642a34c904392d8c859ad74a737ec0bf78\n", "time": "2016-05-17T01:14:06Z\n", "diff": "mmm a / IPython / core / prefilter . py <nl> ppp b / IPython / core / prefilter . py <nl> class PrefilterManager ( Configurable ) : <nl> or : meth : ` sort_transformers ` method after changing the priority . <nl> \" \" \" <nl> <nl> - multi_line_specials = CBool ( True , config = True ) <nl> + multi_line_specials = CBool ( True ) . tag ( config = True ) <nl> shell = Instance ( ' IPython . core . interactiveshell . InteractiveShellABC ' , allow_none = True ) <nl> <nl> def __init__ ( self , shell = None , * * kwargs ) : <nl> def prefilter_lines ( self , lines , continue_prompt = False ) : <nl> class PrefilterTransformer ( Configurable ) : <nl> \" \" \" Transform a line of user input . \" \" \" <nl> <nl> - priority = Integer ( 100 , config = True ) <nl> + priority = Integer ( 100 ) . tag ( config = True ) <nl> # Transformers don ' t currently use shell or prefilter_manager , but as we <nl> # move away from checkers and handlers , they will need them . <nl> shell = Instance ( ' IPython . core . interactiveshell . InteractiveShellABC ' , allow_none = True ) <nl> prefilter_manager = Instance ( ' IPython . core . prefilter . PrefilterManager ' , allow_none = True ) <nl> - enabled = Bool ( True , config = True ) <nl> + enabled = Bool ( True ) . tag ( config = True ) <nl> <nl> def __init__ ( self , shell = None , prefilter_manager = None , * * kwargs ) : <nl> super ( PrefilterTransformer , self ) . __init__ ( <nl> def __repr__ ( self ) : <nl> class PrefilterChecker ( Configurable ) : <nl> \" \" \" Inspect an input line and return a handler for that line . \" \" \" <nl> <nl> - priority = Integer ( 100 , config = True ) <nl> + priority = Integer ( 100 ) . tag ( config = True ) <nl> shell = Instance ( ' IPython . core . interactiveshell . InteractiveShellABC ' , allow_none = True ) <nl> prefilter_manager = Instance ( ' IPython . core . prefilter . PrefilterManager ' , allow_none = True ) <nl> - enabled = Bool ( True , config = True ) <nl> + enabled = Bool ( True ) . tag ( config = True ) <nl> <nl> def __init__ ( self , shell = None , prefilter_manager = None , * * kwargs ) : <nl> super ( PrefilterChecker , self ) . __init__ ( <nl> def __repr__ ( self ) : <nl> <nl> class EmacsChecker ( PrefilterChecker ) : <nl> <nl> - priority = Integer ( 100 , config = True ) <nl> - enabled = Bool ( False , config = True ) <nl> + priority = Integer ( 100 ) . tag ( config = True ) <nl> + enabled = Bool ( False ) . tag ( config = True ) <nl> <nl> def check ( self , line_info ) : <nl> \" Emacs ipython - mode tags certain input lines . \" <nl> def check ( self , line_info ) : <nl> <nl> class MacroChecker ( PrefilterChecker ) : <nl> <nl> - priority = Integer ( 250 , config = True ) <nl> + priority = Integer ( 250 ) . tag ( config = True ) <nl> <nl> def check ( self , line_info ) : <nl> obj = self . shell . user_ns . get ( line_info . ifun ) <nl> def check ( self , line_info ) : <nl> <nl> class IPyAutocallChecker ( PrefilterChecker ) : <nl> <nl> - priority = Integer ( 300 , config = True ) <nl> + priority = Integer ( 300 ) . tag ( config = True ) <nl> <nl> def check ( self , line_info ) : <nl> \" Instances of IPyAutocall in user_ns get autocalled immediately \" <nl> def check ( self , line_info ) : <nl> <nl> class AssignmentChecker ( PrefilterChecker ) : <nl> <nl> - priority = Integer ( 600 , config = True ) <nl> + priority = Integer ( 600 ) . tag ( config = True ) <nl> <nl> def check ( self , line_info ) : <nl> \" \" \" Check to see if user is assigning to a var for the first time , in <nl> def check ( self , line_info ) : <nl> <nl> class AutoMagicChecker ( PrefilterChecker ) : <nl> <nl> - priority = Integer ( 700 , config = True ) <nl> + priority = Integer ( 700 ) . tag ( config = True ) <nl> <nl> def check ( self , line_info ) : <nl> \" \" \" If the ifun is magic , and automagic is on , run it . Note : normal , <nl> def check ( self , line_info ) : <nl> <nl> class PythonOpsChecker ( PrefilterChecker ) : <nl> <nl> - priority = Integer ( 900 , config = True ) <nl> + priority = Integer ( 900 ) . tag ( config = True ) <nl> <nl> def check ( self , line_info ) : <nl> \" \" \" If the ' rest ' of the line begins with a function call or pretty much <nl> def check ( self , line_info ) : <nl> <nl> class AutocallChecker ( PrefilterChecker ) : <nl> <nl> - priority = Integer ( 1000 , config = True ) <nl> + priority = Integer ( 1000 ) . tag ( config = True ) <nl> <nl> - function_name_regexp = CRegExp ( re_fun_name , config = True , <nl> + function_name_regexp = CRegExp ( re_fun_name ) . tag ( config = True , <nl> help = \" RegExp to identify potential function names . \" ) <nl> - exclude_regexp = CRegExp ( re_exclude_auto , config = True , <nl> + exclude_regexp = CRegExp ( re_exclude_auto ) . tag ( config = True , <nl> help = \" RegExp to exclude strings with this start from autocalling . \" ) <nl> <nl> def check ( self , line_info ) : <nl> def handle ( self , line_info ) : <nl> line = line_info . line <nl> ifun = line_info . ifun <nl> the_rest = line_info . the_rest <nl> - pre = line_info . pre <nl> esc = line_info . esc <nl> continue_prompt = line_info . continue_prompt <nl> obj = line_info . ofind ( self . shell ) [ ' obj ' ] <nl> - # print ' pre < % s > ifun < % s > rest < % s > ' % ( pre , ifun , the_rest ) # dbg <nl> <nl> # This should only be active for single - line input ! <nl> if continue_prompt : <nl>\n", "msg": "Update IPython / core / prefilter to new traitlets API .\n"}
{"diff_id": 24017, "repo": "python/cpython\n", "sha": "3bf7a6c1dada9842fb0d53ac3818f20540933c8f\n", "time": "2014-10-30T23:37:03Z\n", "diff": "mmm a / Lib / re . py <nl> ppp b / Lib / re . py <nl> <nl> import sys <nl> import sre_compile <nl> import sre_parse <nl> - import _locale <nl> + try : <nl> + import _locale <nl> + except ImportError : <nl> + _locale = None <nl> <nl> # public symbols <nl> __all__ = [ <nl> def _compile ( pattern , flags ) : <nl> if len ( _cache ) > = _MAXCACHE : <nl> _cache . clear ( ) <nl> if p . flags & LOCALE : <nl> + if not _locale : <nl> + return p <nl> loc = _locale . setlocale ( _locale . LC_CTYPE ) <nl> else : <nl> loc = None <nl>\n", "msg": "Fixed compile error in issue . The _locale module is optional .\n"}
{"diff_id": 24027, "repo": "matplotlib/matplotlib\n", "sha": "68d24f6a7a5b38ff437ea3f78385ec3808617258\n", "time": "2017-11-10T00:26:25Z\n", "diff": "mmm a / lib / matplotlib / font_manager . py <nl> ppp b / lib / matplotlib / font_manager . py <nl> def _normalize_font_family ( family ) : <nl> return family <nl> <nl> <nl> + @ cbook . deprecated ( \" 2 . 2 \" ) <nl> class TempCache ( object ) : <nl> \" \" \" <nl> A class to store temporary caches that are ( a ) not saved to disk <nl> def findfont ( self , prop , fontext = ' ttf ' , directory = None , <nl> < http : / / www . w3 . org / TR / 1998 / REC - CSS2 - 19980512 / > ` _ documentation <nl> for a description of the font finding algorithm . <nl> \" \" \" <nl> + # Pass the relevant rcParams ( and the font manager , as ` self ` ) to <nl> + # _findfont_cached so to prevent using a stale cache entry after an <nl> + # rcParam was changed . <nl> + rc_params = tuple ( tuple ( rcParams [ key ] ) for key in [ <nl> + \" font . serif \" , \" font . sans - serif \" , \" font . cursive \" , \" font . fantasy \" , <nl> + \" font . monospace \" ] ) <nl> + return self . _findfont_cached ( <nl> + prop , fontext , directory , fallback_to_default , rebuild_if_missing , <nl> + rc_params ) <nl> + <nl> + @ lru_cache ( ) <nl> + def _findfont_cached ( self , prop , fontext , directory , fallback_to_default , <nl> + rebuild_if_missing , rc_params ) : <nl> + <nl> if not isinstance ( prop , FontProperties ) : <nl> prop = FontProperties ( prop ) <nl> fname = prop . get_file ( ) <nl> def findfont ( self , prop , fontext = ' ttf ' , directory = None , <nl> else : <nl> fontlist = self . ttflist <nl> <nl> - if directory is None : <nl> - cached = _lookup_cache [ fontext ] . get ( prop ) <nl> - if cached is not None : <nl> - return cached <nl> - else : <nl> + if directory is not None : <nl> directory = os . path . normcase ( directory ) <nl> <nl> best_score = 1e64 <nl> def findfont ( self , prop , fontext = ' ttf ' , directory = None , <nl> else : <nl> raise ValueError ( \" No valid font could be found \" ) <nl> <nl> - if directory is None : <nl> - _lookup_cache [ fontext ] . set ( prop , result ) <nl> return result <nl> <nl> - _is_opentype_cff_font_cache = { } <nl> + @ lru_cache ( ) <nl> def is_opentype_cff_font ( filename ) : <nl> \" \" \" <nl> Returns True if the given font is a Postscript Compact Font Format <nl> def is_opentype_cff_font ( filename ) : <nl> PDF backends that can not subset these fonts . <nl> \" \" \" <nl> if os . path . splitext ( filename ) [ 1 ] . lower ( ) = = ' . otf ' : <nl> - result = _is_opentype_cff_font_cache . get ( filename ) <nl> - if result is None : <nl> - with open ( filename , ' rb ' ) as fd : <nl> - tag = fd . read ( 4 ) <nl> - result = ( tag = = b ' OTTO ' ) <nl> - _is_opentype_cff_font_cache [ filename ] = result <nl> - return result <nl> - return False <nl> + with open ( filename , ' rb ' ) as fd : <nl> + return fd . read ( 4 ) = = b \" OTTO \" <nl> + else : <nl> + return False <nl> <nl> fontManager = None <nl> _fmcache = None <nl> def findfont ( prop , fontext = ' ttf ' ) : <nl> <nl> fontManager = None <nl> <nl> - _lookup_cache = { <nl> - ' ttf ' : TempCache ( ) , <nl> - ' afm ' : TempCache ( ) <nl> - } <nl> - <nl> def _rebuild ( ) : <nl> global fontManager <nl> <nl>\n", "msg": "Rely more on lru_cache in font_manager rather than custom caching .\n"}
{"diff_id": 24501, "repo": "ansible/ansible\n", "sha": "0eef7cadfa08f7e70ac77ddd564208c6e3b5ebc7\n", "time": "2016-12-08T16:23:28Z\n", "diff": "mmm a / lib / ansible / modules / cloud / rackspace / rax_scaling_group . py <nl> ppp b / lib / ansible / modules / cloud / rackspace / rax_scaling_group . py <nl> <nl> - Data to be uploaded to the servers config drive . This option implies <nl> I ( config_drive ) . Can be a file path or a string <nl> version_added : 1 . 8 <nl> - author : \" Matt Martz ( @ sivel ) \" <nl> + wait <nl> + description : <nl> + - wait for the scaling group to finish provisioning the minimum amount of <nl> + servers <nl> + default : \" no \" <nl> + choices : <nl> + - \" yes \" <nl> + - \" no \" <nl> + wait_timeout : <nl> + description : <nl> + - how long before wait gives up , in seconds <nl> + default : 300 <nl> + author : Matt Martz <nl> extends_documentation_fragment : rackspace <nl> ' ' ' <nl> <nl> def rax_asg ( module , cooldown = 300 , disk_config = None , files = { } , flavor = None , <nl> image = None , key_name = None , loadbalancers = [ ] , meta = { } , <nl> min_entities = 0 , max_entities = 0 , name = None , networks = [ ] , <nl> server_name = None , state = ' present ' , user_data = None , <nl> - config_drive = False ) : <nl> + config_drive = False , wait = True , wait_timeout = 300 ) : <nl> changed = False <nl> <nl> au = pyrax . autoscale <nl> def rax_asg ( module , cooldown = 300 , disk_config = None , files = { } , flavor = None , <nl> <nl> sg . get ( ) <nl> <nl> + if wait : <nl> + end_time = time . time ( ) + wait_timeout <nl> + infinite = wait_timeout = = 0 <nl> + while infinite or time . time ( ) < end_time : <nl> + state = sg . get_state ( ) <nl> + if state [ \" pending_capacity \" ] = = 0 : <nl> + break <nl> + <nl> + time . sleep ( 5 ) <nl> + <nl> module . exit_json ( changed = changed , autoscale_group = rax_to_dict ( sg ) ) <nl> <nl> else : <nl> def main ( ) : <nl> server_name = dict ( required = True ) , <nl> state = dict ( default = ' present ' , choices = [ ' present ' , ' absent ' ] ) , <nl> user_data = dict ( no_log = True ) , <nl> + wait = dict ( default = False , type = ' bool ' ) , <nl> + wait_timeout = dict ( default = 300 ) , <nl> ) <nl> ) <nl> <nl>\n", "msg": "Add wait and wait_timeout options for provisioning servers\n"}
{"diff_id": 24666, "repo": "ipython/ipython\n", "sha": "cc3e7f7471ede41820fbcefbf714138c17b45627\n", "time": "2011-06-28T23:47:08Z\n", "diff": "mmm a / IPython / parallel / client / client . py <nl> ppp b / IPython / parallel / client / client . py <nl> def __init__ ( self , url_or_file = None , profile = None , profile_dir = None , ipython_dir <nl> if location is not None : <nl> proto , addr , port = util . split_url ( url ) <nl> if addr = = ' 127 . 0 . 0 . 1 ' and location not in LOCAL_IPS and not sshserver : <nl> - sshserver = location <nl> - warnings . warn ( <nl> - \" Controller appears to be listening on localhost , but is not local . \" <nl> - \" IPython will try to use SSH tunnels to % s \" % location , <nl> - RuntimeWarning ) <nl> + warnings . warn ( \" \" \" <nl> + Controller appears to be listening on localhost , but not on this machine . <nl> + If this is true , you should specify Client ( . . . , sshserver = ' you @ % s ' ) <nl> + or instruct your controller to listen on an external IP . \" \" \" % location , <nl> + RuntimeWarning ) <nl> <nl> self . _config = cfg <nl> <nl>\n", "msg": "better warning on non - local controller without ssh\n"}
{"diff_id": 24903, "repo": "python/cpython\n", "sha": "62fe75509c905bee1181fed9b6714e0de27ff5d4\n", "time": "2003-01-15T22:59:39Z\n", "diff": "mmm a / Lib / _strptime . py <nl> ppp b / Lib / _strptime . py <nl> def __calc_date_time ( self ) : <nl> ( ' 17 ' , ' % d ' ) , ( ' 03 ' , ' % m ' ) , ( ' 3 ' , ' % m ' ) , <nl> # ' 3 ' needed for when no leading zero . <nl> ( ' 2 ' , ' % w ' ) , ( ' 10 ' , ' % I ' ) ) : <nl> - try : <nl> - # Done this way to deal with possible lack of locale info <nl> - # manifesting itself as the empty string ( i . e . , Swedish ' s <nl> - # lack of AM / PM info ) . <nl> + # Must deal with possible lack of locale info <nl> + # manifesting itself as the empty string ( e . g . , Swedish ' s <nl> + # lack of AM / PM info ) or a platform returning a tuple of empty <nl> + # strings ( e . g . , MacOS 9 having timezone as ( ' ' , ' ' ) ) . <nl> + if old : <nl> current_format = current_format . replace ( old , new ) <nl> - except ValueError : <nl> - pass <nl> time_tuple = time . struct_time ( ( 1999 , 1 , 3 , 1 , 1 , 1 , 6 , 3 , 0 ) ) <nl> if time . strftime ( directive , time_tuple ) . find ( ' 00 ' ) : <nl> U_W = ' % U ' <nl> def __getitem__ ( self , fetch ) : <nl> raise <nl> <nl> def __seqToRE ( self , to_convert , directive ) : <nl> - \" \" \" Convert a list to a regex string for matching directive . \" \" \" <nl> + \" \" \" Convert a list to a regex string for matching a directive . \" \" \" <nl> def sorter ( a , b ) : <nl> \" \" \" Sort based on length . <nl> <nl> def sorter ( a , b ) : <nl> return cmp ( b_length , a_length ) <nl> <nl> to_convert = to_convert [ : ] # Don ' t want to change value in - place . <nl> + for value in to_convert : <nl> + if value ! = ' ' : <nl> + break <nl> + else : <nl> + return ' ' <nl> to_convert . sort ( sorter ) <nl> regex = ' | ' . join ( to_convert ) <nl> regex = ' ( ? P < % s > % s ' % ( directive , regex ) <nl> def strptime ( data_string , format = \" % a % b % d % H : % M : % S % Y \" ) : <nl> found_zone = found_dict [ ' Z ' ] . lower ( ) <nl> if locale_time . timezone [ 0 ] = = locale_time . timezone [ 1 ] : <nl> pass # Deals with bad locale setup where timezone info is <nl> - # the same ; first found on FreeBSD 4 . 4 - current <nl> + # the same ; first found on FreeBSD 4 . 4 . <nl> elif locale_time . timezone [ 0 ] . lower ( ) = = found_zone : <nl> tz = 0 <nl> elif locale_time . timezone [ 1 ] . lower ( ) = = found_zone : <nl>\n", "msg": "Checking in Brett Cannon ' s patch , which fixes bug .\n"}
{"diff_id": 25091, "repo": "bokeh/bokeh\n", "sha": "6f4d5fac7b4b951329f904fbde4f5528c7e58c17\n", "time": "2014-08-14T05:28:28Z\n", "diff": "mmm a / bokeh / charts / boxplot . py <nl> ppp b / bokeh / charts / boxplot . py <nl> def __init__ ( self , value , marker = \" circle \" , outliers = True , <nl> self . __outliers = outliers <nl> self . xdr = None <nl> self . ydr = None <nl> - self . data = dict ( ) <nl> - # self . data_segment = dict ( ) <nl> - # self . data_rect = dict ( ) <nl> + self . data_segment = dict ( ) <nl> + self . attr_segment = [ ] <nl> + self . data_rect = dict ( ) <nl> + self . attr_rect = [ ] <nl> self . data_scatter = dict ( ) <nl> self . attr_scatter = [ ] <nl> - self . attr = [ ] <nl> super ( BoxPlot , self ) . __init__ ( title , xlabel , ylabel , legend , <nl> xscale , yscale , width , height , <nl> tools , filename , server , notebook ) <nl> def get_data ( self , marker , outliers , * * value ) : <nl> self . marker = marker <nl> self . outliers = outliers <nl> <nl> - # add cat and witdh to the self . data dict <nl> - self . data [ \" groups \" ] = self . groups <nl> - self . data [ \" width \" ] = [ 0 . 8 ] * len ( self . groups ) <nl> - <nl> - # self . data_segment [ \" groups \" ] = self . groups <nl> - # self . data_rect [ \" groups \" ] = self . groups <nl> - # self . data_rect [ \" width \" ] = [ 0 . 8 ] * len ( self . groups ) <nl> - <nl> - self . nones = [ None ] * len ( self . groups ) <nl> + # add group to the self . data_segment dict <nl> + self . data_segment [ \" groups \" ] = self . groups <nl> + <nl> + # add group and witdh to the self . data_rect dict <nl> + self . data_rect [ \" groups \" ] = self . groups <nl> + self . data_rect [ \" width \" ] = [ 0 . 8 ] * len ( self . groups ) <nl> + <nl> + # all the list we are going to use to save calculated values <nl> + q0_points = [ ] <nl> + q2_points = [ ] <nl> + iqr_centers = [ ] <nl> + iqr_lengths = [ ] <nl> + lower_points = [ ] <nl> + upper_points = [ ] <nl> + upper_center_boxes = [ ] <nl> + upper_height_boxes = [ ] <nl> + lower_center_boxes = [ ] <nl> + lower_height_boxes = [ ] <nl> <nl> for i , level in enumerate ( self . groups ) : <nl> - <nl> - # Initialize all the list to be used to store data <nl> - ( q0_list , q2_list , u_cp_list , u_he_list , <nl> - l_cp_list , l_he_list , iqr_cp_list , iqr_list , <nl> - lower_list , upper_list ) = ( list ( self . nones ) for i in range ( 10 ) ) <nl> - <nl> # Compute quantiles , center points , heights , IQR , etc . <nl> # quantiles <nl> q = np . percentile ( self . value [ level ] , [ 25 , 50 , 75 ] ) <nl> - q0_list [ i ] = q [ 0 ] <nl> - q2_list [ i ] = q [ 2 ] <nl> - <nl> - # rect center points and heights <nl> - u_cp_list [ i ] = ( q [ 2 ] + q [ 1 ] ) / 2 <nl> - u_he_list [ i ] = q [ 2 ] - q [ 1 ] <nl> - l_cp_list [ i ] = ( q [ 1 ] + q [ 0 ] ) / 2 <nl> - l_he_list [ i ] = q [ 1 ] - q [ 0 ] <nl> + q0_points . append ( q [ 0 ] ) <nl> + q2_points . append ( q [ 2 ] ) <nl> <nl> # IQR related stuff . . . <nl> - iqr_cp_list [ i ] = ( q [ 2 ] + q [ 0 ] ) / 2 <nl> + iqr_centers . append ( ( q [ 2 ] + q [ 0 ] ) / 2 ) <nl> iqr = q [ 2 ] - q [ 0 ] <nl> - iqr_list [ i ] = iqr <nl> - <nl> + iqr_lengths . append ( iqr ) <nl> lower = q [ 1 ] - 1 . 5 * iqr <nl> - lower_list [ i ] = lower <nl> - <nl> upper = q [ 1 ] + 1 . 5 * iqr <nl> - upper_list [ i ] = upper <nl> + lower_points . append ( lower ) <nl> + upper_points . append ( upper ) <nl> + <nl> + # rect center points and heights <nl> + upper_center_boxes . append ( ( q [ 2 ] + q [ 1 ] ) / 2 ) <nl> + upper_height_boxes . append ( q [ 2 ] - q [ 1 ] ) <nl> + lower_center_boxes . append ( ( q [ 1 ] + q [ 0 ] ) / 2 ) <nl> + lower_height_boxes . append ( q [ 1 ] - q [ 0 ] ) <nl> <nl> # Store indices of outliers as list <nl> outliers = np . where ( ( self . value [ level ] > upper ) | ( self . value [ level ] < lower ) ) [ 0 ] <nl> def get_data ( self , marker , outliers , * * value ) : <nl> out_x . append ( level ) <nl> out_y . append ( o ) <nl> <nl> - # Store <nl> - self . _set_and_get ( self . data , self . attr , \" q0 \" , level , q0_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" lower_list \" , level , lower_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" q2 \" , level , q2_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" upper_list \" , level , upper_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" iqr_cp_list \" , level , iqr_cp_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" iqr_list \" , level , iqr_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" u_cp_list \" , level , u_cp_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" u_he_list \" , level , u_he_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" l_cp_list \" , level , l_cp_list ) <nl> - self . _set_and_get ( self . data , self . attr , \" l_he_list \" , level , l_he_list ) <nl> - <nl> self . _set_and_get ( self . data_scatter , self . attr_scatter , \" out_x \" , level , out_x ) <nl> self . _set_and_get ( self . data_scatter , self . attr_scatter , \" out_y \" , level , out_y ) <nl> <nl> + # Store outside the loop <nl> + self . colors = self . _set_colors ( self . groups ) <nl> + <nl> + self . _set_and_get ( self . data_segment , self . attr_segment , \" q0 \" , \" \" , q0_points ) <nl> + self . _set_and_get ( self . data_segment , self . attr_segment , \" lower \" , \" \" , lower_points ) <nl> + self . _set_and_get ( self . data_segment , self . attr_segment , \" q2 \" , \" \" , q2_points ) <nl> + self . _set_and_get ( self . data_segment , self . attr_segment , \" upper \" , \" \" , upper_points ) <nl> + <nl> + self . _set_and_get ( self . data_rect , self . attr_rect , \" iqr_centers \" , \" \" , iqr_centers ) <nl> + self . _set_and_get ( self . data_rect , self . attr_rect , \" iqr_lengths \" , \" \" , iqr_lengths ) <nl> + self . _set_and_get ( self . data_rect , self . attr_rect , \" upper_center_boxes \" , \" \" , upper_center_boxes ) <nl> + self . _set_and_get ( self . data_rect , self . attr_rect , \" upper_height_boxes \" , \" \" , upper_height_boxes ) <nl> + self . _set_and_get ( self . data_rect , self . attr_rect , \" lower_center_boxes \" , \" \" , lower_center_boxes ) <nl> + self . _set_and_get ( self . data_rect , self . attr_rect , \" lower_height_boxes \" , \" \" , lower_height_boxes ) <nl> + self . _set_and_get ( self . data_rect , self . attr_rect , \" colors \" , \" \" , self . colors ) <nl> + <nl> def get_source ( self ) : <nl> \" \" \" Get the boxplot data dict into the ColumnDataSource and <nl> calculate the proper ranges . \" \" \" <nl> - self . source = ColumnDataSource ( self . data ) <nl> + self . source_segment = ColumnDataSource ( self . data_segment ) <nl> self . source_scatter = ColumnDataSource ( self . data_scatter ) <nl> - self . xdr = FactorRange ( factors = self . source . data [ \" groups \" ] ) <nl> - lowers , uppers = self . attr [ 1 : : 10 ] , self . attr [ 3 : : 10 ] <nl> + self . source_rect = ColumnDataSource ( self . data_rect ) <nl> + self . xdr = FactorRange ( factors = self . source_segment . data [ \" groups \" ] ) <nl> <nl> - def drop_none ( l ) : <nl> - return [ i for i in l if i is not None ] <nl> - <nl> - start_y = min ( min ( drop_none ( self . data [ i ] ) ) for i in lowers ) <nl> - end_y = max ( max ( drop_none ( self . data [ i ] ) ) for i in uppers ) <nl> + start_y = min ( self . data_segment [ self . attr_segment [ 1 ] ] ) <nl> + end_y = max ( self . data_segment [ self . attr_segment [ 3 ] ] ) <nl> <nl> # # Expand min / max to encompass outliers <nl> if self . outliers : <nl> def draw ( self ) : <nl> display the iqr and rects to display the boxes , taking as reference <nl> points the data loaded at the ColumnDataSurce . <nl> \" \" \" <nl> - self . quartet = list ( self . _chunker ( self . attr , 10 ) ) <nl> - self . duplet = list ( self . _chunker ( self . attr_scatter , 2 ) ) <nl> - colors = self . _set_colors ( self . quartet ) <nl> + self . chart . make_segment ( self . source_segment , \" groups \" , self . attr_segment [ 1 ] , \" groups \" , self . attr_segment [ 0 ] , \" black \" , 2 ) <nl> + self . chart . make_segment ( self . source_segment , \" groups \" , self . attr_segment [ 2 ] , \" groups \" , self . attr_segment [ 3 ] , \" black \" , 2 ) <nl> <nl> - for i , quartet in enumerate ( self . quartet ) : <nl> - self . chart . make_segment ( self . source , \" groups \" , quartet [ 1 ] , \" groups \" , quartet [ 0 ] , \" black \" , 2 ) <nl> - self . chart . make_segment ( self . source , \" groups \" , quartet [ 2 ] , \" groups \" , quartet [ 3 ] , \" black \" , 2 ) <nl> - self . chart . make_rect ( self . source , \" groups \" , quartet [ 4 ] , \" width \" , quartet [ 5 ] , None , \" black \" , 2 ) <nl> - self . chart . make_rect ( self . source , \" groups \" , quartet [ 6 ] , \" width \" , quartet [ 7 ] , colors [ i ] , \" black \" , None ) <nl> - self . chart . make_rect ( self . source , \" groups \" , quartet [ 8 ] , \" width \" , quartet [ 9 ] , colors [ i ] , \" black \" , None ) <nl> + self . chart . make_rect ( self . source_rect , \" groups \" , self . attr_rect [ 0 ] , \" width \" , self . attr_rect [ 1 ] , None , \" black \" , 2 ) <nl> + self . chart . make_rect ( self . source_rect , \" groups \" , self . attr_rect [ 2 ] , \" width \" , self . attr_rect [ 3 ] , self . attr_rect [ 6 ] , \" black \" , None ) <nl> + self . chart . make_rect ( self . source_rect , \" groups \" , self . attr_rect [ 4 ] , \" width \" , self . attr_rect [ 5 ] , self . attr_rect [ 6 ] , \" black \" , None ) <nl> + <nl> + self . duplet = list ( self . _chunker ( self . attr_scatter , 2 ) ) <nl> <nl> for i , duplet in enumerate ( self . duplet ) : <nl> if self . outliers : <nl> - self . chart . make_scatter ( self . source_scatter , duplet [ 0 ] , duplet [ 1 ] , self . marker , colors [ i ] ) <nl> + self . chart . make_scatter ( self . source_scatter , duplet [ 0 ] , duplet [ 1 ] , self . marker , self . colors [ i ] ) <nl> <nl> - # We need to manually select the proper glyphsto be rendered as legends <nl> - indexes = [ 3 , 8 , 13 ] # 1st rect , 2nd rect , 3rd rect <nl> + # We need to manually select the proper glyphs to be rendered as legends <nl> + indexes = [ 5 , 6 , 7 ] # 1st rect , 2nd rect , 3rd rect <nl> self . chart . glyphs = [ self . chart . glyphs [ i ] for i in indexes ] <nl> <nl> def show ( self ) : <nl> def show ( self ) : <nl> # we filled the source and ranges with the calculated data <nl> self . get_source ( ) <nl> # we dinamically inject the source and ranges into the plot <nl> - self . add_data_plot ( self . xdr , self . ydr , [ self . source , self . source_scatter ] ) <nl> + self . add_data_plot ( self . xdr , self . ydr , [ self . source_segment , self . source_rect , self . source_scatter ] ) <nl> # we add the glyphs into the plot <nl> self . draw ( ) <nl> # we pass info to build the legend <nl>\n", "msg": "Box plot rewriting again to use a more simple approach .\n"}
{"diff_id": 25384, "repo": "ansible/ansible\n", "sha": "bc859eec7e5a0446d7ea103dbbae1820334fb388\n", "time": "2016-04-26T13:17:36Z\n", "diff": "mmm a / lib / ansible / module_utils / ec2 . py <nl> ppp b / lib / ansible / module_utils / ec2 . py <nl> def ansible_dict_to_boto3_tag_list ( tags_dict ) : <nl> tags_list . append ( { ' Key ' : k , ' Value ' : v } ) <nl> <nl> return tags_list <nl> + <nl> + <nl> + def get_ec2_security_group_ids_from_names ( sec_group_list , ec2_connection , vpc_id = None , boto3 = True ) : <nl> + <nl> + \" \" \" Return list of security group IDs from security group names . Note that security group names are not unique <nl> + across VPCs . If a name exists across multiple VPCs and no VPC ID is supplied , all matching IDs will be returned . This <nl> + will probably lead to a boto exception if you attempt to assign both IDs to a resource so ensure you wrap the call in <nl> + a try block <nl> + \" \" \" <nl> + <nl> + def get_sg_name ( sg , boto3 ) : <nl> + <nl> + if boto3 : <nl> + return sg [ ' GroupName ' ] <nl> + else : <nl> + return sg . name <nl> + <nl> + <nl> + def get_sg_id ( sg , boto3 ) : <nl> + <nl> + if boto3 : <nl> + return sg [ ' GroupId ' ] <nl> + else : <nl> + return sg . id <nl> + <nl> + <nl> + sec_group_id_list = [ ] <nl> + <nl> + if isinstance ( sec_group_list , basestring ) : <nl> + sec_group_list = [ sec_group_list ] <nl> + <nl> + # Get all security groups <nl> + if boto3 : <nl> + if vpc_id : <nl> + filters = [ <nl> + { <nl> + ' Name ' : ' vpc - id ' , <nl> + ' Values ' : [ <nl> + vpc_id , <nl> + ] <nl> + } <nl> + ] <nl> + all_sec_groups = ec2_connection . describe_security_groups ( Filters = filters ) [ ' SecurityGroups ' ] <nl> + else : <nl> + all_sec_groups = ec2_connection . describe_security_groups ( ) [ ' SecurityGroups ' ] <nl> + else : <nl> + if vpc_id : <nl> + filters = { ' vpc - id ' : vpc_id } <nl> + all_sec_groups = ec2_connection . get_all_security_groups ( filters = filters ) <nl> + else : <nl> + all_sec_groups = ec2_connection . get_all_security_groups ( ) <nl> + <nl> + unmatched = set ( sec_group_list ) . difference ( str ( get_sg_name ( all_sg , boto3 ) ) for all_sg in all_sec_groups ) <nl> + sec_group_name_list = list ( set ( sec_group_list ) - set ( unmatched ) ) <nl> + <nl> + if len ( unmatched ) > 0 : <nl> + # If we have unmatched names that look like an ID , assume they are <nl> + import re <nl> + sec_group_id_list [ : ] = [ sg for sg in unmatched if re . match ( ' sg - [ a - fA - F0 - 9 ] + $ ' , sg ) ] <nl> + still_unmatched = [ sg for sg in unmatched if not re . match ( ' sg - [ a - fA - F0 - 9 ] + $ ' , sg ) ] <nl> + if len ( still_unmatched ) > 0 : <nl> + raise ValueError ( \" The following group names are not valid : % s \" % ' , ' . join ( still_unmatched ) ) <nl> + <nl> + sec_group_id_list + = [ str ( get_sg_id ( all_sg , boto3 ) ) for all_sg in all_sec_groups if str ( get_sg_name ( all_sg , boto3 ) ) in sec_group_name_list ] <nl> + <nl> + return sec_group_id_list <nl> + <nl>\n", "msg": "Add shared functionality to return list of security group IDs from list of names ( )\n"}
{"diff_id": 25439, "repo": "bokeh/bokeh\n", "sha": "76911ac4f813a6e336165b2058935fcf20817376\n", "time": "2014-04-22T03:45:18Z\n", "diff": "mmm a / setup . py <nl> ppp b / setup . py <nl> def getsitepackages ( ) : <nl> <nl> # Set up this checkout or source archive with the right BokehJS files . <nl> <nl> + build_js = False <nl> + <nl> if sys . version_info [ : 2 ] < ( 2 , 6 ) : <nl> raise RuntimeError ( \" Bokeh requires python > = 2 . 6 \" ) <nl> <nl> def getsitepackages ( ) : <nl> <nl> if ' - - build_js ' in sys . argv : <nl> os . chdir ( ' bokehjs ' ) <nl> + build_js = True <nl> try : <nl> - print ( \" deploying bokehjs . . . \" ) <nl> + print ( \" building bokehjs . . . \" ) <nl> out = subprocess . check_output ( [ ' grunt ' , ' deploy ' ] ) <nl> sys . argv . remove ( ' - - build_js ' ) <nl> except subprocess . CalledProcessError : <nl> - print ( \" ERROR : could not deploy bokehjs \" ) <nl> + print ( \" ERROR : could not build bokehjs \" ) <nl> sys . exit ( 1 ) <nl> APP = [ join ( BOKEHJSBUILD , ' js ' , ' bokeh . js ' ) , <nl> join ( BOKEHJSBUILD , ' js ' , ' bokeh . min . js ' ) ] <nl> def getsitepackages ( ) : <nl> path_file = join ( site_packages , \" bokeh . pth \" ) <nl> path = abspath ( dirname ( __file__ ) ) <nl> <nl> + f = open ( ' bokehjs / src / coffee / main . coffee ' ) <nl> + import re <nl> + pat = re . compile ( \" Bokeh . version = ' ( . * ) ' \" ) <nl> + v = pat . search ( f . read ( ) ) . group ( 1 ) <nl> + <nl> + print ( ) <nl> if ' devjs ' in sys . argv or ' develop ' in sys . argv : <nl> with open ( path_file , \" w + \" ) as f : <nl> f . write ( path ) <nl> - print ( \" develop mode , wrote path ( % s ) to ( % s ) \" % ( path , path_file ) ) <nl> + print ( \" Developing bokeh . \" ) <nl> + print ( \" - writing path ' % s ' to % s \" % ( path , path_file ) ) <nl> + if build_js : <nl> + print ( \" - using BUILT bokehjs from bokehjs / build \" ) <nl> + else : <nl> + print ( \" - using RELEASED bokehjs ( version % s ) from bokehjs / release \" % v ) <nl> + print ( ) <nl> sys . exit ( ) <nl> elif ' install ' in sys . argv : <nl> if exists ( path_file ) : <nl> os . remove ( path_file ) <nl> - print ( \" Installing bokeh , removing bokeh . pth if it exists . \" ) <nl> + print ( \" Installing bokeh , removing % s \" % path_file ) <nl> + else : <nl> + print ( \" Installing bokeh . \" ) <nl> + if build_js : <nl> + print ( \" - using BUILT bokehjs from bokehjs / build \" ) <nl> else : <nl> - print ( \" Installing bokeh . . . \" ) <nl> + print ( \" - using RELEASED bokehjs ( version % s ) from bokehjs / release \" % v ) <nl> + print ( ) <nl> <nl> REQUIRES = [ <nl> ' Flask > = 0 . 10 . 1 ' , <nl>\n", "msg": "add some more informative info about what the build config is\n"}
{"diff_id": 25465, "repo": "home-assistant/core\n", "sha": "ab80af099c165af45d08b447074f89bcf2137a02\n", "time": "2015-10-02T03:38:50Z\n", "diff": "mmm a / homeassistant / components / light / limitlessled . py <nl> ppp b / homeassistant / components / light / limitlessled . py <nl> <nl> <nl> light : <nl> platform : limitlessled <nl> - host : 192 . 168 . 1 . 10 <nl> - group_1_name : Living Room <nl> - group_2_name : Bedroom <nl> - group_3_name : Office <nl> - group_4_name : Kitchen <nl> + bridges : <nl> + - host : 192 . 168 . 1 . 10 <nl> + group_1_name : Living Room <nl> + group_2_name : Bedroom <nl> + group_3_name : Office <nl> + group_4_name : Kitchen <nl> + - host : 192 . 168 . 1 . 11 <nl> + group_2_name : Basement <nl> \" \" \" <nl> import logging <nl> <nl> def setup_platform ( hass , config , add_devices_callback , discovery_info = None ) : <nl> \" \" \" Gets the LimitlessLED lights . \" \" \" <nl> import ledcontroller <nl> <nl> - pool = ledcontroller . LedControllerPool ( [ config [ ' host ' ] ] ) <nl> + for bridge_id , bridge in enumerate ( bridges ) : <nl> + bridge [ ' id ' ] = bridge_id <nl> + <nl> + pool = ledcontroller . LedControllerPool ( [ x [ ' host ' ] for x in bridges ] ) <nl> <nl> lights = [ ] <nl> - for i in range ( 1 , 5 ) : <nl> - if ' group_ % d_name ' % ( i ) in config : <nl> - lights . append ( LimitlessLED ( pool , 0 , i , config [ ' group_ % d_name ' % ( i ) ] ) ) <nl> + for bridge in bridges : <nl> + for i in range ( 1 , 5 ) : <nl> + if ' group_ % d_name ' % ( i ) in bridge : <nl> + lights . append ( LimitlessLED ( pool , bridge [ ' id ' ] , i , bridge [ ' group_ % d_name ' % ( i ) ] ) ) <nl> <nl> add_devices_callback ( lights ) <nl> <nl>\n", "msg": "limitlessled : Add support for multiple bridges\n"}
{"diff_id": 25648, "repo": "numpy/numpy\n", "sha": "434d2f141d7e473040effdba37e29ffd3b75a25c\n", "time": "2020-08-19T18:48:47Z\n", "diff": "mmm a / numpy / core / tests / test_regression . py <nl> ppp b / numpy / core / tests / test_regression . py <nl> def test_to_ctypes ( self ) : <nl> assert arr . size * arr . itemsize > 2 * * 31 <nl> c_arr = np . ctypeslib . as_ctypes ( arr ) <nl> assert_equal ( c_arr . _length_ , arr . size ) <nl> + <nl> + def test_complex_conversion_error ( self ) : <nl> + # gh - 17068 <nl> + with pytest . raises ( TypeError , match = r \" Unable to convert dtype . * \" ) : <nl> + complex ( np . array ( \" now \" , np . datetime64 ) ) <nl> + <nl> + def test__array_interface__descr ( self ) : <nl> + # gh - 17068 <nl> + dt = np . dtype ( dict ( names = [ ' a ' , ' b ' ] , <nl> + offsets = [ 0 , 0 ] , <nl> + formats = [ np . int64 , np . int64 ] ) ) <nl> + descr = np . array ( ( 1 , 1 ) , dtype = dt ) . __array_interface__ [ ' descr ' ] <nl> + assert descr = = [ ( ' ' , ' | V8 ' ) ] # instead of [ ( b ' ' , ' | V8 ' ) ] <nl>\n", "msg": "TST : Add tests for bugs fixed in gh - 17068 .\n"}
{"diff_id": 25656, "repo": "python/cpython\n", "sha": "c58e98483788b7e55af871eb9ebdbe56a2b3d109\n", "time": "2002-06-04T20:55:10Z\n", "diff": "mmm a / Lib / distutils / command / bdist_wininst . py <nl> ppp b / Lib / distutils / command / bdist_wininst . py <nl> <nl> __revision__ = \" $ Id $ \" <nl> <nl> import sys , os , string <nl> + import base64 <nl> from distutils . core import Command <nl> from distutils . util import get_platform <nl> from distutils . dir_util import create_tree , remove_tree <nl> def create_exe ( self , arcname , fullname , bitmap = None ) : <nl> # create_exe ( ) <nl> <nl> def get_exe_bytes ( self ) : <nl> - import base64 <nl> return base64 . decodestring ( EXEDATA ) <nl> # class bdist_wininst <nl> <nl> def get_exe_bytes ( self ) : <nl> # - Built wininst . exe from the MSVC project file distutils / misc / wininst . dsw <nl> # - Execute this file ( distutils / distutils / command / bdist_wininst . py ) <nl> <nl> - import re , base64 <nl> + import re <nl> moddata = open ( \" bdist_wininst . py \" , \" r \" ) . read ( ) <nl> exedata = open ( \" . . / . . / misc / wininst . exe \" , \" rb \" ) . read ( ) <nl> print \" wininst . exe length is % d bytes \" % len ( exedata ) <nl>\n", "msg": "import base64 at the top to avoid two different imports at other times\n"}
{"diff_id": 25699, "repo": "ansible/ansible\n", "sha": "4d184a3d5b40728e2359fba91404e6deeadbd989\n", "time": "2016-12-08T16:33:06Z\n", "diff": "mmm a / lib / ansible / modules / extras / packaging / os / apk . py <nl> ppp b / lib / ansible / modules / extras / packaging / os / apk . py <nl> <nl> import os <nl> import re <nl> <nl> - APK_PATH = \" / sbin / apk \" <nl> - <nl> def update_package_db ( module ) : <nl> - cmd = \" apk update \" <nl> + cmd = \" % s update \" % ( APK_PATH ) <nl> rc , stdout , stderr = module . run_command ( cmd , check_rc = False ) <nl> if rc = = 0 : <nl> return True <nl> def update_package_db ( module ) : <nl> module . fail_json ( msg = \" could not update package db \" ) <nl> <nl> def query_package ( module , name ) : <nl> - cmd = \" apk - v info - - installed % s \" % ( name ) <nl> + cmd = \" % s - v info - - installed % s \" % ( APK_PATH , name ) <nl> rc , stdout , stderr = module . run_command ( cmd , check_rc = False ) <nl> if rc = = 0 : <nl> return True <nl> def query_package ( module , name ) : <nl> return False <nl> <nl> def query_latest ( module , name ) : <nl> - cmd = \" apk version % s \" % ( name ) <nl> + cmd = \" % s version % s \" % ( APK_PATH , name ) <nl> rc , stdout , stderr = module . run_command ( cmd , check_rc = False ) <nl> search_pattern = \" ( % s ) - [ \\ d \\ . \\ w ] + - [ \\ d \\ w ] + \\ s + ( . ) \\ s + [ \\ d \\ . \\ w ] + - [ \\ d \\ w ] + \\ s + \" % ( name ) <nl> match = re . search ( search_pattern , stdout ) <nl> def query_latest ( module , name ) : <nl> <nl> def upgrade_packages ( module ) : <nl> if module . check_mode : <nl> - cmd = \" apk upgrade - - simulate \" <nl> + cmd = \" % s upgrade - - simulate \" % ( APK_PATH ) <nl> else : <nl> - cmd = \" apk upgrade \" <nl> + cmd = \" % s upgrade \" % ( APK_PATH ) <nl> rc , stdout , stderr = module . run_command ( cmd , check_rc = False ) <nl> if rc ! = 0 : <nl> module . fail_json ( msg = \" failed to upgrade packages \" ) <nl> def install_packages ( module , names , state ) : <nl> names = \" \" . join ( uninstalled ) <nl> if upgrade : <nl> if module . check_mode : <nl> - cmd = \" apk add - - upgrade - - simulate % s \" % ( names ) <nl> + cmd = \" % s add - - upgrade - - simulate % s \" % ( APK_PATH , names ) <nl> else : <nl> - cmd = \" apk add - - upgrade % s \" % ( names ) <nl> + cmd = \" % s add - - upgrade % s \" % ( APK_PATH , names ) <nl> else : <nl> if module . check_mode : <nl> - cmd = \" apk add - - simulate % s \" % ( names ) <nl> + cmd = \" % s add - - simulate % s \" % ( APK_PATH , names ) <nl> else : <nl> - cmd = \" apk add % s \" % ( names ) <nl> + cmd = \" % s add % s \" % ( APK_PATH , names ) <nl> rc , stdout , stderr = module . run_command ( cmd , check_rc = False ) <nl> if rc ! = 0 : <nl> module . fail_json ( msg = \" failed to install % s \" % ( names ) ) <nl> def remove_packages ( module , names ) : <nl> module . exit_json ( changed = False , msg = \" package ( s ) already removed \" ) <nl> names = \" \" . join ( installed ) <nl> if module . check_mode : <nl> - cmd = \" apk del - - purge - - simulate % s \" % ( names ) <nl> + cmd = \" % s del - - purge - - simulate % s \" % ( APK_PATH , names ) <nl> else : <nl> - cmd = \" apk del - - purge % s \" % ( names ) <nl> + cmd = \" % s del - - purge % s \" % ( APK_PATH , names ) <nl> rc , stdout , stderr = module . run_command ( cmd , check_rc = False ) <nl> if rc ! = 0 : <nl> module . fail_json ( msg = \" failed to remove % s package ( s ) \" % ( names ) ) <nl> def main ( ) : <nl> supports_check_mode = True <nl> ) <nl> <nl> - if not os . path . exists ( APK_PATH ) : <nl> - module . fail_json ( msg = \" cannot find apk , looking for % s \" % ( APK_PATH ) ) <nl> + global APK_PATH <nl> + APK_PATH = module . get_bin_path ( ' apk ' , required = True ) <nl> <nl> p = module . params <nl> <nl>\n", "msg": "Use the module ' s get_bin_path function to find ' apk ' and reuse the return value in all functions\n"}
{"diff_id": 25885, "repo": "mitmproxy/mitmproxy\n", "sha": "2c64b90a3d239c76bca4c334f89d7b53a965d35b\n", "time": "2014-10-31T18:49:45Z\n", "diff": "mmm a / test / test_protocol_http . py <nl> ppp b / test / test_protocol_http . py <nl> def test_stripped_chunked_encoding_no_content ( ) : <nl> <nl> <nl> class TestHTTPRequest : <nl> - def test_asterisk_form ( self ) : <nl> + def test_asterisk_form_in ( self ) : <nl> s = StringIO ( \" OPTIONS * HTTP / 1 . 1 \" ) <nl> f = tutils . tflow ( req = None ) <nl> f . request = HTTPRequest . from_stream ( s ) <nl> def test_asterisk_form ( self ) : <nl> f . request . scheme = \" http \" <nl> assert f . request . assemble ( ) = = \" OPTIONS * HTTP / 1 . 1 \\ r \\ nHost : address : 22 \\ r \\ n \\ r \\ n \" <nl> <nl> - def test_origin_form ( self ) : <nl> + def test_relative_form_in ( self ) : <nl> s = StringIO ( \" GET / foo \\ xff HTTP / 1 . 1 \" ) <nl> tutils . raises ( \" Bad HTTP request line \" , HTTPRequest . from_stream , s ) <nl> s = StringIO ( \" GET / foo HTTP / 1 . 1 \\ r \\ nConnection : Upgrade \\ r \\ nUpgrade : h2c \" ) <nl> def test_origin_form ( self ) : <nl> r . update_host_header ( ) <nl> assert \" Host \" in r . headers <nl> <nl> - <nl> - def test_authority_form ( self ) : <nl> + def test_authority_form_in ( self ) : <nl> s = StringIO ( \" CONNECT oops - no - port . com HTTP / 1 . 1 \" ) <nl> tutils . raises ( \" Bad HTTP request line \" , HTTPRequest . from_stream , s ) <nl> s = StringIO ( \" CONNECT address : 22 HTTP / 1 . 1 \" ) <nl> def test_authority_form ( self ) : <nl> assert r . assemble ( ) = = \" CONNECT address : 22 HTTP / 1 . 1 \\ r \\ nHost : address : 22 \\ r \\ n \\ r \\ n \" <nl> assert r . pretty_url ( False ) = = \" address : 22 \" <nl> <nl> - def test_absolute_form ( self ) : <nl> + def test_absolute_form_in ( self ) : <nl> s = StringIO ( \" GET oops - no - protocol . com HTTP / 1 . 1 \" ) <nl> tutils . raises ( \" Bad HTTP request line \" , HTTPRequest . from_stream , s ) <nl> s = StringIO ( \" GET http : / / address : 22 / HTTP / 1 . 1 \" ) <nl> r = HTTPRequest . from_stream ( s ) <nl> assert r . assemble ( ) = = \" GET http : / / address : 22 / HTTP / 1 . 1 \\ r \\ nHost : address : 22 \\ r \\ n \\ r \\ n \" <nl> <nl> + def test_http_options_relative_form_in ( self ) : <nl> + \" \" \" <nl> + Exercises fix for Issue # xxx . <nl> + \" \" \" <nl> + s = StringIO ( \" OPTIONS / secret / resource HTTP / 1 . 1 \" ) <nl> + r = HTTPRequest . from_stream ( s ) <nl> + r . host = ' address ' <nl> + r . port = 80 <nl> + r . scheme = \" http \" <nl> + assert r . assemble ( ) = = ( \" OPTIONS \" <nl> + \" / secret / resource \" <nl> + \" HTTP / 1 . 1 \\ r \\ nHost : address \\ r \\ n \\ r \\ n \" ) <nl> + <nl> + def test_http_options_absolute_form_in ( self ) : <nl> + s = StringIO ( \" OPTIONS http : / / address / secret / resource HTTP / 1 . 1 \" ) <nl> + r = HTTPRequest . from_stream ( s ) <nl> + r . host = ' address ' <nl> + r . port = 80 <nl> + r . scheme = \" http \" <nl> + assert r . assemble ( ) = = ( \" OPTIONS \" <nl> + \" http : / / address : 80 / secret / resource \" <nl> + \" HTTP / 1 . 1 \\ r \\ nHost : address \\ r \\ n \\ r \\ n \" ) <nl> + <nl> + <nl> def test_assemble_unknown_form ( self ) : <nl> r = tutils . treq ( ) <nl> tutils . raises ( \" Invalid request form \" , r . assemble , \" antiauthority \" ) <nl> def test_relative_request ( self ) : <nl> p . connect ( ) <nl> r = p . request ( \" get : / p / 200 \" ) <nl> assert r . status_code = = 400 <nl> - assert \" Invalid HTTP request form \" in r . content <nl> \\ No newline at end of file <nl> + assert \" Invalid HTTP request form \" in r . content <nl>\n", "msg": "Adding some test coverage for handling HTTP OPTIONS requests .\n"}
{"diff_id": 25950, "repo": "ipython/ipython\n", "sha": "10e40f568e3e5eba31d94897cab95b7abb197cd0\n", "time": "2013-09-20T23:55:09Z\n", "diff": "mmm a / IPython / testing / iptestcontroller . py <nl> ppp b / IPython / testing / iptestcontroller . py <nl> def prepare_py_test_controllers ( inc_slow = False ) : <nl> not_run . append ( controller ) <nl> return to_run , not_run <nl> <nl> - def configure_controllers ( controllers , xunit = False , coverage = False ) : <nl> + def configure_controllers ( controllers , xunit = False , coverage = False , extra_args = ( ) ) : <nl> \" \" \" Apply options for a collection of TestController objects . \" \" \" <nl> for controller in controllers : <nl> if xunit : <nl> controller . add_xunit ( ) <nl> if coverage : <nl> controller . add_coverage ( ) <nl> + controller . cmd . extend ( extra_args ) <nl> <nl> def do_run ( controller ) : <nl> try : <nl> def run_iptestall ( options ) : <nl> coverage : bool or str <nl> Measure code coverage from tests . True will store the raw coverage data , <nl> or pass ' html ' or ' xml ' to get reports . <nl> + <nl> + extra_args : list <nl> + Extra arguments to pass to the test subprocesses , e . g . ' - v ' <nl> \" \" \" <nl> if options . fast ! = 1 : <nl> # If running in parallel , capture output so it doesn ' t get interleaved <nl> def run_iptestall ( options ) : <nl> else : <nl> to_run , not_run = prepare_py_test_controllers ( inc_slow = options . all ) <nl> <nl> - configure_controllers ( to_run , xunit = options . xunit , coverage = options . coverage ) <nl> + configure_controllers ( to_run , xunit = options . xunit , coverage = options . coverage , <nl> + extra_args = options . extra_args ) <nl> <nl> def justify ( ltext , rtext , width = 70 , fill = ' - ' ) : <nl> ltext + = ' ' <nl> def find_code_units ( self , morfs ) : <nl> <nl> <nl> def main ( ) : <nl> + # Arguments after - - should be passed through to nose . Argparse treats <nl> + # everything after - - as regular positional arguments , so we separate them <nl> + # first . <nl> + try : <nl> + ix = sys . argv . index ( ' - - ' ) <nl> + except ValueError : <nl> + to_parse = sys . argv [ 1 : ] <nl> + extra_args = [ ] <nl> + else : <nl> + to_parse = sys . argv [ 1 : ix ] <nl> + extra_args = sys . argv [ ix + 1 : ] <nl> + <nl> parser = argparse . ArgumentParser ( description = ' Run IPython test suite ' ) <nl> parser . add_argument ( ' testgroups ' , nargs = ' * ' , <nl> help = ' Run specified groups of tests . If omitted , run ' <nl> def main ( ) : <nl> help = \" Measure test coverage . Specify ' html ' or \" <nl> \" ' xml ' to get reports . \" ) <nl> <nl> - options = parser . parse_args ( ) <nl> + options = parser . parse_args ( to_parse ) <nl> + options . extra_args = extra_args <nl> <nl> run_iptestall ( options ) <nl> <nl>\n", "msg": "Allow passing extra arguments to iptest through for nose\n"}
{"diff_id": 26077, "repo": "ansible/ansible\n", "sha": "db963449666041414813dfd4486dec9a64ab18de\n", "time": "2014-07-11T18:20:00Z\n", "diff": "mmm a / lib / ansible / utils / __init__ . py <nl> ppp b / lib / ansible / utils / __init__ . py <nl> def _load_vars_from_path ( path , results , vault_password = None ) : <nl> # regular file <nl> elif stat . S_ISREG ( pathstat . st_mode ) : <nl> data = parse_yaml_from_file ( path , vault_password = vault_password ) <nl> - if type ( data ) ! = dict : <nl> + if data and type ( data ) ! = dict : <nl> raise errors . AnsibleError ( <nl> \" % s must be stored as a dictionary / hash \" % path ) <nl> + elif data is None : <nl> + data = { } <nl> <nl> # combine vars overrides by default but can be configured to do a <nl> # hash merge in settings <nl>\n", "msg": "Re - fixing ability to have empty json files after inventory refactoring\n"}
{"diff_id": 26099, "repo": "home-assistant/core\n", "sha": "7dfbff31f5dffb58e7d0adb60c40337ab969d73e\n", "time": "2013-10-13T16:59:13Z\n", "diff": "mmm a / homeassistant / httpinterface . py <nl> ppp b / homeassistant / httpinterface . py <nl> def do_GET ( self ) : # pylint : disable = invalid - name <nl> \" < / form > \" ) ) <nl> <nl> # Describe event bus : <nl> - for category in self . server . eventbus . listeners : <nl> - write ( \" Event { } : { } listeners < br / > \" . format ( category , <nl> - len ( self . server . eventbus . listeners [ category ] ) ) ) <nl> + write ( ( \" < table > < tr > < th > Event < / th > < th > Listeners < / th > < / tr > \" ) ) <nl> + <nl> + for category in sorted ( self . server . eventbus . listeners , <nl> + key = lambda key : key . lower ( ) ) : <nl> + write ( \" < tr > < td > { } < / td > < td > { } < / td > < / tr > \" . <nl> + format ( category , <nl> + len ( self . server . eventbus . listeners [ category ] ) ) ) <nl> <nl> # Form to allow firing events <nl> - write ( ( \" < br / > \" <nl> + write ( ( \" < / table > < br / > \" <nl> \" < form action = ' event / fire ' method = ' POST ' > \" ) ) <nl> <nl> write ( \" < input type = ' hidden ' name = ' api_password ' value = ' { } ' / > \" . <nl>\n", "msg": "Better formatting of events in debug interface\n"}
{"diff_id": 26147, "repo": "ytdl-org/youtube-dl\n", "sha": "d391b7e23d3d6c2af03c6329b4bf059ec095f33d\n", "time": "2018-06-17T21:54:08Z\n", "diff": "mmm a / youtube_dl / extractor / common . py <nl> ppp b / youtube_dl / extractor / common . py <nl> <nl> compat_cookies , <nl> compat_etree_fromstring , <nl> compat_getpass , <nl> + compat_integer_types , <nl> compat_http_client , <nl> compat_os_name , <nl> compat_str , <nl> def ie_key ( cls ) : <nl> def IE_NAME ( self ) : <nl> return compat_str ( type ( self ) . __name__ [ : - 2 ] ) <nl> <nl> - def _request_webpage ( self , url_or_request , video_id , note = None , errnote = None , fatal = True , data = None , headers = { } , query = { } ) : <nl> - \" \" \" Returns the response handle \" \" \" <nl> + @ staticmethod <nl> + def __can_accept_status_code ( err , expected_status ) : <nl> + assert isinstance ( err , compat_urllib_error . HTTPError ) <nl> + if expected_status is None : <nl> + return False <nl> + if isinstance ( expected_status , compat_integer_types ) : <nl> + return err . code = = expected_status <nl> + elif isinstance ( expected_status , ( list , tuple ) ) : <nl> + return err . code in expected_status <nl> + elif callable ( expected_status ) : <nl> + return expected_status ( err . code ) is True <nl> + else : <nl> + assert False <nl> + <nl> + def _request_webpage ( self , url_or_request , video_id , note = None , errnote = None , fatal = True , data = None , headers = { } , query = { } , expected_status = None ) : <nl> + \" \" \" <nl> + Return the response handle . <nl> + <nl> + See _download_webpage docstring for arguments specification . <nl> + \" \" \" <nl> if note is None : <nl> self . report_download_webpage ( video_id ) <nl> elif note is not False : <nl> def _request_webpage ( self , url_or_request , video_id , note = None , errnote = None , fa <nl> try : <nl> return self . _downloader . urlopen ( url_or_request ) <nl> except ( compat_urllib_error . URLError , compat_http_client . HTTPException , socket . error ) as err : <nl> + if isinstance ( err , compat_urllib_error . HTTPError ) : <nl> + if self . __can_accept_status_code ( err , expected_status ) : <nl> + return err . fp <nl> + <nl> if errnote is False : <nl> return False <nl> if errnote is None : <nl> def _request_webpage ( self , url_or_request , video_id , note = None , errnote = None , fa <nl> self . _downloader . report_warning ( errmsg ) <nl> return False <nl> <nl> - def _download_webpage_handle ( self , url_or_request , video_id , note = None , errnote = None , fatal = True , encoding = None , data = None , headers = { } , query = { } ) : <nl> - \" \" \" Returns a tuple ( page content as string , URL handle ) \" \" \" <nl> + def _download_webpage_handle ( self , url_or_request , video_id , note = None , errnote = None , fatal = True , encoding = None , data = None , headers = { } , query = { } , expected_status = None ) : <nl> + \" \" \" <nl> + Return a tuple ( page content as string , URL handle ) . <nl> + <nl> + See _download_webpage docstring for arguments specification . <nl> + \" \" \" <nl> # Strip hashes from the URL ( # 1038 ) <nl> if isinstance ( url_or_request , ( compat_str , str ) ) : <nl> url_or_request = url_or_request . partition ( ' # ' ) [ 0 ] <nl> <nl> - urlh = self . _request_webpage ( url_or_request , video_id , note , errnote , fatal , data = data , headers = headers , query = query ) <nl> + urlh = self . _request_webpage ( url_or_request , video_id , note , errnote , fatal , data = data , headers = headers , query = query , expected_status = expected_status ) <nl> if urlh is False : <nl> assert not fatal <nl> return False <nl> def _webpage_read_content ( self , urlh , url_or_request , video_id , note = None , errno <nl> <nl> return content <nl> <nl> - def _download_webpage ( self , url_or_request , video_id , note = None , errnote = None , fatal = True , tries = 1 , timeout = 5 , encoding = None , data = None , headers = { } , query = { } ) : <nl> - \" \" \" Returns the data of the page as a string \" \" \" <nl> + def _download_webpage ( <nl> + self , url_or_request , video_id , note = None , errnote = None , <nl> + fatal = True , tries = 1 , timeout = 5 , encoding = None , data = None , <nl> + headers = { } , query = { } , expected_status = None ) : <nl> + \" \" \" <nl> + Return the data of the page as a string . <nl> + <nl> + Arguments : <nl> + url_or_request - - plain text URL as a string or <nl> + a compat_urllib_request . Requestobject <nl> + video_id - - Video / playlist / item identifier ( string ) <nl> + <nl> + Keyword arguments : <nl> + note - - note printed before downloading ( string ) <nl> + errnote - - note printed in case of an error ( string ) <nl> + fatal - - flag denoting whether error should be considered fatal , <nl> + i . e . whether it should cause ExtractionError to be raised , <nl> + otherwise a warning will be reported and extraction continued <nl> + tries - - number of tries <nl> + timeout - - sleep interval between tries <nl> + encoding - - encoding for a page content decoding , guessed automatically <nl> + when not explicitly specified <nl> + data - - POST data ( bytes ) <nl> + headers - - HTTP headers ( dict ) <nl> + query - - URL query ( dict ) <nl> + expected_status - - allows to accept failed HTTP requests ( non 2xx <nl> + status code ) by explicitly specifying a set of accepted status <nl> + codes . Can be any of the following entities : <nl> + - an integer type specifying an exact failed status code to <nl> + accept <nl> + - a list or a tuple of integer types specifying a list of <nl> + failed status codes to accept <nl> + - a callable accepting an actual failed status code and <nl> + returning True if it should be accepted <nl> + Note that this argument does not affect success status codes ( 2xx ) <nl> + which are always accepted . <nl> + \" \" \" <nl> + <nl> success = False <nl> try_count = 0 <nl> while success is False : <nl> try : <nl> - res = self . _download_webpage_handle ( url_or_request , video_id , note , errnote , fatal , encoding = encoding , data = data , headers = headers , query = query ) <nl> + res = self . _download_webpage_handle ( <nl> + url_or_request , video_id , note , errnote , fatal , <nl> + encoding = encoding , data = data , headers = headers , query = query , <nl> + expected_status = expected_status ) <nl> success = True <nl> except compat_http_client . IncompleteRead as e : <nl> try_count + = 1 <nl> def _download_webpage ( self , url_or_request , video_id , note = None , errnote = None , f <nl> def _download_xml_handle ( <nl> self , url_or_request , video_id , note = ' Downloading XML ' , <nl> errnote = ' Unable to download XML ' , transform_source = None , <nl> - fatal = True , encoding = None , data = None , headers = { } , query = { } ) : <nl> - \" \" \" Return a tuple ( xml as an xml . etree . ElementTree . Element , URL handle ) \" \" \" <nl> + fatal = True , encoding = None , data = None , headers = { } , query = { } , <nl> + expected_status = None ) : <nl> + \" \" \" <nl> + Return a tuple ( xml as an xml . etree . ElementTree . Element , URL handle ) . <nl> + <nl> + See _download_webpage docstring for arguments specification . <nl> + \" \" \" <nl> res = self . _download_webpage_handle ( <nl> url_or_request , video_id , note , errnote , fatal = fatal , <nl> - encoding = encoding , data = data , headers = headers , query = query ) <nl> + encoding = encoding , data = data , headers = headers , query = query , <nl> + expected_status = expected_status ) <nl> if res is False : <nl> return res <nl> xml_string , urlh = res <nl> def _download_xml_handle ( <nl> xml_string , video_id , transform_source = transform_source , <nl> fatal = fatal ) , urlh <nl> <nl> - def _download_xml ( self , url_or_request , video_id , <nl> - note = ' Downloading XML ' , errnote = ' Unable to download XML ' , <nl> - transform_source = None , fatal = True , encoding = None , <nl> - data = None , headers = { } , query = { } ) : <nl> - \" \" \" Return the xml as an xml . etree . ElementTree . Element \" \" \" <nl> + def _download_xml ( <nl> + self , url_or_request , video_id , <nl> + note = ' Downloading XML ' , errnote = ' Unable to download XML ' , <nl> + transform_source = None , fatal = True , encoding = None , <nl> + data = None , headers = { } , query = { } , expected_status = None ) : <nl> + \" \" \" <nl> + Return the xml as an xml . etree . ElementTree . Element . <nl> + <nl> + See _download_webpage docstring for arguments specification . <nl> + \" \" \" <nl> res = self . _download_xml_handle ( <nl> url_or_request , video_id , note = note , errnote = errnote , <nl> transform_source = transform_source , fatal = fatal , encoding = encoding , <nl> - data = data , headers = headers , query = query ) <nl> + data = data , headers = headers , query = query , <nl> + expected_status = expected_status ) <nl> return res if res is False else res [ 0 ] <nl> <nl> def _parse_xml ( self , xml_string , video_id , transform_source = None , fatal = True ) : <nl> def _parse_xml ( self , xml_string , video_id , transform_source = None , fatal = True ) : <nl> def _download_json_handle ( <nl> self , url_or_request , video_id , note = ' Downloading JSON metadata ' , <nl> errnote = ' Unable to download JSON metadata ' , transform_source = None , <nl> - fatal = True , encoding = None , data = None , headers = { } , query = { } ) : <nl> - \" \" \" Return a tuple ( JSON object , URL handle ) \" \" \" <nl> + fatal = True , encoding = None , data = None , headers = { } , query = { } , <nl> + expected_status = None ) : <nl> + \" \" \" <nl> + Return a tuple ( JSON object , URL handle ) . <nl> + <nl> + See _download_webpage docstring for arguments specification . <nl> + \" \" \" <nl> res = self . _download_webpage_handle ( <nl> url_or_request , video_id , note , errnote , fatal = fatal , <nl> - encoding = encoding , data = data , headers = headers , query = query ) <nl> + encoding = encoding , data = data , headers = headers , query = query , <nl> + expected_status = expected_status ) <nl> if res is False : <nl> return res <nl> json_string , urlh = res <nl> def _download_json_handle ( <nl> def _download_json ( <nl> self , url_or_request , video_id , note = ' Downloading JSON metadata ' , <nl> errnote = ' Unable to download JSON metadata ' , transform_source = None , <nl> - fatal = True , encoding = None , data = None , headers = { } , query = { } ) : <nl> + fatal = True , encoding = None , data = None , headers = { } , query = { } , <nl> + expected_status = None ) : <nl> + \" \" \" <nl> + Return the JSON object as a dict . <nl> + <nl> + See _download_webpage docstring for arguments specification . <nl> + \" \" \" <nl> res = self . _download_json_handle ( <nl> url_or_request , video_id , note = note , errnote = errnote , <nl> transform_source = transform_source , fatal = fatal , encoding = encoding , <nl> - data = data , headers = headers , query = query ) <nl> + data = data , headers = headers , query = query , <nl> + expected_status = expected_status ) <nl> return res if res is False else res [ 0 ] <nl> <nl> def _parse_json ( self , json_string , video_id , transform_source = None , fatal = True ) : <nl>\n", "msg": "[ extractor / common ] Introduce expected_status for convenient accept of failed HTTP requests\n"}
{"diff_id": 26457, "repo": "home-assistant/core\n", "sha": "4e7b35355dcbf0193804a58ed28af0ec585c5088\n", "time": "2020-01-01T23:15:29Z\n", "diff": "mmm a / tests / components / websocket_api / test_init . py <nl> ppp b / tests / components / websocket_api / test_init . py <nl> <nl> \" \" \" Tests for the Home Assistant Websocket API . \" \" \" <nl> - import asyncio <nl> from unittest . mock import Mock , patch <nl> <nl> from aiohttp import WSMsgType <nl> def mock_low_queue ( ) : <nl> yield <nl> <nl> <nl> - @ asyncio . coroutine <nl> - def test_invalid_message_format ( websocket_client ) : <nl> + async def test_invalid_message_format ( websocket_client ) : <nl> \" \" \" Test sending invalid JSON . \" \" \" <nl> - yield from websocket_client . send_json ( { \" type \" : 5 } ) <nl> + await websocket_client . send_json ( { \" type \" : 5 } ) <nl> <nl> - msg = yield from websocket_client . receive_json ( ) <nl> + msg = await websocket_client . receive_json ( ) <nl> <nl> assert msg [ \" type \" ] = = const . TYPE_RESULT <nl> error = msg [ \" error \" ] <nl> def test_invalid_message_format ( websocket_client ) : <nl> assert error [ \" message \" ] . startswith ( \" Message incorrectly formatted \" ) <nl> <nl> <nl> - @ asyncio . coroutine <nl> - def test_invalid_json ( websocket_client ) : <nl> + async def test_invalid_json ( websocket_client ) : <nl> \" \" \" Test sending invalid JSON . \" \" \" <nl> - yield from websocket_client . send_str ( \" this is not JSON \" ) <nl> + await websocket_client . send_str ( \" this is not JSON \" ) <nl> <nl> - msg = yield from websocket_client . receive ( ) <nl> + msg = await websocket_client . receive ( ) <nl> <nl> assert msg . type = = WSMsgType . close <nl> <nl> <nl> - @ asyncio . coroutine <nl> - def test_quiting_hass ( hass , websocket_client ) : <nl> + async def test_quiting_hass ( hass , websocket_client ) : <nl> \" \" \" Test sending invalid JSON . \" \" \" <nl> with patch . object ( hass . loop , \" stop \" ) : <nl> - yield from hass . async_stop ( ) <nl> + await hass . async_stop ( ) <nl> <nl> - msg = yield from websocket_client . receive ( ) <nl> + msg = await websocket_client . receive ( ) <nl> <nl> assert msg . type = = WSMsgType . CLOSE <nl> <nl> <nl> - @ asyncio . coroutine <nl> - def test_pending_msg_overflow ( hass , mock_low_queue , websocket_client ) : <nl> + async def test_pending_msg_overflow ( hass , mock_low_queue , websocket_client ) : <nl> \" \" \" Test get_panels command . \" \" \" <nl> for idx in range ( 10 ) : <nl> - yield from websocket_client . send_json ( { \" id \" : idx + 1 , \" type \" : \" ping \" } ) <nl> - msg = yield from websocket_client . receive ( ) <nl> + await websocket_client . send_json ( { \" id \" : idx + 1 , \" type \" : \" ping \" } ) <nl> + msg = await websocket_client . receive ( ) <nl> assert msg . type = = WSMsgType . close <nl> <nl> <nl> - @ asyncio . coroutine <nl> - def test_unknown_command ( websocket_client ) : <nl> + async def test_unknown_command ( websocket_client ) : <nl> \" \" \" Test get_panels command . \" \" \" <nl> - yield from websocket_client . send_json ( { \" id \" : 5 , \" type \" : \" unknown_command \" } ) <nl> + await websocket_client . send_json ( { \" id \" : 5 , \" type \" : \" unknown_command \" } ) <nl> <nl> - msg = yield from websocket_client . receive_json ( ) <nl> + msg = await websocket_client . receive_json ( ) <nl> assert not msg [ \" success \" ] <nl> assert msg [ \" error \" ] [ \" code \" ] = = const . ERR_UNKNOWN_COMMAND <nl> <nl>\n", "msg": "Migrate websocket_api tests from coroutine to async / await ( )\n"}
{"diff_id": 26695, "repo": "hankcs/HanLP\n", "sha": "e6bc8e0fb60200ebcd1011bae9df60a96a7e5a0a\n", "time": "2020-01-03T20:47:49Z\n", "diff": "mmm a / hanlp / components / parsers / conll . py <nl> ppp b / hanlp / components / parsers / conll . py <nl> class CoNLL_SDP_Transform ( CoNLLTransform ) : <nl> def __init__ ( self , config : SerializableDict = None , map_x = True , map_y = True , lower = True , n_buckets = 32 , <nl> n_tokens_per_batch = 5000 , min_freq = 2 , * * kwargs ) - > None : <nl> super ( ) . __init__ ( config , map_x , map_y , lower , n_buckets , n_tokens_per_batch , min_freq , * * kwargs ) <nl> + self . orphan_relation = ROOT <nl> + <nl> + def lock_vocabs ( self ) : <nl> + super ( ) . lock_vocabs ( ) <nl> + # heuristic to find the orphan relation <nl> + for rel in self . rel_vocab . idx_to_token : <nl> + if ' root ' in rel . lower ( ) : <nl> + self . orphan_relation = rel <nl> + break <nl> <nl> def file_to_inputs ( self , filepath : str , gold = True ) : <nl> assert gold , ' only support gold file for now ' <nl> def Y_to_outputs ( self , Y : Union [ tf . Tensor , Tuple [ tf . Tensor ] ] , gold = False , inputs <nl> ar . append ( ( idx + 1 , self . rel_vocab . idx_to_token [ r ] ) ) <nl> if not ar : <nl> # orphan <nl> - ar . append ( ( 0 , self . rel_vocab . idx_to_token [ 2 ] ) ) <nl> + ar . append ( ( 0 , self . orphan_relation ) ) <nl> sent . append ( ar ) <nl> sents . append ( sent ) <nl> <nl>\n", "msg": "heuristic to find orphan relation in sdp vocab\n"}
{"diff_id": 26701, "repo": "ansible/ansible\n", "sha": "f80e766d979a2bb469fa799db2aadad4ef3df1e4\n", "time": "2014-11-29T23:11:10Z\n", "diff": "mmm a / setup . py <nl> ppp b / setup . py <nl> <nl> package_dir = { ' ansible ' : ' lib / ansible ' } , <nl> packages = find_packages ( ' lib ' ) , <nl> package_data = { <nl> - ' ' : [ ' module_utils / * . ps1 ' , ' modules / core / windows / * . ps1 ' ] , <nl> + ' ' : [ ' module_utils / * . ps1 ' , ' modules / core / windows / * . ps1 ' , ' modules / extras / windows / * . ps1 ' ] , <nl> } , <nl> scripts = [ <nl> ' bin / ansible ' , <nl>\n", "msg": "Need to include extras in setup to accomodate future windows extras modules\n"}
{"diff_id": 26747, "repo": "ansible/ansible\n", "sha": "52d769d36c0f143726224117a57c523ca5664d60\n", "time": "2016-12-08T16:22:40Z\n", "diff": "mmm a / lib / ansible / modules / source_control / hg . py <nl> ppp b / lib / ansible / modules / source_control / hg . py <nl> <nl> aliases : [ version ] <nl> force : <nl> description : <nl> - - Discards uncommitted changes . Runs C ( hg update - C ) . <nl> + - Discards uncommitted changes . Runs C ( hg update - C ) . Prior to <nl> + 1 . 9 , the default was ` yes ` . <nl> required : false <nl> - default : \" yes \" <nl> + default : \" no \" <nl> choices : [ \" yes \" , \" no \" ] <nl> purge : <nl> description : <nl> def main ( ) : <nl> repo = dict ( required = True , aliases = [ ' name ' ] ) , <nl> dest = dict ( required = True ) , <nl> revision = dict ( default = None , aliases = [ ' version ' ] ) , <nl> - force = dict ( default = ' yes ' , type = ' bool ' ) , <nl> + force = dict ( default = ' no ' , type = ' bool ' ) , <nl> purge = dict ( default = ' no ' , type = ' bool ' ) , <nl> executable = dict ( default = None ) , <nl> ) , <nl>\n", "msg": "Reverse the force parameter for the hg module\n"}
{"diff_id": 26899, "repo": "python/cpython\n", "sha": "9fa9a0d635b9fee1ba7b643869cb7b70e9809716\n", "time": "2009-12-10T10:27:09Z\n", "diff": "mmm a / Lib / distutils / sysconfig . py <nl> ppp b / Lib / distutils / sysconfig . py <nl> def get_config_vars ( * args ) : <nl> # are in CFLAGS or LDFLAGS and remove them if they are . <nl> # This is needed when building extensions on a 10 . 3 system <nl> # using a universal build of python . <nl> - for key in ( ' LDFLAGS ' , ' BASECFLAGS ' , <nl> + for key in ( ' LDFLAGS ' , ' BASECFLAGS ' , ' LDSHARED ' , <nl> # a number of derived variables . These need to be <nl> # patched up as well . <nl> ' CFLAGS ' , ' PY_CFLAGS ' , ' BLDSHARED ' ) : <nl> def get_config_vars ( * args ) : <nl> <nl> if ' ARCHFLAGS ' in os . environ : <nl> arch = os . environ [ ' ARCHFLAGS ' ] <nl> - for key in ( ' LDFLAGS ' , ' BASECFLAGS ' , <nl> + for key in ( ' LDFLAGS ' , ' BASECFLAGS ' , ' LDSHARED ' , <nl> # a number of derived variables . These need to be <nl> # patched up as well . <nl> ' CFLAGS ' , ' PY_CFLAGS ' , ' BLDSHARED ' ) : <nl> def get_config_vars ( * args ) : <nl> if m is not None : <nl> sdk = m . group ( 1 ) <nl> if not os . path . exists ( sdk ) : <nl> - for key in ( ' LDFLAGS ' , ' BASECFLAGS ' , <nl> + for key in ( ' LDFLAGS ' , ' BASECFLAGS ' , ' LDSHARED ' , <nl> # a number of derived variables . These need to be <nl> # patched up as well . <nl> ' CFLAGS ' , ' PY_CFLAGS ' , ' BLDSHARED ' ) : <nl>\n", "msg": "Fix an issue with the detection of a non - existing SDK\n"}
{"diff_id": 26964, "repo": "python/cpython\n", "sha": "e676c5ef3ed8a34431f1c30cf91ef67ab70c6d4e\n", "time": "2003-07-21T18:43:33Z\n", "diff": "mmm a / Lib / bsddb / test / test_join . py <nl> ppp b / Lib / bsddb / test / test_join . py <nl> def test01_join ( self ) : <nl> try : <nl> # lets look up all of the red Products <nl> sCursor = secDB . cursor ( ) <nl> - assert sCursor . set ( ' red ' ) <nl> + # Don ' t do the . set ( ) in an assert , or you can get a bogus failure <nl> + # when running python - O <nl> + tmp = sCursor . set ( ' red ' ) <nl> + assert tmp <nl> <nl> # FIXME : jCursor doesn ' t properly hold a reference to its <nl> # cursors , if they are closed before jcursor is used it <nl>\n", "msg": "test01_join ( ) : Fix a test failure when run with \" python - O \" . The\n"}
{"diff_id": 27148, "repo": "ansible/ansible\n", "sha": "2898e000a0b993d5c4445c04c713f6f857ea88e1\n", "time": "2015-09-23T15:13:12Z\n", "diff": "mmm a / lib / ansible / plugins / connection / ssh . py <nl> ppp b / lib / ansible / plugins / connection / ssh . py <nl> def _run ( self , cmd , in_data , sudoable = True ) : <nl> state + = 1 <nl> <nl> while True : <nl> - rfd , wfd , efd = select . select ( rpipes , [ ] , rpipes , timeout ) <nl> + rfd , wfd , efd = select . select ( rpipes , [ ] , rpipes , 0 . 1 ) <nl> <nl> # We pay attention to timeouts only while negotiating a prompt . <nl> <nl>\n", "msg": "Don ' t use the connection timeout for the select poll timeout\n"}
{"diff_id": 27531, "repo": "zulip/zulip\n", "sha": "9582a83d10376cc918d8d5170d334d8ad06e6644\n", "time": "2016-08-11T22:21:30Z\n", "diff": "mmm a / zerver / management / commands / export_single_user . py <nl> ppp b / zerver / management / commands / export_single_user . py <nl> class Command ( BaseCommand ) : <nl> \" \" \" <nl> <nl> def add_arguments ( self , parser ) : <nl> - parser . add_argument ( ' email ' , metavar = ' < realm > ' , type = str , <nl> + parser . add_argument ( ' email ' , metavar = ' < email > ' , type = str , <nl> help = \" email of user to export \" ) <nl> parser . add_argument ( ' - - output ' , <nl> dest = ' output_dir ' , <nl>\n", "msg": "export : Fix usage in export_single_user command .\n"}
{"diff_id": 27963, "repo": "python/cpython\n", "sha": "ff4a23bbcb08e7dbb48fa8b5cfbafaea11e1f8c7\n", "time": "2000-12-04T16:29:13Z\n", "diff": "mmm a / Lib / ConfigParser . py <nl> ppp b / Lib / ConfigParser . py <nl> def remove_option ( self , section , option ) : <nl> sectdict = self . __sections [ section ] <nl> except KeyError : <nl> raise NoSectionError ( section ) <nl> - existed = sectdict . has_key ( key ) <nl> + existed = sectdict . has_key ( option ) <nl> if existed : <nl> - del sectdict [ key ] <nl> + del sectdict [ option ] <nl> return existed <nl> <nl> def remove_section ( self , section ) : <nl>\n", "msg": "remove_option ( ) : Use the right variable name for the option name !\n"}
{"diff_id": 28179, "repo": "3b1b/manim\n", "sha": "54d43313bed4aa3dae3bfc3a74d5d0c7fb9a529b\n", "time": "2018-12-29T07:06:14Z\n", "diff": "mmm a / manimlib / extract_scene . py <nl> ppp b / manimlib / extract_scene . py <nl> def get_scene_classes ( scene_names_to_classes , config ) : <nl> if config [ \" scene_name \" ] in scene_names_to_classes : <nl> return [ scene_names_to_classes [ config [ \" scene_name \" ] ] ] <nl> if config [ \" scene_name \" ] ! = \" \" : <nl> - print ( manimlib . constants . SCENE_NOT_FOUND_MESSAGE ) <nl> - return [ ] <nl> + print ( manimlib . constants . SCENE_NOT_FOUND_MESSAGE , file = sys . stderr ) <nl> + sys . exit ( 2 ) <nl> if config [ \" write_all \" ] : <nl> return list ( scene_names_to_classes . values ( ) ) <nl> return prompt_user_for_choice ( scene_names_to_classes ) <nl>\n", "msg": "print to stderr for scene not found\n"}
{"diff_id": 28279, "repo": "ansible/ansible\n", "sha": "77162e95e49f489a3a62bca68f5027cf8b18ba70\n", "time": "2012-10-29T08:45:44Z\n", "diff": "mmm a / lib / ansible / runner / __init__ . py <nl> ppp b / lib / ansible / runner / __init__ . py <nl> def run ( self ) : <nl> # We aren ' t iterating over all the hosts in this <nl> # group . So , just pick the first host in our group to <nl> # construct the conn object with . <nl> - result_data = self . _executor ( hosts [ 0 ] [ 1 ] ) . result <nl> + result_data = self . _executor ( hosts [ 0 ] ) . result <nl> # Create a ResultData item for each host in this group <nl> # using the returned result . If we didn ' t do this we would <nl> # get false reports of dark hosts . <nl>\n", "msg": "Send a host to runner executor instead of a letter .\n"}
{"diff_id": 28286, "repo": "numpy/numpy\n", "sha": "d2f1e960dfd2dee2e9063f94cafc6e89e4bf5b3c\n", "time": "2006-04-20T15:39:49Z\n", "diff": "mmm a / numpy / distutils / interactive . py <nl> ppp b / numpy / distutils / interactive . py <nl> def show_compilers ( * args ) : <nl> from distutils . ccompiler import show_compilers <nl> show_compilers ( ) <nl> <nl> - def show_tasks ( argv ) : <nl> + def show_tasks ( argv , ccompiler , fcompiler ) : <nl> print \" \" \" \\ <nl> <nl> Tasks : <nl> - i - Show python / platform / machine information <nl> - ie - Show environment information <nl> - f - Show Fortran compilers information <nl> - f < name > - Set Fortran compiler <nl> + i - Show python / platform / machine information <nl> + ie - Show environment information <nl> c - Show C compilers information <nl> - c < name > - Set C compiler <nl> - e - Edit proposed sys . argv [ 1 : ] . <nl> + c < name > - Set C compiler ( current : % s ) <nl> + f - Show Fortran compilers information <nl> + f < name > - Set Fortran compiler ( current : % s ) <nl> + e - Edit proposed sys . argv [ 1 : ] . <nl> <nl> Task aliases : <nl> + 0 - Configure <nl> 1 - Build <nl> 2 - Install <nl> 2 < prefix > - Install with prefix . <nl> def show_tasks ( argv ) : <nl> 5 - Binary distribution <nl> <nl> Proposed sys . argv = % s <nl> - \" \" \" % ( argv ) <nl> + \" \" \" % ( ccompiler , fcompiler , argv ) <nl> <nl> <nl> from exec_command import splitcmdline <nl> <nl> def edit_argv ( * args ) : <nl> argv = args [ 0 ] <nl> - s = raw_input ( ' Edit argv [ % s ] : ' % ( ' ' . join ( argv [ 1 : ] ) ) ) <nl> + readline = args [ 1 ] <nl> + if readline is not None : <nl> + readline . add_history ( ' ' . join ( argv [ 1 : ] ) ) <nl> + try : <nl> + s = raw_input ( ' Edit argv [ UpArrow to retrive % r ] : ' % ( ' ' . join ( argv [ 1 : ] ) ) ) <nl> + except EOFError : <nl> + return <nl> if s : <nl> argv [ 1 : ] = splitcmdline ( s ) <nl> return <nl> def interactive_sys_argv ( argv ) : <nl> print ' Starting interactive session ' <nl> print ' - ' * 72 <nl> <nl> + readline = None <nl> try : <nl> try : <nl> import readline <nl> def interactive_sys_argv ( argv ) : <nl> f_compiler_name = None <nl> <nl> while 1 : <nl> - show_tasks ( argv ) <nl> + show_tasks ( argv , c_compiler_name , f_compiler_name ) <nl> try : <nl> task = raw_input ( ' Choose a task ( ^ D to quit , Enter to continue with setup ) : ' ) . lower ( ) <nl> except EOFError : <nl> def interactive_sys_argv ( argv ) : <nl> if task_func is None : <nl> if task [ 0 ] = = ' c ' : <nl> c_compiler_name = task [ 1 : ] <nl> + if c_compiler_name = = ' none ' : <nl> + c_compiler_name = None <nl> if task [ 0 ] = = ' f ' : <nl> f_compiler_name = task [ 1 : ] <nl> + if f_compiler_name = = ' none ' : <nl> + f_compiler_name = None <nl> if task [ 0 ] = = ' 2 ' and len ( task ) > 1 : <nl> prefix = task [ 1 : ] <nl> task = task [ 0 ] <nl> def interactive_sys_argv ( argv ) : <nl> prefix = None <nl> if task = = ' 4 ' : <nl> argv [ 1 : ] = [ ' sdist ' , ' - f ' ] <nl> - elif task in ' 1235 ' : <nl> + elif task in ' 01235 ' : <nl> cmd_opts = { ' config ' : [ ] , ' config_fc ' : [ ] , <nl> ' build_ext ' : [ ] , ' build_src ' : [ ] , <nl> ' build_clib ' : [ ] } <nl> if c_compiler_name is not None : <nl> c = ' - - compiler = % s ' % ( c_compiler_name ) <nl> cmd_opts [ ' config ' ] . append ( c ) <nl> - cmd_opts [ ' build_ext ' ] . append ( c ) <nl> - cmd_opts [ ' build_clib ' ] . append ( c ) <nl> + if task ! = ' 0 ' : <nl> + cmd_opts [ ' build_ext ' ] . append ( c ) <nl> + cmd_opts [ ' build_clib ' ] . append ( c ) <nl> if f_compiler_name is not None : <nl> c = ' - - fcompiler = % s ' % ( f_compiler_name ) <nl> cmd_opts [ ' config_fc ' ] . append ( c ) <nl> - cmd_opts [ ' build_ext ' ] . append ( c ) <nl> - cmd_opts [ ' build_clib ' ] . append ( c ) <nl> + if task ! = ' 0 ' : <nl> + cmd_opts [ ' build_ext ' ] . append ( c ) <nl> + cmd_opts [ ' build_clib ' ] . append ( c ) <nl> if task = = ' 3 ' : <nl> cmd_opts [ ' build_ext ' ] . append ( ' - - inplace ' ) <nl> cmd_opts [ ' build_src ' ] . append ( ' - - inplace ' ) <nl> def interactive_sys_argv ( argv ) : <nl> for k , opts in cmd_opts . items ( ) : <nl> if opts : <nl> conf . extend ( [ k ] + opts ) <nl> - if task = = ' 1 ' : <nl> + if task = = ' 0 ' : <nl> + if ' config ' not in conf : <nl> + conf . append ( ' config ' ) <nl> + argv [ 1 : ] = conf <nl> + elif task = = ' 1 ' : <nl> argv [ 1 : ] = conf + [ ' build ' ] <nl> elif task = = ' 2 ' : <nl> if prefix is not None : <nl> def interactive_sys_argv ( argv ) : <nl> else : <nl> print ' - ' * 68 <nl> try : <nl> - task_func ( argv ) <nl> + task_func ( argv , readline ) <nl> except Exception , msg : <nl> print ' Failed running task % s : % s ' % ( task , msg ) <nl> break <nl>\n", "msg": "Added configure alias to interactive sys . argv setter , number of minor improvements .\n"}
{"diff_id": 28438, "repo": "zulip/zulip\n", "sha": "92978d6fb22d623631a3edcb2386ce0e9b02345f\n", "time": "2017-04-28T23:15:07Z\n", "diff": "mmm a / analytics / management / commands / update_analytics_counts . py <nl> ppp b / analytics / management / commands / update_analytics_counts . py <nl> def add_arguments ( self , parser ) : <nl> help = ' Update stat tables from current state to - - time . Defaults to the current time . ' , <nl> default = timezone_now ( ) . isoformat ( ) ) <nl> parser . add_argument ( ' - - utc ' , <nl> - type = bool , <nl> + action = ' store_true ' , <nl> help = \" Interpret - - time in UTC . \" , <nl> default = False ) <nl> parser . add_argument ( ' - - stat ' , ' - s ' , <nl>\n", "msg": "analytics : Fix - - utc argument in update_analytics_counts . py .\n"}
{"diff_id": 28595, "repo": "docker/compose\n", "sha": "5fdb75b541cee7a6a26e7b9e5a6483afebc88174\n", "time": "2015-10-19T19:16:19Z\n", "diff": "mmm a / compose / config / validation . py <nl> ppp b / compose / config / validation . py <nl> def _parse_valid_types_from_validator ( validator ) : <nl> if len ( validator ) > = 2 : <nl> first_type = anglicize_validator ( validator [ 0 ] ) <nl> last_type = anglicize_validator ( validator [ - 1 ] ) <nl> - types_from_validator = \" { } { } \" . format ( first_type , \" , \" . join ( validator [ 1 : - 1 ] ) ) <nl> + types_from_validator = \" { } \" . format ( \" , \" . join ( [ first_type ] + validator [ 1 : - 1 ] ) ) <nl> <nl> msg = \" { } or { } \" . format ( <nl> types_from_validator , <nl> def _parse_oneof_validator ( error ) : <nl> Inspecting the context value of a ValidationError gives us information about <nl> which sub schema failed and which kind of error it is . <nl> \" \" \" <nl> - <nl> required = [ context for context in error . context if context . validator = = ' required ' ] <nl> if required : <nl> return required [ 0 ] . message <nl>\n", "msg": "Improve error message for type constraints\n"}
{"diff_id": 28632, "repo": "ansible/ansible\n", "sha": "30a38f94ce81314019fb2d9496602eef0a5b36fe\n", "time": "2016-04-22T18:21:14Z\n", "diff": "mmm a / lib / ansible / playbook / base . py <nl> ppp b / lib / ansible / playbook / base . py <nl> def post_validate ( self , templar ) : <nl> if isinstance ( value , string_types ) and ' % ' in value : <nl> value = value . replace ( ' % ' , ' ' ) <nl> value = float ( value ) <nl> - elif attribute . isa = = ' list ' : <nl> + elif attribute . isa in ( ' list ' , ' barelist ' ) : <nl> if value is None : <nl> value = [ ] <nl> elif not isinstance ( value , list ) : <nl> - if isinstance ( value , string_types ) : <nl> + if isinstance ( value , string_types ) and attribute . isa = = ' barelist ' : <nl> + display . deprecated ( <nl> + \" Using comma separated values for a list has been deprecated . \" \\ <nl> + \" You should instead use the correct YAML syntax for lists . \" \\ <nl> + ) <nl> value = value . split ( ' , ' ) <nl> else : <nl> value = [ value ] <nl>\n", "msg": "Create a special class of list FieldAttribute for splitting on commas\n"}
{"diff_id": 29013, "repo": "ansible/ansible\n", "sha": "8a7a81904093862fc424c1040d3ad22d76bae4d9\n", "time": "2018-10-17T20:28:20Z\n", "diff": "mmm a / lib / ansible / plugins / inventory / netbox . py <nl> ppp b / lib / ansible / plugins / inventory / netbox . py <nl> <nl> - device_roles <nl> - device_types <nl> - manufacturers <nl> + - platforms <nl> default : [ ] <nl> query_filters : <nl> description : List of parameters passed to the query string ( Multiple values may be separated by commas ) <nl> def group_extractors ( self ) : <nl> \" memory \" : self . extract_memory , <nl> \" vcpus \" : self . extract_vcpus , <nl> \" device_roles \" : self . extract_device_role , <nl> + \" platforms \" : self . extract_platform , <nl> \" device_types \" : self . extract_device_type , <nl> \" manufacturers \" : self . extract_manufacturer <nl> } <nl> def extract_vcpus ( self , host ) : <nl> def extract_memory ( self , host ) : <nl> return host . get ( \" memory \" ) <nl> <nl> + def extract_platform ( self , host ) : <nl> + try : <nl> + return self . platforms_lookup [ host [ \" platform \" ] [ \" id \" ] ] <nl> + except Exception : <nl> + return <nl> + <nl> def extract_device_type ( self , host ) : <nl> try : <nl> return [ self . device_types_lookup [ host [ \" device_type \" ] [ \" id \" ] ] ] <nl> def extract_primary_ip6 ( self , host ) : <nl> def extract_tags ( self , host ) : <nl> return host [ \" tags \" ] <nl> <nl> + def refresh_platforms_lookup ( self ) : <nl> + url = urljoin ( self . api_endpoint , \" / api / dcim / platforms / ? limit = 0 \" ) <nl> + platforms = self . get_resource_list ( api_url = url ) <nl> + self . platforms_lookup = dict ( ( platform [ \" id \" ] , platform [ \" name \" ] ) for platform in platforms ) <nl> + <nl> def refresh_sites_lookup ( self ) : <nl> url = urljoin ( self . api_endpoint , \" / api / dcim / sites / ? limit = 0 \" ) <nl> sites = self . get_resource_list ( api_url = url ) <nl>\n", "msg": "Add support for platform grouping ( )\n"}
{"diff_id": 29043, "repo": "python/cpython\n", "sha": "1c92a76a69a4024316ddc6d20245b70ad830ade7\n", "time": "2016-11-06T16:25:54Z\n", "diff": "mmm a / Lib / test / test_fstring . py <nl> ppp b / Lib / test / test_fstring . py <nl> def test_no_backslashes_in_expression_part ( self ) : <nl> ] ) <nl> <nl> def test_no_escapes_for_braces ( self ) : <nl> - # \\ x7b is ' { ' . Make sure it doesn ' t start an expression . <nl> - self . assertEqual ( f ' \\ x7b2 } } ' , ' { 2 } ' ) <nl> - self . assertEqual ( f ' \\ x7b2 ' , ' { 2 ' ) <nl> - self . assertEqual ( f ' \\ u007b2 ' , ' { 2 ' ) <nl> - self . assertEqual ( f ' \\ N { LEFT CURLY BRACKET } 2 \\ N { RIGHT CURLY BRACKET } ' , ' { 2 } ' ) <nl> + \" \" \" <nl> + Only literal curly braces begin an expression . <nl> + \" \" \" <nl> + # \\ x7b is ' { ' . <nl> + self . assertEqual ( f ' \\ x7b1 + 1 } } ' , ' { 1 + 1 } ' ) <nl> + self . assertEqual ( f ' \\ x7b1 + 1 ' , ' { 1 + 1 ' ) <nl> + self . assertEqual ( f ' \\ u007b1 + 1 ' , ' { 1 + 1 ' ) <nl> + self . assertEqual ( f ' \\ N { LEFT CURLY BRACKET } 1 + 1 \\ N { RIGHT CURLY BRACKET } ' , ' { 1 + 1 } ' ) <nl> <nl> def test_newlines_in_expressions ( self ) : <nl> self . assertEqual ( f ' { 0 } ' , ' 0 ' ) <nl>\n", "msg": "Update test_no_escapes_for_braces to clarify behavior with a docstring and expressions that clearly are not evaluated .\n"}
{"diff_id": 29064, "repo": "ansible/ansible\n", "sha": "5ff79d05968604f42e63be2ef3c8d5ce84cae262\n", "time": "2016-12-08T16:24:09Z\n", "diff": "mmm a / lib / ansible / modules / network / eos / eos_template . py <nl> ppp b / lib / ansible / modules / network / eos / eos_template . py <nl> <nl> sample : [ ' . . . ' , ' . . . ' ] <nl> \" \" \" <nl> <nl> + import re <nl> + <nl> def compare ( this , other ) : <nl> parents = [ item . text for item in this . parents ] <nl> for entry in other : <nl> def main ( ) : <nl> <nl> commands = flatten ( commands , list ( ) ) <nl> <nl> + # Filter out configuration mode commands followed immediately by an <nl> + # exit command indented by one level only , e . g . <nl> + # - route - map map01 permit 10 <nl> + # - exit <nl> + # <nl> + # Build a temporary list as we filter , then copy the temp list <nl> + # back onto the commands list . <nl> + temp = [ ] <nl> + ind_prev = 999 <nl> + count = 0 <nl> + for c in commands : <nl> + ind_this = c . count ( ' ' ) <nl> + if re . search ( r \" ^ \\ s * exit $ \" , c ) and ind_this = = ind_prev + 1 : <nl> + temp . pop ( ) <nl> + count - = 1 <nl> + if count ! = 0 : <nl> + ind_prev = temp [ - 1 ] . count ( ' ' ) <nl> + continue <nl> + temp . append ( c ) <nl> + ind_prev = ind_this <nl> + count + = 1 <nl> + <nl> + commands = temp <nl> + <nl> if commands : <nl> if not module . check_mode : <nl> commands = [ str ( c ) . strip ( ) for c in commands ] <nl> def main ( ) : <nl> from ansible . module_utils . eos import * <nl> if __name__ = = ' __main__ ' : <nl> main ( ) <nl> - <nl>\n", "msg": "Add filter to eos_template to remove configuration mode command followed immediately by an exit command indented one level .\n"}
{"diff_id": 29134, "repo": "zulip/zulip\n", "sha": "7914bb78c642cd0fecc9d112195853188aa6cd87\n", "time": "2012-11-29T18:57:06Z\n", "diff": "mmm a / api / common . py <nl> ppp b / api / common . py <nl> <nl> # Check that we have a recent enough version <nl> # Older versions don ' t provide the ' json ' attribute on responses . <nl> assert ( requests . __version__ > ' 0 . 12 ' ) <nl> + API_VERSTRING = \" / api / v1 / \" <nl> <nl> class HumbugAPI ( object ) : <nl> def __init__ ( self , email , api_key = None , api_key_file = None , <nl> def __init__ ( self , email , api_key = None , api_key_file = None , <nl> self . client_name = client <nl> <nl> def do_api_query ( self , request , url , longpolling = False ) : <nl> - had_error_retry = False <nl> request [ \" email \" ] = self . email <nl> request [ \" api - key \" ] = self . api_key <nl> request [ \" client \" ] = self . client_name <nl> - failures = 0 <nl> <nl> for ( key , val ) in request . iteritems ( ) : <nl> if not ( isinstance ( val , str ) or isinstance ( val , unicode ) ) : <nl> request [ key ] = simplejson . dumps ( val ) <nl> <nl> + query_state = { <nl> + ' had_error_retry ' : False , <nl> + ' request ' : request , <nl> + ' failures ' : 0 , <nl> + } <nl> + <nl> + def error_retry ( error_string ) : <nl> + if not self . retry_on_errors or query_state [ \" failures \" ] > = 10 : <nl> + return False <nl> + if self . verbose : <nl> + if not query_state [ \" had_error_retry \" ] : <nl> + sys . stdout . write ( \" humbug API ( % s ) : connection error % s - - retrying . \" % \\ <nl> + ( url . split ( API_VERSTRING , 2 ) [ 1 ] , error_string , ) ) <nl> + query_state [ \" had_error_retry \" ] = True <nl> + else : <nl> + sys . stdout . write ( \" . \" ) <nl> + sys . stdout . flush ( ) <nl> + query_state [ \" request \" ] [ \" dont_block \" ] = simplejson . dumps ( True ) <nl> + time . sleep ( 1 ) <nl> + query_state [ \" failures \" ] + = 1 <nl> + return True <nl> + <nl> + def end_error_retry ( succeeded ) : <nl> + if query_state [ \" had_error_retry \" ] and self . verbose : <nl> + if succeeded : <nl> + print \" Success ! \" <nl> + else : <nl> + print \" Failed ! \" <nl> + <nl> while True : <nl> try : <nl> - res = requests . post ( urlparse . urljoin ( self . base_url , url ) , data = request , <nl> + res = requests . post ( urlparse . urljoin ( self . base_url , url ) , <nl> + data = query_state [ \" request \" ] , <nl> verify = True , timeout = 55 ) <nl> <nl> # On 50x errors , try again after a short sleep <nl> - if str ( res . status_code ) . startswith ( ' 5 ' ) and self . retry_on_errors and failures < 10 : <nl> - if self . verbose : <nl> - if not had_error_retry : <nl> - sys . stdout . write ( \" connection error % s - - retrying . \" % ( res . status_code , ) ) <nl> - had_error_retry = True <nl> - request [ \" dont_block \" ] = simplejson . dumps ( True ) <nl> - else : <nl> - sys . stdout . write ( \" . \" ) <nl> - sys . stdout . flush ( ) <nl> - time . sleep ( 1 ) <nl> - failures + = 1 <nl> - continue <nl> + if str ( res . status_code ) . startswith ( ' 5 ' ) : <nl> + if error_retry ( \" ( server % s ) \" % ( res . status_code , ) ) : <nl> + continue <nl> + # Otherwise fall through and process the python - requests error normally <nl> except ( requests . exceptions . Timeout , requests . exceptions . SSLError ) as e : <nl> # Timeouts are either a Timeout or an SSLError ; we <nl> # want the later exception handlers to deal with any <nl> def do_api_query ( self , request , url , longpolling = False ) : <nl> # and the correct response is to just retry <nl> continue <nl> else : <nl> + end_error_retry ( False ) <nl> return { ' msg ' : \" Connection error : \\ n % s \" % traceback . format_exc ( ) , <nl> \" result \" : \" connection - error \" } <nl> except requests . exceptions . ConnectionError : <nl> - if self . retry_on_errors and failures < 10 : <nl> - if self . verbose : <nl> - if not had_error_retry : <nl> - sys . stdout . write ( \" connection error - - retrying . \" ) <nl> - had_error_retry = True <nl> - request [ \" dont_block \" ] = simplejson . dumps ( True ) <nl> - else : <nl> - sys . stdout . write ( \" . \" ) <nl> - sys . stdout . flush ( ) <nl> - time . sleep ( 1 ) <nl> - failures + = 1 <nl> + if error_retry ( \" \" ) : <nl> continue <nl> + end_error_retry ( False ) <nl> return { ' msg ' : \" Connection error : \\ n % s \" % traceback . format_exc ( ) , <nl> \" result \" : \" connection - error \" } <nl> except Exception : <nl> def do_api_query ( self , request , url , longpolling = False ) : <nl> return { ' msg ' : \" Unexpected error : \\ n % s \" % traceback . format_exc ( ) , <nl> \" result \" : \" unexpected - error \" } <nl> <nl> - if self . verbose and had_error_retry : <nl> - print \" Success ! \" <nl> if res . json is not None : <nl> + end_error_retry ( True ) <nl> return res . json <nl> + end_error_retry ( False ) <nl> return { ' msg ' : res . text , \" result \" : \" http - error \" , <nl> \" status_code \" : res . status_code } <nl> <nl> def _register ( cls , name , url = None , make_request = ( lambda request = { } : request ) , * * <nl> url = name <nl> def call ( self , * args , * * kwargs ) : <nl> request = make_request ( * args , * * kwargs ) <nl> - return self . do_api_query ( request , ' / api / v1 / ' + url , * * query_kwargs ) <nl> + return self . do_api_query ( request , API_VERSTRING + url , * * query_kwargs ) <nl> call . func_name = name <nl> setattr ( cls , name , call ) <nl> <nl>\n", "msg": "do_api_query : Reduce error handling code duplication .\n"}
{"diff_id": 29211, "repo": "bokeh/bokeh\n", "sha": "26591e83f05e08987956d394accb7c1d2188d83c\n", "time": "2015-10-20T18:37:07Z\n", "diff": "mmm a / bokeh / client . py <nl> ppp b / bokeh / client . py <nl> def send_message ( self , message ) : <nl> sent = message . send ( self . _socket ) <nl> log . debug ( \" Sent % r [ % d bytes ] \" , message , sent ) <nl> <nl> - def _send_patch_document ( self , sessionid , event ) : <nl> - msg = self . _protocol . create ( ' PATCH - DOC ' , sessionid , [ event ] ) <nl> + def _send_patch_document ( self , session_id , event ) : <nl> + msg = self . _protocol . create ( ' PATCH - DOC ' , session_id , [ event ] ) <nl> self . send_message ( msg ) <nl> <nl> def _send_message_wait_for_reply ( self , message ) : <nl> def have_reply_or_disconnected ( ) : <nl> self . _loop_until ( have_reply_or_disconnected ) <nl> return waiter . reply <nl> <nl> - def push_session ( self , doc , sessionid = DEFAULT_SESSION_ID ) : <nl> + def push_session ( self , doc , session_id = DEFAULT_SESSION_ID ) : <nl> ' ' ' Create a session by pushing the given document to the server , overwriting any existing server - side doc <nl> <nl> Args : <nl> doc : bokeh . document . Document <nl> The Document to initialize the session with . <nl> <nl> - sessionid : string , optional <nl> + session_id : string , optional <nl> The name of the session ( omit to use ' default ' , None to use random unique id ) <nl> <nl> Returns : <nl> session : a ClientSession with the given document and ID <nl> ' ' ' <nl> - session = ClientSession ( self , doc , sessionid ) <nl> + session = ClientSession ( self , doc , session_id ) <nl> msg = self . _protocol . create ( ' PUSH - DOC ' , session . id , session . document ) <nl> reply = self . _send_message_wait_for_reply ( msg ) <nl> if reply is None : <nl> def push_session ( self , doc , sessionid = DEFAULT_SESSION_ID ) : <nl> else : <nl> return session <nl> <nl> - def pull_session ( self , sessionid = DEFAULT_SESSION_ID ) : <nl> + def pull_session ( self , session_id = DEFAULT_SESSION_ID ) : <nl> ' ' ' Create a session by pulling the document from the given session on the server . <nl> Args : <nl> - sessionid : string , optional <nl> + session_id : string , optional <nl> The name of the session ( omit to use ' default ' , None to use random unique id ) <nl> <nl> Returns : <nl> session : a ClientSession with the given ID <nl> ' ' ' <nl> - sessionid = ClientSession . _ensure_session_id ( sessionid ) <nl> - msg = self . _protocol . create ( ' PULL - DOC - REQ ' , sessionid ) <nl> + session_id = ClientSession . _ensure_session_id ( session_id ) <nl> + msg = self . _protocol . create ( ' PULL - DOC - REQ ' , session_id ) <nl> reply = self . _send_message_wait_for_reply ( msg ) <nl> if reply is None : <nl> raise RuntimeError ( \" Connection to server was lost \" ) <nl> def pull_session ( self , sessionid = DEFAULT_SESSION_ID ) : <nl> else : <nl> doc = Document ( ) <nl> reply . push_to_document ( doc ) <nl> - session = ClientSession ( self , doc , sessionid ) <nl> + session = ClientSession ( self , doc , session_id ) <nl> return session <nl> <nl> def _send_request_server_info ( self ) : <nl> def _tell_sessions_about_disconnect ( self ) : <nl> <nl> class ClientSession ( object ) : <nl> <nl> - def __init__ ( self , connection , doc , sessionid = DEFAULT_SESSION_ID ) : <nl> + def __init__ ( self , connection , doc , session_id = DEFAULT_SESSION_ID ) : <nl> ' ' ' <nl> Attaches to a particular named session on the server . <nl> ' ' ' <nl> self . _connection = connection <nl> self . _document = doc <nl> - self . _id = self . _ensure_session_id ( sessionid ) <nl> + self . _id = self . _ensure_session_id ( session_id ) <nl> self . _document . on_change ( self . _document_changed ) <nl> # registering may remove the on_change above as side effect <nl> self . _connection . _register_session ( self ) <nl> self . _current_patch = None <nl> <nl> @ classmethod <nl> - def _ensure_session_id ( cls , sessionid = DEFAULT_SESSION_ID ) : <nl> - if sessionid is None : <nl> - sessionid = str ( uuid . uuid4 ( ) ) <nl> - return sessionid <nl> + def _ensure_session_id ( cls , session_id = DEFAULT_SESSION_ID ) : <nl> + if session_id is None : <nl> + session_id = str ( uuid . uuid4 ( ) ) <nl> + return session_id <nl> <nl> @ property <nl> def document ( self ) : <nl>\n", "msg": "Use session_id not sessionid in client . py to match most other places\n"}
{"diff_id": 29548, "repo": "huggingface/transformers\n", "sha": "80eacb8f16208bcc7ffd8ed5b5750d6fc6854a24\n", "time": "2019-12-13T13:10:22Z\n", "diff": "mmm a / transformers / configuration_utils . py <nl> ppp b / transformers / configuration_utils . py <nl> def __init__ ( self , * * kwargs ) : <nl> self . use_bfloat16 = kwargs . pop ( ' use_bfloat16 ' , False ) <nl> self . pruned_heads = kwargs . pop ( ' pruned_heads ' , { } ) <nl> self . is_decoder = kwargs . pop ( ' is_decoder ' , False ) <nl> - self . idx2label = kwargs . pop ( ' idx2label ' , { i : ' LABEL_ { } ' . format ( i ) for i in range ( self . num_labels ) } ) <nl> - self . label2idx = kwargs . pop ( ' label2idx ' , dict ( zip ( self . idx2label . values ( ) , self . idx2label . keys ( ) ) ) ) <nl> + self . id2label = kwargs . pop ( ' id2label ' , { i : ' LABEL_ { } ' . format ( i ) for i in range ( self . num_labels ) } ) <nl> + self . label2id = kwargs . pop ( ' label2id ' , dict ( zip ( self . id2label . values ( ) , self . id2label . keys ( ) ) ) ) <nl> <nl> def save_pretrained ( self , save_directory ) : <nl> \" \" \" Save a configuration object to the directory ` save_directory ` , so that it <nl>\n", "msg": "Adding labels mapping for classification models in their respective config .\n"}
{"diff_id": 29828, "repo": "ansible/ansible\n", "sha": "ca8668f08b5c87115df50c66dadc3528faa69e88\n", "time": "2017-11-01T20:46:22Z\n", "diff": "mmm a / lib / ansible / cli / adhoc . py <nl> ppp b / lib / ansible / cli / adhoc . py <nl> <nl> from ansible . executor . task_queue_manager import TaskQueueManager <nl> from ansible . module_utils . _text import to_text <nl> from ansible . parsing . splitter import parse_kv <nl> + from ansible . playbook import Playbook <nl> from ansible . playbook . play import Play <nl> from ansible . plugins . loader import get_all_plugin_loaders <nl> <nl> def run ( self ) : <nl> play_ds = self . _play_ds ( pattern , self . options . seconds , self . options . poll_interval ) <nl> play = Play ( ) . load ( play_ds , variable_manager = variable_manager , loader = loader ) <nl> <nl> + # used in start callback <nl> + playbook = Playbook ( loader ) <nl> + playbook . _entries . append ( play ) <nl> + playbook . _file_name = ' __adhoc_playbook__ ' <nl> + <nl> if self . callback : <nl> cb = self . callback <nl> elif self . options . one_line : <nl> def run ( self ) : <nl> run_tree = run_tree , <nl> ) <nl> <nl> + self . _tqm . send_callback ( ' v2_playbook_on_start ' , playbook ) <nl> + <nl> result = self . _tqm . run ( play ) <nl> + <nl> + self . _tqm . send_callback ( ' v2_playbook_on_stats ' , self . _tqm . _stats ) <nl> finally : <nl> if self . _tqm : <nl> self . _tqm . cleanup ( ) <nl>\n", "msg": "Use playbook callbacks for adhoc commands too\n"}
{"diff_id": 29832, "repo": "zulip/zulip\n", "sha": "1b4cac6734fe8ca78ca8ec55d91f0d82a74289ef\n", "time": "2020-03-06T20:02:02Z\n", "diff": "mmm a / zerver / models . py <nl> ppp b / zerver / models . py <nl> def get_user_profile_by_email ( email : str ) - > UserProfile : <nl> return UserProfile . objects . select_related ( ) . get ( delivery_email__iexact = email . strip ( ) ) <nl> <nl> @ cache_with_key ( user_profile_by_api_key_cache_key , timeout = 3600 * 24 * 7 ) <nl> + def maybe_get_user_profile_by_api_key ( api_key : str ) - > Optional [ UserProfile ] : <nl> + try : <nl> + return UserProfile . objects . select_related ( ) . get ( api_key = api_key ) <nl> + except UserProfile . DoesNotExist : <nl> + # We will cache failed lookups with None . The <nl> + # use case here is that broken API clients may <nl> + # continually ask for the same wrong API key , and <nl> + # we want to handle that as quickly as possible . <nl> + return None <nl> + <nl> def get_user_profile_by_api_key ( api_key : str ) - > UserProfile : <nl> - return UserProfile . objects . select_related ( ) . get ( api_key = api_key ) <nl> + user_profile = maybe_get_user_profile_by_api_key ( api_key ) <nl> + if user_profile is None : <nl> + raise UserProfile . DoesNotExist ( ) <nl> + <nl> + return user_profile <nl> <nl> def get_user_by_delivery_email ( email : str , realm : Realm ) - > UserProfile : <nl> \" \" \" Fetches a user given their delivery email . For use in <nl>\n", "msg": "models : Cache failures to find user in get_user_by_api_key .\n"}
{"diff_id": 29879, "repo": "scikit-learn/scikit-learn\n", "sha": "1dc6c49525f0bc37759d5c352161b61bdce641a8\n", "time": "2011-05-15T18:12:45Z\n", "diff": "mmm a / scikits / learn / datasets / samples_generator . py <nl> ppp b / scikits / learn / datasets / samples_generator . py <nl> <nl> import numpy as np <nl> import numpy . random as nr <nl> <nl> + from . . utils import check_random_state <nl> + <nl> <nl> def test_dataset_classif ( n_samples = 100 , n_features = 100 , param = [ 1 , 1 ] , <nl> n_informative = 0 , k = 0 , seed = None ) : <nl> def swiss_roll ( n_samples , noise = 0 . 0 ) : <nl> X = np . transpose ( X ) <nl> t = np . squeeze ( t ) <nl> return X , t <nl> + <nl> + <nl> + def make_blobs ( n_samples = 100 , n_features = 2 , centers = 3 , cluster_std = 1 . 0 , <nl> + center_box = ( - 10 . 0 , 10 . 0 ) , shuffle = True , random_state = 0 ) : <nl> + \" \" \" Generate isotropic Gaussian blobs for clustering <nl> + <nl> + Parameters <nl> + mmmmmmmmm - <nl> + n_samples : int ( default 100 ) <nl> + Total number of points equally divided among clusters <nl> + <nl> + n_features : int ( default 2 ) <nl> + Number of dimensions for each sample <nl> + <nl> + centers : int or array of shape [ n_centers , n_features ] ( default 3 ) <nl> + Number of centers to generate or fixed center location <nl> + <nl> + cluster_std : float or sequace of floats <nl> + Standard deviation of the clusters <nl> + <nl> + center_box : pair of floats ( min , max ) <nl> + Bounding box for each cluster center when randomly generated <nl> + positions <nl> + <nl> + shuffle : boolean ( default True ) <nl> + Shuffle the order of the sample <nl> + <nl> + random_state : int or RandomState instance ( default 0 ) <nl> + Seed used by the pseudo random number generator <nl> + <nl> + Return <nl> + mmmmmm <nl> + samples : array of shape [ n_samples , n_features ] <nl> + The generated samples <nl> + <nl> + labels : array of shape [ n_samples ] <nl> + The integer labels for class membership of each sample <nl> + <nl> + Example <nl> + mmmmmm - <nl> + <nl> + > > > samples , labels = make_blobs ( n_samples = 10 , centers = 3 , n_features = 2 ) <nl> + > > > samples . shape <nl> + ( 10 , 2 ) <nl> + > > > labels <nl> + array ( [ 0 , 0 , 1 , 0 , 2 , 2 , 2 , 1 , 1 , 0 ] ) <nl> + <nl> + \" \" \" <nl> + random_state = check_random_state ( random_state ) <nl> + <nl> + if isinstance ( centers , int ) : <nl> + centers = random_state . uniform ( center_box [ 0 ] , center_box [ 1 ] , <nl> + size = ( centers , n_features ) ) <nl> + else : <nl> + centers = np . atleast_2d ( centers ) <nl> + n_features = centers . shape [ 1 ] <nl> + <nl> + blobs = [ ] <nl> + labels = [ ] <nl> + <nl> + n_centers = centers . shape [ 0 ] <nl> + n_samples_per_center = [ n_samples / n_centers ] * n_centers <nl> + n_samples_per_center [ 0 ] + = n_samples % n_centers <nl> + <nl> + for i , n in enumerate ( n_samples_per_center ) : <nl> + blobs . append ( centers [ i ] + random_state . normal ( scale = cluster_std , <nl> + size = ( n , n_features ) ) ) <nl> + labels + = [ i ] * n <nl> + <nl> + samples = np . concatenate ( blobs ) <nl> + labels = np . array ( labels ) <nl> + if shuffle : <nl> + indices = np . arange ( samples . shape [ 0 ] ) <nl> + random_state . shuffle ( indices ) <nl> + samples = samples [ indices ] <nl> + labels = labels [ indices ] <nl> + return samples , labels <nl>\n", "msg": "new utility function to generate blobby datasets\n"}
{"diff_id": 29906, "repo": "ytdl-org/youtube-dl\n", "sha": "e68dd1921ad7528d225a8571066f99b9934b6a06\n", "time": "2015-11-06T05:33:05Z\n", "diff": "mmm a / youtube_dl / extractor / miomio . py <nl> ppp b / youtube_dl / extractor / miomio . py <nl> def _real_extract ( self , url ) : <nl> mioplayer_path = self . _search_regex ( <nl> r ' src = \" ( / mioplayer / [ ^ \" ] + ) \" ' , webpage , ' ref_path ' ) <nl> <nl> + http_headers = { ' Referer ' : ' http : / / www . miomio . tv % s ' % mioplayer_path , } <nl> + <nl> xml_config = self . _search_regex ( <nl> r ' flashvars = \" type = ( ? : sina | video ) & amp ; ( . + ? ) & amp ; ' , <nl> webpage , ' xml config ' ) <nl> def _real_extract ( self , url ) : <nl> <nl> vid_config_request = compat_urllib_request . Request ( <nl> ' http : / / www . miomio . tv / mioplayer / mioplayerconfigfiles / sina . php ? { 0 } ' . format ( xml_config ) , <nl> - headers = { ' Referer ' : ' http : / / www . miomio . tv / mioplayer / mioplayer - v3 . 0 . swf ' } ) <nl> + headers = http_headers ) <nl> <nl> # the following xml contains the actual configuration information on the video file ( s ) <nl> vid_config = self . _download_xml ( vid_config_request , video_id ) <nl> <nl> - http_headers = { <nl> - ' Referer ' : ' http : / / www . miomio . tv % s ' % mioplayer_path , <nl> - } <nl> - <nl> if not int_or_none ( xpath_text ( vid_config , ' timelength ' ) ) : <nl> raise ExtractorError ( ' Unable to load videos ! ' , expected = True ) <nl> <nl>\n", "msg": "[ miomio ] use the formats urls headers for downloading xml\n"}
{"diff_id": 30237, "repo": "getredash/redash\n", "sha": "ebb07946acfcc2e4a18367dcafde973d7baca394\n", "time": "2016-04-20T14:54:20Z\n", "diff": "mmm a / redash / query_runner / influx_db . py <nl> ppp b / redash / query_runner / influx_db . py <nl> def _transform_result ( results ) : <nl> result_rows = [ ] <nl> <nl> for result in results : <nl> - if not result_columns : <nl> - for c in result . raw [ ' series ' ] [ 0 ] [ ' columns ' ] : <nl> - result_columns . append ( { \" name \" : c } ) <nl> + for series in result . raw [ ' series ' ] : <nl> + for column in series [ ' columns ' ] : <nl> + if column not in result_columns : <nl> + result_columns . append ( column ) <nl> + for key in series [ ' tags ' ] . keys ( ) : <nl> + if key not in result_columns : <nl> + result_columns . append ( key ) <nl> <nl> - for point in result . get_points ( ) : <nl> - result_rows . append ( point ) <nl> + for result in results : <nl> + for series in result . raw [ ' series ' ] : <nl> + for point in series [ ' values ' ] : <nl> + result_row = { } <nl> + for column in result_columns : <nl> + if column in series [ ' tags ' ] : <nl> + result_row [ column ] = series [ ' tags ' ] [ column ] <nl> + elif column in series [ ' columns ' ] : <nl> + index = series [ ' columns ' ] . index ( column ) <nl> + value = point [ index ] <nl> + result_row [ column ] = value <nl> + result_rows . append ( result_row ) <nl> <nl> return json . dumps ( { <nl> - \" columns \" : result_columns , <nl> - \" rows \" : result_rows <nl> + \" columns \" : [ { ' name ' : c } for c in result_columns ] , <nl> + \" rows \" : result_rows <nl> } , cls = JSONEncoder ) <nl> <nl> <nl>\n", "msg": "Added support for multiple queries and tags\n"}
{"diff_id": 30267, "repo": "encode/django-rest-framework\n", "sha": "e03bb9c2fe53591c40707569d091b8793ea1000e\n", "time": "2012-11-21T07:17:30Z\n", "diff": "mmm a / rest_framework / pagination . py <nl> ppp b / rest_framework / pagination . py <nl> def __init__ ( self , * args , * * kwargs ) : <nl> super ( BasePaginationSerializer , self ) . __init__ ( * args , * * kwargs ) <nl> results_field = self . results_field <nl> object_serializer = self . opts . object_serializer_class <nl> - self . fields [ results_field ] = object_serializer ( source = ' object_list ' ) <nl> + self . serialize_fields [ results_field ] = object_serializer ( source = ' object_list ' ) <nl> <nl> def to_native ( self , obj ) : <nl> \" \" \" <nl>\n", "msg": "Change pagination to update Serializer . serialize_fields\n"}
{"diff_id": 30279, "repo": "explosion/spaCy\n", "sha": "2b5cde87fd0053c21c00bfd6b76551bba21cecda\n", "time": "2015-07-26T20:40:04Z\n", "diff": "mmm a / fabfile . py <nl> ppp b / fabfile . py <nl> <nl> - from fabric . api import local , lcd , env , settings <nl> + from fabric . api import local , lcd , env , settings , prefix <nl> from os . path import exists as file_exists <nl> from fabtools . python import virtualenv <nl> from os import path <nl> + import os <nl> + import shutil <nl> <nl> <nl> PWD = path . dirname ( __file__ ) <nl> VENV_DIR = path . join ( PWD , ' . env ' ) <nl> <nl> <nl> - def sdist ( ) : <nl> - if file_exists ( ' dist / ' ) : <nl> - local ( ' rm - rf dist / ' ) <nl> - local ( ' mkdir dist ' ) <nl> - with virtualenv ( VENV_DIR ) : <nl> - local ( ' python setup . py sdist ' ) <nl> + def counts ( ) : <nl> + pass <nl> + # Tokenize the corpus <nl> + # tokenize ( ) <nl> + # get_freqs ( ) <nl> + # Collate the counts <nl> + # cat freqs | sort - k2 | gather_freqs ( ) <nl> + # gather_freqs ( ) <nl> + # smooth ( ) <nl> + <nl> + <nl> + # clean , make , sdist <nl> + # cd to new env , install from sdist , <nl> + # Push changes to server <nl> + # Pull changes on server <nl> + # clean make init model <nl> + # test - - vectors - - slow <nl> + # train <nl> + # test - - vectors - - slow - - models <nl> + # sdist <nl> + # upload data to server <nl> + # change to clean venv <nl> + # py2 : install from sdist , test - - slow , download data , test - - models - - vectors <nl> + # py3 : install from sdist , test - - slow , download data , test - - models - - vectors <nl> + <nl> + <nl> + def prebuild ( build_dir = ' / tmp / build_spacy ' ) : <nl> + if file_exists ( build_dir ) : <nl> + shutil . rmtree ( build_dir ) <nl> + os . mkdir ( build_dir ) <nl> + spacy_dir = path . dirname ( __file__ ) <nl> + wn_url = ' http : / / wordnetcode . princeton . edu / 3 . 0 / WordNet - 3 . 0 . tar . gz ' <nl> + build_venv = path . join ( build_dir , ' . env ' ) <nl> + with lcd ( build_dir ) : <nl> + local ( ' git clone % s . ' % spacy_dir ) <nl> + local ( ' virtualenv ' + build_venv ) <nl> + with prefix ( ' cd % s & & PYTHONPATH = ` pwd ` & & source % s / bin / activate ' % ( build_dir , build_venv ) ) : <nl> + local ( ' pip install cython fabric fabtools ' ) <nl> + local ( ' pip install - r requirements . txt ' ) <nl> + local ( ' fab clean make ' ) <nl> + local ( ' cp - r % s / corpora / en / wordnet corpora / en / ' % spacy_dir ) <nl> + local ( ' cp % s / corpora / en / freqs . txt . gz corpora / en / ' % spacy_dir ) <nl> + local ( ' python bin / init_model . py lang_data / en corpora / en spacy / en / data ' ) <nl> + local ( ' fab test ' ) <nl> + local ( ' python setup . py sdist ' ) <nl> <nl> def docs ( ) : <nl> with virtualenv ( VENV_DIR ) : <nl>\n", "msg": "* Add prebuild command , to test clean builds\n"}
{"diff_id": 30328, "repo": "python/cpython\n", "sha": "bace6764fac7800883029d7f34dc56d17a1530a0\n", "time": "2010-07-05T17:13:21Z\n", "diff": "mmm a / Lib / test / test_math . py <nl> ppp b / Lib / test / test_math . py <nl> class TestNoTrunc ( object ) : <nl> <nl> self . assertRaises ( TypeError , math . trunc ) <nl> self . assertRaises ( TypeError , math . trunc , 1 , 2 ) <nl> - # XXX : This is not ideal , but see the comment in math_trunc ( ) . <nl> - self . assertRaises ( AttributeError , math . trunc , TestNoTrunc ( ) ) <nl> + self . assertRaises ( ( AttributeError , TypeError ) , math . trunc , <nl> + TestNoTrunc ( ) ) <nl> <nl> t = TestNoTrunc ( ) <nl> t . __trunc__ = lambda * args : args <nl>\n", "msg": "be more generous to implementations that have implemented correctly\n"}
{"diff_id": 30406, "repo": "home-assistant/core\n", "sha": "ecb0567d0396d5f31c4e490137c8924886662004\n", "time": "2013-10-06T20:13:54Z\n", "diff": "mmm a / homeassistant / observers . py <nl> ppp b / homeassistant / observers . py <nl> def track_sun ( eventbus , statemachine , latitude , longitude ) : <nl> logger = logging . getLogger ( __name__ ) <nl> <nl> def update_sun_state ( now ) : <nl> + \" \" \" Method to update the current state of the sun and time the next update . \" \" \" <nl> observer = ephem . Observer ( ) <nl> observer . lat = latitude <nl> observer . long = longitude <nl>\n", "msg": "Missing doc string for one method .\n"}
{"diff_id": 30566, "repo": "ansible/ansible\n", "sha": "9b8a55032dd19a0a185f8c687d3f095b774083ff\n", "time": "2014-12-14T18:27:17Z\n", "diff": "mmm a / lib / ansible / runner / action_plugins / debug . py <nl> ppp b / lib / ansible / runner / action_plugins / debug . py <nl> def run ( self , conn , tmp , module_name , module_args , inject , complex_args = None , * * <nl> result = dict ( msg = args [ ' msg ' ] ) <nl> elif ' var ' in args and not utils . LOOKUP_REGEX . search ( args [ ' var ' ] ) : <nl> results = template . template ( self . basedir , args [ ' var ' ] , inject , convert_bare = True ) <nl> - result [ args [ ' var ' ] ] = results <nl> + result [ ' var ' ] = { args [ ' var ' ] : results } <nl> <nl> # force flag to make debug output module always verbose <nl> result [ ' verbose_always ' ] = True <nl>\n", "msg": "Do not use the variable name as a key for the result of the module\n"}
{"diff_id": 30584, "repo": "tornadoweb/tornado\n", "sha": "11fdd479b2ff45a3071672fd483cdb4aa81783ec\n", "time": "2010-10-19T19:28:36Z\n", "diff": "mmm a / tornado / testing . py <nl> ppp b / tornado / testing . py <nl> def get_app ( self ) : <nl> return Application ( [ ( ' / ' , MyHandler ) . . . ] ) <nl> <nl> def test_homepage ( self ) : <nl> + # The following two lines are equivalent to <nl> + # response = self . fetch ( ' / ' ) <nl> + # but are shown in full here to demonstrate explicit use <nl> + # of self . stop and self . wait . <nl> self . http_client . fetch ( self . get_url ( ' / ' ) , self . stop ) <nl> response = self . wait ( ) <nl> # test contents of response <nl> def get_app ( self ) : <nl> \" \" \" <nl> raise NotImplementedError ( ) <nl> <nl> + def fetch ( self , path , * * kwargs ) : <nl> + \" \" \" Convenience method to synchronously fetch a url . <nl> + <nl> + The given path will be appended to the local server ' s host and port . <nl> + Any additional kwargs will be passed directly to <nl> + AsyncHTTPClient . fetch ( and so could be used to pass method = \" POST \" , <nl> + body = \" . . . \" , etc ) . <nl> + \" \" \" <nl> + self . http_client . fetch ( self . get_url ( path ) , self . stop , * * kwargs ) <nl> + return self . wait ( ) <nl> + <nl> def get_httpserver_options ( self ) : <nl> \" \" \" May be overridden by subclasses to return additional <nl> keyword arguments for HTTPServer . <nl>\n", "msg": "Add a convenience method for synchronous fetches in AsyncHTTPTestCase\n"}
{"diff_id": 30601, "repo": "locustio/locust\n", "sha": "d71eb78d2a556d3bd05ce720c76159a087ddd196\n", "time": "2012-08-21T21:08:40Z\n", "diff": "mmm a / locust / clients . py <nl> ppp b / locust / clients . py <nl> class HttpBrowser ( object ) : <nl> def __init__ ( self , base_url , gzip = False ) : <nl> self . base_url = base_url <nl> self . gzip = gzip <nl> + self . new_session ( ) <nl> + <nl> + def new_session ( self ) : <nl> + \" \" \" <nl> + Get a new HTTP session for this HttpBrowser instance <nl> + \" \" \" <nl> handlers = [ urllib2 . HTTPCookieProcessor ( ) ] <nl> <nl> # Check for basic authentication <nl>\n", "msg": "Added a new_session ( ) method to HttpBrowser that can be used to give the client a new session\n"}
{"diff_id": 30662, "repo": "python/cpython\n", "sha": "49413418582f1e2c94fc77275833cd83ea8bd6c9\n", "time": "1998-05-19T15:15:59Z\n", "diff": "mmm a / Lib / mimetypes . py <nl> ppp b / Lib / mimetypes . py <nl> def guess_extension ( type ) : <nl> <nl> Return value is a string giving a filename extension , including the <nl> leading dot ( ' . ' ) . The extension is not guaranteed to have been <nl> - associated with any particular data stream , but has been known to be <nl> - used for streams of the MIME type given by ` type ' . If ` type ' is not <nl> - known , None is returned . <nl> + associated with any particular data stream , but would be mapped to the <nl> + MIME type ` type ' by guess_type ( ) . If no extension can be guessed for <nl> + ` type ' , None is returned . <nl> \" \" \" <nl> + global inited <nl> + if not inited : <nl> + init ( ) <nl> type = string . lower ( type ) <nl> for ext , stype in types_map . items ( ) : <nl> if type = = stype : <nl>\n", "msg": "guess_extension ( ) : Revise documentation string to be more clear . If not\n"}
{"diff_id": 30770, "repo": "ipython/ipython\n", "sha": "3ffb5e70e589c2508420758b01b2a1bac2e9bc7f\n", "time": "2011-06-20T23:40:12Z\n", "diff": "mmm a / IPython / frontend / qt / console / ipython_widget . py <nl> ppp b / IPython / frontend / qt / console / ipython_widget . py <nl> <nl> # Base path for most payload sources . <nl> zmq_shell_source = ' IPython . zmq . zmqshell . ZMQInteractiveShell ' <nl> <nl> + if sys . platform . startswith ( ' win ' ) : <nl> + default_editor = ' notepad ' <nl> + else : <nl> + default_editor = ' ' <nl> + <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> # IPythonWidget class <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> class IPythonWidget ( FrontendWidget ) : <nl> custom_edit = Bool ( False ) <nl> custom_edit_requested = QtCore . Signal ( object , object ) <nl> <nl> - editor = Unicode ( ' default ' , config = True , <nl> + editor = Unicode ( default_editor , config = True , <nl> help = \" \" \" <nl> A command for invoking a system text editor . If the string contains a <nl> { filename } format specifier , it will be used . Otherwise , the filename will <nl> def _edit ( self , filename , line = None ) : <nl> \" \" \" <nl> if self . custom_edit : <nl> self . custom_edit_requested . emit ( filename , line ) <nl> - elif self . editor = = ' default ' : <nl> - self . _append_plain_text ( ' No default editor available . \\ n ' ) <nl> + elif not self . editor : <nl> + self . _append_plain_text ( ' No default editor available . \\ n ' <nl> + ' Specify a GUI text editor in the ` IPythonWidget . editor ` configurable \\ n ' <nl> + ' to enable the % edit magic ' ) <nl> else : <nl> try : <nl> filename = ' \" % s \" ' % filename <nl>\n", "msg": "expand default_editor message to include configurable\n"}
{"diff_id": 30793, "repo": "ansible/ansible\n", "sha": "6f2f07ef511bce438a270a92a5e5bbc29860eae4\n", "time": "2016-12-08T16:22:59Z\n", "diff": "mmm a / lib / ansible / modules / database / mysql / mysql_db . py <nl> ppp b / lib / ansible / modules / database / mysql / mysql_db . py <nl> <nl> target : <nl> description : <nl> - Location , on the remote host , of the dump file to read from or write to . Uncompressed SQL <nl> - files ( C ( . sql ) ) as well as bzip2 ( C ( . bz2 ) ) and gzip ( C ( . gz ) ) compressed files are supported . <nl> + files ( C ( . sql ) ) as well as bzip2 ( C ( . bz2 ) ) , gzip ( C ( . gz ) ) and xz compressed files are supported . <nl> required : false <nl> notes : <nl> - Requires the MySQLdb Python package on the remote host . For Ubuntu , this <nl> def db_dump ( module , host , user , password , db_name , target , all_databases , port , <nl> cmd = cmd + ' | gzip > ' + pipes . quote ( target ) <nl> elif os . path . splitext ( target ) [ - 1 ] = = ' . bz2 ' : <nl> cmd = cmd + ' | bzip2 > ' + pipes . quote ( target ) <nl> + elif os . path . splitext ( target ) [ - 1 ] = = ' . xz ' : <nl> + cmd = cmd + ' | xz > ' + pipes . quote ( target ) <nl> else : <nl> cmd + = \" > % s \" % pipes . quote ( target ) <nl> rc , stdout , stderr = module . run_command ( cmd , use_unsafe_shell = True ) <nl> def db_import ( module , host , user , password , db_name , target , all_databases , port <nl> finally : <nl> # bzip2 file back up <nl> rc , stdout , stderr = module . run_command ( ' % s % s ' % ( bzip2_path , os . path . splitext ( target ) [ 0 ] ) ) <nl> + elif os . path . splitext ( target ) [ - 1 ] = = ' . xz ' : <nl> + xz_path = module . get_bin_path ( ' xz ' ) <nl> + if not xz_path : <nl> + module . fail_json ( msg = \" xz command not found \" ) <nl> + # xz - d file ( uncompress ) <nl> + rc , stdout , stderr = module . run_command ( ' % s - d % s ' % ( xz_path , target ) ) <nl> + if rc ! = 0 : <nl> + return rc , stdout , stderr <nl> + # Import sql <nl> + cmd + = \" < % s \" % pipes . quote ( os . path . splitext ( target ) [ 0 ] ) <nl> + try : <nl> + rc , stdout , stderr = module . run_command ( cmd , use_unsafe_shell = True ) <nl> + if rc ! = 0 : <nl> + return rc , stdout , stderr <nl> + finally : <nl> + # xz file back up <nl> + rc , stdout , stderr = module . run_command ( ' % s % s ' % ( xz_path , os . path . splitext ( target ) [ 0 ] ) ) <nl> else : <nl> cmd + = \" < % s \" % pipes . quote ( target ) <nl> rc , stdout , stderr = module . run_command ( cmd , use_unsafe_shell = True ) <nl>\n", "msg": "Add support for xz compression ( for dump and import ) in mysql_db module\n"}
{"diff_id": 30819, "repo": "python/cpython\n", "sha": "5935ff07beb0ed2b37dc7a85f04861122eaa5a9a\n", "time": "2001-12-19T16:54:23Z\n", "diff": "mmm a / Lib / test / test_weakref . py <nl> ppp b / Lib / test / test_weakref . py <nl> <nl> import sys <nl> import unittest <nl> + import UserList <nl> import weakref <nl> <nl> import test_support <nl> def test_basic_proxy ( self ) : <nl> o = C ( ) <nl> self . check_proxy ( o , weakref . proxy ( o ) ) <nl> <nl> + L = UserList . UserList ( ) <nl> + p = weakref . proxy ( L ) <nl> + self . failIf ( p , \" proxy for empty UserList should be false \" ) <nl> + p . append ( 12 ) <nl> + self . assertEqual ( len ( L ) , 1 ) <nl> + self . failUnless ( p , \" proxy for non - empty UserList should be true \" ) <nl> + p [ : ] = [ 2 , 3 ] <nl> + self . assertEqual ( len ( L ) , 2 ) <nl> + self . assertEqual ( len ( p ) , 2 ) <nl> + self . failUnless ( 3 in p , \" proxy didn ' t support __contains__ ( ) properly \" ) <nl> + p [ 1 ] = 5 <nl> + self . assertEqual ( L [ 1 ] , 5 ) <nl> + self . assertEqual ( p [ 1 ] , 5 ) <nl> + L2 = UserList . UserList ( L ) <nl> + p2 = weakref . proxy ( L2 ) <nl> + self . assertEqual ( p , p2 ) <nl> + <nl> def test_callable_proxy ( self ) : <nl> o = Callable ( ) <nl> ref1 = weakref . proxy ( o ) <nl>\n", "msg": "Add some additional tests that check more proxy behaviors .\n"}
{"diff_id": 30845, "repo": "python/cpython\n", "sha": "ecd79eb7dbde19ea2adbf2a912caa5b284b477b9\n", "time": "2003-01-29T00:35:32Z\n", "diff": "mmm a / Lib / test / regrtest . py <nl> ppp b / Lib / test / regrtest . py <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_socketserver <nl> test_sunaudiodev <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' mac ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_sundry <nl> test_timing <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' unixware7 ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_sunaudiodev <nl> test_sundry <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' openunix8 ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_sunaudiodev <nl> test_sundry <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' sco_sv3 ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_threadedtempfile <nl> test_threading <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' riscos ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_threading <nl> test_timing <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' darwin ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_socketserver <nl> test_sunaudiodev <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' sunos5 ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_mpz <nl> test_openpty <nl> test_socketserver <nl> - test_winreg <nl> - test_winsound <nl> test_zipfile <nl> test_zlib <nl> \" \" \" , <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_socketserver <nl> test_sunaudiodev <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> test_zipfile <nl> test_zlib <nl> \" \" \" , <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_socketserver <nl> test_sunaudiodev <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' cygwin ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_socketserver <nl> test_sunaudiodev <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> ' os2emx ' : <nl> \" \" \" <nl> def printlist ( x , width = 70 , indent = 4 ) : <nl> test_signal <nl> test_sunaudiodev <nl> test_unicode_file <nl> - test_winreg <nl> - test_winsound <nl> \" \" \" , <nl> } <nl> <nl> def __init__ ( self ) : <nl> if test_socket_ssl . skip_expected : <nl> self . expected . add ( ' test_socket_ssl ' ) <nl> <nl> + if sys . platform ! = \" mac \" : <nl> + self . expected . add ( \" test_macostools \" ) <nl> + self . expected . add ( \" test_macfs \" ) <nl> + <nl> + if sys . platform ! = \" win32 \" : <nl> + self . expected . add ( \" test_winreg \" ) <nl> + self . expected . add ( \" test_winsound \" ) <nl> + <nl> self . valid = True <nl> <nl> def isvalid ( self ) : <nl>\n", "msg": "Expect test_macostools and test_macfs to get skipped whenever\n"}
{"diff_id": 30978, "repo": "celery/celery\n", "sha": "c3d34632d8d8acf2e250c6e045cc1bf0d5ea1be0\n", "time": "2012-05-08T18:29:34Z\n", "diff": "mmm a / celery / app / base . py <nl> ppp b / celery / app / base . py <nl> <nl> from celery . utils . imports import instantiate , symbol_by_name <nl> <nl> from . annotations import prepare as prepare_annotations <nl> - from . builtins import load_builtin_tasks <nl> + from . builtins import builtin_task , load_builtin_tasks <nl> from . defaults import DEFAULTS , find_deprecated_settings <nl> from . state import _tls , get_current_app <nl> from . utils import AppPickler , Settings , bugreport , _unpickle_app <nl> def worker_main ( self , argv = None ) : <nl> return instantiate ( \" celery . bin . celeryd : WorkerCommand \" , app = self ) \\ <nl> . execute_from_commandline ( argv ) <nl> <nl> - def task ( self , * args , * * options ) : <nl> + def task ( self , * args , * * opts ) : <nl> \" \" \" Creates new task class from any callable . \" \" \" <nl> <nl> - def inner_create_task_cls ( * * options ) : <nl> + def inner_create_task_cls ( builtin = False , * * opts ) : <nl> <nl> def _create_task_cls ( fun ) : <nl> + if builtin : <nl> + cons = lambda app : app . _task_from_fun ( fun , * * opts ) <nl> + cons . __name__ = fun . __name__ <nl> + builtin_task ( cons ) <nl> if self . accept_magic_kwargs : # compat mode <nl> - return self . _task_from_fun ( fun , * * options ) <nl> + return self . _task_from_fun ( fun , * * opts ) <nl> <nl> # return a proxy object that is only evaluated when first used <nl> - promise = PromiseProxy ( self . _task_from_fun , ( fun , ) , options ) <nl> + promise = PromiseProxy ( self . _task_from_fun , ( fun , ) , opts ) <nl> self . _pending . append ( promise ) <nl> return promise <nl> <nl> return _create_task_cls <nl> <nl> if len ( args ) = = 1 and callable ( args [ 0 ] ) : <nl> - return inner_create_task_cls ( * * options ) ( * args ) <nl> - return inner_create_task_cls ( * * options ) <nl> + return inner_create_task_cls ( * * opts ) ( * args ) <nl> + return inner_create_task_cls ( * * opts ) <nl> <nl> def _task_from_fun ( self , fun , * * options ) : <nl> base = options . pop ( \" base \" , None ) or self . Task <nl>\n", "msg": "Task decorator now support a builtin argument , to makes the task be registered in all subsequent apps\n"}
{"diff_id": 31106, "repo": "ansible/ansible\n", "sha": "1670d9b03080cb17b4cb0e473d808f8233cc83ed\n", "time": "2016-12-08T16:24:47Z\n", "diff": "mmm a / lib / ansible / modules / cloud / docker / docker_service . py <nl> ppp b / lib / ansible / modules / cloud / docker / docker_service . py <nl> <nl> type : bool <nl> required : false <nl> default : true <nl> + pull : <nl> + description : <nl> + - Use with state I ( present ) to always pull images prior to starting the application . <nl> + - Same as running docker - compose pull . <nl> + - When a new image is pulled , services using the image will be recreated unless C ( recreate ) is I ( never ) . <nl> + type : bool <nl> + required : false <nl> + default : false <nl> remove_images : <nl> description : <nl> - Use with state I ( absent ) to remove the all images or only local images . <nl> def __init__ ( self , client ) : <nl> self . services = None <nl> self . scale = None <nl> self . debug = None <nl> + self . pull = None <nl> <nl> for key , value in client . module . params . items ( ) : <nl> setattr ( self , key , value ) <nl> def cmd_up ( self ) : <nl> converge = convergence_strategy_from_opts ( up_options ) <nl> self . log ( \" convergence strategy : % s \" % converge ) <nl> <nl> + if self . pull : <nl> + result . update ( self . cmd_pull ( ) ) <nl> + <nl> for service in self . project . services : <nl> if not service_names or service . name in service_names : <nl> plan = service . convergence_plan ( strategy = converge ) <nl> if plan . action ! = ' noop ' : <nl> result [ ' changed ' ] = True <nl> if self . debug or self . check_mode : <nl> - result [ ' actions ' ] [ service . name ] = dict ( ) <nl> + if not result [ ' actions ' ] . get ( service . name ) : <nl> + result [ ' actions ' ] [ service . name ] = dict ( ) <nl> result [ ' actions ' ] [ service . name ] [ plan . action ] = [ ] <nl> for container in plan . containers : <nl> result [ ' actions ' ] [ service . name ] [ plan . action ] . append ( dict ( <nl> def cmd_up ( self ) : <nl> <nl> return result <nl> <nl> + def cmd_pull ( self ) : <nl> + result = dict ( <nl> + changed = False , <nl> + actions = dict ( ) , <nl> + ) <nl> + <nl> + if not self . check_mode : <nl> + for service in self . project . get_services ( self . services , include_deps = False ) : <nl> + self . log ( ' Pulling image for service % s ' % service . name ) <nl> + # store the existing image ID <nl> + image = service . image ( ) <nl> + old_image_id = None <nl> + if image and image . get ( ' Id ' ) : <nl> + old_image_id = image [ ' Id ' ] <nl> + <nl> + # pull the image <nl> + service . pull ( ignore_pull_failures = False ) <nl> + <nl> + # store the new image ID <nl> + image = service . image ( ) <nl> + new_image_id = None <nl> + if image and image . get ( ' Id ' ) : <nl> + new_image_id = image [ ' Id ' ] <nl> + <nl> + if new_image_id ! = old_image_id : <nl> + # if a new image was pulled <nl> + result [ ' changed ' ] = True <nl> + result [ ' actions ' ] [ service . name ] = dict ( ) <nl> + result [ ' actions ' ] [ service . name ] [ ' pulled_image ' ] = dict ( <nl> + name = service . image_name , <nl> + id = service . image ( ) [ ' Id ' ] <nl> + ) <nl> + return result <nl> + <nl> def cmd_down ( self ) : <nl> result = dict ( <nl> changed = False , <nl> def main ( ) : <nl> scale = dict ( type = ' dict ' ) , <nl> services = dict ( type = ' list ' ) , <nl> dependencies = dict ( type = ' bool ' , default = True ) , <nl> + pull = dict ( type = ' bool ' , default = False ) , <nl> debug = dict ( type = ' bool ' , default = False ) <nl> ) <nl> <nl>\n", "msg": "Add pull option to pull images prior to evaluating service state .\n"}
{"diff_id": 31208, "repo": "bokeh/bokeh\n", "sha": "de37f80ced9b20b9c86ef2060fdf1b4346ba0b85\n", "time": "2013-07-08T14:25:47Z\n", "diff": "mmm a / bokeh / session . py <nl> ppp b / bokeh / session . py <nl> <nl> import logging <nl> import urlparse <nl> import uuid <nl> + import warnings <nl> <nl> import requests <nl> <nl> def add ( self , * objects ) : <nl> \" \" \" <nl> for obj in objects : <nl> if obj is None : <nl> - import pdb ; pdb . set_trace ( ) <nl> - obj . session = self <nl> - self . _models [ obj . _id ] = obj <nl> + warnings . warn ( \" Null object passed to Session . add ( ) \" ) <nl> + else : <nl> + obj . session = self <nl> + self . _models [ obj . _id ] = obj <nl> <nl> def view ( self ) : <nl> \" \" \" Triggers the OS to open a web browser pointing to the file <nl> class HTMLFileSession ( BaseHTMLSession ) : <nl> div_template = \" plots . html \" # template for just the plot < div > <nl> html_template = \" base . html \" # template for the entire HTML file <nl> <nl> + inline_js = True <nl> + inline_css = True <nl> + rootdir = abspath ( split ( __file__ ) [ 0 ] ) <nl> + <nl> def __init__ ( self , filename = \" bokehplot . html \" , plot = None ) : <nl> self . filename = filename <nl> super ( HTMLFileSession , self ) . __init__ ( plot = plot ) <nl> def _load_template ( self , filename ) : <nl> with open ( join ( self . template_dir , filename ) ) as f : <nl> return jinja2 . Template ( f . read ( ) ) <nl> <nl> - def dumps ( self , js = \" inline \" , css = \" inline \" , <nl> - rootdir = abspath ( split ( __file__ ) [ 0 ] ) ) : <nl> + def dumps ( self , js = None , css = None , rootdir = None ) : <nl> \" \" \" Returns the HTML contents as a string <nl> <nl> - * * js * * and * * css * * can be \" inline \" or \" relative \" . In the latter case , <nl> + * * js * * and * * css * * can be \" inline \" or \" relative \" , and they default <nl> + to the values of self . inline_js and self . inline_css . <nl> + <nl> + If these are set to be \" relative \" ( or self . inline_js / css are False ) , <nl> * * rootdir * * can be specified to indicate the base directory from which <nl> - the path to the various static files should be computed . <nl> + the path to the various static files should be computed . * * rootdir * * <nl> + defaults to the value of self . rootdir . <nl> \" \" \" <nl> # FIXME : Handle this more intelligently <nl> the_plot = [ m for m in self . _models . itervalues ( ) if isinstance ( m , Plot ) ] [ 0 ] <nl> def dumps ( self , js = \" inline \" , css = \" inline \" , <nl> div = self . _load_template ( self . div_template ) . render ( <nl> elementid = elementid <nl> ) <nl> - <nl> - if js = = \" inline \" : <nl> + <nl> + if rootdir is None : <nl> + rootdir = self . rootdir <nl> + <nl> + if js = = \" inline \" or self . inline_js : <nl> rawjs = self . _inline_scripts ( self . js_paths ( ) ) . decode ( \" utf - 8 \" ) <nl> jsfiles = [ ] <nl> else : <nl> rawjs = None <nl> jsfiles = [ os . path . relpath ( p , rootdir ) for p in self . js_paths ( ) ] <nl> <nl> - if css = = \" inline \" : <nl> + if css = = \" inline \" or self . inline_css : <nl> rawcss = self . _inline_css ( self . css_paths ( ) ) . decode ( \" utf - 8 \" ) <nl> cssfiles = [ ] <nl> else : <nl> def _inline_scripts ( self , paths ) : <nl> return \" \" <nl> strings = [ ] <nl> for script in paths : <nl> - f_name = join ( self . server_static_dir , script ) <nl> + f_name = abspath ( join ( self . server_static_dir , script ) ) <nl> strings . append ( \" \" \" <nl> / / BEGIN % s <nl> \" \" \" % f_name + open ( f_name ) . read ( ) + \\ <nl> def _inline_css ( self , paths ) : <nl> return \" \" . join ( strings ) <nl> <nl> <nl> - def save ( self , filename = None , js = \" inline \" , css = \" inline \" , <nl> - rootdir = abspath ( split ( __file__ ) [ 0 ] ) ) : <nl> + def save ( self , filename = None , js = None , css = None , rootdir = None ) : <nl> \" \" \" Saves the file contents . Uses self . filename if * * filename * * <nl> is not provided . Overwrites the contents . <nl> <nl> - * * js * * and * * css * * can be \" inline \" or \" relative \" . In the latter case , <nl> + * * js * * and * * css * * can be \" inline \" or \" relative \" , and they default <nl> + to the values of self . inline_js and self . inline_css . <nl> + <nl> + If these are set to be \" relative \" ( or self . inline_js / css are False ) , <nl> * * rootdir * * can be specified to indicate the base directory from which <nl> - the path to the various static files should be computed . <nl> + the path to the various static files should be computed . * * rootdir * * <nl> + defaults to the value of self . rootdir . <nl> \" \" \" <nl> s = self . dumps ( js , css , rootdir ) <nl> if filename is None : <nl>\n", "msg": "Adding inline_js and inline_css attributes to HTMLFileSession , and changing dumps ( ) and save ( ) to also use these\n"}
{"diff_id": 31285, "repo": "matplotlib/matplotlib\n", "sha": "8584bec6b6c360fd5e56f85c0b33b20826b2be87\n", "time": "2009-09-08T17:43:04Z\n", "diff": "mmm a / lib / mpl_toolkits / axes_grid / axislines . py <nl> ppp b / lib / mpl_toolkits / axes_grid / axislines . py <nl> def new_floating_axis ( self , nth_coord , value , <nl> return axisline <nl> <nl> <nl> + def get_gridlines ( self ) : <nl> + \" \" \" <nl> + return list of gridline coordinates in data coordinates . <nl> + \" \" \" <nl> + <nl> + gridlines = [ ] <nl> + <nl> + locs = [ ] <nl> + y1 , y2 = self . axes . get_ylim ( ) <nl> + if self . axes . xaxis . _gridOnMajor : <nl> + locs . extend ( self . axes . xaxis . major . locator ( ) ) <nl> + if self . axes . xaxis . _gridOnMinor : <nl> + locs . extend ( self . axes . xaxis . minor . locator ( ) ) <nl> + <nl> + for x in locs : <nl> + gridlines . append ( [ [ x , x ] , [ y1 , y2 ] ] ) <nl> + <nl> + <nl> + x1 , x2 = self . axes . get_xlim ( ) <nl> + locs = [ ] <nl> + if self . axes . yaxis . _gridOnMajor : <nl> + locs . extend ( self . axes . yaxis . major . locator ( ) ) <nl> + if self . axes . yaxis . _gridOnMinor : <nl> + locs . extend ( self . axes . yaxis . minor . locator ( ) ) <nl> + <nl> + for y in locs : <nl> + gridlines . append ( [ [ x1 , x2 ] , [ y , y ] ] ) <nl> + <nl> + return gridlines <nl> <nl> <nl> from matplotlib . lines import Line2D <nl> def __init__ ( self , * kl , * * kw ) : <nl> <nl> helper = kw . pop ( \" grid_helper \" , None ) <nl> <nl> + self . _axisline_on = True <nl> + <nl> if helper : <nl> self . _grid_helper = helper <nl> else : <nl> self . _grid_helper = GridHelperRectlinear ( self ) <nl> <nl> - self . _axisline_on = True <nl> - <nl> super ( Axes , self ) . __init__ ( * kl , * * kw ) <nl> <nl> self . toggle_axisline ( True ) <nl> def _init_gridlines ( self , grid_helper = None ) : <nl> if grid_helper is None : <nl> grid_helper = self . get_grid_helper ( ) <nl> gridlines . set_grid_helper ( grid_helper ) <nl> - gridlines . set_clip_on ( True ) <nl> + <nl> + self . axes . _set_artist_props ( gridlines ) <nl> + # gridlines . set_clip_path ( self . axes . patch ) <nl> + # set_clip_path need to be defered after Axes . cla is completed . <nl> + # It is done inside the cla . <nl> <nl> self . gridlines = gridlines <nl> <nl> def cla ( self ) : <nl> # gridlines need to b created before cla ( ) since cla calls grid ( ) <nl> - self . _init_gridlines ( ) <nl> <nl> + self . _init_gridlines ( ) <nl> super ( Axes , self ) . cla ( ) <nl> + <nl> + # the clip_path should be set after Axes . cla ( ) since that ' s <nl> + # when a patch is created . <nl> + self . gridlines . set_clip_path ( self . axes . patch ) <nl> + <nl> self . _init_axis_artists ( ) <nl> <nl> def get_grid_helper ( self ) : <nl> def get_grid_helper ( self ) : <nl> <nl> <nl> def grid ( self , b = None , * * kwargs ) : <nl> + \" \" \" <nl> + Toggel the gridlines , and optionally set the properties of the lines . <nl> + \" \" \" <nl> + # their are some discrepancy between the behavior of grid in <nl> + # axes_grid and the original mpl ' s grid , because axes_grid <nl> + # explicitly set the visibility of the gridlines . <nl> + <nl> + super ( Axes , self ) . grid ( b , * * kwargs ) <nl> if not self . _axisline_on : <nl> - super ( Axes , self ) . grid ( b , * * kwargs ) <nl> return <nl> <nl> if b is None : <nl> - b = not self . gridlines . get_visible ( ) <nl> <nl> + if self . axes . xaxis . _gridOnMinor or self . axes . xaxis . _gridOnMajor or \\ <nl> + self . axes . yaxis . _gridOnMinor or self . axes . yaxis . _gridOnMajor : <nl> + b = True <nl> + else : <nl> + b = False <nl> + <nl> self . gridlines . set_visible ( b ) <nl> <nl> if len ( kwargs ) : <nl>\n", "msg": "axes_grid : add grid support for rectlinear axes\n"}
{"diff_id": 31397, "repo": "scrapy/scrapy\n", "sha": "ee9881d2704798c9cd61b6da503bb0694227c58c\n", "time": "2019-12-18T11:08:34Z\n", "diff": "mmm a / scrapy / linkextractors / __init__ . py <nl> ppp b / scrapy / linkextractors / __init__ . py <nl> def __new__ ( cls , * args , * * kwargs ) : <nl> warn ( ' scrapy . linkextractors . FilteringLinkExtractor is deprecated , ' <nl> ' please use scrapy . linkextractors . LinkExtractor instead ' , <nl> ScrapyDeprecationWarning , stacklevel = 2 ) <nl> - return super ( FilteringLinkExtractor , cls ) . __new__ ( cls ) <nl> + return super ( ) . __new__ ( cls , * args , * * kwargs ) <nl> <nl> def __init__ ( self , link_extractor , allow , deny , allow_domains , deny_domains , <nl> restrict_xpaths , canonicalize , deny_extensions , restrict_css , restrict_text ) : <nl>\n", "msg": "Improve FilteringLinkExtractor . __new__\n"}
{"diff_id": 31401, "repo": "zulip/zulip\n", "sha": "979d997cf80f29aac08add4a2dce614e6040ab5e\n", "time": "2012-11-08T15:48:13Z\n", "diff": "mmm a / zephyr / models . py <nl> ppp b / zephyr / models . py <nl> def bulk_create_huddles ( users , huddle_user_list ) : <nl> huddles = { } <nl> huddles_by_id = { } <nl> huddle_set = set ( ) <nl> - existing_huddles = { } <nl> + existing_huddles = set ( ) <nl> for huddle in Huddle . objects . all ( ) : <nl> - existing_huddles [ huddle . huddle_hash ] = True <nl> + existing_huddles . add ( huddle . huddle_hash ) <nl> for huddle_users in huddle_user_list : <nl> user_ids = [ users [ email ] . id for email in huddle_users ] <nl> huddle_hash = get_huddle_hash ( user_ids ) <nl>\n", "msg": "bulk_create_huddles : Use a set for existing_huddles\n"}
{"diff_id": 31471, "repo": "ansible/ansible\n", "sha": "ef23cf31b096396e939e1f0c075c53460d06ba12\n", "time": "2016-12-08T16:34:32Z\n", "diff": "mmm a / lib / ansible / modules / extras / clustering / consul_acl . py <nl> ppp b / lib / ansible / modules / extras / clustering / consul_acl . py <nl> def yml_to_rules ( module , yml_rules ) : <nl> rules . add_rule ( ' key ' , Rule ( rule [ ' key ' ] , rule [ ' policy ' ] ) ) <nl> elif ( ' service ' in rule and ' policy ' in rule ) : <nl> rules . add_rule ( ' service ' , Rule ( rule [ ' service ' ] , rule [ ' policy ' ] ) ) <nl> + elif ( ' event ' in rule and ' policy ' in rule ) : <nl> + rules . add_rule ( ' event ' , Rule ( rule [ ' event ' ] , rule [ ' policy ' ] ) ) <nl> + elif ( ' query ' in rule and ' policy ' in rule ) : <nl> + rules . add_rule ( ' query ' , Rule ( rule [ ' query ' ] , rule [ ' policy ' ] ) ) <nl> else : <nl> - module . fail_json ( msg = \" a rule requires a key / service and a policy . \" ) <nl> + module . fail_json ( msg = \" a rule requires a key / service / event or query and a policy . \" ) <nl> return rules <nl> <nl> template = ' ' ' % s \" % s \" { <nl> def yml_to_rules ( module , yml_rules ) : <nl> } <nl> ' ' ' <nl> <nl> - RULE_TYPES = [ ' key ' , ' service ' ] <nl> + RULE_TYPES = [ ' key ' , ' service ' , ' event ' , ' query ' ] <nl> <nl> class Rules : <nl> <nl>\n", "msg": "Add ability to create event and query acl rules for a given acl token ( )\n"}
{"diff_id": 31529, "repo": "matplotlib/matplotlib\n", "sha": "b5cf81d43e573a475b172ab7f1dc5ccd345f8347\n", "time": "2020-06-25T22:42:11Z\n", "diff": "mmm a / lib / matplotlib / backends / _backend_tk . py <nl> ppp b / lib / matplotlib / backends / _backend_tk . py <nl> def showtip ( self , text ) : <nl> except tk . TclError : <nl> pass <nl> label = tk . Label ( tw , text = self . text , justify = tk . LEFT , <nl> - background = \" # ffffe0 \" , relief = tk . SOLID , borderwidth = 1 ) <nl> + relief = tk . SOLID , borderwidth = 1 ) <nl> label . pack ( ipadx = 1 ) <nl> <nl> def hidetip ( self ) : <nl>\n", "msg": "Backport PR : Fix tk tooltips for dark themes .\n"}
{"diff_id": 31534, "repo": "spotify/luigi\n", "sha": "5ac2c241ca108e4e93574e84114f28ce2841c5e6\n", "time": "2015-09-11T15:35:27Z\n", "diff": "mmm a / luigi / scheduler . py <nl> ppp b / luigi / scheduler . py <nl> def __init__ ( self , state_path ) : <nl> self . _status_tasks = collections . defaultdict ( dict ) <nl> self . _active_workers = { } # map from id to a Worker object <nl> <nl> + def get_state ( self ) : <nl> + return self . _tasks , self . _active_workers <nl> + <nl> + def set_state ( self , state ) : <nl> + self . _tasks , self . _active_workers = state <nl> + <nl> def dump ( self ) : <nl> - state = ( self . _tasks , self . _active_workers ) <nl> try : <nl> with open ( self . _state_path , ' wb ' ) as fobj : <nl> - pickle . dump ( state , fobj ) <nl> + pickle . dump ( self . get_state ( ) , fobj ) <nl> except IOError : <nl> logger . warning ( \" Failed saving scheduler state \" , exc_info = 1 ) <nl> else : <nl> def load ( self ) : <nl> logger . exception ( \" Error when loading state . Starting from clean slate . \" ) <nl> return <nl> <nl> - self . _tasks , self . _active_workers = state <nl> + self . set_state ( state ) <nl> self . _status_tasks = collections . defaultdict ( dict ) <nl> for task in six . itervalues ( self . _tasks ) : <nl> self . _status_tasks [ task . status ] [ task . id ] = task <nl>\n", "msg": "Refactor getter / setter of scheduler state to their own methods .\n"}
{"diff_id": 31683, "repo": "quantopian/zipline\n", "sha": "b3c23b0d8b539b4cb019a62e3b052b0c632954eb\n", "time": "2013-04-02T15:27:34Z\n", "diff": "mmm a / zipline / finance / risk . py <nl> ppp b / zipline / finance / risk . py <nl> def advance_by_months ( dt , jump_in_months ) : <nl> # # # # # # # # # # # # # # # # # # # # # # # # # # # # <nl> <nl> <nl> - def sharpe ( algorithm_volatility , algorithm_return , treasury_return ) : <nl> + def sharpe_ratio ( algorithm_volatility , algorithm_return , treasury_return ) : <nl> \" \" \" <nl> http : / / en . wikipedia . org / wiki / Sharpe_ratio <nl> <nl> def sharpe ( algorithm_volatility , algorithm_return , treasury_return ) : <nl> return ( algorithm_return - treasury_return ) / algorithm_volatility <nl> <nl> <nl> - def sortino ( algorithm_returns , algorithm_period_return , mar ) : <nl> + def sortino_ratio ( algorithm_returns , algorithm_period_return , mar ) : <nl> \" \" \" <nl> http : / / en . wikipedia . org / wiki / Sortino_ratio <nl> <nl> def sortino ( algorithm_returns , algorithm_period_return , mar ) : <nl> return ( algorithm_period_return - mar ) / dr <nl> <nl> <nl> - def information ( algorithm_returns , benchmark_returns ) : <nl> + def information_ratio ( algorithm_returns , benchmark_returns ) : <nl> \" \" \" <nl> http : / / en . wikipedia . org / wiki / Information_ratio <nl> <nl> def alpha ( algorithm_period_return , treasury_period_return , <nl> Return percentage for treasury period . <nl> benchmark_period_return ( float ) : <nl> Return percentage for benchmark period . <nl> - beat ( float ) : <nl> + beta ( float ) : <nl> beta value for the same period as all other values <nl> <nl> Returns : <nl> def calculate_sharpe ( self ) : <nl> \" \" \" <nl> http : / / en . wikipedia . org / wiki / Sharpe_ratio <nl> \" \" \" <nl> - return sharpe ( self . algorithm_volatility , <nl> - self . algorithm_period_returns , <nl> - self . treasury_period_return ) <nl> + return sharpe_ratio ( self . algorithm_volatility , <nl> + self . algorithm_period_returns , <nl> + self . treasury_period_return ) <nl> <nl> def calculate_sortino ( self , mar = None ) : <nl> \" \" \" <nl> def calculate_sortino ( self , mar = None ) : <nl> if mar is None : <nl> mar = self . treasury_period_return <nl> <nl> - return sortino ( self . algorithm_returns , <nl> - self . algorithm_period_returns , <nl> - mar ) <nl> + return sortino_ratio ( self . algorithm_returns , <nl> + self . algorithm_period_returns , <nl> + mar ) <nl> <nl> def calculate_information ( self ) : <nl> \" \" \" <nl> http : / / en . wikipedia . org / wiki / Information_ratio <nl> \" \" \" <nl> - return information ( self . algorithm_returns , self . benchmark_returns ) <nl> + return information_ratio ( self . algorithm_returns , <nl> + self . benchmark_returns ) <nl> <nl> def calculate_beta ( self ) : <nl> \" \" \" <nl> def calculate_sharpe ( self ) : <nl> \" \" \" <nl> http : / / en . wikipedia . org / wiki / Sharpe_ratio <nl> \" \" \" <nl> - return sharpe ( self . algorithm_volatility [ - 1 ] , <nl> - self . algorithm_period_returns [ - 1 ] , <nl> - self . treasury_period_return ) <nl> + return sharpe_ratio ( self . algorithm_volatility [ - 1 ] , <nl> + self . algorithm_period_returns [ - 1 ] , <nl> + self . treasury_period_return ) <nl> <nl> def calculate_sortino ( self , mar = None ) : <nl> \" \" \" <nl> def calculate_sortino ( self , mar = None ) : <nl> if mar is None : <nl> mar = self . treasury_period_return <nl> <nl> - return sortino ( np . array ( self . algorithm_returns ) , <nl> - self . algorithm_period_returns [ - 1 ] , <nl> - mar ) <nl> + return sortino_ratio ( np . array ( self . algorithm_returns ) , <nl> + self . algorithm_period_returns [ - 1 ] , <nl> + mar ) <nl> <nl> def calculate_information ( self ) : <nl> \" \" \" <nl> http : / / en . wikipedia . org / wiki / Information_ratio <nl> \" \" \" <nl> A = np . array <nl> - return information ( A ( self . algorithm_returns ) , <nl> - A ( self . benchmark_returns ) ) <nl> + return information_ratio ( A ( self . algorithm_returns ) , <nl> + A ( self . benchmark_returns ) ) <nl> <nl> def calculate_alpha ( self ) : <nl> \" \" \" <nl>\n", "msg": "MAINT : Rename risk metric function names to include ' _ratio ' suffix .\n"}
{"diff_id": 31740, "repo": "zulip/zulip\n", "sha": "0badbbf0e281bec39613924bd0e1756d549adc0c\n", "time": "2017-02-07T06:34:01Z\n", "diff": "mmm a / zerver / tests / test_decorators . py <nl> ppp b / zerver / tests / test_decorators . py <nl> <nl> # - * - coding : utf - 8 - * - <nl> import mock <nl> <nl> - from typing import Any , Iterable , Optional , Text <nl> + from typing import Any , Iterable , List , Optional , Text , Tuple <nl> from django . test import TestCase <nl> from django . utils . translation import ugettext as _ <nl> from django . http import HttpResponse , HttpRequest <nl> <nl> return_success_on_head_request <nl> ) <nl> from zerver . lib . validator import ( <nl> - check_string , check_dict , check_bool , check_int , check_list <nl> + check_string , check_dict , check_bool , check_int , check_list , Validator <nl> ) <nl> from zerver . models import \\ <nl> get_realm , get_user_profile_by_email , UserProfile , Client <nl> def test_check_dict ( self ) : <nl> keys = [ <nl> ( ' names ' , check_list ( check_string ) ) , <nl> ( ' city ' , check_string ) , <nl> - ] <nl> + ] # type : List [ Tuple [ str , Validator ] ] <nl> <nl> x = { <nl> ' names ' : [ ' alice ' , ' bob ' ] , <nl>\n", "msg": "Type annotate a variable to prevent future errors .\n"}
{"diff_id": 31864, "repo": "scrapy/scrapy\n", "sha": "20efdc027355e4e8a880de5af5214e3f10db41fe\n", "time": "2010-10-29T18:21:36Z\n", "diff": "mmm a / scrapy / commands / deploy . py <nl> ppp b / scrapy / commands / deploy . py <nl> def add_options ( self , parser ) : <nl> help = \" list available targets \" ) <nl> parser . add_option ( \" - l \" , \" - - list - projects \" , metavar = \" TARGET \" , \\ <nl> help = \" list available projects on TARGET \" ) <nl> + parser . add_option ( \" - - egg \" , metavar = \" FILE \" , <nl> + help = \" use the given egg , instead of building it \" ) <nl> <nl> def run ( self , args , opts ) : <nl> try : <nl> def run ( self , args , opts ) : <nl> return <nl> target , project = _get_target_project ( args ) <nl> version = _get_version ( opts ) <nl> - egg = _build_egg ( ) <nl> + if opts . egg : <nl> + egg = open ( opts . egg , ' rb ' ) <nl> + else : <nl> + _log ( \" Bulding egg of % s - % s \" % ( project , version ) ) <nl> + egg = _build_egg ( ) <nl> _upload_egg ( target , egg , project , version ) <nl> <nl> def _log ( message ) : <nl>\n", "msg": "added - - egg argument to scrapy deploy command , and log message when building the egg\n"}
{"diff_id": 31865, "repo": "python/cpython\n", "sha": "6d394d9b6875a750cc57283a6e205fbdbd0cb5de\n", "time": "2005-07-08T15:03:37Z\n", "diff": "mmm a / Tools / bgen / bgen / bgenObjectDefinition . py <nl> ppp b / Tools / bgen / bgen / bgenObjectDefinition . py <nl> class ObjectDefinition ( GeneratorGroup ) : <nl> basechain = \" NULL \" <nl> tp_flags = \" Py_TPFLAGS_DEFAULT \" <nl> basetype = None <nl> + argref = \" \" # set to \" * \" if arg to < type > _New should be pointer <nl> <nl> def __init__ ( self , name , prefix , itselftype ) : <nl> \" \" \" ObjectDefinition constructor . May be extended , but do not override . <nl> def __init__ ( self , name , prefix , itselftype ) : <nl> self . itselftype = itselftype <nl> self . objecttype = name + ' Object ' <nl> self . typename = name + ' _Type ' <nl> - self . argref = \" \" # set to \" * \" if arg to < type > _New should be pointer <nl> self . static = \" static \" # set to \" \" to make < type > _New and < type > _Convert public <nl> self . modulename = None <nl> if hasattr ( self , \" assertions \" ) : <nl>\n", "msg": "Handle argref so it can be overridden more easily in a subclass .\n"}
{"diff_id": 31975, "repo": "explosion/spaCy\n", "sha": "6bbdcc5db5bf8a96e7110db3bc64a51306b86073\n", "time": "2015-05-30T03:23:02Z\n", "diff": "mmm a / bin / parser / train . py <nl> ppp b / bin / parser / train . py <nl> def score_model ( scorer , nlp , raw_text , annot_tuples ) : <nl> scorer . score ( tokens , gold , verbose = False ) <nl> <nl> <nl> + def _merge_sents ( sents ) : <nl> + m_deps = [ [ ] , [ ] , [ ] , [ ] , [ ] , [ ] ] <nl> + m_brackets = [ ] <nl> + i = 0 <nl> + for ( ids , words , tags , heads , labels , ner ) , brackets in sents : <nl> + m_deps [ 0 ] . extend ( id_ + i for id_ in ids ) <nl> + m_deps [ 1 ] . extend ( words ) <nl> + m_deps [ 2 ] . extend ( tags ) <nl> + m_deps [ 3 ] . extend ( head + i for head in heads ) <nl> + m_deps [ 4 ] . extend ( labels ) <nl> + m_deps [ 5 ] . extend ( ner ) <nl> + m_brackets . extend ( ( b [ ' first ' ] + i , b [ ' last ' ] + i , b [ ' label ' ] ) for b in brackets ) <nl> + i + = len ( ids ) <nl> + return [ ( m_deps , m_brackets ) ] <nl> + <nl> + <nl> def train ( Language , gold_tuples , model_dir , n_iter = 15 , feat_set = u ' basic ' , seed = 0 , <nl> gold_preproc = False , n_sents = 0 , corruption_level = 0 ) : <nl> dep_model_dir = path . join ( model_dir , ' deps ' ) <nl> def train ( Language , gold_tuples , model_dir , n_iter = 15 , feat_set = u ' basic ' , seed = 0 <nl> scorer = Scorer ( ) <nl> loss = 0 <nl> for raw_text , sents in gold_tuples : <nl> - if not gold_preproc : <nl> + if gold_preproc : <nl> + raw_text = None <nl> + else : <nl> sents = _merge_sents ( sents ) <nl> for annot_tuples , ctnt in sents : <nl> score_model ( scorer , nlp , raw_text , annot_tuples ) <nl> - if raw_text is None or gold_preproc : <nl> + if raw_text is None : <nl> tokens = nlp . tokenizer . tokens_from_list ( annot_tuples [ 1 ] ) <nl> else : <nl> tokens = nlp . tokenizer ( raw_text ) <nl> def train ( Language , gold_tuples , model_dir , n_iter = 15 , feat_set = u ' basic ' , seed = 0 <nl> nlp . vocab . strings . dump ( path . join ( model_dir , ' vocab ' , ' strings . txt ' ) ) <nl> <nl> <nl> - def evaluate ( Language , gold_tuples , model_dir , gold_preproc = False , verbose = True ) : <nl> + def evaluate ( Language , gold_tuples , model_dir , gold_preproc = False , verbose = False ) : <nl> nlp = Language ( data_dir = model_dir ) <nl> scorer = Scorer ( ) <nl> for raw_text , sents in gold_tuples : <nl> + if gold_preproc : <nl> + raw_text = None <nl> + else : <nl> + sents = _merge_sents ( sents ) <nl> for annot_tuples , brackets in sents : <nl> - if raw_text is None or gold_preproc : <nl> + if raw_text is None : <nl> tokens = nlp . tokenizer . tokens_from_list ( annot_tuples [ 1 ] ) <nl> nlp . tagger ( tokens ) <nl> nlp . entity ( tokens ) <nl> def evaluate ( Language , gold_tuples , model_dir , gold_preproc = False , verbose = True ) <nl> tokens = nlp ( raw_text , merge_mwes = False ) <nl> gold = GoldParse ( tokens , annot_tuples ) <nl> scorer . score ( tokens , gold , verbose = verbose ) <nl> - for t in tokens : <nl> - print t . orth_ , t . dep_ , t . head . orth_ , t . ent_type_ <nl> return scorer <nl> <nl> <nl> def main ( train_loc , dev_loc , model_dir , n_sents = 0 , n_iter = 15 , out_loc = \" \" , verbos <nl> feat_set = ' basic ' if not debug else ' debug ' , <nl> gold_preproc = gold_preproc , n_sents = n_sents , <nl> corruption_level = corruption_level , n_iter = n_iter ) <nl> - # if out_loc : <nl> - # write_parses ( English , dev_loc , model_dir , out_loc ) <nl> + if out_loc : <nl> + write_parses ( English , dev_loc , model_dir , out_loc ) <nl> scorer = evaluate ( English , list ( read_json_file ( dev_loc ) ) , <nl> model_dir , gold_preproc = gold_preproc , verbose = verbose ) <nl> print ' TOK ' , 100 - scorer . token_acc <nl>\n", "msg": "* Fix gold_preproc flag in train . py\n"}
{"diff_id": 32029, "repo": "quantopian/zipline\n", "sha": "0e0fecf4ee73144604085a45411de17577800ac2\n", "time": "2013-03-21T19:13:56Z\n", "diff": "mmm a / zipline / utils / simfactory . py <nl> ppp b / zipline / utils / simfactory . py <nl> def create_test_zipline ( * * config ) : <nl> test_algo . sim_params , <nl> concurrent = concurrent_trades <nl> ) <nl> - <nl> - test_algo . set_sources ( [ trade_source ] ) <nl> + if trade_source : <nl> + test_algo . set_sources ( [ trade_source ] ) <nl> <nl> # mmmmmmmmmmmmmmmmmm - <nl> # Transforms <nl>\n", "msg": "allows config to force no datasources by passing a None .\n"}
{"diff_id": 32048, "repo": "ansible/ansible\n", "sha": "7f351a46e5506631d522c0495e1a0c4b345030f6\n", "time": "2016-12-08T16:35:10Z\n", "diff": "mmm a / lib / ansible / modules / extras / system / locale_gen . py <nl> ppp b / lib / ansible / modules / extras / system / locale_gen . py <nl> def is_present ( name ) : <nl> def fix_case ( name ) : <nl> \" \" \" locale - a might return the encoding in either lower or upper case . <nl> Passing through this function makes them uniform for comparisons . \" \" \" <nl> - for s , r in LOCALE_NORMALIZATION . iteritems ( ) : <nl> + for s , r in LOCALE_NORMALIZATION . items ( ) : <nl> name = name . replace ( s , r ) <nl> return name <nl> <nl>\n", "msg": "replace iteritems with items to ensure python3 compatibility\n"}
{"diff_id": 32084, "repo": "matplotlib/matplotlib\n", "sha": "8d3defcebe8fe2c7b0dcb53f146d919620024fbb\n", "time": "2020-12-09T21:17:19Z\n", "diff": "mmm a / lib / matplotlib / axes / _base . py <nl> ppp b / lib / matplotlib / axes / _base . py <nl> def __init__ ( self , bounds , transform ) : <nl> self . _transform = transform <nl> <nl> def __call__ ( self , ax , renderer ) : <nl> - # Subtracting transFigure will typically rely on inverted ( ) , freezing <nl> - # the transform ; thus , this needs to be delayed until draw time as <nl> - # transFigure may otherwise change after this is evaluated . <nl> + # Subtracting transSubfigure will typically rely on inverted ( ) , <nl> + # freezing the transform ; thus , this needs to be delayed until draw <nl> + # time as transSubfigure may otherwise change after this is evaluated . <nl> return mtransforms . TransformedBbox ( <nl> mtransforms . Bbox . from_bounds ( * self . _bounds ) , <nl> - self . _transform - ax . figure . transFigure ) <nl> + self . _transform - ax . figure . transSubfigure ) <nl> <nl> <nl> def _process_plot_format ( fmt ) : <nl> def set_position ( self , pos , which = ' both ' ) : <nl> Axes have two position attributes . The ' original ' position is the <nl> position allocated for the Axes . The ' active ' position is the <nl> position the Axes is actually drawn at . These positions are usually <nl> - the same unless a fixed aspect is set to the Axes . See ` . set_aspect ` <nl> - for details . <nl> + the same unless a fixed aspect is set to the Axes . See <nl> + ` . Axes . set_aspect ` for details . <nl> <nl> Parameters <nl> mmmmmmmmm - <nl> def set_aspect ( self , aspect , adjustable = None , anchor = None , share = False ) : <nl> etc . <nl> = = = = = = = = = = = = = = = = = = = = = = = = = = <nl> <nl> - See ` . set_anchor ` for further details . <nl> + See ` ~ . Axes . set_anchor ` for further details . <nl> <nl> share : bool , default : False <nl> If ` ` True ` ` , apply the settings to all shared Axes . <nl>\n", "msg": "FIX : update a transform from transFigure to transSubfigure\n"}
{"diff_id": 32245, "repo": "encode/django-rest-framework\n", "sha": "959e23426088661e12f5c8c3fafc64da42d1fc24\n", "time": "2014-09-05T22:33:47Z\n", "diff": "mmm a / tests / test_parsers . py <nl> ppp b / tests / test_parsers . py <nl> <nl> - # - * - coding : utf - 8 - * - <nl> + # - * - coding : utf - 8 - * - <nl> <nl> from __future__ import unicode_literals <nl> from rest_framework . compat import StringIO <nl> def test_get_encoded_filename ( self ) : <nl> <nl> def __replace_content_disposition ( self , disposition ) : <nl> self . parser_context [ ' request ' ] . META [ ' HTTP_CONTENT_DISPOSITION ' ] = disposition <nl> - <nl> - <nl>\n", "msg": "Move parser tests to correct directory\n"}
{"diff_id": 32266, "repo": "ytdl-org/youtube-dl\n", "sha": "6bceb36b9939b2fcc8417cc734fa9e376d67e4f2\n", "time": "2017-05-27T22:43:04Z\n", "diff": "mmm a / youtube_dl / extractor / beampro . py <nl> ppp b / youtube_dl / extractor / beampro . py <nl> <nl> <nl> <nl> class BeamProBaseIE ( InfoExtractor ) : <nl> + _API_BASE = ' https : / / mixer . com / api / v1 ' <nl> _RATINGS = { ' family ' : 0 , ' teen ' : 13 , ' 18 + ' : 18 } <nl> <nl> def _extract_channel_info ( self , chan ) : <nl> def _extract_channel_info ( self , chan ) : <nl> <nl> <nl> class BeamProLiveIE ( BeamProBaseIE ) : <nl> - IE_NAME = ' Beam : live ' <nl> - _VALID_URL = r ' https ? : / / ( ? : \\ w + \\ . ) ? beam \\ . pro / ( ? P < id > [ ^ / ? # & ] + ) ' <nl> + IE_NAME = ' Mixer : live ' <nl> + _VALID_URL = r ' https ? : / / ( ? : \\ w + \\ . ) ? ( ? : beam \\ . pro | mixer \\ . com ) / ( ? P < id > [ ^ / ? # & ] + ) ' <nl> _TEST = { <nl> - ' url ' : ' http : / / www . beam . pro / niterhayven ' , <nl> + ' url ' : ' http : / / mixer . com / niterhayven ' , <nl> ' info_dict ' : { <nl> ' id ' : ' 261562 ' , <nl> ' ext ' : ' mp4 ' , <nl> class BeamProLiveIE ( BeamProBaseIE ) : <nl> } , <nl> } <nl> <nl> + _MANIFEST_URL_TEMPLATE = ' % s / channels / % % s / manifest . % % s ' % BeamProBaseIE . _API_BASE <nl> + <nl> @ classmethod <nl> def suitable ( cls , url ) : <nl> return False if BeamProVodIE . suitable ( url ) else super ( BeamProLiveIE , cls ) . suitable ( url ) <nl> def _real_extract ( self , url ) : <nl> channel_name = self . _match_id ( url ) <nl> <nl> chan = self . _download_json ( <nl> - ' https : / / beam . pro / api / v1 / channels / % s ' % channel_name , channel_name ) <nl> + ' % s / channels / % s ' % ( self . _API_BASE , channel_name ) , channel_name ) <nl> <nl> if chan . get ( ' online ' ) is False : <nl> raise ExtractorError ( <nl> def _real_extract ( self , url ) : <nl> <nl> channel_id = chan [ ' id ' ] <nl> <nl> + def manifest_url ( kind ) : <nl> + return self . _MANIFEST_URL_TEMPLATE % ( channel_id , kind ) <nl> + <nl> formats = self . _extract_m3u8_formats ( <nl> - ' https : / / beam . pro / api / v1 / channels / % s / manifest . m3u8 ' % channel_id , <nl> - channel_name , ext = ' mp4 ' , m3u8_id = ' hls ' , fatal = False ) <nl> + manifest_url ( ' m3u8 ' ) , channel_name , ext = ' mp4 ' , m3u8_id = ' hls ' , <nl> + fatal = False ) <nl> + formats . extend ( self . _extract_smil_formats ( <nl> + manifest_url ( ' smil ' ) , channel_name , fatal = False ) ) <nl> self . _sort_formats ( formats ) <nl> <nl> info = { <nl> ' id ' : compat_str ( chan . get ( ' id ' ) or channel_name ) , <nl> ' title ' : self . _live_title ( chan . get ( ' name ' ) or channel_name ) , <nl> ' description ' : clean_html ( chan . get ( ' description ' ) ) , <nl> - ' thumbnail ' : try_get ( chan , lambda x : x [ ' thumbnail ' ] [ ' url ' ] , compat_str ) , <nl> + ' thumbnail ' : try_get ( <nl> + chan , lambda x : x [ ' thumbnail ' ] [ ' url ' ] , compat_str ) , <nl> ' timestamp ' : parse_iso8601 ( chan . get ( ' updatedAt ' ) ) , <nl> ' is_live ' : True , <nl> ' view_count ' : int_or_none ( chan . get ( ' viewersTotal ' ) ) , <nl> def _real_extract ( self , url ) : <nl> <nl> <nl> class BeamProVodIE ( BeamProBaseIE ) : <nl> - IE_NAME = ' Beam : vod ' <nl> - _VALID_URL = r ' https ? : / / ( ? : \\ w + \\ . ) ? beam \\ . pro / [ ^ / ? # & ] + . * [ ? & ] vod = ( ? P < id > \\ d + ) ' <nl> + IE_NAME = ' Mixer : vod ' <nl> + _VALID_URL = r ' https ? : / / ( ? : \\ w + \\ . ) ? ( ? : beam \\ . pro | mixer \\ . com ) / [ ^ / ? # & ] + \\ ? . * ? \\ bvod = ( ? P < id > \\ d + ) ' <nl> _TEST = { <nl> - ' url ' : ' https : / / beam . pro / willow8714 ? vod = 2259830 ' , <nl> + ' url ' : ' https : / / mixer . com / willow8714 ? vod = 2259830 ' , <nl> ' md5 ' : ' b2431e6e8347dc92ebafb565d368b76b ' , <nl> ' info_dict ' : { <nl> ' id ' : ' 2259830 ' , <nl> class BeamProVodIE ( BeamProBaseIE ) : <nl> ' age_limit ' : 13 , <nl> ' view_count ' : int , <nl> } , <nl> + ' params ' : { <nl> + ' skip_download ' : True , <nl> + } , <nl> } <nl> <nl> - def _extract_format ( self , vod , vod_type ) : <nl> + @ staticmethod <nl> + def _extract_format ( vod , vod_type ) : <nl> if not vod . get ( ' baseUrl ' ) : <nl> return [ ] <nl> <nl> if vod_type = = ' hls ' : <nl> - filename , protocol = ' manifest . m3u8 ' , ' m3u8 ' <nl> + filename , protocol = ' manifest . m3u8 ' , ' m3u8_native ' <nl> elif vod_type = = ' raw ' : <nl> filename , protocol = ' source . mp4 ' , ' https ' <nl> else : <nl> - return [ ] <nl> + assert False <nl> <nl> - data = vod . get ( ' data ' ) or { } <nl> + data = vod . get ( ' data ' ) if isinstance ( vod . get ( ' data ' ) , dict ) else { } <nl> <nl> format_id = [ vod_type ] <nl> - if ' Height ' in data : <nl> + if isinstance ( data . get ( ' Height ' ) , compat_str ) : <nl> format_id . append ( ' % sp ' % data [ ' Height ' ] ) <nl> <nl> return [ { <nl> def _real_extract ( self , url ) : <nl> vod_id = self . _match_id ( url ) <nl> <nl> vod_info = self . _download_json ( <nl> - ' https : / / beam . pro / api / v1 / recordings / % s ' % vod_id , vod_id ) <nl> + ' % s / recordings / % s ' % ( self . _API_BASE , vod_id ) , vod_id ) <nl> <nl> state = vod_info . get ( ' state ' ) <nl> if state ! = ' AVAILABLE ' : <nl> raise ExtractorError ( <nl> - ' VOD % s is not available ( state : % s ) ' % ( vod_id , state ) , expected = True ) <nl> + ' VOD % s is not available ( state : % s ) ' % ( vod_id , state ) , <nl> + expected = True ) <nl> <nl> formats = [ ] <nl> thumbnail_url = None <nl> def _real_extract ( self , url ) : <nl> ' view_count ' : int_or_none ( vod_info . get ( ' viewsTotal ' ) ) , <nl> ' formats ' : formats , <nl> } <nl> - <nl> - chan = vod_info . get ( ' channel ' ) or { } <nl> - info . update ( self . _extract_channel_info ( chan ) ) <nl> + info . update ( self . _extract_channel_info ( vod_info . get ( ' channel ' ) or { } ) ) <nl> <nl> return info <nl>\n", "msg": "[ beam ] Improve and add support for mixer . com ( closes )\n"}
{"diff_id": 32286, "repo": "ipython/ipython\n", "sha": "fff0bd6f8b0b5e2424734d4587c4d69c040417bb\n", "time": "2016-05-25T08:04:01Z\n", "diff": "mmm a / IPython / terminal / ptshell . py <nl> ppp b / IPython / terminal / ptshell . py <nl> <nl> <nl> from prompt_toolkit . completion import Completer , Completion <nl> from prompt_toolkit . enums import DEFAULT_BUFFER , SEARCH_BUFFER , EditingMode <nl> - from prompt_toolkit . filters import HasFocus , HasSelection , Condition , ViInsertMode , EmacsInsertMode <nl> + from prompt_toolkit . filters import HasFocus , HasSelection , Condition , ViInsertMode , EmacsInsertMode , IsDone <nl> from prompt_toolkit . history import InMemoryHistory <nl> from prompt_toolkit . shortcuts import create_prompt_application , create_eventloop , create_prompt_layout <nl> from prompt_toolkit . interface import CommandLineInterface <nl> <nl> from prompt_toolkit . keys import Keys <nl> from prompt_toolkit . layout . lexers import Lexer <nl> from prompt_toolkit . layout . lexers import PygmentsLexer <nl> + from prompt_toolkit . layout . processors import ConditionalProcessor , HighlightMatchingBracketProcessor <nl> from prompt_toolkit . styles import PygmentsStyle , DynamicStyle <nl> <nl> from pygments . styles import get_style_by_name , get_all_styles <nl> def _highlighting_style_changed ( self , change ) : <nl> help = \" Display a multi column completion menu . \" , <nl> ) . tag ( config = True ) <nl> <nl> - <nl> + highlight_matching_brackets = Bool ( False , <nl> + help = \" Highlight matching brackets . \" , <nl> + ) . tag ( config = True ) <nl> + <nl> @ observe ( ' term_title ' ) <nl> def init_term_title ( self , change = None ) : <nl> # Enable or disable the terminal title . <nl> def _layout_options ( self ) : <nl> ' get_continuation_tokens ' : self . get_continuation_tokens , <nl> ' multiline ' : True , <nl> ' display_completions_in_columns ' : self . display_completions_in_columns , <nl> + <nl> + # Highlight matching brackets , but only when this setting is <nl> + # enabled , and only when the DEFAULT_BUFFER has the focus . <nl> + ' extra_input_processors ' : [ ConditionalProcessor ( <nl> + processor = HighlightMatchingBracketProcessor ( chars = ' [ ] ( ) { } ' ) , <nl> + filter = HasFocus ( DEFAULT_BUFFER ) & ~ IsDone ( ) & <nl> + Condition ( lambda cli : self . highlight_matching_brackets ) ) ] , <nl> } <nl> <nl> def _update_layout ( self ) : <nl>\n", "msg": "Added option to ptshell for highlighting matching brackets .\n"}
{"diff_id": 32326, "repo": "ansible/ansible\n", "sha": "7f7e730dea36dbb709b47c39ca1a28cb9f6cb3f1\n", "time": "2015-12-11T19:55:44Z\n", "diff": "mmm a / lib / ansible / plugins / strategy / __init__ . py <nl> ppp b / lib / ansible / plugins / strategy / __init__ . py <nl> <nl> <nl> from ansible import constants as C <nl> from ansible . errors import AnsibleError , AnsibleParserError , AnsibleUndefinedVariable <nl> + < < < < < < < Updated upstream <nl> + = = = = = = = <nl> + from ansible . executor . play_iterator import PlayIterator <nl> + from ansible . executor . process . worker import WorkerProcess <nl> + > > > > > > > Stashed changes <nl> from ansible . executor . task_result import TaskResult <nl> from ansible . inventory . host import Host <nl> from ansible . inventory . group import Group <nl> def _process_pending_results ( self , iterator ) : <nl> [ iterator . mark_host_failed ( h ) for h in self . _inventory . get_hosts ( iterator . _play . hosts ) if h . name not in self . _tqm . _unreachable_hosts ] <nl> else : <nl> iterator . mark_host_failed ( host ) <nl> - self . _tqm . _failed_hosts [ host . name ] = True <nl> - self . _tqm . _stats . increment ( ' failures ' , host . name ) <nl> + ( state , tmp_task ) = iterator . get_next_task_for_host ( host , peek = True ) <nl> + if state . run_state ! = PlayIterator . ITERATING_RESCUE : <nl> + self . _tqm . _failed_hosts [ host . name ] = True <nl> + self . _tqm . _stats . increment ( ' failures ' , host . name ) <nl> else : <nl> self . _tqm . _stats . increment ( ' ok ' , host . name ) <nl> self . _tqm . send_callback ( ' v2_runner_on_failed ' , task_result , ignore_errors = task . ignore_errors ) <nl>\n", "msg": "Don ' t mark hosts failed if they ' ve moved to a rescue portion of a block\n"}
{"diff_id": 32381, "repo": "scikit-learn/scikit-learn\n", "sha": "c041d69acf452fd7b8693ecccdb725c3a0ca9061\n", "time": "2013-08-15T13:45:16Z\n", "diff": "mmm a / sklearn / cluster / k_means_ . py <nl> ppp b / sklearn / cluster / k_means_ . py <nl> def predict ( self , X ) : <nl> <nl> Returns <nl> mmmmmm - <nl> - Y : array , shape [ n_samples , ] <nl> - Index of the closest center each sample belongs to . <nl> + labels : array , shape [ n_samples , ] <nl> + Index of the cluster each sample belongs to . <nl> \" \" \" <nl> self . _check_fitted ( ) <nl> X = self . _check_test_data ( X ) <nl>\n", "msg": "Better docstring for KMeans . predict .\n"}
{"diff_id": 32382, "repo": "bokeh/bokeh\n", "sha": "5e4d33c1eb65aaaec59f95a2afe3713074f0a7ad\n", "time": "2014-07-25T18:41:14Z\n", "diff": "mmm a / bokeh / charts / _charts . py <nl> ppp b / bokeh / charts / _charts . py <nl> def end_plot ( self ) : <nl> def make_axis ( self , dimension , scale , label ) : <nl> \" Create linear , date or categorical axis depending on the scale and dimension . \" <nl> if scale = = \" linear \" : <nl> + dim_loc_map = { 0 : \" bottom \" , 1 : \" left \" } <nl> axis = LinearAxis ( plot = self . plot , <nl> dimension = dimension , <nl> - location = \" min \" , <nl> + location = dim_loc_map [ dimension ] , <nl> axis_label = label ) <nl> elif scale = = \" date \" : <nl> axis = DatetimeAxis ( plot = self . plot , <nl> dimension = dimension , <nl> - location = \" min \" , <nl> + location = dim_loc_map [ dimension ] , <nl> axis_label = label ) <nl> elif scale = = \" categorical \" : <nl> axis = CategoricalAxis ( plot = self . plot , <nl>\n", "msg": "make_axis ( ) needs to use new ' location ' values after layout PR merge\n"}
{"diff_id": 32473, "repo": "matplotlib/matplotlib\n", "sha": "01c21a057bb5b6451a661a3c4ae5a040b1fe1903\n", "time": "2015-12-29T06:58:18Z\n", "diff": "mmm a / lib / matplotlib / animation . py <nl> ppp b / lib / matplotlib / animation . py <nl> <nl> except ImportError : <nl> # python2 <nl> from base64 import encodestring as encodebytes <nl> + import abc <nl> import contextlib <nl> import tempfile <nl> from matplotlib . cbook import iterable , is_string_like <nl> def __getitem__ ( self , name ) : <nl> writers = MovieWriterRegistry ( ) <nl> <nl> <nl> - class MovieWriter ( object ) : <nl> + class AbstractMovieWriter ( six . with_metaclass ( abc . ABCMeta ) ) : <nl> + ' ' ' <nl> + Abstract base class for writing movies . Fundamentally , what a MovieWriter <nl> + does is provide is a way to grab frames by calling grab_frame ( ) . <nl> + <nl> + setup ( ) is called to start the process and finish ( ) is called afterwards . <nl> + <nl> + This class is set up to provide for writing movie frame data to a pipe . <nl> + saving ( ) is provided as a context manager to facilitate this process as : : <nl> + <nl> + with moviewriter . saving ( fig , outfile = ' myfile . mp4 ' , dpi = 100 ) : <nl> + # Iterate over frames <nl> + moviewriter . grab_frame ( * * savefig_kwargs ) <nl> + <nl> + The use of the context manager ensures that setup ( ) and finish ( ) are <nl> + performed as necessary . <nl> + <nl> + An instance of a concrete subclass of this class can be given as the <nl> + ` writer ` argument of ` Animation . save ( ) ` . <nl> + ' ' ' <nl> + <nl> + @ abc . abstractmethod <nl> + def setup ( self , fig , outfile , dpi , * args ) : <nl> + ' ' ' <nl> + Perform setup for writing the movie file . <nl> + <nl> + fig : ` matplotlib . Figure ` instance <nl> + The figure object that contains the information for frames <nl> + outfile : string <nl> + The filename of the resulting movie file <nl> + dpi : int <nl> + The DPI ( or resolution ) for the file . This controls the size <nl> + in pixels of the resulting movie file . <nl> + ' ' ' <nl> + <nl> + @ abc . abstractmethod <nl> + def grab_frame ( self , * * savefig_kwargs ) : <nl> + ' ' ' <nl> + Grab the image information from the figure and save as a movie frame . <nl> + All keyword arguments in savefig_kwargs are passed on to the ' savefig ' <nl> + command that saves the figure . <nl> + ' ' ' <nl> + <nl> + @ abc . abstractmethod <nl> + def finish ( self ) : <nl> + ' Finish any processing for writing the movie . ' <nl> + <nl> + @ contextlib . contextmanager <nl> + def saving ( self , fig , outfile , dpi , * args ) : <nl> + ' ' ' <nl> + Context manager to facilitate writing the movie file . <nl> + <nl> + All arguments are passed on to ` setup ` . <nl> + ' ' ' <nl> + self . setup ( fig , outfile , dpi , * args ) <nl> + yield <nl> + self . finish ( ) <nl> + <nl> + <nl> + class MovieWriter ( AbstractMovieWriter ) : <nl> ' ' ' <nl> Base class for writing movies . Fundamentally , what a MovieWriter does <nl> is provide is a way to grab frames by calling grab_frame ( ) . setup ( ) <nl> class MovieWriter ( object ) : <nl> This class is set up to provide for writing movie frame data to a pipe . <nl> saving ( ) is provided as a context manager to facilitate this process as : : <nl> <nl> - with moviewriter . saving ( ' myfile . mp4 ' ) : <nl> + with moviewriter . saving ( fig , outfile = ' myfile . mp4 ' , dpi = 100 ) : <nl> # Iterate over frames <nl> - moviewriter . grab_frame ( ) <nl> + moviewriter . grab_frame ( * * savefig_kwargs ) <nl> <nl> The use of the context manager ensures that setup and cleanup are <nl> performed as necessary . <nl> def setup ( self , fig , outfile , dpi , * args ) : <nl> # eliminates the need for temp files . <nl> self . _run ( ) <nl> <nl> - @ contextlib . contextmanager <nl> - def saving ( self , * args ) : <nl> - ' ' ' <nl> - Context manager to facilitate writing the movie file . <nl> - <nl> - ` ` * args ` ` are any parameters that should be passed to ` setup ` . <nl> - ' ' ' <nl> - # This particular sequence is what contextlib . contextmanager wants <nl> - self . setup ( * args ) <nl> - yield <nl> - self . finish ( ) <nl> - <nl> def _run ( self ) : <nl> # Uses subprocess to call the program for assembling frames into a <nl> # movie file . * args * returns the sequence of command line arguments <nl> def save ( self , filename , writer = None , fps = None , dpi = None , codec = None , <nl> <nl> * filename * is the output filename , e . g . , : file : ` mymovie . mp4 ` <nl> <nl> - * writer * is either an instance of : class : ` MovieWriter ` or a string <nl> - key that identifies a class to use , such as ' ffmpeg ' or ' mencoder ' . <nl> - If nothing is passed , the value of the rcparam ` animation . writer ` is <nl> - used . <nl> + * writer * is either an instance of : class : ` AbstractMovieWriter ` or <nl> + a string key that identifies a class to use , such as ' ffmpeg ' or <nl> + ' mencoder ' . If nothing is passed , the value of the rcparam <nl> + ` animation . writer ` is used . <nl> <nl> * fps * is the frames per second in the movie . Defaults to None , <nl> which will use the animation ' s specified interval to set the frames <nl>\n", "msg": "MAINT : Create an abstract base class for movie writers .\n"}
{"diff_id": 32492, "repo": "ytdl-org/youtube-dl\n", "sha": "de7aade2f872d6de2dbd0d82624e51c24968e057\n", "time": "2019-12-31T20:31:22Z\n", "diff": "mmm a / youtube_dl / extractor / soundcloud . py <nl> ppp b / youtube_dl / extractor / soundcloud . py <nl> <nl> compat_urlparse , <nl> ) <nl> from . . utils import ( <nl> + error_to_compat_str , <nl> ExtractorError , <nl> float_or_none , <nl> HEADRequest , <nl> class SoundcloudIE ( InfoExtractor ) : <nl> ' original ' : 0 , <nl> } <nl> <nl> + def _store_client_id ( self , client_id ) : <nl> + self . _downloader . cache . store ( ' soundcloud ' , ' client_id ' , client_id ) <nl> + <nl> def _update_client_id ( self ) : <nl> webpage = self . _download_webpage ( ' https : / / soundcloud . com / ' , None ) <nl> for src in reversed ( re . findall ( r ' < script [ ^ > ] + src = \" ( [ ^ \" ] + ) \" ' , webpage ) ) : <nl> def _update_client_id ( self ) : <nl> script , ' client id ' , default = None ) <nl> if client_id : <nl> self . _CLIENT_ID = client_id <nl> - self . _downloader . cache . store ( ' soundcloud ' , ' client_id ' , client_id ) <nl> + self . _store_client_id ( client_id ) <nl> return <nl> raise ExtractorError ( ' Unable to extract client id ' ) <nl> <nl> def _download_json ( self , * args , * * kwargs ) : <nl> + non_fatal = kwargs . get ( ' fatal ' ) is False <nl> + if non_fatal : <nl> + del kwargs [ ' fatal ' ] <nl> query = kwargs . get ( ' query ' , { } ) . copy ( ) <nl> for _ in range ( 2 ) : <nl> query [ ' client_id ' ] = self . _CLIENT_ID <nl> def _download_json ( self , * args , * * kwargs ) : <nl> return super ( SoundcloudIE , self ) . _download_json ( * args , * * compat_kwargs ( kwargs ) ) <nl> except ExtractorError as e : <nl> if isinstance ( e . cause , compat_HTTPError ) and e . cause . code = = 401 : <nl> + self . _store_client_id ( None ) <nl> self . _update_client_id ( ) <nl> continue <nl> + elif non_fatal : <nl> + self . _downloader . report_warning ( error_to_compat_str ( e ) ) <nl> + return False <nl> raise <nl> <nl> def _real_initialize ( self ) : <nl>\n", "msg": "[ soundcloud ] fix client id extraction for non fatal requests\n"}
{"diff_id": 32532, "repo": "pypa/pipenv\n", "sha": "2d1a0e06023b27cd9cb577a55ecafdc50f772802\n", "time": "2017-10-04T05:29:01Z\n", "diff": "mmm a / tests / test_pipenv . py <nl> ppp b / tests / test_pipenv . py <nl> <nl> <nl> os . environ [ ' PIPENV_DONT_USE_PYENV ' ] = ' 1 ' <nl> <nl> + <nl> class PipenvInstance ( ) : <nl> \" \" \" An instance of a Pipenv Project . . . \" \" \" <nl> def __init__ ( self , pipfile = True , chdir = False ) : <nl> def test_sequential_mode ( self ) : <nl> <nl> c = p . pipenv ( ' run python - c \" import requests ; import idna ; import certifi ; import records ; import tpfd ; import parse ; \" ' ) <nl> assert c . return_code = = 0 <nl> - <nl> + <nl> @ pytest . mark . sequential <nl> @ pytest . mark . install <nl> @ pytest . mark . update <nl> def test_sequential_update_mode ( self ) : <nl> assert ' urllib3 ' in p . lockfile [ ' default ' ] <nl> assert ' certifi ' in p . lockfile [ ' default ' ] <nl> assert ' records ' in p . lockfile [ ' default ' ] <nl> - <nl> + <nl> c = p . pipenv ( ' run python - c \" import requests ; import idna ; import certifi ; import records ; \" ' ) <nl> assert c . return_code = = 0 <nl> - <nl> + <nl> c = p . pipenv ( ' update - - sequential ' ) <nl> - assert c . return_code = = 0 <nl> - <nl> + assert c . return_code = = 0 <nl> + <nl> assert ' requests ' in p . lockfile [ ' default ' ] <nl> assert ' idna ' in p . lockfile [ ' default ' ] <nl> assert ' urllib3 ' in p . lockfile [ ' default ' ] <nl> def test_sequential_update_mode ( self ) : <nl> assert ' records ' in p . lockfile [ ' default ' ] <nl> <nl> c = p . pipenv ( ' run python - c \" import requests ; import idna ; import certifi ; import records ; \" ' ) <nl> - assert c . return_code = = 0 <nl> + assert c . return_code = = 0 <nl> <nl> @ pytest . mark . run <nl> @ pytest . mark . markers <nl> def test_e_dot ( self ) : <nl> assert ' path ' in p . pipfile [ ' dev - packages ' ] [ key ] <nl> assert ' requests ' in p . lockfile [ ' develop ' ] <nl> <nl> - <nl> @ pytest . mark . code <nl> @ pytest . mark . install <nl> def test_code_import_manual ( self ) : <nl> def test_lock_requirements_file ( self ) : <nl> for req in req_list : <nl> assert req in c . out <nl> <nl> - <nl> @ pytest . mark . lock <nl> @ pytest . mark . requirements <nl> @ pytest . mark . complex <nl>\n", "msg": "flake 8 stuff for the main test suite\n"}
{"diff_id": 32718, "repo": "matplotlib/matplotlib\n", "sha": "58a27bf4979dd991ee58756b4ff957fd62af9575\n", "time": "2016-03-29T15:53:42Z\n", "diff": "mmm a / lib / mpl_toolkits / axisartist / grid_finder . py <nl> ppp b / lib / mpl_toolkits / axisartist / grid_finder . py <nl> def __init__ ( self , <nl> <nl> <nl> class MaxNLocator ( mticker . MaxNLocator ) : <nl> - def __init__ ( self , nbins = 10 , steps = None , <nl> - trim = True , <nl> + def __init__ ( self , nbins = 10 , steps = None , <nl> + trim = True , <nl> integer = False , <nl> symmetric = False , <nl> prune = None ) : <nl> - <nl> + # trim argument has no effect . It has been left for API compatibility <nl> mticker . MaxNLocator . __init__ ( self , nbins , steps = steps , <nl> - trim = trim , integer = integer , <nl> + integer = integer , <nl> symmetric = symmetric , prune = prune ) <nl> self . create_dummy_axis ( ) <nl> self . _factor = None <nl>\n", "msg": "Dont forward trim argument to ticker where it has no effect\n"}
{"diff_id": 32861, "repo": "python/cpython\n", "sha": "d594849c42b6141622f8e442e26b49e2df6ef4ff\n", "time": "2004-02-26T15:22:17Z\n", "diff": "mmm a / Lib / codecs . py <nl> ppp b / Lib / codecs . py <nl> def readlines ( self , sizehint = None ) : <nl> Line breaks are implemented using the codec ' s decoder <nl> method and are included in the list entries . <nl> <nl> - sizehint , if given , is passed as size argument to the <nl> - stream ' s . read ( ) method . <nl> + sizehint , if given , is ignored since there is no efficient <nl> + way to finding the true end - of - line . <nl> <nl> \" \" \" <nl> - if sizehint is None : <nl> - data = self . stream . read ( ) <nl> - else : <nl> - data = self . stream . read ( sizehint ) <nl> + data = self . stream . read ( ) <nl> return self . decode ( data , self . errors ) [ 0 ] . splitlines ( 1 ) <nl> <nl> def reset ( self ) : <nl> def readline ( self , size = None ) : <nl> <nl> def readlines ( self , sizehint = None ) : <nl> <nl> - if sizehint is None : <nl> - data = self . reader . read ( ) <nl> - else : <nl> - data = self . reader . read ( sizehint ) <nl> + data = self . reader . read ( ) <nl> data , bytesencoded = self . encode ( data , self . errors ) <nl> return data . splitlines ( 1 ) <nl> <nl>\n", "msg": "Ignore sizehint argument . Fixes SF .\n"}
{"diff_id": 32885, "repo": "celery/celery\n", "sha": "04ddf8ea88a39448185effbf01ba186f88d27dc2\n", "time": "2014-05-20T15:00:20Z\n", "diff": "mmm a / celery / concurrency / asynpool . py <nl> ppp b / celery / concurrency / asynpool . py <nl> def _make_process_result ( self , hub ) : <nl> fileno_to_outq = self . fileno_to_outq <nl> on_state_change = self . on_state_change <nl> add_reader = hub . add_reader <nl> - hub_remove = hub . remove <nl> + remove_reader = hub . remove_reader <nl> recv_message = self . _recv_message <nl> <nl> def on_result_readable ( fileno ) : <nl> try : <nl> fileno_to_outq [ fileno ] <nl> except KeyError : # process gone <nl> - return hub_remove ( fileno ) <nl> + return remove_reader ( fileno ) <nl> it = recv_message ( add_reader , fileno , on_state_change ) <nl> try : <nl> next ( it ) <nl> except StopIteration : <nl> pass <nl> except ( IOError , OSError , EOFError ) : <nl> - hub_remove ( fileno ) <nl> + remove_reader ( fileno ) <nl> else : <nl> add_reader ( fileno , it ) <nl> return on_result_readable <nl> def on_job_ready ( self , job , i , obj , inqW_fd ) : <nl> def _create_process_handlers ( self , hub , READ = READ , ERR = ERR ) : <nl> \" \" \" For async pool this will create the handlers called <nl> when a process is up / down and etc . \" \" \" <nl> - add_reader , hub_remove = hub . add_reader , hub . remove <nl> + add_reader , remove_reader , remove_writer = hub . add_reader , hub . remove_reader , hub . remove_writer <nl> cache = self . _cache <nl> all_inqueues = self . _all_inqueues <nl> fileno_to_inq = self . _fileno_to_inq <nl> def on_process_up ( proc ) : <nl> <nl> self . on_process_up = on_process_up <nl> <nl> - def _remove_from_index ( obj , proc , index , callback = None ) : <nl> + def _remove_from_index ( obj , proc , index , remove_func , callback = None ) : <nl> # this remove the file descriptors for a process from <nl> # the indices . we have to make sure we don ' t overwrite <nl> # another processes fds , as the fds may be reused . <nl> def _remove_from_index ( obj , proc , index , callback = None ) : <nl> except KeyError : <nl> pass <nl> else : <nl> - hub_remove ( fd ) <nl> + remove_func ( fd ) <nl> if callback is not None : <nl> callback ( fd ) <nl> return fd <nl> def on_process_down ( proc ) : <nl> if proc . dead : <nl> return <nl> process_flush_queues ( proc ) <nl> - _remove_from_index ( proc . outq . _reader , proc , fileno_to_outq ) <nl> + _remove_from_index ( proc . outq . _reader , proc , fileno_to_outq , remove_func = remove_reader ) <nl> if proc . synq : <nl> - _remove_from_index ( proc . synq . _writer , proc , fileno_to_synq ) <nl> + _remove_from_index ( proc . synq . _writer , proc , fileno_to_synq , remove_func = remove_writer ) <nl> inq = _remove_from_index ( proc . inq . _writer , proc , fileno_to_inq , <nl> + remove_func = remove_writer , <nl> callback = all_inqueues . discard ) <nl> if inq : <nl> busy_workers . discard ( inq ) <nl> - hub_remove ( proc . sentinel ) <nl> + remove_reader ( proc . sentinel ) <nl> waiting_to_start . discard ( proc ) <nl> self . _active_writes . discard ( proc . inqW_fd ) <nl> - hub_remove ( proc . inqW_fd ) <nl> - hub_remove ( proc . outqR_fd ) <nl> + remove_writer ( proc . inqW_fd ) <nl> + remove_reader ( proc . outqR_fd ) <nl> if proc . synqR_fd : <nl> - hub_remove ( proc . synqR_fd ) <nl> + remove_reader ( proc . synqR_fd ) <nl> if proc . synqW_fd : <nl> self . _active_writes . discard ( proc . synqW_fd ) <nl> - hub_remove ( proc . synqW_fd ) <nl> + remove_reader ( proc . synqW_fd ) <nl> self . on_process_down = on_process_down <nl> <nl> def _create_write_handlers ( self , hub , <nl>\n", "msg": "Be more selective about how file descriptors get removed from Kombu ' s hub .\n"}
{"diff_id": 32976, "repo": "zulip/zulip\n", "sha": "6d29525ef03702f4daecdd309a4713019e00e374\n", "time": "2018-12-19T19:24:57Z\n", "diff": "mmm a / zerver / views / email_log . py <nl> ppp b / zerver / views / email_log . py <nl> <nl> from zerver . models import get_realm , get_user_by_delivery_email <nl> from zerver . lib . notifications import enqueue_welcome_emails <nl> from zerver . lib . response import json_success <nl> + from zerver . lib . actions import do_change_user_delivery_email <nl> from zproject . email_backends import ( <nl> get_forward_address , <nl> set_forward_address , <nl> def generate_all_emails ( request : HttpRequest ) - > HttpResponse : <nl> assert result . status_code = = 200 <nl> <nl> # Reset the email value so we can run this again <nl> - user_profile . email = registered_email <nl> - user_profile . save ( update_fields = [ ' email ' ] ) <nl> + do_change_user_delivery_email ( user_profile , registered_email ) <nl> <nl> # Follow up day1 day2 emails for normal user <nl> enqueue_welcome_emails ( user_profile ) <nl>\n", "msg": "emails : Fix broken email revert process in email_log .\n"}
{"diff_id": 33063, "repo": "ansible/ansible\n", "sha": "4614a574eadb0a245edae3bcf7acee1f84cd37b3\n", "time": "2015-07-31T08:39:02Z\n", "diff": "mmm a / lib / ansible / module_utils / facts . py <nl> ppp b / lib / ansible / module_utils / facts . py <nl> class AIXNetwork ( GenericBsdIfconfigNetwork , Network ) : <nl> \" \" \" <nl> platform = ' AIX ' <nl> <nl> + def get_default_interfaces ( self , route_path ) : <nl> + netstat_path = module . get_bin_path ( ' netstat ' ) <nl> + <nl> + rc , out , err = module . run_command ( [ netstat_path , ' - nr ' ] ) <nl> + <nl> + interface = dict ( v4 = { } , v6 = { } ) <nl> + <nl> + lines = out . split ( ' \\ n ' ) <nl> + for line in lines : <nl> + words = line . split ( ) <nl> + if len ( words ) > 1 and words [ 0 ] = = ' default ' : <nl> + if ' . ' in words [ 1 ] : <nl> + interface [ ' v4 ' ] [ ' gateway ' ] = words [ 1 ] <nl> + interface [ ' v4 ' ] [ ' interface ' ] = words [ 5 ] <nl> + elif ' : ' in words [ 1 ] : <nl> + interface [ ' v6 ' ] [ ' gateway ' ] = words [ 1 ] <nl> + interface [ ' v6 ' ] [ ' interface ' ] = words [ 5 ] <nl> + <nl> + return interface [ ' v4 ' ] , interface [ ' v6 ' ] <nl> + <nl> # AIX ' ifconfig - a ' does not have three words in the interface line <nl> def get_interfaces_info ( self , ifconfig_path , ifconfig_options ) : <nl> interfaces = { } <nl>\n", "msg": "Allows network network interface facts collection as an unprivileged user and adds more facts\n"}
{"diff_id": 33134, "repo": "zulip/zulip\n", "sha": "faa3c275de3cf2511aa0ffcf5ed3fe8e85c308ca\n", "time": "2018-02-22T13:32:03Z\n", "diff": "mmm a / zerver / tests / test_narrow . py <nl> ppp b / zerver / tests / test_narrow . py <nl> def _do_add_term_test ( self , term : Dict [ str , Any ] , where_clause : Text , <nl> if params is not None : <nl> actual_params = query . compile ( ) . params <nl> self . assertEqual ( actual_params , params ) <nl> - self . assertTrue ( where_clause in str ( query ) ) <nl> + self . assertIn ( where_clause , str ( query ) ) <nl> <nl> def _build_query ( self , term : Dict [ str , Any ] ) - > Query : <nl> return self . builder . add_term ( self . raw_query , term ) <nl>\n", "msg": "test_narrow : Use a better assert for easier debugging .\n"}
{"diff_id": 33198, "repo": "ansible/ansible\n", "sha": "25822330a2642eacebde1c038561913ba9d0b9db\n", "time": "2013-06-06T20:27:30Z\n", "diff": "mmm a / lib / ansible / inventory / expand_hosts . py <nl> ppp b / lib / ansible / inventory / expand_hosts . py <nl> def expand_hostname_range ( line = None ) : <nl> # nrange : [ 1 : 6 ] ; range ( ) is a built - in . Can ' t use the name <nl> # tail : ' - node ' <nl> <nl> - ( head , nrange , tail ) = line . replace ( ' [ ' , ' | ' ) . replace ( ' ] ' , ' | ' ) . split ( ' | ' ) <nl> + # Add support for multiple ranges in a host so : <nl> + # db [ 01 : 10 : 3 ] node - [ 01 : 10 ] <nl> + # - to do this we split off at the first [ . . . ] set , getting the list <nl> + # of hosts and then repeat until none left . <nl> + # - also add an optional third parameter which contains the step . ( Default : 1 ) <nl> + # so range can be [ 01 : 10 : 2 ] - > 01 03 05 07 09 <nl> + # FIXME : make this work for alphabetic sequences too . <nl> + <nl> + ( head , nrange , tail ) = line . replace ( ' [ ' , ' | ' , 1 ) . replace ( ' ] ' , ' | ' , 1 ) . split ( ' | ' ) <nl> bounds = nrange . split ( \" : \" ) <nl> - if len ( bounds ) ! = 2 : <nl> + if len ( bounds ) ! = 2 and len ( bounds ) ! = 3 : <nl> raise errors . AnsibleError ( \" host range incorrectly specified \" ) <nl> beg = bounds [ 0 ] <nl> end = bounds [ 1 ] <nl> + if len ( bounds ) = = 2 : <nl> + step = 1 <nl> + else : <nl> + step = bounds [ 2 ] <nl> if not beg : <nl> beg = \" 0 \" <nl> if not end : <nl> def expand_hostname_range ( line = None ) : <nl> raise errors . AnsibleError ( \" host range format incorrectly specified ! \" ) <nl> seq = string . ascii_letters [ i_beg : i_end + 1 ] <nl> except ValueError : # not a alpha range <nl> - seq = range ( int ( beg ) , int ( end ) + 1 ) <nl> + seq = range ( int ( beg ) , int ( end ) + 1 , int ( step ) ) <nl> <nl> for rseq in seq : <nl> hname = ' ' . join ( ( head , fill ( rseq ) , tail ) ) <nl> - all_hosts . append ( hname ) <nl> + <nl> + if detect_range ( hname ) : <nl> + all_hosts . extend ( expand_hostname_range ( hname ) ) <nl> + else : <nl> + all_hosts . append ( hname ) <nl> <nl> return all_hosts <nl>\n", "msg": "Add support for multiple ranges in a host\n"}
{"diff_id": 33203, "repo": "zulip/zulip\n", "sha": "aad99ce951e3f999cff77f4d1a5344b421b96bd8\n", "time": "2019-11-05T02:10:37Z\n", "diff": "mmm a / zerver / data_import / mattermost . py <nl> ppp b / zerver / data_import / mattermost . py <nl> def get_invite_only_value_from_channel_type ( channel_type : str ) - > bool : <nl> for username in channel_members_map [ stream_name ] : <nl> channel_users . add ( user_id_mapper . get ( username ) ) <nl> <nl> - if channel_users : <nl> - subscriber_handler . set_info ( <nl> - users = channel_users , <nl> - stream_id = stream_id , <nl> - ) <nl> + subscriber_handler . set_info ( <nl> + users = channel_users , <nl> + stream_id = stream_id , <nl> + ) <nl> streams . append ( stream ) <nl> return streams <nl> <nl>\n", "msg": "mattermost import : Fix handling of channels with no subscribers .\n"}
{"diff_id": 33256, "repo": "python/cpython\n", "sha": "1d376687b7176fc0e52e85f0a6cd98b53fd77d41\n", "time": "2009-02-02T19:19:36Z\n", "diff": "mmm a / Lib / importlib / _bootstrap . py <nl> ppp b / Lib / importlib / _bootstrap . py <nl> def __init__ ( self , path_entry ) : <nl> super ( PyFileImporter , self ) . __init__ ( path_entry ) <nl> <nl> <nl> + class SysPathFinder : <nl> + <nl> + \" \" \" Meta path finder for sys . ( path | path_hooks | path_importer_cache ) . \" \" \" <nl> + <nl> + def _default_hook ( self , path ) : <nl> + \" \" \" Use the default hook on ' path ' . <nl> + <nl> + If the path will not work for the default hook then raise ImportError . <nl> + <nl> + \" \" \" <nl> + # TODO ( brett . cannon ) Implement <nl> + raise ImportError <nl> + <nl> + # The list of implicit hooks cannot be a class attribute because of <nl> + # bootstrapping issues for accessing imp . <nl> + def _implicit_hooks ( self , path ) : <nl> + \" \" \" Return a list of the implicit path hooks . \" \" \" <nl> + return [ self . _default_hook , imp . NullImporter ] <nl> + <nl> + def _path_hooks ( self , path ) : <nl> + \" \" \" Search sys . path_hooks for a finder for ' path ' . <nl> + <nl> + Guaranteed to return a finder for the path as NullImporter is the <nl> + default importer for any path that does not have an explicit finder . <nl> + <nl> + \" \" \" <nl> + for hook in sys . path_hooks + self . _implicit_hooks ( ) : <nl> + try : <nl> + return hook ( path ) <nl> + except ImportError : <nl> + continue <nl> + else : <nl> + # This point should never be reached thanks to NullImporter . <nl> + raise SystemError ( \" no hook could find an importer for \" <nl> + \" { 0 } \" . format ( path ) ) <nl> + <nl> + def _path_importer_cache ( self , path ) : <nl> + \" \" \" Get the finder for the path from sys . path_importer_cache . <nl> + <nl> + If the path is not in the cache , find the appropriate finder and cache <nl> + it . If None is cached , get the default finder and cache that <nl> + ( if applicable ) . <nl> + <nl> + Because of NullImporter , some finder should be returned . The only <nl> + explicit fail case is if None is cached but the path cannot be used for <nl> + the default hook , for which ImportError is raised . <nl> + <nl> + \" \" \" <nl> + try : <nl> + finder = sys . path_importer_cache ( path ) ; <nl> + except KeyError : <nl> + finder = self . _path_hooks ( path ) <nl> + sys . path_importer_cache [ path ] = finder <nl> + else : <nl> + if finder is None : <nl> + # Raises ImportError on failure . <nl> + finder = self . _default_hook ( path ) <nl> + sys . path_importer_cache [ path ] = finder <nl> + return finder <nl> + <nl> + def find_module ( self , fullname , path = None ) : <nl> + \" \" \" Find the module on sys . path or ' path ' . \" \" \" <nl> + if not path : <nl> + path = sys . path <nl> + for entry in path : <nl> + try : <nl> + finder = self . _path_importer_cache ( entry ) <nl> + except ImportError : <nl> + continue <nl> + loader = finder . find_module ( fullname ) <nl> + if loader : <nl> + return loader <nl> + else : <nl> + return None <nl> + <nl> + <nl> class ImportLockContext ( object ) : <nl> <nl> \" \" \" Context manager for the import lock . \" \" \" <nl>\n", "msg": "To prevent another screw - up on my part where my prototype gets lost thanks to\n"}
{"diff_id": 33295, "repo": "python-telegram-bot/python-telegram-bot\n", "sha": "433110abe9304a0e18eaafbf38a0c83815ac71ca\n", "time": "2016-03-09T15:47:33Z\n", "diff": "mmm a / telegram / bot . py <nl> ppp b / telegram / bot . py <nl> def _post_message ( url , data , kwargs , timeout = None , network_delay = 2 . ) : <nl> reply_to_message_id = kwargs . get ( ' reply_to_message_id ' ) <nl> data [ ' reply_to_message_id ' ] = reply_to_message_id <nl> <nl> + if kwargs . get ( ' disable_notification ' ) : <nl> + disable_notification = kwargs . get ( ' disable_notification ' ) <nl> + data [ ' disable_notification ' ] = disable_notification <nl> + <nl> if kwargs . get ( ' reply_markup ' ) : <nl> reply_markup = kwargs . get ( ' reply_markup ' ) <nl> if isinstance ( reply_markup , ReplyMarkup ) : <nl> def sendMessage ( self , <nl> Text of the message to be sent . <nl> disable_web_page_preview : <nl> Disables link previews for links in this message . [ Optional ] <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl> def forwardMessage ( self , <nl> - Chat id . <nl> message_id : <nl> Unique message identifier . <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> <nl> Returns : <nl> A telegram . Message instance representing the message forwarded . <nl> def sendPhoto ( self , <nl> caption : <nl> Photo caption ( may also be used when resending photos by file_id ) . <nl> [ Optional ] <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl> def sendAudio ( self , <nl> Performer of sent audio . [ Optional ] <nl> title : <nl> Title of sent audio . [ Optional ] <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl> def sendDocument ( self , <nl> filename : <nl> File name that shows in telegram message ( it is usefull when you <nl> send file generated by temp module , for example ) . [ Optional ] <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl> def sendSticker ( self , <nl> Sticker to send . You can either pass a file_id as String to resend <nl> a sticker that is already on the Telegram servers , or upload a new <nl> sticker using multipart / form - data . <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl> def sendVideo ( self , <nl> timeout : <nl> float . If this value is specified , use it as the definitive timeout <nl> ( in seconds ) for urlopen ( ) operations . [ Optional ] <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl> def sendVoice ( self , <nl> a new audio file using multipart / form - data . <nl> duration : <nl> Duration of sent audio in seconds . [ Optional ] <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl> def sendLocation ( self , <nl> Latitude of location . <nl> longitude : <nl> Longitude of location . <nl> + disable_notification : <nl> + Sends the message silently . iOS users will not receive <nl> + a notification , Android users will receive a notification <nl> + with no sound . Other apps coming soon . [ Optional ] <nl> reply_to_message_id : <nl> If the message is a reply , ID of the original message . [ Optional ] <nl> reply_markup : <nl>\n", "msg": "Added disable_notification parameter for silent messages\n"}
{"diff_id": 33369, "repo": "python/cpython\n", "sha": "ed0392ae06a52f768123aad0e478b5bcfac143e2\n", "time": "2015-03-02T05:40:36Z\n", "diff": "mmm a / Lib / test / test_smtpnet . py <nl> ppp b / Lib / test / test_smtpnet . py <nl> def check_ssl_verifiy ( host , port ) : <nl> <nl> class SmtpTest ( unittest . TestCase ) : <nl> testServer = ' smtp . gmail . com ' <nl> - remotePort = 25 <nl> + remotePort = 587 <nl> <nl> def test_connect_starttls ( self ) : <nl> support . get_attribute ( smtplib , ' SMTP_SSL ' ) <nl>\n", "msg": "Issue : Update Gmail port number for STARTTLS to 587 .\n"}
{"diff_id": 33505, "repo": "scrapy/scrapy\n", "sha": "0c374c00fb7c8d24a60bec8a94f7cbb06accb980\n", "time": "2018-02-08T00:09:02Z\n", "diff": "mmm a / scrapy / extensions / telnet . py <nl> ppp b / scrapy / extensions / telnet . py <nl> def from_crawler ( cls , crawler ) : <nl> def start_listening ( self ) : <nl> self . port = listen_tcp ( self . portrange , self . host , self ) <nl> h = self . port . getHost ( ) <nl> - logger . debug ( \" Telnet console listening on % ( host ) s : % ( port ) d \" , <nl> - { ' host ' : h . host , ' port ' : h . port } , <nl> - extra = { ' crawler ' : self . crawler } ) <nl> + logger . info ( \" Telnet console listening on % ( host ) s : % ( port ) d \" , <nl> + { ' host ' : h . host , ' port ' : h . port } , <nl> + extra = { ' crawler ' : self . crawler } ) <nl> <nl> def stop_listening ( self ) : <nl> self . port . stopListening ( ) <nl>\n", "msg": "use INFO log level to show telnet host / port\n"}
{"diff_id": 33722, "repo": "ansible/ansible\n", "sha": "4a8a052e2e754a3ed3f74431f26ec62205c0b36d\n", "time": "2016-12-08T16:24:44Z\n", "diff": "mmm a / lib / ansible / modules / cloud / amazon / ec2_lc . py <nl> ppp b / lib / ansible / modules / cloud / amazon / ec2_lc . py <nl> <nl> required : false <nl> security_groups : <nl> description : <nl> - - A list of security groups into which instances should be found <nl> + - A list of security groups to apply to the instances . For VPC instances , specify security group IDs . For EC2 - Classic , specify either security group names or IDs . <nl> required : false <nl> region : <nl> description : <nl>\n", "msg": "Improve documentation on security_groups - option\n"}
{"diff_id": 33767, "repo": "keras-team/keras\n", "sha": "b200b7d19c6b00a99bd890b925c1c0d887c1b303\n", "time": "2017-08-07T19:35:18Z\n", "diff": "mmm a / keras / initializers . py <nl> ppp b / keras / initializers . py <nl> def get_config ( self ) : <nl> <nl> @ classmethod <nl> def from_config ( cls , config ) : <nl> + if ' dtype ' in config : <nl> + # Initializers saved from ` tf . keras ` <nl> + # may contain an unused ` dtype ` argument . <nl> + config . pop ( ' dtype ' ) <nl> return cls ( * * config ) <nl> <nl> <nl>\n", "msg": "Add handling for ` dtype ` arg in initializer config .\n"}
{"diff_id": 33790, "repo": "bokeh/bokeh\n", "sha": "7cc473438b4974edcb5c38396a1f3740c712d8ce\n", "time": "2014-07-02T20:52:50Z\n", "diff": "mmm a / bokeh / plotting_helpers . py <nl> ppp b / bokeh / plotting_helpers . py <nl> <nl> from . objects import ( <nl> BoxSelectionOverlay , BoxSelectTool , BoxZoomTool , CategoricalAxis , <nl> ColumnDataSource , CrosshairTool , DataRange1d , DatetimeAxis , <nl> - EmbedTool , FactorRange , Grid , HoverTool , Legend , LinearAxis , <nl> + EmbedTool , FactorRange , Grid , HoverTool , Legend , LinearAxis , LogAxis , <nl> ObjectExplorerTool , PanTool , Plot , PreviewSaveTool , Range , Range1d , <nl> ResetTool , ResizeTool , WheelZoomTool , Tool <nl> ) <nl> def _get_axis_class ( axis_type , range_input ) : <nl> return None <nl> elif axis_type is \" linear \" : <nl> return LinearAxis <nl> + elif axis_type is \" log \" : <nl> + return LogAxis <nl> elif axis_type = = \" datetime \" : <nl> return DatetimeAxis <nl> elif axis_type = = \" auto \" : <nl> def _new_xy_plot ( x_range = None , y_range = None , plot_width = None , plot_height = None , <nl> <nl> x_axiscls = _get_axis_class ( x_axis_type , p . x_range ) <nl> if x_axiscls : <nl> - xaxis = x_axiscls ( plot = p , dimension = 0 , location = \" min \" , bounds = \" auto \" ) <nl> + if x_axiscls is LogAxis : <nl> + xaxis = x_axiscls ( plot = p , dimension = 0 , location = \" min \" , bounds = \" auto \" , x_mapper_type = \" log \" ) <nl> + else : <nl> + xaxis = x_axiscls ( plot = p , dimension = 0 , location = \" min \" , bounds = \" auto \" ) <nl> xgrid = Grid ( plot = p , dimension = 0 , axis = xaxis ) <nl> <nl> - y_axiscls = _get_axis_class ( y_axis_type , p . y_range ) <nl> + y_axiscls , p . y_mapper_type = _get_axis_class ( y_axis_type , p . y_range ) <nl> if y_axiscls : <nl> - yaxis = y_axiscls ( plot = p , dimension = 1 , location = \" min \" , bounds = \" auto \" ) <nl> + if y_axiscls is LogAxis : <nl> + yaxis = y_axiscls ( plot = p , dimension = 1 , location = \" min \" , bounds = \" auto \" , y_mapper_type = \" log \" ) <nl> + else : <nl> + yaxis = y_axiscls ( plot = p , dimension = 1 , location = \" min \" , bounds = \" auto \" ) <nl> ygrid = Grid ( plot = p , dimension = 1 , axis = yaxis ) <nl> <nl> + <nl> border_args = [ \" min_border \" , \" min_border_top \" , \" min_border_bottom \" , \" min_border_left \" , \" min_border_right \" ] <nl> for arg in border_args : <nl> if arg in kw : <nl>\n", "msg": "fixed new plotting helpers with log mapper type\n"}
{"diff_id": 33805, "repo": "nicolargo/glances\n", "sha": "ec54bf903353d37ecc3e75c53adf905bf99f3555\n", "time": "2015-05-02T13:32:25Z\n", "diff": "mmm a / glances / __init__ . py <nl> ppp b / glances / __init__ . py <nl> <nl> __license__ = ' LGPL ' <nl> <nl> # Import system lib <nl> + import locale <nl> import platform <nl> import signal <nl> import sys <nl> - import locale <nl> <nl> # Import psutil <nl> try : <nl> <nl> from glances . core . glances_logging import logger <nl> from glances . core . glances_main import GlancesMain <nl> <nl> - # Setup translations <nl> try : <nl> locale . setlocale ( locale . LC_ALL , ' ' ) <nl> except locale . Error : <nl> - # Issue # 517 <nl> - # Setting LC_ALL to ' ' should not generate an error unless LC_ALL is not <nl> - # defined in the user environment , which can be the case when used via SSH . <nl> - # So simply skip this error , as python will use the C locale by default . <nl> - logger . warning ( \" No locale LC_ALL variable found . Use the default C locale . \" ) <nl> - pass <nl> + print ( \" Warning : Unable to set locale . Expect encoding problems . \" ) <nl> <nl> # Check Python version <nl> if sys . version_info < ( 2 , 6 ) or ( 3 , 0 ) < = sys . version_info < ( 3 , 3 ) : <nl>\n", "msg": "Be more explicit about when the modification of the locale fails\n"}
{"diff_id": 33968, "repo": "zulip/zulip\n", "sha": "9b25f8789f9f0684cc888ecf07baa1eeddc7ee35\n", "time": "2019-01-31T20:40:05Z\n", "diff": "mmm a / zerver / data_import / hipchat . py <nl> ppp b / zerver / data_import / hipchat . py <nl> def get_raw_message ( d : Dict [ str , Any ] ) - > Optional [ ZerverFieldsT ] : <nl> return None <nl> <nl> if is_pm_data : <nl> - if int ( sender_id ) ! = int ( fn_id ) : <nl> + # We need to compare with str ( ) on both sides here . <nl> + # In Stride , user IDs are strings , but in HipChat , <nl> + # they are integers , and fn_id is always a string . <nl> + if str ( sender_id ) ! = str ( fn_id ) : <nl> # PMs are in multiple places in the Hipchat export , <nl> # and we only use the copy from the sender <nl> return None <nl>\n", "msg": "hipchat : Fix handling of user IDs in Stride import .\n"}
{"diff_id": 34078, "repo": "numpy/numpy\n", "sha": "796f998cfaf8cba4b65fc32287c7694d1d9b68db\n", "time": "2004-04-12T19:18:44Z\n", "diff": "mmm a / scipy_test / testing . py <nl> ppp b / scipy_test / testing . py <nl> def _get_module_tests ( self , module , level ) : <nl> return self . _get_suite_list ( test_module , level , module . __name__ ) <nl> <nl> def _get_suite_list ( self , test_module , level , module_name = ' __main__ ' ) : <nl> + mstr = self . _module_str <nl> if hasattr ( test_module , ' test_suite ' ) : <nl> # Using old styled test suite <nl> try : <nl>\n", "msg": "added missing definition of ' mstr ' for use in failure reporting\n"}
{"diff_id": 34083, "repo": "python/cpython\n", "sha": "293fdeb31e664d68d894978c6b85ca97599c1349\n", "time": "2002-11-18T15:29:02Z\n", "diff": "mmm a / Mac / Tools / IDE / PythonIDE . py <nl> ppp b / Mac / Tools / IDE / PythonIDE . py <nl> def init ( ) : <nl> macresource . need ( ' DITL ' , 468 , \" PythonIDE . rsrc \" ) <nl> widgetrespathsegs = [ sys . exec_prefix , \" Mac \" , \" Tools \" , \" IDE \" , \" Widgets . rsrc \" ] <nl> widgetresfile = os . path . join ( * widgetrespathsegs ) <nl> + if not os . path . exists ( widgetresfile ) : <nl> + widgetrespathsegs = [ os . pardir , \" Tools \" , \" IDE \" , \" Widgets . rsrc \" ] <nl> + widgetresfile = os . path . join ( * widgetrespathsegs ) <nl> refno = macresource . need ( ' CURS ' , 468 , widgetresfile ) <nl> if os . environ . has_key ( ' PYTHONIDEPATH ' ) : <nl> # For development set this environment variable <nl> def init ( ) : <nl> # We ' re not a fullblown application <nl> idepathsegs = [ sys . exec_prefix , \" Mac \" , \" Tools \" , \" IDE \" ] <nl> ide_path = os . path . join ( * idepathsegs ) <nl> + if not os . path . exists ( ide_path ) : <nl> + idepathsegs = [ os . pardir , \" Tools \" , \" IDE \" ] <nl> + for p in sys . path : <nl> + ide_path = os . path . join ( * ( [ p ] + idepathsegs ) ) <nl> + if os . path . exists ( ide_path ) : <nl> + break <nl> + <nl> else : <nl> # We are a fully frozen application <nl> ide_path = sys . argv [ 0 ] <nl>\n", "msg": "Another workaround , to find the IDE directory when we ' re in MacPython - OSX\n"}
{"diff_id": 34193, "repo": "matplotlib/matplotlib\n", "sha": "e644548742266275060d7fbc87b81d6e7d15062e\n", "time": "2007-05-21T21:41:48Z\n", "diff": "mmm a / lib / matplotlib / axes . py <nl> ppp b / lib / matplotlib / axes . py <nl> def errorbar ( self , x , y , yerr = None , xerr = None , <nl> lower = y - yerr [ 0 ] <nl> upper = y + yerr [ 1 ] <nl> <nl> - barcols . append ( self . vlines ( x , lower , upper , label = ' _nolegend_ ' ) ) <nl> - caplines . extend ( self . plot ( x , lower , ' _ ' , ms = 2 * capsize , label = ' _nolegend_ ' ) ) <nl> - caplines . extend ( self . plot ( x , upper , ' _ ' , ms = 2 * capsize , label = ' _nolegend_ ' ) ) <nl> + vlines_kw = { ' label ' : ' _nolegend_ ' } <nl> + if ' linewidth ' in kwargs : <nl> + vlines_kw [ ' linewidth ' ] = kwargs [ ' linewidth ' ] <nl> + if ' lw ' in kwargs : <nl> + vlines_kw [ ' lw ' ] = kwargs [ ' lw ' ] <nl> + barcols . append ( self . vlines ( x , lower , upper , * * vlines_kw ) ) <nl> + <nl> + plot_kw = { <nl> + ' ms ' : 2 * capsize , <nl> + ' label ' : ' _nolegend_ ' } <nl> + if ' markeredgewidth ' in kwargs : <nl> + plot_kw [ ' markeredgewidth ' ] = kwargs [ ' markeredgewidth ' ] <nl> + if ' mew ' in kwargs : <nl> + plot_kw [ ' mew ' ] = kwargs [ ' mew ' ] <nl> + caplines . extend ( self . plot ( x , lower , ' _ ' , * * plot_kw ) ) <nl> + caplines . extend ( self . plot ( x , upper , ' _ ' , * * plot_kw ) ) <nl> <nl> if not barsabove and fmt is not None : <nl> l0 , = self . plot ( x , y , fmt , * * kwargs ) <nl>\n", "msg": "fix ' linewidth ' and ' markeredgewidth ' kwargs to errorbar\n"}
{"diff_id": 34211, "repo": "ytdl-org/youtube-dl\n", "sha": "efd6c574a2cabc860f54018af726cd291cbec868\n", "time": "2013-11-18T15:35:41Z\n", "diff": "mmm a / youtube_dl / YoutubeDL . py <nl> ppp b / youtube_dl / YoutubeDL . py <nl> def save_console_title ( self ) : <nl> if not self . params . get ( ' consoletitle ' , False ) : <nl> return <nl> if ' TERM ' in os . environ : <nl> - write_string ( u ' \\ 033 [ 22t ' , self . _screen_file ) <nl> + # Save the title on stack <nl> + write_string ( u ' \\ 033 [ 22 ; 0t ' , self . _screen_file ) <nl> <nl> def restore_console_title ( self ) : <nl> if not self . params . get ( ' consoletitle ' , False ) : <nl> return <nl> if ' TERM ' in os . environ : <nl> - write_string ( u ' \\ 033 [ 23t ' , self . _screen_file ) <nl> + # Restore the title from stack <nl> + write_string ( u ' \\ 033 [ 23 ; 0t ' , self . _screen_file ) <nl> <nl> def __enter__ ( self ) : <nl> self . save_console_title ( ) <nl>\n", "msg": "Correctly write and restore the console title on the stack ( fixes )\n"}
{"diff_id": 34280, "repo": "zulip/zulip\n", "sha": "6952dcbdacbf32588acaf3b4ac8ffc15f526213f\n", "time": "2017-11-27T01:14:23Z\n", "diff": "mmm a / zerver / lib / bot_lib . py <nl> ppp b / zerver / lib / bot_lib . py <nl> <nl> import time <nl> import re <nl> import importlib <nl> - from zerver . lib . actions import internal_send_message <nl> - from zerver . models import UserProfile <nl> + from zerver . lib . actions import internal_send_private_message , \\ <nl> + internal_send_stream_message , internal_send_huddle_message <nl> + from zerver . models import UserProfile , get_user <nl> from zerver . lib . bot_storage import get_bot_state , set_bot_state , \\ <nl> is_key_in_bot_state , get_bot_state_size , remove_bot_state <nl> from zerver . lib . bot_config import get_bot_config <nl> def __init__ ( self , user_profile : UserProfile ) - > None : <nl> def send_message ( self , message : Dict [ str , Any ] ) - > None : <nl> if not self . _rate_limit . is_legal ( ) : <nl> self . _rate_limit . show_error_and_exit ( ) <nl> - recipients = message [ ' to ' ] if message [ ' type ' ] = = ' stream ' else ' , ' . join ( message [ ' to ' ] ) <nl> - internal_send_message ( realm = self . user_profile . realm , sender_email = self . user_profile . email , <nl> - recipient_type_name = message [ ' type ' ] , recipients = recipients , <nl> - topic_name = message . get ( ' subject ' , None ) , content = message [ ' content ' ] ) <nl> + <nl> + if message [ ' type ' ] = = ' stream ' : <nl> + internal_send_stream_message ( self . user_profile . realm , self . user_profile , message [ ' to ' ] , <nl> + message [ ' subject ' ] , message [ ' content ' ] ) <nl> + return <nl> + <nl> + assert message [ ' type ' ] = = ' private ' <nl> + # Ensure that it ' s a comma - separated list , even though the <nl> + # usual ' to ' field could be either a List [ str ] or a str . <nl> + recipients = ' , ' . join ( message [ ' to ' ] ) . split ( ' , ' ) <nl> + <nl> + if len ( message [ ' to ' ] ) = = 1 : <nl> + recipient_user = get_user ( recipients [ 0 ] , self . user_profile . realm ) <nl> + internal_send_private_message ( self . user_profile . realm , self . user_profile , <nl> + recipient_user , message [ ' content ' ] ) <nl> + else : <nl> + internal_send_huddle_message ( self . user_profile . realm , self . user_profile , <nl> + recipients , message [ ' content ' ] ) <nl> <nl> def send_reply ( self , message : Dict [ str , Any ] , response : str ) - > None : <nl> if message [ ' type ' ] = = ' private ' : <nl>\n", "msg": "embedded bots : Stop using internal_send_message for non - system - bots .\n"}
{"diff_id": 34385, "repo": "python/cpython\n", "sha": "8e3315d5ae89ec40e0536dc035325e99d43e090f\n", "time": "2007-08-24T11:48:37Z\n", "diff": "mmm a / setup . py <nl> ppp b / setup . py <nl> def detect_modules ( self ) : <nl> # strip out double - dashes first so that we don ' t end up with <nl> # substituting \" - - Long \" to \" - Long \" and thus lead to \" ong \" being <nl> # used for a library directory . <nl> - env_val = re . sub ( r ' ( ^ | \\ s + ) - ( - | ( ? ! % s ) ) ' % arg_name [ 1 ] , ' ' , env_val ) <nl> + env_val = re . sub ( r ' ( ^ | \\ s + ) - ( - | ( ? ! % s ) ) ' % arg_name [ 1 ] , <nl> + ' ' , env_val ) <nl> parser = optparse . OptionParser ( ) <nl> # Make sure that allowing args interspersed with options is <nl> # allowed <nl>\n", "msg": "Bug : fix stripping of unwanted LDFLAGS .\n"}
{"diff_id": 34448, "repo": "StevenBlack/hosts\n", "sha": "59ddd34d0edce556289bb9bdf8f1cabe1e41014b\n", "time": "2020-08-28T05:51:14Z\n", "diff": "mmm a / updateHostsFile . py <nl> ppp b / updateHostsFile . py <nl> <nl> <nl> try : <nl> import requests <nl> - except ModuleNotFoundError : # noqa : F821 <nl> - raise ModuleNotFoundError ( \" This project ' s dependencies have changed . The Requests library ( \" # noqa : F821 <nl> - \" https : / / requests . readthedocs . io / en / master / ) is now required . \" ) <nl> + except ImportError : <nl> + raise ImportError ( \" This project ' s dependencies have changed . The Requests library ( \" <nl> + \" https : / / requests . readthedocs . io / en / master / ) is now required . \" ) <nl> <nl> <nl> # Syntactic sugar for \" sudo \" command in UNIX / Linux <nl>\n", "msg": "Changed dependency - related exception to be compatible with Python versions < 3 . 6\n"}
{"diff_id": 34773, "repo": "numpy/numpy\n", "sha": "99f428eeb52ce65e79defff4306aebc601ee1c42\n", "time": "2009-01-08T19:22:21Z\n", "diff": "mmm a / numpy / core / tests / test_print . py <nl> ppp b / numpy / core / tests / test_print . py <nl> def _test_locale_independance ( tp ) : <nl> else : <nl> locale . setlocale ( locale . LC_NUMERIC , ' FRENCH ' ) <nl> <nl> - assert_equal ( locale . format ( \" % f \" , tp ( 1 . 2 ) ) , locale . format ( \" % f \" , float ( 1 . 2 ) ) , <nl> + assert_equal ( str ( tp ( 1 . 2 ) ) , str ( float ( 1 . 2 ) ) , <nl> err_msg = ' Failed locale test for type % s ' % tp ) <nl> finally : <nl> locale . setlocale ( locale . LC_NUMERIC , locale = curloc ) <nl>\n", "msg": "Revert buggy test fix for locale independecce .\n"}
{"diff_id": 34821, "repo": "python/cpython\n", "sha": "b69fa1f8b7b935c79b193502358bd59b685afd77\n", "time": "2012-02-21T15:22:34Z\n", "diff": "mmm a / Lib / json / __init__ . py <nl> ppp b / Lib / json / __init__ . py <nl> <nl> Compact encoding : : <nl> <nl> > > > import json <nl> - > > > json . dumps ( [ 1 , 2 , 3 , { ' 4 ' : 5 , ' 6 ' : 7 } ] , separators = ( ' , ' , ' : ' ) ) <nl> + > > > json . dumps ( [ 1 , 2 , 3 , { ' 4 ' : 5 , ' 6 ' : 7 } ] , sort_keys = True , separators = ( ' , ' , ' : ' ) ) <nl> ' [ 1 , 2 , 3 , { \" 4 \" : 5 , \" 6 \" : 7 } ] ' <nl> <nl> Pretty printing ( using repr ( ) because of extraneous whitespace in the output ) : : <nl>\n", "msg": "Let ' s sort the keys so that this test passes even with random hashes .\n"}
{"diff_id": 34829, "repo": "scikit-learn/scikit-learn\n", "sha": "97445e201c980dbbdd2c3e91980bc2aea654ecb8\n", "time": "2012-11-24T10:43:10Z\n", "diff": "mmm a / sklearn / feature_extraction / text . py <nl> ppp b / sklearn / feature_extraction / text . py <nl> def fit_transform ( self , raw_documents , y = None ) : <nl> vocab = dict ( ( ( t , i ) for i , t in enumerate ( sorted ( terms ) ) ) ) <nl> if not vocab : <nl> raise ValueError ( \" empty vocabulary ; training set may have \" <nl> - \" contained only stop words \" ) <nl> + \" contained only stop words or min_df ( resp . \" <nl> + \" max_df ) may be too high ( resp . too low ) . \" ) <nl> self . vocabulary_ = vocab <nl> <nl> # the term_counts and document_counts might be useful statistics , are <nl>\n", "msg": "Improve error message when vocabulary is empty .\n"}
{"diff_id": 34954, "repo": "ipython/ipython\n", "sha": "b1780dcc5562e216f1bea2a2f9495875471c1e9f\n", "time": "2010-10-26T07:16:36Z\n", "diff": "mmm a / IPython / core / builtin_trap . py <nl> ppp b / IPython / core / builtin_trap . py <nl> <nl> import __builtin__ <nl> <nl> from IPython . config . configurable import Configurable <nl> - from IPython . core . quitter import Quitter <nl> <nl> from IPython . utils . traitlets import Instance <nl> <nl> similarity index 100 % <nl> rename from IPython / core / quitter . py <nl> rename to IPython / deathrow / quitter . py <nl>\n", "msg": "moved Quitter to deathrow as it is no longer used by anything\n"}
{"diff_id": 35137, "repo": "scrapy/scrapy\n", "sha": "0522fe35c334141e90741644fec368cdbd12044e\n", "time": "2019-03-29T19:15:34Z\n", "diff": "mmm a / scrapy / commands / parse . py <nl> ppp b / scrapy / commands / parse . py <nl> def add_options ( self , parser ) : <nl> parser . add_option ( \" - m \" , \" - - meta \" , dest = \" meta \" , <nl> help = \" inject extra meta into the Request , it must be a valid raw json string \" ) <nl> parser . add_option ( \" - - cbkwargs \" , dest = \" cbkwargs \" , <nl> - help = \" inject extra cbkwargs into the Request , it must be a valid raw json string \" ) <nl> + help = \" inject extra callback kwargs into the Request , it must be a valid raw json string \" ) <nl> parser . add_option ( \" - d \" , \" - - depth \" , dest = \" depth \" , type = \" int \" , default = 1 , <nl> help = \" maximum depth for parsing requests [ default : % default ] \" ) <nl> parser . add_option ( \" - v \" , \" - - verbose \" , dest = \" verbose \" , action = \" store_true \" , <nl>\n", "msg": "parse command : improve option description\n"}
{"diff_id": 35140, "repo": "fxsjy/jieba\n", "sha": "c8df565981f9f7488474484865247cb09813d502\n", "time": "2013-04-26T09:43:24Z\n", "diff": "mmm a / jieba / __init__ . py <nl> ppp b / jieba / __init__ . py <nl> def initialize ( dictionary = DICTIONARY ) : <nl> cache_file = os . path . join ( tempfile . gettempdir ( ) , \" jieba . cache \" ) <nl> load_from_cache_fail = True <nl> if os . path . exists ( cache_file ) and os . path . getmtime ( cache_file ) > os . path . getmtime ( os . path . join ( _curpath , dictionary ) ) : <nl> - print > > sys . stderr , \" loading model from cache \" <nl> + print > > sys . stderr , \" loading model from cache \" + cache_file <nl> try : <nl> trie , FREQ , total , min_freq = marshal . load ( open ( cache_file , ' rb ' ) ) <nl> load_from_cache_fail = False <nl> def initialize ( dictionary = DICTIONARY ) : <nl> trie , FREQ , total = gen_trie ( os . path . join ( _curpath , dictionary ) ) <nl> FREQ = dict ( [ ( k , log ( float ( v ) / total ) ) for k , v in FREQ . iteritems ( ) ] ) # normalize <nl> min_freq = min ( FREQ . itervalues ( ) ) <nl> - print > > sys . stderr , \" dumping model to file cache \" <nl> + print > > sys . stderr , \" dumping model to file cache \" + cache_file <nl> tmp_suffix = \" . \" + str ( random . random ( ) ) <nl> marshal . dump ( ( trie , FREQ , total , min_freq ) , open ( cache_file + tmp_suffix , ' wb ' ) ) <nl> if os . name = = ' nt ' : <nl>\n", "msg": "more log trace for trouble shooting\n"}
{"diff_id": 35220, "repo": "huggingface/transformers\n", "sha": "8ba4c5885f03e437edff225e53f8fd334ebc3819\n", "time": "2020-04-28T23:13:59Z\n", "diff": "mmm a / src / transformers / tokenization_utils . py <nl> ppp b / src / transformers / tokenization_utils . py <nl> def max_len_single_sentence ( self ) : <nl> def max_len_sentences_pair ( self ) : <nl> return self . model_max_length - self . num_special_tokens_to_add ( pair = True ) <nl> <nl> + @ max_len_single_sentence . setter <nl> + def max_len_single_sentence ( self , value ) : <nl> + \" \" \" For backward compatibility , allow to try to setup ' max_len_single_sentence ' \" \" \" <nl> + if value = = self . model_max_length - self . num_special_tokens_to_add ( pair = False ) : <nl> + logger . warning ( <nl> + \" Setting ' max_len_single_sentence ' is now deprecated . \" \" This value is automatically set up . \" <nl> + ) <nl> + else : <nl> + raise ValueError ( <nl> + \" Setting ' max_len_single_sentence ' is now deprecated . \" \" This value is automatically set up . \" <nl> + ) <nl> + <nl> + @ max_len_sentences_pair . setter <nl> + def max_len_sentences_pair ( self , value ) : <nl> + \" \" \" For backward compatibility , allow to try to setup ' max_len_sentences_pair ' \" \" \" <nl> + if value = = self . model_max_length - self . num_special_tokens_to_add ( pair = True ) : <nl> + logger . warning ( <nl> + \" Setting ' max_len_sentences_pair ' is now deprecated . \" \" This value is automatically set up . \" <nl> + ) <nl> + else : <nl> + raise ValueError ( <nl> + \" Setting ' max_len_sentences_pair ' is now deprecated . \" \" This value is automatically set up . \" <nl> + ) <nl> + <nl> def get_vocab ( self ) : <nl> \" \" \" Returns the vocabulary as a dict of { token : index } pairs . ` tokenizer . get_vocab ( ) [ token ] ` is equivalent to ` tokenizer . convert_tokens_to_ids ( token ) ` when ` token ` is in the vocab . \" \" \" <nl> raise NotImplementedError ( ) <nl>\n", "msg": "Allow a more backward compatible behavior of max_len_single_sentence and max_len_sentences_pair ( )\n"}
{"diff_id": 35238, "repo": "python/cpython\n", "sha": "592f28272e1287e8cdb57fb1887c4782fa07281e\n", "time": "2000-02-18T00:14:21Z\n", "diff": "mmm a / Lib / distutils / core . py <nl> ppp b / Lib / distutils / core . py <nl> class from it , and returns the class object . <nl> expected class was not found in it . \" \" \" <nl> <nl> module_name = ' distutils . command . ' + command <nl> - klass_name = string . join \\ <nl> - ( map ( string . capitalize , string . split ( command , ' _ ' ) ) , ' ' ) <nl> + klass_name = command <nl> <nl> try : <nl> __import__ ( module_name ) <nl>\n", "msg": "Command classes are now named identically to their commands , so reflect this\n"}
{"diff_id": 35366, "repo": "python/cpython\n", "sha": "04e25a1bdfd8fd724ab46a9c946ae8aa11cd09af\n", "time": "2000-08-22T01:48:54Z\n", "diff": "mmm a / Lib / distutils / archive_util . py <nl> ppp b / Lib / distutils / archive_util . py <nl> <nl> import os <nl> from distutils . errors import DistutilsExecError <nl> from distutils . spawn import spawn <nl> - <nl> + from distutils . dir_util import mkpath <nl> <nl> def make_tarball ( base_name , base_dir , compress = \" gzip \" , <nl> verbose = 0 , dry_run = 0 ) : <nl> def make_tarball ( base_name , base_dir , compress = \" gzip \" , <nl> \" bad value for ' compress ' : must be None , ' gzip ' , or ' compress ' \" <nl> <nl> archive_name = base_name + \" . tar \" <nl> + mkpath ( os . path . dirname ( archive_name ) , verbose = verbose , dry_run = dry_run ) <nl> cmd = [ \" tar \" , \" - cf \" , archive_name , base_dir ] <nl> spawn ( cmd , verbose = verbose , dry_run = dry_run ) <nl> <nl> def make_zipfile ( base_name , base_dir , verbose = 0 , dry_run = 0 ) : <nl> # no changes needed ! <nl> <nl> zip_filename = base_name + \" . zip \" <nl> + mkpath ( os . path . dirname ( zip_filename ) , verbose = verbose , dry_run = dry_run ) <nl> try : <nl> spawn ( [ \" zip \" , \" - rq \" , zip_filename , base_dir ] , <nl> verbose = verbose , dry_run = dry_run ) <nl> def visit ( z , dirname , names ) : <nl> ' bztar ' : ( make_tarball , [ ( ' compress ' , ' bzip2 ' ) ] , \" bzip2 ' ed tar - file \" ) , <nl> ' ztar ' : ( make_tarball , [ ( ' compress ' , ' compress ' ) ] , \" compressed tar file \" ) , <nl> ' tar ' : ( make_tarball , [ ( ' compress ' , None ) ] , \" uncompressed tar file \" ) , <nl> - ' zip ' : ( make_zipfile , [ ] , \" zip - file \" ) <nl> + ' zip ' : ( make_zipfile , [ ] , \" ZIP file \" ) <nl> } <nl> <nl> def check_archive_formats ( formats ) : <nl>\n", "msg": "Ensure destination directory exists before trying to create a tarball\n"}
{"diff_id": 35443, "repo": "ipython/ipython\n", "sha": "75b852da393422658518a51ed7e619b25fbbd5a8\n", "time": "2012-01-31T13:49:46Z\n", "diff": "mmm a / IPython / zmq / ipkernel . py <nl> ppp b / IPython / zmq / ipkernel . py <nl> def embed_kernel ( module = None , local_ns = None ) : <nl> if local_ns is None : <nl> local_ns = caller_locals <nl> app = IPKernelApp . instance ( user_module = module , user_ns = local_ns ) <nl> - app . initialize ( ) <nl> + app . initialize ( [ ] ) <nl> app . start ( ) <nl> <nl> def main ( ) : <nl>\n", "msg": "embed_kernel : pass [ ] to IPKernelApp . initialize to prevent argv parsing\n"}
{"diff_id": 35535, "repo": "keras-team/keras\n", "sha": "d635a60140c11c64db4ac887bc79396484bb55e3\n", "time": "2015-07-23T16:22:06Z\n", "diff": "mmm a / keras / utils / model_utils . py <nl> ppp b / keras / utils / model_utils . py <nl> <nl> import numpy as np <nl> import theano <nl> <nl> + def print_graph_layer_shapes ( graph , input_shapes ) : <nl> + \" \" \" <nl> + Utility function to print the shape of the output at each layer of a Graph <nl> + <nl> + Arguments : <nl> + graph : An instance of models . Graph <nl> + input_shapes : A dict that gives a shape for each input to the Graph <nl> + \" \" \" <nl> + input_vars = [ graph . inputs [ name ] . input <nl> + for name in graph . input_order ] <nl> + output_vars = [ graph . outputs [ name ] . get_output ( ) <nl> + for name in graph . output_order ] <nl> + input_dummy = [ np . zeros ( input_shapes [ name ] , dtype = np . float32 ) <nl> + for name in graph . input_order ] <nl> + <nl> + print ( \" input shapes : \" , input_shapes ) <nl> + for name , l in graph . nodes . items ( ) : <nl> + shape_f = theano . function ( input_vars , <nl> + l . get_output ( train = False ) . shape , <nl> + on_unused_input = ' ignore ' ) <nl> + out_shape = shape_f ( * input_dummy ) <nl> + print ( ' shape after ' , l . get_config ( ) [ ' name ' ] , \" ( \" , name , \" ) : \" , out_shape ) <nl> <nl> - def print_layer_shapes ( model , input_shape ) : <nl> + def print_model_layer_shapes ( model , input_shapes ) : <nl> \" \" \" <nl> Utility function that prints the shape of the output at each layer . <nl> <nl> Arguments : <nl> model : An instance of models . Model <nl> input_shape : The shape of the input you will provide to the model . <nl> + Either a tuple ( for a single input ) or a list of tuple <nl> \" \" \" <nl> # This is to handle the case where a model has been connected to a previous <nl> # layer ( and therefore get_input would recurse into previous layer ' s <nl> def print_layer_shapes ( model , input_shape ) : <nl> raise Exception ( \" This function doesn ' t work on model used as subparts \" <nl> \" for other models \" ) <nl> <nl> - input_var = model . get_input ( train = False ) <nl> - input_tmp = np . zeros ( input_shape , dtype = np . float32 ) <nl> - print ( \" input shape : \" , input_shape ) <nl> + # We allow the shortcut input_shapes = ( 1 , 1 , 28 ) instead of <nl> + # input_shapes = [ ( 1 , 1 , 28 ) ] . <nl> + if not isinstance ( input_shapes [ 0 ] , tuple ) : <nl> + input_shapes = [ input_shapes ] <nl> + input_vars = model . get_input ( train = False ) <nl> + # theano . function excepts a list of variables <nl> + if not isinstance ( input_vars , list ) : <nl> + input_vars = [ input_vars ] <nl> + input_dummy = [ np . zeros ( shape , dtype = np . float32 ) <nl> + for shape in input_shapes ] <nl> + <nl> + print ( \" input shapes : \" , input_shapes ) <nl> for l in model . layers : <nl> - shape_f = theano . function ( [ input_var ] , l . get_output ( train = False ) . shape ) <nl> - out_shape = shape_f ( input_tmp ) <nl> + shape_f = theano . function ( input_vars , <nl> + l . get_output ( train = False ) . shape ) <nl> + out_shape = shape_f ( * input_dummy ) <nl> print ( ' shape after ' , l . get_config ( ) [ ' name ' ] , \" : \" , out_shape ) <nl>\n", "msg": "Add model_utils . print_graph_layer_shapes to handle Graph models .\n"}
{"diff_id": 35684, "repo": "scikit-learn/scikit-learn\n", "sha": "114920e6ea61794fbae562f5081f15fbdb490e90\n", "time": "2015-12-01T07:33:17Z\n", "diff": "mmm a / sklearn / pipeline . py <nl> ppp b / sklearn / pipeline . py <nl> def make_pipeline ( * steps ) : <nl> \" \" \" Construct a Pipeline from the given estimators . <nl> <nl> This is a shorthand for the Pipeline constructor ; it does not require , and <nl> - does not permit , naming the estimators . Instead , they will be given names <nl> - automatically based on their types . <nl> + does not permit , naming the estimators . Instead , their names will be set <nl> + to the lowercase of their types automatically . <nl> <nl> Examples <nl> mmmmmm - - <nl>\n", "msg": "edited docstring for clarity regarding the naming of the pipeline components .\n"}
{"diff_id": 35732, "repo": "ansible/ansible\n", "sha": "06062334316ad8d030cd78e389fbdb339a481dd5\n", "time": "2016-12-08T16:33:38Z\n", "diff": "mmm a / lib / ansible / modules / extras / cloud / vmware / vmware_vm_shell . py <nl> ppp b / lib / ansible / modules / extras / cloud / vmware / vmware_vm_shell . py <nl> <nl> vm_id_type : <nl> description : <nl> - The identification tag for the VM <nl> - default : dns_name <nl> + default : vm_name <nl> choices : <nl> - ' uuid ' <nl> - ' dns_name ' <nl> - ' inventory_path ' <nl> + - ' vm_name ' <nl> required : False <nl> vm_username : <nl> description : <nl>\n", "msg": "Changing docs to reflect vm_name as the default vm_id_type\n"}
{"diff_id": 35819, "repo": "zulip/zulip\n", "sha": "81fe519b24ac4e9c4e9dae85212e725df97d1bc2\n", "time": "2012-11-28T19:49:21Z\n", "diff": "mmm a / zephyr / views . py <nl> ppp b / zephyr / views . py <nl> def api_github_landing ( request , user_profile , event = POST , <nl> <nl> subject = \" % s : pull request % d \" % ( repository [ ' name ' ] , <nl> pull_req [ ' number ' ] ) <nl> - content = ( \" New [ pull request ] ( % s ) from % s : \\ n \\ n % s \\ n \\ n > % s \" <nl> - % ( pull_req [ ' html_url ' ] , <nl> - pull_req [ ' user ' ] [ ' login ' ] , <nl> + content = ( \" Pull request from % s [ % s ] ( % s ) : \\ n \\ n % s \\ n \\ n > % s \" <nl> + % ( pull_req [ ' user ' ] [ ' login ' ] , <nl> + payload [ ' action ' ] , <nl> + pull_req [ ' html_url ' ] , <nl> pull_req [ ' title ' ] , <nl> pull_req [ ' body ' ] ) ) <nl> elif event = = ' push ' : <nl>\n", "msg": "github hooks : Display different actions for pull requests sanely\n"}
{"diff_id": 35820, "repo": "matplotlib/matplotlib\n", "sha": "bf5a0a4c61bbeda0d8b716bce0cb94174fdf3549\n", "time": "2009-09-14T19:16:49Z\n", "diff": "mmm a / doc / sphinxext / gen_gallery . py <nl> ppp b / doc / sphinxext / gen_gallery . py <nl> def gen_gallery ( app , doctree ) : <nl> ' matplotlib_icon ' , <nl> ] ) <nl> <nl> - print <nl> - print \" generating gallery : \" , <nl> data = [ ] <nl> + thumbnails = { } <nl> + <nl> for subdir in ( ' api ' , ' pylab_examples ' , ' mplot3d ' , ' widgets ' , ' axes_grid ' ) : <nl> origdir = os . path . join ( ' build ' , rootdir , subdir ) <nl> thumbdir = os . path . join ( outdir , rootdir , subdir , ' thumbnails ' ) <nl> if not os . path . exists ( thumbdir ) : <nl> os . makedirs ( thumbdir ) <nl> - print subdir , <nl> <nl> for filename in sorted ( glob . glob ( os . path . join ( origdir , ' * . png ' ) ) ) : <nl> if filename . endswith ( \" hires . png \" ) : <nl> def gen_gallery ( app , doctree ) : <nl> path , filename = os . path . split ( filename ) <nl> basename , ext = os . path . splitext ( filename ) <nl> if basename in skips : <nl> - sys . stdout . write ( ' [ skipping % s ] ' % basename ) <nl> - sys . stdout . flush ( ) <nl> continue <nl> <nl> # Create thumbnails based on images in tmpdir , and place <nl> # them within the build tree <nl> orig_path = str ( os . path . join ( origdir , filename ) ) <nl> thumb_path = str ( os . path . join ( thumbdir , filename ) ) <nl> - if out_of_date ( orig_path , thumb_path ) : <nl> - image . thumbnail ( orig_path , thumb_path , scale = 0 . 3 ) <nl> + if out_of_date ( orig_path , thumb_path ) or True : <nl> + thumbnails [ orig_path ] = thumb_path <nl> <nl> m = multiimage . match ( basename ) <nl> if m is None : <nl> def gen_gallery ( app , doctree ) : <nl> data . append ( ( subdir , basename , <nl> os . path . join ( rootdir , subdir , ' thumbnails ' , filename ) ) ) <nl> <nl> - sys . stdout . write ( \" . \" ) <nl> - sys . stdout . flush ( ) <nl> - print <nl> - <nl> link_template = \" \" \" \\ <nl> < a href = \" % s \" > < img src = \" % s \" border = \" 0 \" alt = \" % s \" / > < / a > <nl> \" \" \" <nl> def gen_gallery ( app , doctree ) : <nl> fh . write ( template % ' \\ n ' . join ( rows ) ) <nl> fh . close ( ) <nl> <nl> + try : <nl> + import multiprocessing <nl> + def make_thumbnail ( args ) : <nl> + image . thumbnail ( args [ 0 ] , args [ 1 ] , 0 . 3 ) <nl> + <nl> + app . builder . info ( \" generating thumbnails . . . \" , nonl = True ) <nl> + pool = multiprocessing . Pool ( ) <nl> + pool . map ( make_thumbnail , thumbnails . iteritems ( ) ) <nl> + app . builder . info ( \" done \" ) <nl> + <nl> + except ImportError : <nl> + for key in app . builder . status_iterator ( <nl> + thumbnails . iterkeys ( ) , \" generating thumbnails . . . \" , <nl> + length = len ( thumbnails ) ) : <nl> + image . thumbnail ( key , thumbnails [ key ] , 0 . 3 ) <nl> + <nl> def setup ( app ) : <nl> app . connect ( ' env - updated ' , gen_gallery ) <nl>\n", "msg": "Improve speed of gallery thumbnail generation using multiprocessing\n"}
{"diff_id": 35833, "repo": "zulip/zulip\n", "sha": "caa939d2d5723ae901e977f2c4c1ac81b9d4823c\n", "time": "2020-10-15T22:12:05Z\n", "diff": "mmm a / zerver / lib / actions . py <nl> ppp b / zerver / lib / actions . py <nl> def get_non_subscribed_tups ( ) - > List [ Tuple [ UserProfile , Stream ] ] : <nl> <nl> our_realm = users [ 0 ] . realm <nl> <nl> - # TODO : XXX : This transaction really needs to be done at the serializeable <nl> - # transaction isolation level . <nl> + # We do all the database changes in a transaction to ensure <nl> + # RealmAuditLog entries are atomically created when making changes . <nl> with transaction . atomic ( ) : <nl> occupied_streams_before = list ( get_occupied_streams ( our_realm ) ) <nl> Subscription . objects . filter ( <nl> def get_non_subscribed_tups ( ) - > List [ Tuple [ UserProfile , Stream ] ] : <nl> ) . update ( active = False ) <nl> occupied_streams_after = list ( get_occupied_streams ( our_realm ) ) <nl> <nl> - # Log Subscription Activities in RealmAuditLog <nl> - event_time = timezone_now ( ) <nl> - event_last_message_id = get_last_message_id ( ) <nl> - all_subscription_logs : ( List [ RealmAuditLog ] ) = [ ] <nl> - for ( sub , stream ) in subs_to_deactivate : <nl> - all_subscription_logs . append ( RealmAuditLog ( realm = sub . user_profile . realm , <nl> - acting_user = acting_user , <nl> - modified_user = sub . user_profile , <nl> - modified_stream = stream , <nl> - event_last_message_id = event_last_message_id , <nl> - event_type = RealmAuditLog . SUBSCRIPTION_DEACTIVATED , <nl> - event_time = event_time ) ) <nl> - # Now since we have all log objects generated we can do a bulk insert <nl> - RealmAuditLog . objects . bulk_create ( all_subscription_logs ) <nl> + # Log Subscription Activities in RealmAuditLog <nl> + event_time = timezone_now ( ) <nl> + event_last_message_id = get_last_message_id ( ) <nl> + all_subscription_logs : ( List [ RealmAuditLog ] ) = [ ] <nl> + for ( sub , stream ) in subs_to_deactivate : <nl> + audit_log_entry = RealmAuditLog ( <nl> + realm = sub . user_profile . realm , <nl> + acting_user = acting_user , <nl> + modified_user = sub . user_profile , <nl> + modified_stream = stream , <nl> + event_last_message_id = event_last_message_id , <nl> + event_type = RealmAuditLog . SUBSCRIPTION_DEACTIVATED , <nl> + event_time = event_time ) <nl> + all_subscription_logs . append ( audit_log_entry ) <nl> + # Now since we have all log objects generated we can do a bulk insert <nl> + RealmAuditLog . objects . bulk_create ( all_subscription_logs ) <nl> <nl> altered_user_dict : Dict [ int , Set [ int ] ] = defaultdict ( set ) <nl> streams_by_user : Dict [ int , List [ Stream ] ] = defaultdict ( list ) <nl>\n", "msg": "actions : Use transaction . atomic properly when removing subscriptions .\n"}
{"diff_id": 35945, "repo": "plotly/dash\n", "sha": "6ff9900100dea7d79aa7f9b2185dc54031638fcc\n", "time": "2019-09-04T15:40:11Z\n", "diff": "mmm a / dash / dash . py <nl> ppp b / dash / dash . py <nl> def _add_url ( self , name , view_func , methods = ( ' GET ' , ) ) : <nl> # e . g . for adding authentication with flask_login <nl> self . routes . append ( name ) <nl> <nl> + @ property <nl> + def layout ( self ) : <nl> + return self . _layout <nl> + <nl> def _layout_value ( self ) : <nl> if isinstance ( self . _layout , _patch_collections_abc ( ' Callable ' ) ) : <nl> self . _cached_layout = self . _layout ( ) <nl> def _layout_value ( self ) : <nl> self . _cached_layout = self . _layout <nl> return self . _cached_layout <nl> <nl> - @ property <nl> - def layout ( self ) : <nl> - return self . _layout <nl> - <nl> @ layout . setter <nl> def layout ( self , value ) : <nl> if ( not isinstance ( value , Component ) and <nl>\n", "msg": "Revert \" mypy wants property getter and setter to be consecutive to each other . \"\n"}
{"diff_id": 36148, "repo": "explosion/spaCy\n", "sha": "ffedff9e6c544b4becb455e3b483bed0fea32ff6\n", "time": "2015-11-03T07:54:05Z\n", "diff": "mmm a / spacy / en / download . py <nl> ppp b / spacy / en / download . py <nl> def install_data ( url , extract_path , download_path ) : <nl> assert tmp = = download_path <nl> t = tarfile . open ( download_path ) <nl> t . extractall ( extract_path ) <nl> + os . unlink ( download_path ) <nl> <nl> <nl> @ plac . annotations ( <nl>\n", "msg": "* Remove the archive after download , to save disk space\n"}
{"diff_id": 36235, "repo": "python/cpython\n", "sha": "c15a82813acd556c93cdea6db3d40413e34e422a\n", "time": "2001-08-27T21:45:32Z\n", "diff": "mmm a / Lib / test / test_unary . py <nl> ppp b / Lib / test / test_unary . py <nl> def test_invert ( self ) : <nl> self . assert_ ( - - 2 = = 2 ) <nl> self . assert_ ( - 2L = = 0 - 2L ) <nl> <nl> - def test_overflow ( self ) : <nl> - self . assertRaises ( OverflowError , eval , \" + \" + ( \" 9 \" * 32 ) ) <nl> - self . assertRaises ( OverflowError , eval , \" - \" + ( \" 9 \" * 32 ) ) <nl> - self . assertRaises ( OverflowError , eval , \" ~ \" + ( \" 9 \" * 32 ) ) <nl> + def test_no_overflow ( self ) : <nl> + nines = \" 9 \" * 32 <nl> + self . assert_ ( eval ( \" + \" + nines ) = = eval ( \" + \" + nines + \" L \" ) ) <nl> + self . assert_ ( eval ( \" - \" + nines ) = = eval ( \" - \" + nines + \" L \" ) ) <nl> + self . assert_ ( eval ( \" ~ \" + nines ) = = eval ( \" ~ \" + nines + \" L \" ) ) <nl> <nl> def test_bad_types ( self ) : <nl> for op in ' + ' , ' - ' , ' ~ ' : <nl>\n", "msg": "Change test_overflow to test_no_overflow ; looks like big int literals\n"}
{"diff_id": 36417, "repo": "home-assistant/core\n", "sha": "3453d31f010c12757f72fe690b6bc6c5368a07ba\n", "time": "2019-01-09T04:23:44Z\n", "diff": "mmm a / homeassistant / components / zha / helpers . py <nl> ppp b / homeassistant / components / zha / helpers . py <nl> async def configure_reporting ( entity_id , cluster , attr , skip_bind = False , <nl> <nl> attr_name = cluster . attributes . get ( attr , [ attr ] ) [ 0 ] <nl> cluster_name = cluster . ep_attribute <nl> + kwargs = { } <nl> + if manufacturer : <nl> + kwargs [ ' manufacturer ' ] = manufacturer <nl> try : <nl> res = await cluster . configure_reporting ( attr , min_report , <nl> max_report , reportable_change , <nl> - manufacturer = manufacturer ) <nl> + * * kwargs ) <nl> _LOGGER . debug ( <nl> \" % s : reporting ' % s ' attr on ' % s ' cluster : % d / % d / % d : Result : ' % s ' \" , <nl> entity_id , attr_name , cluster_name , min_report , max_report , <nl>\n", "msg": "Use manufacturer id only for configure_reporting only when specified . ( )\n"}
{"diff_id": 36432, "repo": "ansible/ansible\n", "sha": "a64bae30b5a75a7a9f34d60624da4d1938be2390\n", "time": "2016-12-08T16:33:35Z\n", "diff": "mmm a / lib / ansible / modules / extras / cloud / docker / docker_login . py <nl> ppp b / lib / ansible / modules / extras / cloud / docker / docker_login . py <nl> def login ( self ) : <nl> reauth = self . reauth , <nl> dockercfg_path = self . dockercfg_path <nl> ) <nl> + except DockerAPIError as e : <nl> + self . module . fail_json ( msg = \" Docker API Error : % s \" % e . explanation ) <nl> except Exception as e : <nl> self . module . fail_json ( msg = \" failed to login to the remote registry \" , error = repr ( e ) ) <nl> <nl>\n", "msg": "Added more meaningful fail messages on Docker API\n"}
{"diff_id": 36700, "repo": "zulip/zulip\n", "sha": "7e62ef5a036ebab5dd4cadc931b7ac2d9740623d\n", "time": "2013-01-11T21:11:07Z\n", "diff": "mmm a / zephyr / decorator . py <nl> ppp b / zephyr / decorator . py <nl> def _wrapped_view_func ( request , * args , * * kwargs ) : <nl> return view_func ( request , * args , * * kwargs ) <nl> return _wrapped_view_func <nl> <nl> - class RequestVariableMissingError ( Exception ) : <nl> + class JsonableError ( Exception ) : <nl> + def __init__ ( self , error ) : <nl> + self . error = error <nl> + <nl> + def __str__ ( self ) : <nl> + return self . to_json_error_msg ( ) <nl> + <nl> + def to_json_error_msg ( self ) : <nl> + return self . error <nl> + <nl> + class RequestVariableMissingError ( JsonableError ) : <nl> def __init__ ( self , var_name ) : <nl> self . var_name = var_name <nl> <nl> def to_json_error_msg ( self ) : <nl> return \" Missing ' % s ' argument \" % ( self . var_name , ) <nl> <nl> - def __str__ ( self ) : <nl> - return self . to_json_error_msg ( ) <nl> - <nl> - class RequestVariableConversionError ( Exception ) : <nl> + class RequestVariableConversionError ( JsonableError ) : <nl> def __init__ ( self , var_name , bad_value ) : <nl> self . var_name = var_name <nl> self . bad_value = bad_value <nl> def __init__ ( self , var_name , bad_value ) : <nl> def to_json_error_msg ( self ) : <nl> return \" Bad value for ' % s ' : % s \" % ( self . var_name , self . bad_value ) <nl> <nl> - def __str__ ( self ) : <nl> - return self . to_json_error_msg ( ) <nl> - <nl> # Used in conjunction with @ has_request_variables , below <nl> class POST ( object ) : <nl> # NotSpecified is a sentinel value for determining whether a <nl>\n", "msg": "Add a common base class for the RequestVariable * Error classes .\n"}
{"diff_id": 36743, "repo": "ansible/ansible\n", "sha": "f6d66bb3dbce02a78594f701ac6174035d3fa88e\n", "time": "2016-12-08T16:33:00Z\n", "diff": "mmm a / lib / ansible / modules / extras / windows / win_unzip . py <nl> ppp b / lib / ansible / modules / extras / windows / win_unzip . py <nl> <nl> DOCUMENTATION = ' ' ' <nl> mmm <nl> module : win_unzip <nl> - version_added : \" \" <nl> - short_description : Unzips compressed files on the Windows node <nl> + version_added : \" 2 . 0 \" <nl> + short_description : Unzips compressed files and archives on the Windows node <nl> description : <nl> - - Unzips compressed files , and can force reboot ( if needed , i . e . such as hotfixes ) . Has ability to recursively unzip files within the src zip file provided using Read - Archive and piping to Expand - Archive ( Using PSCX ) . If the destination directory does not exist , it will be created before unzipping the file . If a . zip file is specified as src and recurse is true then PSCX will be installed . Specifying rm parameter will allow removal of the src file after extraction . <nl> + - Unzips compressed files and archives . For extracting any compression types other than . zip , the PowerShellCommunityExtensions ( PSCX ) Module is required . This module ( in conjunction with PSCX ) has the ability to recursively unzip files within the src zip file provided and also functionality for many other compression types . If the destination directory does not exist , it will be created before unzipping the file . Specifying rm parameter will force removal of the src file after extraction . <nl> options : <nl> src : <nl> description : <nl>\n", "msg": "minor doc fixes that had lingering description of deprecated functions\n"}
{"diff_id": 36851, "repo": "matplotlib/matplotlib\n", "sha": "118876e867f416c6d22752c3955c9427cefbc46c\n", "time": "2020-07-02T12:12:35Z\n", "diff": "mmm a / lib / matplotlib / backends / _backend_tk . py <nl> ppp b / lib / matplotlib / backends / _backend_tk . py <nl> def destroy ( self , * args ) : <nl> <nl> self . window . destroy ( ) <nl> <nl> - if not Gcf . get_num_fig_managers ( ) and self . _owns_mainloop : <nl> + if self . _owns_mainloop and not Gcf . get_num_fig_managers ( ) : <nl> self . window . quit ( ) <nl> <nl> def get_window_title ( self ) : <nl> def mainloop ( ) : <nl> if managers : <nl> first_manager = managers [ 0 ] <nl> manager_class = type ( first_manager ) <nl> + if manager_class . _owns_mainloop : <nl> + return <nl> manager_class . _owns_mainloop = True <nl> - first_manager . window . mainloop ( ) <nl> - manager_class . _owns_mainloop = False <nl> + try : <nl> + first_manager . window . mainloop ( ) <nl> + finally : <nl> + manager_class . _owns_mainloop = False <nl>\n", "msg": "Use finally to reset mainloop ownership\n"}
{"diff_id": 36927, "repo": "openai/gym\n", "sha": "1183e8f6b56f3fce04f09e3cef32f264956f9cae\n", "time": "2017-01-11T22:09:18Z\n", "diff": "mmm a / gym / benchmarks / __init__ . py <nl> ppp b / gym / benchmarks / __init__ . py <nl> <nl> tasks = [ <nl> { ' env_id ' : ' MinecraftObstacles - v0 ' , <nl> ' trials ' : 1 , <nl> - ' max_timesteps ' : 900 , <nl> + ' max_timesteps ' : 900000 , <nl> ' reward_floor ' : - 1000 . 0 , <nl> ' reward_ceiling ' : 2080 . 0 , <nl> } , <nl> { ' env_id ' : ' MinecraftSimpleRoomMaze - v0 ' , <nl> ' trials ' : 1 , <nl> - ' max_timesteps ' : 900 , <nl> + ' max_timesteps ' : 900000 , <nl> ' reward_floor ' : - 1000 . 0 , <nl> ' reward_ceiling ' : 4160 . 0 , <nl> } , <nl> { ' env_id ' : ' MinecraftAttic - v0 ' , <nl> ' trials ' : 1 , <nl> - ' max_timesteps ' : 600 , <nl> + ' max_timesteps ' : 600000 , <nl> ' reward_floor ' : - 1000 . 0 , <nl> ' reward_ceiling ' : 1040 . 0 , <nl> } , <nl> { ' env_id ' : ' MinecraftComplexityUsage - v0 ' , <nl> ' trials ' : 1 , <nl> - ' max_timesteps ' : 600 , <nl> + ' max_timesteps ' : 600000 , <nl> ' reward_floor ' : - 1000 . 0 , <nl> ' reward_ceiling ' : 1000 . 0 , <nl> } , <nl>\n", "msg": "Fixed timesteps in MinecraftHard - v0 benchmark .\n"}
{"diff_id": 37093, "repo": "python/cpython\n", "sha": "ca67412f282c8e821b27097aa812f582e6f2b13f\n", "time": "2008-03-29T05:06:52Z\n", "diff": "mmm a / Lib / test / test_threading . py <nl> ppp b / Lib / test / test_threading . py <nl> def __init__ ( self , name , testcase , sema , mutex , nrunning ) : <nl> def run ( self ) : <nl> delay = random . random ( ) / 10000 . 0 <nl> if verbose : <nl> - print ( ' task ' , self . getName ( ) , ' will run for ' , delay , ' sec ' ) <nl> + print ( ' task % s will run for % . 1f usec ' % <nl> + ( self . getName ( ) , delay * 1e6 ) ) <nl> <nl> with self . sema : <nl> with self . mutex : <nl> def run ( self ) : <nl> self . testcase . assert_ ( self . nrunning . get ( ) > = 0 ) <nl> if verbose : <nl> print ( ' % s is finished . % d tasks are running ' % <nl> - self . getName ( ) , self . nrunning . get ( ) ) <nl> + ( self . getName ( ) , self . nrunning . get ( ) ) ) <nl> <nl> class ThreadTests ( unittest . TestCase ) : <nl> <nl> def test_enumerate_after_join ( self ) : <nl> enum = threading . enumerate <nl> old_interval = sys . getcheckinterval ( ) <nl> try : <nl> - for i in range ( 1 , 1000 ) : <nl> + for i in range ( 1 , 100 ) : <nl> + # Try a couple times at each thread - switching interval <nl> + # to get more interleavings . <nl> + sys . setcheckinterval ( i / / 5 ) <nl> t = threading . Thread ( target = lambda : None ) <nl> t . start ( ) <nl> t . join ( ) <nl>\n", "msg": "Update test_threading with a couple changes from trunk that got lost due , I\n"}
{"diff_id": 37204, "repo": "pypa/pipenv\n", "sha": "467d1e0d42b161b33c96b60069cc925bc8890a48\n", "time": "2018-03-30T18:37:12Z\n", "diff": "mmm a / pipenv / core . py <nl> ppp b / pipenv / core . py <nl> def warn_in_virtualenv ( ) : <nl> ' { 0 } : Pipenv found itself running within a virtual environment , ' <nl> ' so it will automatically use that environment , instead of ' <nl> ' creating its own for any project . You can set ' <nl> - ' PIPENV_IGNORE_VIRTUALENVS = 1 to force pipenv to ignore that ' <nl> - ' environment and create its own instead . ' . format ( <nl> - crayons . green ( ' Courtesy Notice ' ) <nl> + ' { 1 } to force pipenv to ignore that environment and create ' <nl> + ' its own instead . ' . format ( <nl> + crayons . green ( ' Courtesy Notice ' ) , <nl> + crayons . normal ( ' PIPENV_IGNORE_VIRTUALENVS = 1 ' , bold = True ) , <nl> ) , <nl> err = True , <nl> ) <nl>\n", "msg": "Use bold formatting for required variable\n"}
{"diff_id": 37223, "repo": "keras-team/keras\n", "sha": "33a03fdb7908f84adaf4827adfc308365fb73ff2\n", "time": "2017-08-13T17:28:27Z\n", "diff": "mmm a / keras / preprocessing / image . py <nl> ppp b / keras / preprocessing / image . py <nl> def __init__ ( self , x , y , image_data_generator , <nl> ' with shape ' , self . x . shape ) <nl> channels_axis = 3 if data_format = = ' channels_last ' else 1 <nl> if self . x . shape [ channels_axis ] not in { 1 , 3 , 4 } : <nl> - raise ValueError ( ' NumpyArrayIterator is set to use the ' <nl> - ' data format convention \" ' + data_format + ' \" ' <nl> - ' ( channels on axis ' + str ( channels_axis ) + ' ) , i . e . expected ' <nl> - ' either 1 , 3 or 4 channels on axis ' + str ( channels_axis ) + ' . ' <nl> - ' However , it was passed an array with shape ' + str ( self . x . shape ) + <nl> - ' ( ' + str ( self . x . shape [ channels_axis ] ) + ' channels ) . ' ) <nl> + warnings . warn ( ' NumpyArrayIterator is set to use the ' <nl> + ' data format convention \" ' + data_format + ' \" ' <nl> + ' ( channels on axis ' + str ( channels_axis ) + ' ) , i . e . expected ' <nl> + ' either 1 , 3 or 4 channels on axis ' + str ( channels_axis ) + ' . ' <nl> + ' However , it was passed an array with shape ' + str ( self . x . shape ) + <nl> + ' ( ' + str ( self . x . shape [ channels_axis ] ) + ' channels ) . ' ) <nl> if y is not None : <nl> self . y = np . asarray ( y ) <nl> else : <nl>\n", "msg": "Changed exception to warning for NumpyIterator ( )\n"}
{"diff_id": 37539, "repo": "scikit-learn/scikit-learn\n", "sha": "d11e6d19987a983c7bd3e134b06e1e21462cb12d\n", "time": "2012-09-04T19:48:12Z\n", "diff": "mmm a / sklearn / tests / test_common . py <nl> ppp b / sklearn / tests / test_common . py <nl> <nl> from sklearn . utils import shuffle <nl> from sklearn . preprocessing import Scaler <nl> # from sklearn . cross_validation import train_test_split <nl> - from sklearn . datasets import load_iris , load_boston <nl> + from sklearn . datasets import load_iris , load_boston , make_blobs <nl> from sklearn . metrics import zero_one_score , adjusted_rand_score <nl> from sklearn . lda import LDA <nl> from sklearn . svm . base import BaseLibSVM <nl> def test_transformers ( ) : <nl> estimators = all_estimators ( ) <nl> transformers = [ ( name , E ) for name , E in estimators if issubclass ( E , <nl> TransformerMixin ) ] <nl> - iris = load_iris ( ) <nl> - X , y = iris . data , iris . target <nl> - X , y = shuffle ( X , y , random_state = 0 ) <nl> - X , y = X [ : 10 ] , y [ : 10 ] <nl> + X , y = make_blobs ( n_samples = 30 , centers = [ [ 0 , 0 , 0 ] , [ 1 , 1 , 1 ] ] , <nl> + random_state = 0 , n_features = 2 , cluster_std = 0 . 1 ) <nl> n_samples , n_features = X . shape <nl> X = Scaler ( ) . fit_transform ( X ) <nl> X - = X . min ( ) <nl>\n", "msg": "ENH more robust transformer testing . . . . don ' t ask why that came up\n"}
{"diff_id": 37691, "repo": "bokeh/bokeh\n", "sha": "ff08befdb482ba887d0d19e6b075ab64ec883ccf\n", "time": "2015-06-16T21:34:00Z\n", "diff": "mmm a / bokeh / embed . py <nl> ppp b / bokeh / embed . py <nl> def notebook_div ( plot_object ) : <nl> return encode_utf8 ( html ) <nl> <nl> <nl> - def file_html ( plot_object , resources , title , template = FILE ) : <nl> + def file_html ( plot_object , resources , title , template = FILE , template_variables = None ) : <nl> ' ' ' Return an HTML document that embeds a Bokeh plot . <nl> <nl> The data for the plot is stored directly in the returned HTML . <nl> def file_html ( plot_object , resources , title , template = FILE ) : <nl> template ( Template , optional ) : HTML document template ( default : FILE ) <nl> A Jinja2 Template , see bokeh . templates . FILE for the required <nl> template parameters <nl> + template_variables ( dict , optional ) : variables to be used in the Jinja2 <nl> + template . If used , the following variable names will be overwritten : <nl> + title , plot_resources , plot_script , plot_div <nl> <nl> Returns : <nl> html : standalone HTML document with embedded plot <nl> def file_html ( plot_object , resources , title , template = FILE ) : <nl> css_files = resources . css_files , <nl> ) <nl> script , div = components ( plot_object ) <nl> - html = template . render ( <nl> - title = title , <nl> - plot_resources = plot_resources , <nl> - plot_script = script , <nl> - plot_div = div , <nl> + template_variables_full = \\ <nl> + template_variables . copy ( ) if template_variables is not None else { } <nl> + template_variables_full . update ( <nl> + { <nl> + ' title ' : title , <nl> + ' plot_resources ' : plot_resources , <nl> + ' plot_script ' : script , <nl> + ' plot_div ' : div , <nl> + } <nl> ) <nl> + html = template . render ( template_variables_full ) <nl> return encode_utf8 ( html ) <nl> <nl> <nl>\n", "msg": "Allow users to specify Jinja2 template variables\n"}
{"diff_id": 37710, "repo": "matplotlib/matplotlib\n", "sha": "bb75ff0decbd2d2d9e1bd33ae77f0d47817cc7f6\n", "time": "2013-01-23T17:29:16Z\n", "diff": "mmm a / lib / matplotlib / mlab . py <nl> ppp b / lib / matplotlib / mlab . py <nl> def csv2rec ( fname , comments = ' # ' , skiprows = 0 , checkrows = 0 , delimiter = ' , ' , <nl> files is automatic , if the filename ends in ' . gz ' <nl> <nl> - * comments * : the character used to indicate the start of a comment <nl> - in the file <nl> + in the file , or None to switch off the removal of comments <nl> <nl> - * skiprows * : is the number of rows from the top to skip <nl> <nl> def get_converters ( reader ) : <nl> if needheader : <nl> for row in reader : <nl> # print ' csv2rec ' , row <nl> - if len ( row ) and row [ 0 ] . startswith ( comments ) : <nl> + if len ( row ) and comments ! = None and row [ 0 ] . startswith ( comments ) : <nl> continue <nl> headers = row <nl> break <nl> def get_converters ( reader ) : <nl> while 1 : <nl> # skip past any comments and consume one line of column header <nl> row = next ( reader ) <nl> - if len ( row ) and row [ 0 ] . startswith ( comments ) : <nl> + if len ( row ) and comments ! = None and row [ 0 ] . startswith ( comments ) : <nl> continue <nl> break <nl> <nl> def get_converters ( reader ) : <nl> rowmasks = [ ] <nl> for i , row in enumerate ( reader ) : <nl> if not len ( row ) : continue <nl> - if row [ 0 ] . startswith ( comments ) : continue <nl> + if comments ! = None and row [ 0 ] . startswith ( comments ) : continue <nl> # Ensure that the row returned always has the same nr of elements <nl> row . extend ( [ ' ' ] * ( len ( converters ) - len ( row ) ) ) <nl> rows . append ( [ func ( name , val ) for func , name , val in zip ( converters , names , row ) ] ) <nl>\n", "msg": "Enable to switch off the removal of comments in csv2rec .\n"}
{"diff_id": 37761, "repo": "apache/superset\n", "sha": "e42d1a1fc5c576722bcb50ff1f70e1c35aa04111\n", "time": "2016-02-16T04:10:33Z\n", "diff": "mmm a / panoramix / viz . py <nl> ppp b / panoramix / viz . py <nl> class DistributionBarViz ( DistributionPieViz ) : <nl> ' fields ' : ( <nl> ' granularity ' , <nl> ( ' since ' , ' until ' ) , <nl> - ' metrics ' , ' groupby ' , <nl> - ' limit ' , <nl> + ) <nl> + } , { <nl> + ' label ' : ' Chart Options ' , <nl> + ' fields ' : ( <nl> + ' groupby ' , <nl> + ' columns ' , <nl> + ' metrics ' , <nl> + ' row_limit ' , <nl> ( ' show_legend ' , ' bar_stacked ' ) , <nl> ) <nl> } , ) <nl> + form_overrides = { <nl> + ' groupby ' : { <nl> + ' label ' : ' Series ' , <nl> + } , <nl> + ' columns ' : { <nl> + ' label ' : ' Breakdowns ' , <nl> + ' description ' : \" Defines how each series is broken down \" , <nl> + } , <nl> + } <nl> + <nl> + def query_obj ( self ) : <nl> + d = super ( DistributionPieViz , self ) . query_obj ( ) <nl> + fd = self . form_data <nl> + d [ ' is_timeseries ' ] = False <nl> + gb = fd . get ( ' groupby ' ) or [ ] <nl> + cols = fd . get ( ' columns ' ) or [ ] <nl> + d [ ' groupby ' ] = set ( gb + cols ) <nl> + if len ( d [ ' groupby ' ] ) < len ( gb ) + len ( cols ) : <nl> + raise Exception ( \" Can ' t have overlap between Series and Breakdowns \" ) <nl> + if not self . metrics : <nl> + raise Exception ( \" Pick at least one metric \" ) <nl> + if not self . groupby : <nl> + raise Exception ( \" Pick at least one field for [ Series ] \" ) <nl> + return d <nl> <nl> def get_df ( self ) : <nl> df = super ( DistributionPieViz , self ) . get_df ( ) <nl> - df = df . pivot_table ( <nl> + fd = self . form_data <nl> + <nl> + row = df . groupby ( self . groupby ) . sum ( ) [ self . metrics [ 0 ] ] . copy ( ) <nl> + row . sort ( ascending = False ) <nl> + columns = fd . get ( ' columns ' ) or [ ] <nl> + pt = df . pivot_table ( <nl> index = self . groupby , <nl> + columns = columns , <nl> values = self . metrics ) <nl> - df = df . sort ( self . metrics [ 0 ] , ascending = False ) <nl> - return df <nl> + pt = pt . reindex ( row . index ) <nl> + return pt <nl> <nl> def get_json_data ( self ) : <nl> df = self . get_df ( ) <nl> def get_json_data ( self ) : <nl> for name , ys in series . items ( ) : <nl> if df [ name ] . dtype . kind not in \" biufc \" : <nl> continue <nl> - df [ ' timestamp ' ] = pd . to_datetime ( df . index , utc = False ) <nl> if isinstance ( name , string_types ) : <nl> series_title = name <nl> elif len ( self . metrics ) > 1 : <nl> series_title = \" , \" . join ( name ) <nl> else : <nl> - series_title = \" , \" . join ( name [ 1 : ] ) <nl> + l = [ str ( s ) for s in name [ 1 : ] ] <nl> + series_title = \" , \" . join ( l ) <nl> d = { <nl> \" key \" : series_title , <nl> \" values \" : [ <nl> - { ' x ' : ds , ' y ' : ys [ i ] } <nl> - for i , ds in enumerate ( df . timestamp ) ] <nl> + { ' x ' : i , ' y ' : v } <nl> + for i , v in ys . iteritems ( ) ] <nl> } <nl> chart_data . append ( d ) <nl> return dumps ( chart_data ) <nl>\n", "msg": "Improved the bar char to allow for dimensional breakdowns\n"}
{"diff_id": 37830, "repo": "ansible/ansible\n", "sha": "17c185c8d1040d6441a66779125dcb67c68a99d8\n", "time": "2016-12-08T16:23:09Z\n", "diff": "mmm a / lib / ansible / modules / cloud / azure / azure . py <nl> ppp b / lib / ansible / modules / cloud / azure / azure . py <nl> def create_virtual_machine ( module , azure ) : <nl> authorized_keys_path = u ' / home / % s / . ssh / authorized_keys ' % user <nl> ssh_config . public_keys . public_keys . append ( PublicKey ( path = authorized_keys_path , fingerprint = fingerprint ) ) <nl> # Append ssh config to linux machine config <nl> - linux_config . ssh = ssh_config <nl> + vm_config . ssh = ssh_config <nl> <nl> # Create network configuration <nl> network_config = ConfigurationSetInputEndpoints ( ) <nl> def main ( ) : <nl> cloud_service_raw = None <nl> if module . params . get ( ' state ' ) = = ' absent ' : <nl> ( changed , public_dns_name , deployment ) = terminate_virtual_machine ( module , azure ) <nl> - <nl> + <nl> elif module . params . get ( ' state ' ) = = ' present ' : <nl> # Changed is always set to true when provisioning new instances <nl> if not module . params . get ( ' name ' ) : <nl>\n", "msg": "enable azure to provision windows instances\n"}
{"diff_id": 37937, "repo": "ytdl-org/youtube-dl\n", "sha": "bc93bdb5bbf48b121977a5088ab9607f0fbeb83e\n", "time": "2015-06-27T07:19:46Z\n", "diff": "mmm a / youtube_dl / extractor / youtube . py <nl> ppp b / youtube_dl / extractor / youtube . py <nl> def add_dash_mpd ( video_info ) : <nl> add_dash_mpd ( video_info ) <nl> else : <nl> age_gate = False <nl> + video_info = None <nl> # Try looking directly into the video webpage <nl> mobj = re . search ( r ' ; ytplayer \\ . config \\ s * = \\ s * ( { . * ? } ) ; ' , video_webpage ) <nl> if mobj : <nl>\n", "msg": "[ youtube ] Fix reference before assignment for video_info\n"}
{"diff_id": 37973, "repo": "ansible/ansible\n", "sha": "a620ef414e3713c9524ab6825501e7900f0dfbcd\n", "time": "2014-08-11T16:23:05Z\n", "diff": "mmm a / lib / ansible / cache / memcached . py <nl> ppp b / lib / ansible / cache / memcached . py <nl> <nl> # You should have received a copy of the GNU General Public License <nl> # along with Ansible . If not , see < http : / / www . gnu . org / licenses / > . <nl> import collections <nl> + import os <nl> import sys <nl> import time <nl> + import threading <nl> + from itertools import chain <nl> <nl> from ansible import constants as C <nl> from ansible . cache . base import BaseCacheModule <nl> <nl> sys . exit ( 1 ) <nl> <nl> <nl> + class ProxyClientPool ( object ) : <nl> + \" \" \" <nl> + Memcached connection pooling for thread / fork safety . Inspired by py - redis <nl> + connection pool . <nl> + <nl> + Available connections are maintained in a deque and released in a FIFO manner . <nl> + \" \" \" <nl> + <nl> + def __init__ ( self , * args , * * kwargs ) : <nl> + self . max_connections = kwargs . pop ( ' max_connections ' , 1024 ) <nl> + self . connection_args = args <nl> + self . connection_kwargs = kwargs <nl> + self . reset ( ) <nl> + <nl> + def reset ( self ) : <nl> + self . pid = os . getpid ( ) <nl> + self . _num_connections = 0 <nl> + self . _available_connections = collections . deque ( maxlen = self . max_connections ) <nl> + self . _locked_connections = set ( ) <nl> + self . _lock = threading . Lock ( ) <nl> + <nl> + def _check_safe ( self ) : <nl> + if self . pid ! = os . getpid ( ) : <nl> + with self . _lock : <nl> + if self . pid = = os . getpid ( ) : <nl> + # bail out - another thread already acquired the lock <nl> + return <nl> + self . disconnect_all ( ) <nl> + self . reset ( ) <nl> + <nl> + def get_connection ( self ) : <nl> + self . _check_safe ( ) <nl> + try : <nl> + connection = self . _available_connections . popleft ( ) <nl> + except IndexError : <nl> + connection = self . create_connection ( ) <nl> + self . _locked_connections . add ( connection ) <nl> + return connection <nl> + <nl> + def create_connection ( self ) : <nl> + if self . _num_connections > = self . max_connections : <nl> + raise RuntimeError ( \" Too many memcached connections \" ) <nl> + self . _num_connections + = 1 <nl> + return memcache . Client ( * self . connection_args , * * self . connection_kwargs ) <nl> + <nl> + def release_connection ( self , connection ) : <nl> + self . _check_safe ( ) <nl> + self . _locked_connections . remove ( connection ) <nl> + self . _available_connections . append ( connection ) <nl> + <nl> + def disconnect_all ( self ) : <nl> + for conn in chain ( self . _available_connections , self . _locked_connections ) : <nl> + conn . disconnect_all ( ) <nl> + <nl> + def __getattr__ ( self , name ) : <nl> + def wrapped ( * args , * * kwargs ) : <nl> + return self . _proxy_client ( name , * args , * * kwargs ) <nl> + return wrapped <nl> + <nl> + def _proxy_client ( self , name , * args , * * kwargs ) : <nl> + conn = self . get_connection ( ) <nl> + <nl> + try : <nl> + return getattr ( conn , name ) ( * args , * * kwargs ) <nl> + finally : <nl> + self . release_connection ( conn ) <nl> + <nl> + <nl> class CacheModuleKeys ( collections . MutableSet ) : <nl> \" \" \" <nl> A set subclass that keeps track of insertion time and persists <nl> def __init__ ( self , * args , * * kwargs ) : <nl> <nl> self . _timeout = C . CACHE_PLUGIN_TIMEOUT <nl> self . _prefix = C . CACHE_PLUGIN_PREFIX <nl> - self . _cache = memcache . Client ( connection , debug = 0 ) <nl> + self . _cache = ProxyClientPool ( connection , debug = 0 ) <nl> self . _keys = CacheModuleKeys ( self . _cache , self . _cache . get ( CacheModuleKeys . PREFIX ) or [ ] ) <nl> <nl> def _make_key ( self , key ) : <nl> def _expire_keys ( self ) : <nl> <nl> def get ( self , key ) : <nl> value = self . _cache . get ( self . _make_key ( key ) ) <nl> - # guard against the key not being removed from the zset ; <nl> + # guard against the key not being removed from the keyset ; <nl> # this could happen in cases where the timeout value is changed <nl> # between invocations <nl> if value is None : <nl>\n", "msg": "Implement connection pooling for memcached cache plugin .\n"}
{"diff_id": 38128, "repo": "ansible/ansible\n", "sha": "99e2557b428930c690733a1ffe891ef28cd38bea\n", "time": "2016-12-08T16:23:11Z\n", "diff": "mmm a / lib / ansible / modules / cloud / amazon / route53 . py <nl> ppp b / lib / ansible / modules / cloud / amazon / route53 . py <nl> <nl> required : false <nl> default : false <nl> version_added : \" 1 . 9 \" <nl> + identifier : <nl> + description : <nl> + - Weighted and latency - based resource record sets only . An identifier <nl> + that differentiates among multiple resource record sets that have the <nl> + same combination of DNS name and type . <nl> + required : false <nl> + default : null <nl> + version_added : \" 2 . 0 \" <nl> + weight : <nl> + description : <nl> + - Weighted resource record sets only . Among resource record sets that <nl> + have the same combination of DNS name and type , a value that <nl> + determines what portion of traffic for the current resource record set <nl> + is routed to the associated location . <nl> + required : false <nl> + default : null <nl> + version_added : \" 2 . 0 \" <nl> + region : <nl> + description : <nl> + - Latency - based resource record sets only Among resource record sets <nl> + that have the same combination of DNS name and type , a value that <nl> + determines which region this should be associated with for the <nl> + latency - based routing <nl> + required : false <nl> + default : null <nl> + version_added : \" 2 . 0 \" <nl> + health_check : <nl> + description : <nl> + - Health check to associate with this record <nl> + required : false <nl> + default : null <nl> + version_added : \" 2 . 0 \" <nl> + failover : <nl> + description : <nl> + - Failover resource record sets only . Whether this is the primary or <nl> + secondary resource record set . <nl> + required : false <nl> + default : null <nl> + version_added : \" 2 . 0 \" <nl> author : \" Bruce Pennypacker ( @ bpennypacker ) \" <nl> extends_documentation_fragment : aws <nl> ' ' ' <nl> <nl> alias = True <nl> alias_hosted_zone_id = \" { { elb_zone_id } } \" <nl> <nl> + # Use a routing policy to distribute traffic : <nl> + - route53 : <nl> + command : \" create \" <nl> + zone : \" foo . com \" <nl> + record : \" www . foo . com \" <nl> + type : \" CNAME \" <nl> + value : \" host1 . foo . com \" <nl> + ttl : 30 <nl> + # Routing policy <nl> + identifier : \" host1 @ www \" <nl> + weight : 100 <nl> + health_check : \" d994b780 - 3150 - 49fd - 9205 - 356abdd42e75 \" <nl> <nl> ' ' ' <nl> <nl> <nl> import boto <nl> from boto import route53 <nl> from boto . route53 import Route53Connection <nl> - from boto . route53 . record import ResourceRecordSets <nl> + from boto . route53 . record import Record , ResourceRecordSets <nl> HAS_BOTO = True <nl> except ImportError : <nl> HAS_BOTO = False <nl> <nl> + def get_zone_by_name ( conn , module , zone_name , want_private ) : <nl> + \" \" \" Finds a zone by name \" \" \" <nl> + for zone in conn . get_zones ( ) : <nl> + # only save this zone id if the private status of the zone matches <nl> + # the private_zone_in boolean specified in the params <nl> + private_zone = module . boolean ( zone . config . get ( ' PrivateZone ' , False ) ) <nl> + if private_zone = = want_private and zone . name = = zone_name : <nl> + return zone <nl> + return None <nl> + <nl> <nl> def commit ( changes , retry_interval ) : <nl> \" \" \" Commit changes , but retry PriorRequestNotComplete errors . \" \" \" <nl> def main ( ) : <nl> overwrite = dict ( required = False , type = ' bool ' ) , <nl> retry_interval = dict ( required = False , default = 500 ) <nl> private_zone = dict ( required = False , type = ' bool ' , default = False ) , <nl> + identifier = dict ( required = False ) , <nl> + weight = dict ( required = False , type = ' int ' ) , <nl> + region = dict ( required = False ) , <nl> + health_check = dict ( required = False ) , <nl> + failover = dict ( required = False ) , <nl> ) <nl> ) <nl> module = AnsibleModule ( argument_spec = argument_spec ) <nl> def main ( ) : <nl> alias_hosted_zone_id_in = module . params . get ( ' alias_hosted_zone_id ' ) <nl> retry_interval_in = module . params . get ( ' retry_interval ' ) <nl> private_zone_in = module . params . get ( ' private_zone ' ) <nl> + identifier_in = module . params . get ( ' identifier ' ) <nl> + weight_in = module . params . get ( ' weight ' ) <nl> + region_in = module . params . get ( ' region ' ) <nl> + health_check_in = module . params . get ( ' health_check ' ) <nl> + failover_in = module . params . get ( ' failover ' ) <nl> <nl> region , ec2_url , aws_connect_kwargs = get_aws_connection_info ( module ) <nl> <nl> def main ( ) : <nl> except boto . exception . BotoServerError , e : <nl> module . fail_json ( msg = e . error_message ) <nl> <nl> - # Get all the existing hosted zones and save their ID ' s <nl> - zones = { } <nl> - results = conn . get_all_hosted_zones ( ) <nl> - for r53zone in results [ ' ListHostedZonesResponse ' ] [ ' HostedZones ' ] : <nl> - # only save this zone id if the private status of the zone matches <nl> - # the private_zone_in boolean specified in the params <nl> - if module . boolean ( r53zone [ ' Config ' ] . get ( ' PrivateZone ' , False ) ) = = private_zone_in : <nl> - zone_id = r53zone [ ' Id ' ] . replace ( ' / hostedzone / ' , ' ' ) <nl> - zones [ r53zone [ ' Name ' ] ] = zone_id <nl> + # Find the named zone ID <nl> + zone = get_zone_by_name ( conn , module , zone_in , private_zone_in ) <nl> <nl> # Verify that the requested zone is already defined in Route53 <nl> - if not zone_in in zones : <nl> + if zone is None : <nl> errmsg = \" Zone % s does not exist in Route53 \" % zone_in <nl> module . fail_json ( msg = errmsg ) <nl> <nl> record = { } <nl> <nl> found_record = False <nl> - sets = conn . get_all_rrsets ( zones [ zone_in ] ) <nl> + wanted_rset = Record ( name = record_in , type = type_in , ttl = ttl_in , <nl> + identifier = identifier_in , weight = weight_in , region = region_in , <nl> + health_check = health_check_in , failover = failover_in ) <nl> + for v in value_list : <nl> + if alias_in : <nl> + wanted_rset . set_alias ( alias_hosted_zone_id_in , v ) <nl> + else : <nl> + wanted_rset . add_value ( v ) <nl> + <nl> + sets = conn . get_all_rrsets ( zone . id , name = record_in , type = type_in , identifier = identifier_in ) <nl> for rset in sets : <nl> # Due to a bug in either AWS or Boto , \" special \" characters are returned as octals , preventing round <nl> # tripping of things like * and @ . <nl> decoded_name = rset . name . replace ( r ' \\ 052 ' , ' * ' ) <nl> decoded_name = decoded_name . replace ( r ' \\ 100 ' , ' @ ' ) <nl> <nl> - if rset . type = = type_in and decoded_name . lower ( ) = = record_in . lower ( ) : <nl> + if rset . type = = type_in and decoded_name . lower ( ) = = record_in . lower ( ) and rset . identifier = = identifier_in : <nl> found_record = True <nl> record [ ' zone ' ] = zone_in <nl> record [ ' type ' ] = rset . type <nl> def main ( ) : <nl> record [ ' ttl ' ] = rset . ttl <nl> record [ ' value ' ] = ' , ' . join ( sorted ( rset . resource_records ) ) <nl> record [ ' values ' ] = sorted ( rset . resource_records ) <nl> + record [ ' identifier ' ] = rset . identifier <nl> + record [ ' weight ' ] = rset . weight <nl> + record [ ' region ' ] = rset . region <nl> + record [ ' failover ' ] = rset . failover <nl> + record [ ' health_check ' ] = rset . health_check <nl> if rset . alias_dns_name : <nl> record [ ' alias ' ] = True <nl> record [ ' value ' ] = rset . alias_dns_name <nl> def main ( ) : <nl> record [ ' alias ' ] = False <nl> record [ ' value ' ] = ' , ' . join ( sorted ( rset . resource_records ) ) <nl> record [ ' values ' ] = sorted ( rset . resource_records ) <nl> - if value_list = = sorted ( rset . resource_records ) and int ( record [ ' ttl ' ] ) = = ttl_in and command_in = = ' create ' : <nl> + if command_in = = ' create ' and rset . to_xml ( ) = = wanted_rset . to_xml ( ) : <nl> module . exit_json ( changed = False ) <nl> + break <nl> <nl> if command_in = = ' get ' : <nl> module . exit_json ( changed = False , set = record ) <nl> def main ( ) : <nl> if command_in = = ' delete ' and not found_record : <nl> module . exit_json ( changed = False ) <nl> <nl> - changes = ResourceRecordSets ( conn , zones [ zone_in ] ) <nl> - <nl> - if command_in = = ' create ' and found_record : <nl> - if not module . params [ ' overwrite ' ] : <nl> - module . fail_json ( msg = \" Record already exists with different value . Set ' overwrite ' to replace it \" ) <nl> - else : <nl> - change = changes . add_change ( \" DELETE \" , record_in , type_in , record [ ' ttl ' ] ) <nl> - for v in record [ ' values ' ] : <nl> - if record [ ' alias ' ] : <nl> - change . set_alias ( record [ ' alias_hosted_zone_id ' ] , v ) <nl> - else : <nl> - change . add_value ( v ) <nl> + changes = ResourceRecordSets ( conn , zone . id ) <nl> <nl> if command_in = = ' create ' or command_in = = ' delete ' : <nl> - change = changes . add_change ( command_in . upper ( ) , record_in , type_in , ttl_in ) <nl> - for v in value_list : <nl> - if module . params [ ' alias ' ] : <nl> - change . set_alias ( alias_hosted_zone_id_in , v ) <nl> - else : <nl> - change . add_value ( v ) <nl> + if command_in = = ' create ' and found_record : <nl> + if not module . params [ ' overwrite ' ] : <nl> + module . fail_json ( msg = \" Record already exists with different value . Set ' overwrite ' to replace it \" ) <nl> + command = ' UPSERT ' <nl> + else : <nl> + command = command_in . upper ( ) <nl> + changes . add_change_record ( command , wanted_rset ) <nl> <nl> try : <nl> result = commit ( changes , retry_interval_in ) <nl>\n", "msg": "route53 : add support for routing policies\n"}
{"diff_id": 38214, "repo": "python/cpython\n", "sha": "574a25127a5e0d21483f3b7432798329b49fe112\n", "time": "2004-08-04T14:22:56Z\n", "diff": "mmm a / Lib / test / test_threadsignals . py <nl> ppp b / Lib / test / test_threadsignals . py <nl> <nl> import signal <nl> import os <nl> import sys <nl> - from test import test_support , TestSkipped <nl> + from test . test_support import run_unittest , TestSkipped <nl> <nl> if sys . platform [ : 3 ] in ( ' win ' , ' os2 ' ) or sys . platform = = ' riscos ' : <nl> raise TestSkipped , \" Can ' t test signal on % s \" % sys . platform <nl> <nl> - signal_blackboard = { signal . SIGUSR1 : { ' tripped ' : 0 , ' tripped_by ' : 0 } , <nl> - signal . SIGUSR2 : { ' tripped ' : 0 , ' tripped_by ' : 0 } , <nl> - signal . SIGALRM : { ' tripped ' : 0 , ' tripped_by ' : 0 } } <nl> - <nl> process_pid = os . getpid ( ) <nl> signalled_all = thread . allocate_lock ( ) <nl> <nl> def test_signals ( self ) : <nl> self . assertEqual ( signal_blackboard [ signal . SIGUSR2 ] [ ' tripped ' ] , 1 ) <nl> self . assertEqual ( signal_blackboard [ signal . SIGUSR2 ] [ ' tripped_by ' ] , <nl> thread . get_ident ( ) ) <nl> + signalled_all . release ( ) <nl> <nl> def spawnSignallingThread ( self ) : <nl> thread . start_new_thread ( send_signals , ( ) ) <nl> <nl> <nl> def test_main ( ) : <nl> + global signal_blackboard <nl> + <nl> + signal_blackboard = { signal . SIGUSR1 : { ' tripped ' : 0 , ' tripped_by ' : 0 } , <nl> + signal . SIGUSR2 : { ' tripped ' : 0 , ' tripped_by ' : 0 } , <nl> + signal . SIGALRM : { ' tripped ' : 0 , ' tripped_by ' : 0 } } <nl> + <nl> oldsigs = registerSignals ( ( handle_signals , handle_signals , handle_signals ) ) <nl> try : <nl> - test_support . run_unittest ( ThreadSignals ) <nl> + run_unittest ( ThreadSignals ) <nl> finally : <nl> registerSignals ( oldsigs ) <nl> <nl>\n", "msg": "To ever run this test \" you must import TestSkipped \" from the right\n"}
{"diff_id": 38303, "repo": "zulip/zulip\n", "sha": "cb842c1b838eaddf32a9341a0eb0b7745746404f\n", "time": "2016-08-02T21:31:07Z\n", "diff": "mmm a / zulip_tools . py <nl> ppp b / zulip_tools . py <nl> def su_to_zulip ( ) : <nl> pwent = pwd . getpwnam ( \" zulip \" ) <nl> os . setgid ( pwent . pw_gid ) <nl> os . setuid ( pwent . pw_uid ) <nl> + os . environ [ ' HOME ' ] = os . path . abspath ( os . path . join ( DEPLOYMENTS_DIR , ' . . ' ) ) <nl> <nl> def make_deploy_path ( ) : <nl> # type : ( ) - > str <nl>\n", "msg": "Improve su_to_zulip setting of home directory .\n"}
{"diff_id": 38403, "repo": "sherlock-project/sherlock\n", "sha": "f29cab49e4bf5379b612532ba94824abc3546800\n", "time": "2019-12-31T20:48:21Z\n", "diff": "mmm a / sherlock / sites . py <nl> ppp b / sherlock / sites . py <nl> <nl> import logging <nl> import os <nl> import json <nl> + import operator <nl> import requests <nl> + import sys <nl> <nl> <nl> class SiteInformation ( ) : <nl> - def __init__ ( self , name , url_home , url_username_format , <nl> + def __init__ ( self , name , url_home , url_username_format , popularity_rank , <nl> username_claimed , username_unclaimed , <nl> information ) : <nl> \" \" \" Create Site Information Object . <nl> def __init__ ( self , name , url_home , url_username_format , <nl> usernames would show up under the <nl> \" https : / / somesite . com / users / \" area of <nl> the web site . <nl> + popularity_rank - - Integer indicating popularity of site . <nl> + In general , smaller numbers mean more <nl> + popular ( \" 0 \" or None means ranking <nl> + information not available ) . <nl> username_claimed - - String containing username which is known <nl> to be claimed on web site . <nl> username_unclaimed - - String containing username which is known <nl> def __init__ ( self , name , url_home , url_username_format , <nl> self . name = name <nl> self . url_home = url_home <nl> self . url_username_format = url_username_format <nl> + <nl> + if ( popularity_rank is None ) or ( popularity_rank = = 0 ) : <nl> + # We do not know the popularity , so make site go to bottom of list . <nl> + popularity_rank = sys . maxsize <nl> + self . popularity_rank = popularity_rank <nl> + <nl> self . username_claimed = username_claimed <nl> self . username_unclaimed = username_unclaimed <nl> self . information = information <nl> def __init__ ( self , data_file_path = None ) : <nl> # Add all of site information from the json file to internal site list . <nl> for site_name in site_data : <nl> try : <nl> + # If popularity unknown , make site be at bottom of list . <nl> + popularity_rank = site_data [ site_name ] . get ( \" rank \" , sys . maxsize ) <nl> + <nl> self . sites [ site_name ] = \\ <nl> SiteInformation ( site_name , <nl> site_data [ site_name ] [ \" urlMain \" ] , <nl> site_data [ site_name ] [ \" url \" ] , <nl> + popularity_rank , <nl> site_data [ site_name ] [ \" username_claimed \" ] , <nl> site_data [ site_name ] [ \" username_unclaimed \" ] , <nl> site_data [ site_name ] <nl> def __init__ ( self , data_file_path = None ) : <nl> <nl> return <nl> <nl> + def site_name_list ( self , popularity_rank = False ) : <nl> + \" \" \" Get Site Name List . <nl> + <nl> + Keyword Arguments : <nl> + self - - This object . <nl> + popularity_rank - - Boolean indicating if list should be sorted <nl> + by popularity rank . <nl> + Default value is False . <nl> + NOTE : List is sorted in ascending <nl> + alphabetical order is popularity rank <nl> + is not requested . <nl> + <nl> + Return Value : <nl> + List of strings containing names of sites . <nl> + \" \" \" <nl> + <nl> + if popularity_rank = = True : <nl> + # Sort in ascending popularity rank order . <nl> + site_rank_name = \\ <nl> + sorted ( [ ( site . popularity_rank , site . name ) for site in self ] , <nl> + key = operator . itemgetter ( 0 ) <nl> + ) <nl> + site_names = [ name for _ , name in site_rank_name ] <nl> + else : <nl> + # Sort in ascending alphabetical order . <nl> + site_names = sorted ( [ site . name for site in self ] , key = str . lower ) <nl> + <nl> + return site_names <nl> + <nl> def __iter__ ( self ) : <nl> \" \" \" Iterator For Object . <nl> <nl>\n", "msg": "Add popularity rank to Site Information object . Add method to retrieve list of names of the sites ( sorted by alphabetical or popularity rank ) .\n"}
{"diff_id": 38597, "repo": "zulip/zulip\n", "sha": "a21856c569ab2c1ae18f64e1d23492e335055918\n", "time": "2019-09-25T14:36:47Z\n", "diff": "mmm a / zerver / data_import / mattermost . py <nl> ppp b / zerver / data_import / mattermost . py <nl> def fix_mentions ( content : str , mention_user_ids : Set [ int ] ) - > str : <nl> continue <nl> <nl> pub_date = raw_message [ ' pub_date ' ] <nl> + sender_user_id = raw_message [ ' sender_id ' ] <nl> try : <nl> recipient_id = get_recipient_id_from_receiver_name ( raw_message [ \" receiver_id \" ] , Recipient . STREAM ) <nl> except KeyError : <nl> def fix_mentions ( content : str , mention_user_ids : Set [ int ] ) - > str : <nl> rendered_content = None <nl> <nl> topic_name = ' imported from mattermost ' <nl> - user_id = raw_message [ ' sender_id ' ] <nl> <nl> message = build_message ( <nl> content = content , <nl> def fix_mentions ( content : str , mention_user_ids : Set [ int ] ) - > str : <nl> recipient_id = recipient_id , <nl> rendered_content = rendered_content , <nl> topic_name = topic_name , <nl> - user_id = user_id , <nl> + user_id = sender_user_id , <nl> has_attachment = False , <nl> ) <nl> zerver_message . append ( message ) <nl>\n", "msg": "mattermost : Rename user_id to sender_user_id in process_raw_message_batch .\n"}
{"diff_id": 38636, "repo": "ansible/ansible\n", "sha": "5ae50eb8cf8c5608a13db3d01cf1a4c9ec25ff67\n", "time": "2016-12-08T16:22:38Z\n", "diff": "mmm a / lib / ansible / modules / cloud / rackspace / rax_cbs_attachments . py <nl> ppp b / lib / ansible / modules / cloud / rackspace / rax_cbs_attachments . py <nl> <nl> <nl> def cloud_block_storage_attachments ( module , state , volume , server , device , <nl> wait , wait_timeout ) : <nl> - for arg in ( state , volume , server , device ) : <nl> - if not arg : <nl> - module . fail_json ( msg = ' % s is required for rax_cbs_attachments ' % <nl> - arg ) <nl> - <nl> cbs = pyrax . cloud_blockstorage <nl> cs = pyrax . cloudservers <nl> <nl> def cloud_block_storage_attachments ( module , state , volume , server , device , <nl> not key . startswith ( ' _ ' ) ) : <nl> instance [ key ] = value <nl> <nl> - result = dict ( changed = changed , volume = instance ) <nl> + result = dict ( changed = changed ) <nl> <nl> if volume . status = = ' error ' : <nl> result [ ' msg ' ] = ' % s failed to build ' % volume . id <nl> def cloud_block_storage_attachments ( module , state , volume , server , device , <nl> pyrax . utils . wait_until ( volume , ' status ' , ' in - use ' , <nl> interval = 5 , attempts = attempts ) <nl> <nl> + volume . get ( ) <nl> + result [ ' volume ' ] = rax_to_dict ( volume ) <nl> + <nl> if ' msg ' in result : <nl> module . fail_json ( * * result ) <nl> else : <nl> def cloud_block_storage_attachments ( module , state , volume , server , device , <nl> elif volume . attachments : <nl> module . fail_json ( msg = ' Volume is attached to another server ' ) <nl> <nl> - for key , value in vars ( volume ) . iteritems ( ) : <nl> - if ( isinstance ( value , NON_CALLABLES ) and <nl> - not key . startswith ( ' _ ' ) ) : <nl> - instance [ key ] = value <nl> - <nl> - result = dict ( changed = changed , volume = instance ) <nl> + result = dict ( changed = changed , volume = rax_to_dict ( volume ) ) <nl> <nl> if volume . status = = ' error ' : <nl> result [ ' msg ' ] = ' % s failed to build ' % volume . id <nl>\n", "msg": "Small fix - ups to convert objects to dicts , update volume details at the appropriate time , and remove unnecessary required argument logic\n"}
{"diff_id": 38794, "repo": "zulip/zulip\n", "sha": "ed1d7f79974ec01fe08f7800900efbc2440005af\n", "time": "2016-11-11T23:36:45Z\n", "diff": "mmm a / zerver / lib / webhooks / git . py <nl> ppp b / zerver / lib / webhooks / git . py <nl> <nl> TAG_WITH_URL_TEMPLATE = u \" [ { tag_name } ] ( { tag_url } ) \" <nl> TAG_WITHOUT_URL_TEMPLATE = u \" { tag_name } \" <nl> <nl> - def get_push_commits_event_message ( user_name , compare_url , branch_name , commits_data ) : <nl> - # type : ( text_type , Optional [ text_type ] , text_type , List [ Dict [ str , Any ] ] ) - > text_type <nl> + def get_push_commits_event_message ( user_name , compare_url , branch_name , commits_data , is_truncated = False ) : <nl> + # type : ( text_type , Optional [ text_type ] , text_type , List [ Dict [ str , Any ] ] , Optional [ bool ] ) - > text_type <nl> if compare_url : <nl> pushed_text_message = PUSH_PUSHED_TEXT_WITH_URL . format ( compare_url = compare_url ) <nl> else : <nl> def get_push_commits_event_message ( user_name , compare_url , branch_name , commits_ <nl> user_name = user_name , <nl> pushed_text = pushed_text_message , <nl> branch_name = branch_name , <nl> - commits_data = get_commits_content ( commits_data ) , <nl> + commits_data = get_commits_content ( commits_data , is_truncated ) , <nl> ) . rstrip ( ) <nl> <nl> def get_force_push_commits_event_message ( user_name , url , branch_name , head ) : <nl> def get_commits_comment_action_message ( user_name , action , commit_url , sha , messa <nl> ) <nl> return content <nl> <nl> - def get_commits_content ( commits_data ) : <nl> - # type : ( List [ Dict [ str , Any ] ] ) - > text_type <nl> + def get_commits_content ( commits_data , is_truncated = False ) : <nl> + # type : ( List [ Dict [ str , Any ] ] , Optional [ bool ] ) - > text_type <nl> commits_content = u ' ' <nl> for commit in commits_data [ : COMMITS_LIMIT ] : <nl> commits_content + = COMMIT_ROW_TEMPLATE . format ( <nl> def get_commits_content ( commits_data ) : <nl> <nl> if len ( commits_data ) > COMMITS_LIMIT : <nl> commits_content + = COMMITS_MORE_THAN_LIMIT_TEMPLATE . format ( <nl> - commits_number = len ( commits_data ) - COMMITS_LIMIT ) <nl> + commits_number = len ( commits_data ) - COMMITS_LIMIT <nl> + ) <nl> + elif is_truncated : <nl> + commits_content + = COMMITS_MORE_THAN_LIMIT_TEMPLATE . format ( <nl> + commits_number = ' ' <nl> + ) . replace ( ' ' , ' ' ) <nl> return commits_content . rstrip ( ) <nl> <nl> def get_short_sha ( sha ) : <nl>\n", "msg": "Add is_truncated param to git integrations push commits event .\n"}
{"diff_id": 39276, "repo": "celery/celery\n", "sha": "885eca59952365f24698b28d9969defc20cc170b\n", "time": "2016-10-12T09:38:50Z\n", "diff": "mmm a / celery / events / state . py <nl> ppp b / celery / events / state . py <nl> def __init__ ( self , uuid = None , cluster_state = None , children = None , * * kwargs ) : <nl> ) <nl> self . _serializer_handlers = { <nl> ' children ' : self . _serializable_children , <nl> + ' root ' : self . _serializable_root , <nl> + ' parent ' : self . _serializable_parent , <nl> } <nl> if kwargs : <nl> self . __dict__ . update ( kwargs ) <nl> def as_dict ( self ) : <nl> def _serializable_children ( self , value ) : <nl> return [ task . id for task in self . children ] <nl> <nl> + def _serializable_root ( self , value ) : <nl> + return self . root_id <nl> + <nl> + def _serializable_parent ( self , value ) : <nl> + return self . parent_id <nl> + <nl> def __reduce__ ( self ) : <nl> return _depickle_task , ( self . __class__ , self . as_dict ( ) ) <nl> <nl>\n", "msg": "Add serializer handlers for root and parent ( )\n"}
{"diff_id": 39358, "repo": "ipython/ipython\n", "sha": "8e2720fc8038b68a9ce4ccc7f147d368d5911ad9\n", "time": "2013-04-24T15:45:49Z\n", "diff": "mmm a / IPython / extensions / storemagic . py <nl> ppp b / IPython / extensions / storemagic . py <nl> def store ( self , parameter_s = ' ' ) : <nl> * ` ` % store - z ` ` - Remove all variables from storage <nl> * ` ` % store - r ` ` - Refresh all variables from store ( delete <nl> current vals ) <nl> - * ` ` % store - r spam ` ` - Refresh specified variable from store ( delete <nl> - current val ) <nl> + * ` ` % store - r spam bar ` ` - Refresh specified variables from store <nl> + ( delete current val ) <nl> * ` ` % store foo > a . txt ` ` - Store value of foo to new file a . txt <nl> * ` ` % store foo > > a . txt ` ` - Append value of foo to file a . txt <nl> <nl> def store ( self , parameter_s = ' ' ) : <nl> <nl> elif ' r ' in opts : <nl> if args : <nl> - try : <nl> - obj = db [ ' autorestore / ' + args [ 0 ] ] <nl> - except KeyError : <nl> - print \" Unable to restore variable ' % s ' , ( use % % store - d to forget ! ) \" % args [ 0 ] <nl> - print \" The error was : \" , sys . exc_info ( ) [ 0 ] <nl> - else : <nl> - ip . user_ns [ args [ 0 ] ] = obj <nl> + for arg in args : <nl> + try : <nl> + obj = db [ ' autorestore / ' + arg ] <nl> + except KeyError : <nl> + print \" no stored variable % s \" % arg <nl> + else : <nl> + ip . user_ns [ arg ] = obj <nl> else : <nl> refresh_variables ( ip ) <nl> <nl>\n", "msg": "Added handling of multiple args to store - r and removed error message\n"}
{"diff_id": 39594, "repo": "ipython/ipython\n", "sha": "6c11e817cbda01e5317cc2e26544579db2ea1d54\n", "time": "2013-02-04T05:12:04Z\n", "diff": "mmm a / IPython / kernel / zmq / session . py <nl> ppp b / IPython / kernel / zmq / session . py <nl> def _keyfile_changed ( self , name , old , new ) : <nl> with open ( new , ' rb ' ) as f : <nl> self . key = f . read ( ) . strip ( ) <nl> <nl> + # for protecting against sends from forks <nl> + pid = Integer ( ) <nl> + <nl> # serialization traits : <nl> <nl> pack = Any ( default_packer ) # the actual packer function <nl> def _unpack_changed ( self , name , old , new ) : <nl> Containers larger than this are pickled outright . <nl> \" \" \" <nl> ) <nl> + <nl> <nl> def __init__ ( self , * * kwargs ) : <nl> \" \" \" create a Session object <nl> def __init__ ( self , * * kwargs ) : <nl> self . none = self . pack ( { } ) <nl> # ensure self . _session_default ( ) if necessary , so bsession is defined : <nl> self . session <nl> + self . pid = os . getpid ( ) <nl> <nl> @ property <nl> def msg_id ( self ) : <nl> def send ( self , stream , msg_or_type , content = None , parent = None , ident = None , <nl> else : <nl> msg = self . msg ( msg_or_type , content = content , parent = parent , <nl> header = header , metadata = metadata ) <nl> - <nl> + if not os . getpid ( ) = = self . pid : <nl> + io . rprint ( \" WARNING : attempted to send message from fork \" ) <nl> + io . rprint ( msg ) <nl> + return <nl> buffers = [ ] if buffers is None else buffers <nl> to_send = self . serialize ( msg , ident ) <nl> to_send . extend ( buffers ) <nl>\n", "msg": "use Session to protect from fork - unsafe sockets\n"}
{"diff_id": 39667, "repo": "home-assistant/core\n", "sha": "8c010c8df434f63680579db6fcbeba20483159ed\n", "time": "2015-12-21T23:09:27Z\n", "diff": "mmm a / homeassistant / components / automation / sun . py <nl> ppp b / homeassistant / components / automation / sun . py <nl> <nl> <nl> CONF_OFFSET = ' offset ' <nl> CONF_EVENT = ' event ' <nl> + CONF_BEFORE = \" before \" <nl> + CONF_BEFORE_OFFSET = \" before_offset \" <nl> + CONF_AFTER = \" after \" <nl> + CONF_AFTER_OFFSET = \" after_offset \" <nl> <nl> EVENT_SUNSET = ' sunset ' <nl> EVENT_SUNRISE = ' sunrise ' <nl> def trigger ( hass , config , action ) : <nl> _LOGGER . error ( \" Invalid value for % s : % s \" , CONF_EVENT , event ) <nl> return False <nl> <nl> - if CONF_OFFSET in config : <nl> - raw_offset = config . get ( CONF_OFFSET ) <nl> - <nl> - negative_offset = False <nl> - if raw_offset . startswith ( ' - ' ) : <nl> - negative_offset = True <nl> - raw_offset = raw_offset [ 1 : ] <nl> - <nl> - try : <nl> - ( hour , minute , second ) = [ int ( x ) for x in raw_offset . split ( ' : ' ) ] <nl> - except ValueError : <nl> - _LOGGER . error ( ' Could not parse offset % s ' , raw_offset ) <nl> - return False <nl> - <nl> - offset = timedelta ( hours = hour , minutes = minute , seconds = second ) <nl> - <nl> - if negative_offset : <nl> - offset * = - 1 <nl> - else : <nl> - offset = timedelta ( 0 ) <nl> + offset = _parse_offset ( config . get ( CONF_OFFSET ) ) <nl> + if offset is False : <nl> + return False <nl> <nl> # Do something to call action <nl> if event = = EVENT_SUNRISE : <nl> def trigger ( hass , config , action ) : <nl> return True <nl> <nl> <nl> + def if_action ( hass , config ) : <nl> + \" \" \" Wraps action method with sun based condition . \" \" \" <nl> + before = config . get ( CONF_BEFORE ) <nl> + after = config . get ( CONF_AFTER ) <nl> + <nl> + # Make sure required configuration keys are present <nl> + if before is None and after is None : <nl> + logging . getLogger ( __name__ ) . error ( <nl> + \" Missing if - condition configuration key % s or % s \" , <nl> + CONF_BEFORE , CONF_AFTER ) <nl> + return None <nl> + <nl> + # Make sure configuration keys have the right value <nl> + if before is not None and before not in ( EVENT_SUNRISE , EVENT_SUNSET ) or \\ <nl> + after is not None and after not in ( EVENT_SUNRISE , EVENT_SUNSET ) : <nl> + logging . getLogger ( __name__ ) . error ( <nl> + \" % s and % s can only be set to % s or % s \" , <nl> + CONF_BEFORE , CONF_AFTER , EVENT_SUNRISE , EVENT_SUNSET ) <nl> + return None <nl> + <nl> + before_offset = _parse_offset ( config . get ( CONF_BEFORE_OFFSET ) ) <nl> + after_offset = _parse_offset ( config . get ( CONF_AFTER_OFFSET ) ) <nl> + if before_offset is False or after_offset is False : <nl> + return None <nl> + <nl> + if before is None : <nl> + before_func = lambda : None <nl> + elif before = = EVENT_SUNRISE : <nl> + before_func = lambda : sun . next_rising_utc ( hass ) + before_offset <nl> + else : <nl> + before_func = lambda : sun . next_setting_utc ( hass ) + before_offset <nl> + <nl> + if after is None : <nl> + after_func = lambda : None <nl> + elif after = = EVENT_SUNRISE : <nl> + after_func = lambda : sun . next_rising_utc ( hass ) + after_offset <nl> + else : <nl> + after_func = lambda : sun . next_setting_utc ( hass ) + after_offset <nl> + <nl> + # This is needed for testing <nl> + time_func = dt_util . utcnow <nl> + <nl> + def time_if ( ) : <nl> + \" \" \" Validate time based if - condition \" \" \" <nl> + <nl> + # This is needed for testing . <nl> + nonlocal time_func <nl> + now = time_func ( ) <nl> + before = before_func ( ) <nl> + after = after_func ( ) <nl> + <nl> + if before is not None and now > now . replace ( hour = before . hour , <nl> + minute = before . minute ) : <nl> + return False <nl> + <nl> + if after is not None and now < now . replace ( hour = after . hour , <nl> + minute = after . minute ) : <nl> + return False <nl> + <nl> + return True <nl> + <nl> + return time_if <nl> + <nl> + <nl> def trigger_sunrise ( hass , action , offset ) : <nl> \" \" \" Trigger action at next sun rise . \" \" \" <nl> def next_rise ( ) : <nl> def sunset_automation_listener ( now ) : <nl> action ( ) <nl> <nl> track_point_in_utc_time ( hass , sunset_automation_listener , next_set ( ) ) <nl> + <nl> + <nl> + def _parse_offset ( raw_offset ) : <nl> + if raw_offset is None : <nl> + return timedelta ( 0 ) <nl> + <nl> + negative_offset = False <nl> + if raw_offset . startswith ( ' - ' ) : <nl> + negative_offset = True <nl> + raw_offset = raw_offset [ 1 : ] <nl> + <nl> + try : <nl> + ( hour , minute , second ) = [ int ( x ) for x in raw_offset . split ( ' : ' ) ] <nl> + except ValueError : <nl> + _LOGGER . error ( ' Could not parse offset % s ' , raw_offset ) <nl> + return False <nl> + <nl> + offset = timedelta ( hours = hour , minutes = minute , seconds = second ) <nl> + <nl> + if negative_offset : <nl> + offset * = - 1 <nl> + <nl> + return offset <nl>\n", "msg": "Add ability to use sun as condition in automation\n"}
{"diff_id": 39765, "repo": "python/cpython\n", "sha": "33d7f1a76c3544d2901492cfb6fc9db85f2dfbd6\n", "time": "1998-05-20T16:28:24Z\n", "diff": "mmm a / Lib / random . py <nl> ppp b / Lib / random . py <nl> <nl> <nl> # Translated from anonymously contributed C / C + + source . <nl> <nl> + import whrandom <nl> from whrandom import random , uniform , randint , choice # Also for export ! <nl> from math import log , exp , pi , e , sqrt , acos , cos , sin <nl> <nl> + # Interfaces to replace remaining needs for importing whrandom <nl> + # XXX TO DO : make the distribution functions below into methods . <nl> + <nl> + def makeseed ( a = None ) : <nl> + \" \" \" Turn a hashable value into three seed values for whrandom . seed ( ) . <nl> + <nl> + None or no argument returns ( 0 , 0 , 0 ) , to seed from current time . <nl> + <nl> + \" \" \" <nl> + if a is None : <nl> + return ( 0 , 0 , 0 ) <nl> + a = hash ( a ) <nl> + a , x = divmod ( a , 256 ) <nl> + a , y = divmod ( a , 256 ) <nl> + a , z = divmod ( a , 256 ) <nl> + x = ( x + a ) % 256 or 1 <nl> + y = ( y + a ) % 256 or 1 <nl> + z = ( z + a ) % 256 or 1 <nl> + return ( x , y , z ) <nl> + <nl> + def seed ( a = None ) : <nl> + \" \" \" Seed the default generator from any hashable value . <nl> + <nl> + None or no argument returns ( 0 , 0 , 0 ) to seed from current time . <nl> + <nl> + \" \" \" <nl> + x , y , z = makeseed ( a ) <nl> + whrandom . seed ( x , y , z ) <nl> + <nl> + class generator ( whrandom . whrandom ) : <nl> + \" \" \" Random generator class . \" \" \" <nl> + <nl> + def __init__ ( self , a = None ) : <nl> + \" \" \" Constructor . Seed from current time or hashable value . \" \" \" <nl> + self . seed ( a ) <nl> + <nl> + def seed ( self , a = None ) : <nl> + \" \" \" Seed the generator from current time or hashable value . \" \" \" <nl> + x , y , z = makeseed ( a ) <nl> + whrandom . whrandom . seed ( self , x , y , z ) <nl> + <nl> + def new_generator ( a = None ) : <nl> + \" \" \" Return a new random generator instance . \" \" \" <nl> + return generator ( a ) <nl> + <nl> # Housekeeping function to verify that magic constants have been <nl> # computed correctly <nl> <nl>\n", "msg": "Add Interfaces to replace remaining needs for importing whrandom .\n"}
{"diff_id": 39949, "repo": "zulip/zulip\n", "sha": "399f4f8870c803243d84537aed75fc512f34151d\n", "time": "2016-08-19T03:12:36Z\n", "diff": "new file mode 100644 <nl> index 000000000000 . . 58cddbd3b99d <nl> mmm / dev / null <nl> ppp b / zerver / management / commands / send_password_reset_email . py <nl> <nl> + from __future__ import absolute_import <nl> + <nl> + import logging <nl> + from typing import Any <nl> + <nl> + from argparse import ArgumentParser <nl> + from django . core . management . base import BaseCommand , CommandError <nl> + from django . conf import settings <nl> + from django . core . mail import send_mail , BadHeaderError <nl> + from zerver . forms import PasswordResetForm <nl> + from zerver . models import UserProfile , get_user_profile_by_email , get_realm <nl> + from django . template import loader <nl> + from django . core . mail import EmailMultiAlternatives <nl> + <nl> + from django . utils . http import urlsafe_base64_encode <nl> + from django . utils . encoding import force_bytes <nl> + <nl> + <nl> + from django . contrib . auth . tokens import default_token_generator <nl> + <nl> + class Command ( BaseCommand ) : <nl> + help = \" \" \" Send email to specified email address . \" \" \" <nl> + <nl> + def add_arguments ( self , parser ) : <nl> + # type : ( ArgumentParser ) - > None <nl> + parser . add_argument ( ' - - to ' , metavar = ' < to > ' , type = str , <nl> + help = \" email of user to send the email \" ) <nl> + parser . add_argument ( ' - - realm ' , metavar = ' < realm > ' , type = str , <nl> + help = \" realm to send the email to all users in \" ) <nl> + parser . add_argument ( ' - - server ' , metavar = ' < server > ' , type = str , <nl> + help = \" If you specify ' YES ' will send to everyone on server \" ) <nl> + <nl> + def handle ( self , * args , * * options ) : <nl> + # type : ( * Any , * * str ) - > None <nl> + if options [ \" to \" ] : <nl> + users = [ get_user_profile_by_email ( options [ \" to \" ] ) ] <nl> + elif options [ \" realm \" ] : <nl> + realm = get_realm ( options [ \" realm \" ] ) <nl> + users = UserProfile . objects . filter ( realm = realm , is_active = True , is_bot = False , <nl> + is_mirror_dummy = False ) <nl> + elif options [ \" server \" ] = = \" YES \" : <nl> + users = UserProfile . objects . filter ( is_active = True , is_bot = False , <nl> + is_mirror_dummy = False ) <nl> + else : <nl> + raise RuntimeError ( \" Missing arguments \" ) <nl> + self . send ( users ) <nl> + <nl> + def send ( self , users , domain_override = None , <nl> + subject_template_name = ' registration / password_reset_subject . txt ' , <nl> + email_template_name = ' registration / password_reset_email . txt ' , <nl> + use_https = True , token_generator = default_token_generator , <nl> + from_email = None , request = None , html_email_template_name = None ) : <nl> + \" \" \" Sends one - use only links for resetting password to target users <nl> + <nl> + \" \" \" <nl> + for user_profile in users : <nl> + context = { <nl> + ' email ' : user_profile . email , <nl> + ' domain ' : user_profile . realm . host , <nl> + ' site_name ' : \" zulipo \" , <nl> + ' uid ' : urlsafe_base64_encode ( force_bytes ( user_profile . pk ) ) , <nl> + ' user ' : user_profile , <nl> + ' token ' : token_generator . make_token ( user_profile ) , <nl> + ' protocol ' : ' https ' if use_https else ' http ' , <nl> + } <nl> + <nl> + logging . warning ( \" Sending % s email to % s \" % ( email_template_name , user_profile . email , ) ) <nl> + self . send_mail ( subject_template_name , email_template_name , <nl> + context , from_email , user_profile . email , <nl> + html_email_template_name = html_email_template_name ) <nl> + <nl> + def send_mail ( self , subject_template_name , email_template_name , <nl> + context , from_email , to_email , html_email_template_name = None ) : <nl> + \" \" \" <nl> + Sends a django . core . mail . EmailMultiAlternatives to ` to_email ` . <nl> + \" \" \" <nl> + subject = loader . render_to_string ( subject_template_name , context ) <nl> + # Email subject * must not * contain newlines <nl> + subject = ' ' . join ( subject . splitlines ( ) ) <nl> + body = loader . render_to_string ( email_template_name , context ) <nl> + <nl> + email_message = EmailMultiAlternatives ( subject , body , from_email , [ to_email ] ) <nl> + if html_email_template_name is not None : <nl> + html_email = loader . render_to_string ( html_email_template_name , context ) <nl> + email_message . attach_alternative ( html_email , ' text / html ' ) <nl> + <nl> + email_message . send ( ) <nl>\n", "msg": "Add management command to send password reset emails .\n"}
{"diff_id": 39973, "repo": "ytdl-org/youtube-dl\n", "sha": "a9bbd26f1d2bb45205f9fbd2626569522049e40e\n", "time": "2016-01-10T04:49:27Z\n", "diff": "mmm a / youtube_dl / extractor / bigflix . py <nl> ppp b / youtube_dl / extractor / bigflix . py <nl> class BigflixIE ( InfoExtractor ) : <nl> ' description ' : ' md5 : 3d2ba5815f14911d5cc6a501ae0cf65d ' , <nl> } <nl> } , { <nl> - # multiple formats <nl> + # 2 formats <nl> ' url ' : ' http : / / www . bigflix . com / Tamil - movies / Drama - movies / Madarasapatinam / 16070 ' , <nl> ' info_dict ' : { <nl> ' id ' : ' 16070 ' , <nl> class BigflixIE ( InfoExtractor ) : <nl> ' params ' : { <nl> ' skip_download ' : True , <nl> } <nl> + } , { <nl> + # multiple formats <nl> + ' url ' : ' http : / / www . bigflix . com / Malayalam - movies / Drama - movies / Indian - Rupee / 15967 ' , <nl> + ' only_matching ' : True , <nl> } ] <nl> <nl> def _real_extract ( self , url ) : <nl> def _real_extract ( self , url ) : <nl> <nl> def decode_url ( quoted_b64_url ) : <nl> return base64 . b64decode ( compat_urllib_parse_unquote ( <nl> - quoted_b64_url ) ) . encode ( ' ascii ' ) . decode ( ' utf - 8 ' ) <nl> + quoted_b64_url ) . encode ( ' ascii ' ) ) . decode ( ' utf - 8 ' ) <nl> + <nl> + formats = [ ] <nl> + for height , encoded_url in re . findall ( <nl> + r ' ContentURL_ ( \\ d { 3 , 4 } ) [ pP ] [ ^ = ] + = ( [ ^ & ] + ) ' , webpage ) : <nl> + video_url = decode_url ( encoded_url ) <nl> + f = { <nl> + ' url ' : video_url , <nl> + ' format_id ' : ' % sp ' % height , <nl> + ' height ' : int ( height ) , <nl> + } <nl> + if video_url . startswith ( ' rtmp ' ) : <nl> + f [ ' ext ' ] = ' flv ' <nl> + formats . append ( f ) <nl> <nl> - formats = [ { <nl> - ' url ' : decode_url ( encoded_url ) , <nl> - ' format_id ' : ' % sp ' % height , <nl> - ' height ' : int ( height ) , <nl> - } for height , encoded_url in re . findall ( <nl> - r ' ContentURL_ ( \\ d { 3 , 4 } ) [ pP ] [ ^ = ] + = ( [ ^ & ] + ) ' , webpage ) ] <nl> + file_url = self . _search_regex ( <nl> + r ' file = ( [ ^ & ] + ) ' , webpage , ' video url ' , default = None ) <nl> + if file_url : <nl> + video_url = decode_url ( file_url ) <nl> + if all ( f [ ' url ' ] ! = video_url for f in formats ) : <nl> + formats . append ( { <nl> + ' url ' : decode_url ( file_url ) , <nl> + } ) <nl> <nl> - if not formats : <nl> - formats . append ( { <nl> - ' url ' : decode_url ( self . _search_regex ( <nl> - r ' file = ( [ ^ & ] + ) ' , webpage , ' video url ' ) ) , <nl> - } ) <nl> + self . _sort_formats ( formats ) <nl> <nl> description = self . _html_search_meta ( ' description ' , webpage ) <nl> <nl>\n", "msg": "[ bigflix ] Improve formats extraction\n"}
{"diff_id": 40038, "repo": "ansible/ansible\n", "sha": "4f42e752e65d7ab2e4fbd1255ce2290753649c81\n", "time": "2016-12-08T16:23:03Z\n", "diff": "mmm a / lib / ansible / modules / database / mysql / mysql_user . py <nl> ppp b / lib / ansible / modules / database / mysql / mysql_user . py <nl> class InvalidPrivsError ( Exception ) : <nl> # MySQL module specific support methods . <nl> # <nl> <nl> - def connect ( module , login_user , login_password , config_file = ' ~ / . my . cnf ' ) : <nl> + def connect ( module , login_user = None , login_password = None , config_file = ' ~ / . my . cnf ' ) : <nl> config = { <nl> ' host ' : module . params [ ' login_host ' ] , <nl> ' db ' : ' mysql ' <nl>\n", "msg": "Allow playbook specified login_user and login_password to override config file settings\n"}
{"diff_id": 40202, "repo": "ansible/ansible\n", "sha": "4e7b67a45a3f46d0e770a547d51bc65f850d8096\n", "time": "2012-08-30T00:08:45Z\n", "diff": "mmm a / lib / ansible / module_common . py <nl> ppp b / lib / ansible / module_common . py <nl> def _log_invocation ( self ) : <nl> log_args = re . sub ( r ' login_password = . + ( . * ) ' , r \" login_password = NOT_LOGGING_PASSWORD \\ 1 \" , log_args ) <nl> syslog . syslog ( syslog . LOG_NOTICE , ' Invoked with % s ' % log_args ) <nl> <nl> - def get_bin_path ( self , arg ) : <nl> + def get_bin_path ( self , arg , opt_dirs = [ ] ) : <nl> ' ' ' <nl> find system executable in PATH . <nl> if found return full path ; otherwise return None <nl> ' ' ' <nl> sbin_paths = [ ' / sbin ' , ' / usr / sbin ' , ' / usr / local / sbin ' ] <nl> - paths = os . environ . get ( ' PATH ' ) . split ( ' : ' ) <nl> + paths = [ ] <nl> + for d in opt_dirs : <nl> + if d is not None and os . path . exists ( d ) : <nl> + paths . append ( d ) <nl> + paths + = os . environ . get ( ' PATH ' ) . split ( ' : ' ) <nl> bin_path = None <nl> # mangle PATH to include / sbin dirs <nl> for p in sbin_paths : <nl> if p not in paths and os . path . exists ( p ) : <nl> paths . append ( p ) <nl> for d in paths : <nl> - path = ' % s / % s ' % ( d , arg ) <nl> + path = os . path . join ( d , arg ) <nl> if os . path . exists ( path ) and os . access ( path , os . X_OK ) : <nl> bin_path = path <nl> break <nl>\n", "msg": "Add option to pass list of dirs to get_bin_path in module_common . py\n"}
{"diff_id": 40387, "repo": "python/cpython\n", "sha": "8c0e0ab767f0d6f8395d8c317b08977563b70d41\n", "time": "2014-09-25T03:21:39Z\n", "diff": "mmm a / Lib / asyncio / unix_events . py <nl> ppp b / Lib / asyncio / unix_events . py <nl> <nl> \" \" \" Selector event loop for Unix with signal handling . \" \" \" <nl> <nl> import errno <nl> - import fcntl <nl> import os <nl> import signal <nl> import socket <nl> def create_unix_server ( self , protocol_factory , path = None , * , <nl> def _set_nonblocking ( fd ) : <nl> os . set_blocking ( fd , False ) <nl> else : <nl> + import fcntl <nl> + <nl> def _set_nonblocking ( fd ) : <nl> flags = fcntl . fcntl ( fd , fcntl . F_GETFL ) <nl> flags = flags | os . O_NONBLOCK <nl>\n", "msg": "asyncio . unix_events : Move import statement to match tulip code\n"}
{"diff_id": 40516, "repo": "ansible/ansible\n", "sha": "c648c95eb74fceb4478d674242603f6db037796d\n", "time": "2016-05-16T18:39:49Z\n", "diff": "mmm a / lib / ansible / inventory / yaml . py <nl> ppp b / lib / ansible / inventory / yaml . py <nl> <nl> from ansible . inventory . expand_hosts import detect_range <nl> from ansible . inventory . expand_hosts import expand_hostname_range <nl> from ansible . parsing . utils . addresses import parse_address <nl> + from ansible . compat . six import string_types <nl> <nl> class InventoryParser ( object ) : <nl> \" \" \" <nl> def _parse_groups ( self , group , group_data ) : <nl> self . groups [ group ] = Group ( name = group ) <nl> <nl> if isinstance ( group_data , dict ) : <nl> + # make sure they are dicts <nl> + for section in [ ' vars ' , ' children ' , ' hosts ' ] : <nl> + if section in group_data and isinstance ( group_data [ section ] , string_types ) : <nl> + group_data [ section ] = { group_data [ section ] : None } <nl> + <nl> if ' vars ' in group_data : <nl> for var in group_data [ ' vars ' ] : <nl> if var ! = ' ansible_group_priority ' : <nl>\n", "msg": "made format more flexible and allow for non dict entries\n"}
{"diff_id": 40546, "repo": "numpy/numpy\n", "sha": "3757ca12afa8b89e92e16b6814eb1888611b4db0\n", "time": "2007-05-11T08:20:41Z\n", "diff": "mmm a / numpy / core / setup . py <nl> ppp b / numpy / core / setup . py <nl> def generate_config_h ( ext , build_dir ) : <nl> tc = generate_testcode ( target ) <nl> from distutils import sysconfig <nl> python_include = sysconfig . get_python_inc ( ) <nl> + python_h = join ( python_include , ' Python . h ' ) <nl> + if not os . path . isfile ( python_h ) : <nl> + raise SystemError , \\ <nl> + \" Non - existing % s . Perhaps you need to install \" \\ <nl> + \" python - dev | python - devel . \" % ( python_h ) <nl> result = config_cmd . try_run ( tc , include_dirs = [ python_include ] , <nl> library_dirs = default_lib_dirs ) <nl> if not result : <nl> - raise \" ERROR : Failed to test configuration \" <nl> + raise SystemError , \" Failed to test configuration . \" \\ <nl> + \" See previous error messages for more information . \" <nl> <nl> # Python 2 . 3 causes a segfault when <nl> # trying to re - acquire the thread - state <nl>\n", "msg": "Improved error message for missing Python . h .\n"}
{"diff_id": 40621, "repo": "ansible/ansible\n", "sha": "0ebf4b2d5aea04c072138abc9d1c9ed724dcd379\n", "time": "2012-11-20T14:39:48Z\n", "diff": "mmm a / lib / ansible / runner / action_plugins / fetch . py <nl> ppp b / lib / ansible / runner / action_plugins / fetch . py <nl> def run ( self , conn , tmp , module_name , module_args , inject ) : <nl> f . close ( ) <nl> new_md5 = utils . md5 ( dest ) <nl> if new_md5 ! = remote_md5 : <nl> - result = dict ( failed = True , md5sum = new_md5 , msg = \" md5 mismatch \" , file = source ) <nl> + result = dict ( failed = True , md5sum = new_md5 , msg = \" md5 mismatch \" , file = source , dest = dest ) <nl> return ReturnData ( conn = conn , result = result ) <nl> - result = dict ( changed = True , md5sum = new_md5 ) <nl> + result = dict ( changed = True , md5sum = new_md5 , dest = dest ) <nl> return ReturnData ( conn = conn , result = result ) <nl> else : <nl> - result = dict ( changed = False , md5sum = local_md5 , file = source ) <nl> + result = dict ( changed = False , md5sum = local_md5 , file = source , dest = dest ) <nl> return ReturnData ( conn = conn , result = result ) <nl> <nl>\n", "msg": "Add destination path to fetch result\n"}
{"diff_id": 40630, "repo": "quantopian/zipline\n", "sha": "45e28a4b9d10652596cbeaeb2115b1b2ad0b868d\n", "time": "2013-03-26T03:51:20Z\n", "diff": "mmm a / zipline / gens / utils . py <nl> ppp b / zipline / gens / utils . py <nl> <nl> # <nl> - # Copyright 2012 Quantopian , Inc . <nl> + # Copyright 2013 Quantopian , Inc . <nl> # <nl> # Licensed under the Apache License , Version 2 . 0 ( the \" License \" ) ; <nl> # you may not use this file except in compliance with the License . <nl>\n", "msg": "MAINT : Updates copyright year on gens . utils module .\n"}
{"diff_id": 40631, "repo": "ytdl-org/youtube-dl\n", "sha": "3c4fc580bbf7a37b4ed0f244010c248840d86afe\n", "time": "2012-06-06T13:24:12Z\n", "diff": "mmm a / youtube_dl / utils . py <nl> ppp b / youtube_dl / utils . py <nl> <nl> import StringIO <nl> <nl> std_headers = { <nl> - ' User - Agent ' : ' Mozilla / 5 . 0 ( X11 ; Linux x86_64 ; rv : 5 . 0 . 1 ) Gecko / 20100101 Firefox / 5 . 0 . 1 ' , <nl> + ' User - Agent ' : ' iTunes / 10 . 6 . 1 ' , <nl> ' Accept - Charset ' : ' ISO - 8859 - 1 , utf - 8 ; q = 0 . 7 , * ; q = 0 . 7 ' , <nl> ' Accept ' : ' text / html , application / xhtml + xml , application / xml ; q = 0 . 9 , * / * ; q = 0 . 8 ' , <nl> ' Accept - Encoding ' : ' gzip , deflate ' , <nl>\n", "msg": "Use an User - Agent that will allow downloading from blip . tv fixes\n"}
{"diff_id": 40928, "repo": "pypa/pipenv\n", "sha": "105a2fc0e84ea69b598f1a8ddf6e4f8e694410ed\n", "time": "2017-10-25T06:45:19Z\n", "diff": "mmm a / pipenv / cli . py <nl> ppp b / pipenv / cli . py <nl> def uninstall ( <nl> @ click . option ( ' - - python ' , default = False , nargs = 1 , help = \" Specify which version of Python virtualenv should use . \" ) <nl> @ click . option ( ' - - verbose ' , is_flag = True , default = False , help = \" Verbose mode . \" ) <nl> @ click . option ( ' - - requirements ' , ' - r ' , is_flag = True , default = False , help = \" Generate output compatible with requirements . txt . \" ) <nl> + @ click . option ( ' - - dev ' , ' - d ' , is_flag = True , default = False , help = \" Generate output compatible with requirements . txt for the development dependencies . \" ) <nl> @ click . option ( ' - - clear ' , is_flag = True , default = False , help = \" Clear the dependency cache . \" ) <nl> @ click . option ( ' - - pre ' , is_flag = True , default = False , help = u \" Allow pre \u2013 releases . \" ) <nl> - def lock ( three = None , python = False , verbose = False , requirements = False , clear = False , pre = False ) : <nl> + def lock ( three = None , python = False , verbose = False , requirements = False , dev = False , clear = False , pre = False ) : <nl> <nl> # Ensure that virtualenv is available . <nl> ensure_project ( three = three , python = python ) <nl> def lock ( three = None , python = False , verbose = False , requirements = False , clear = Fals <nl> pre = project . settings . get ( ' pre ' ) <nl> <nl> if requirements : <nl> - do_init ( dev = True , requirements = requirements ) <nl> + do_init ( dev = dev , requirements = requirements ) <nl> <nl> do_lock ( verbose = verbose , clear = clear , pre = pre ) <nl> <nl>\n", "msg": "Added dev option to lock - r to generate requirements with and without development requirements\n"}
{"diff_id": 40941, "repo": "home-assistant/core\n", "sha": "0aac4d64e1d8336ebe71fe108245409646a82c75\n", "time": "2016-12-09T07:26:02Z\n", "diff": "mmm a / homeassistant / components / climate / radiotherm . py <nl> ppp b / homeassistant / components / climate / radiotherm . py <nl> <nl> ATTR_MODE = ' mode ' <nl> <nl> CONF_HOLD_TEMP = ' hold_temp ' <nl> + CONF_AWAY_TEMPERATURE_HEAT = ' away_temperature_heat ' <nl> + CONF_AWAY_TEMPERATURE_COOL = ' away_temperature_cool ' <nl> + <nl> + DEFAULT_AWAY_TEMPERATURE_HEAT = 60 <nl> + DEFAULT_AWAY_TEMPERATURE_COOL = 85 <nl> <nl> PLATFORM_SCHEMA = PLATFORM_SCHEMA . extend ( { <nl> vol . Optional ( CONF_HOST ) : vol . All ( cv . ensure_list , [ cv . string ] ) , <nl> vol . Optional ( CONF_HOLD_TEMP , default = False ) : cv . boolean , <nl> + vol . Optional ( CONF_AWAY_TEMPERATURE_HEAT , <nl> + default = DEFAULT_AWAY_TEMPERATURE_HEAT ) : vol . Coerce ( float ) , <nl> + vol . Optional ( CONF_AWAY_TEMPERATURE_COOL , <nl> + default = DEFAULT_AWAY_TEMPERATURE_COOL ) : vol . Coerce ( float ) , <nl> } ) <nl> <nl> <nl> def setup_platform ( hass , config , add_devices , discovery_info = None ) : <nl> return False <nl> <nl> hold_temp = config . get ( CONF_HOLD_TEMP ) <nl> + away_temps = [ <nl> + config . get ( CONF_AWAY_TEMPERATURE_HEAT ) , <nl> + config . get ( CONF_AWAY_TEMPERATURE_COOL ) <nl> + ] <nl> tstats = [ ] <nl> <nl> for host in hosts : <nl> try : <nl> tstat = radiotherm . get_thermostat ( host ) <nl> - tstats . append ( RadioThermostat ( tstat , hold_temp ) ) <nl> + tstats . append ( RadioThermostat ( tstat , hold_temp , away_temps ) ) <nl> except OSError : <nl> _LOGGER . exception ( \" Unable to connect to Radio Thermostat : % s \" , <nl> host ) <nl> def setup_platform ( hass , config , add_devices , discovery_info = None ) : <nl> class RadioThermostat ( ClimateDevice ) : <nl> \" \" \" Representation of a Radio Thermostat . \" \" \" <nl> <nl> - def __init__ ( self , device , hold_temp ) : <nl> + def __init__ ( self , device , hold_temp , away_temps ) : <nl> \" \" \" Initialize the thermostat . \" \" \" <nl> self . device = device <nl> self . set_time ( ) <nl> def __init__ ( self , device , hold_temp ) : <nl> self . _name = None <nl> self . _fmode = None <nl> self . _tmode = None <nl> - self . hold_temp = hold_temp <nl> + self . _hold_temp = hold_temp <nl> + self . _away = False <nl> + self . _away_temps = away_temps <nl> + self . _prev_temp = None <nl> self . update ( ) <nl> self . _operation_list = [ STATE_AUTO , STATE_COOL , STATE_HEAT , STATE_OFF ] <nl> <nl> def target_temperature ( self ) : <nl> \" \" \" Return the temperature we try to reach . \" \" \" <nl> return self . _target_temperature <nl> <nl> + @ property <nl> + def is_away_mode_on ( self ) : <nl> + \" \" \" Return true if away mode is on . \" \" \" <nl> + return self . _away <nl> + <nl> def update ( self ) : <nl> \" \" \" Update the data from the thermostat . \" \" \" <nl> self . _current_temperature = self . device . temp [ ' raw ' ] <nl> def set_temperature ( self , * * kwargs ) : <nl> self . device . t_cool = round ( temperature * 2 . 0 ) / 2 . 0 <nl> elif self . _current_operation = = STATE_HEAT : <nl> self . device . t_heat = round ( temperature * 2 . 0 ) / 2 . 0 <nl> - if self . hold_temp : <nl> + if self . _hold_temp or self . _away : <nl> self . device . hold = 1 <nl> else : <nl> self . device . hold = 0 <nl> def set_operation_mode ( self , operation_mode ) : <nl> self . device . t_cool = round ( self . _target_temperature * 2 . 0 ) / 2 . 0 <nl> elif operation_mode = = STATE_HEAT : <nl> self . device . t_heat = round ( self . _target_temperature * 2 . 0 ) / 2 . 0 <nl> + <nl> + def turn_away_mode_on ( self ) : <nl> + \" \" \" Turn away on . <nl> + <nl> + The RTCOA app simulates away mode by using a hold . <nl> + \" \" \" <nl> + away_temp = None <nl> + if not self . _away : <nl> + self . _prev_temp = self . _target_temperature <nl> + if self . _current_operation = = STATE_HEAT : <nl> + away_temp = self . _away_temps [ 0 ] <nl> + elif self . _current_operation = = STATE_COOL : <nl> + away_temp = self . _away_temps [ 1 ] <nl> + self . _away = True <nl> + self . set_temperature ( temperature = away_temp ) <nl> + <nl> + def turn_away_mode_off ( self ) : <nl> + \" \" \" Turn away off . \" \" \" <nl> + self . _away = False <nl> + self . set_temperature ( temperature = self . _prev_temp ) <nl>\n", "msg": "Add away mode for Radio Thermostat / 3M Filtrete ( )\n"}
{"diff_id": 40994, "repo": "ansible/ansible\n", "sha": "9b96da9aa16dc2abc8b2d999df8b49519a9aa715\n", "time": "2019-06-27T15:40:20Z\n", "diff": "mmm a / lib / ansible / modules / monitoring / zabbix / zabbix_host . py <nl> ppp b / lib / ansible / modules / monitoring / zabbix / zabbix_host . py <nl> <nl> host_name : <nl> description : <nl> - Name of the host in Zabbix . <nl> - - host_name is the unique identifier used and cannot be updated using this module . <nl> + - I ( host_name ) is the unique identifier used and cannot be updated using this module . <nl> required : true <nl> visible_name : <nl> description : <nl> <nl> description : <nl> - Add Facts for a zabbix inventory ( e . g . Tag ) ( see example below ) . <nl> - Please review the interface documentation for more information on the supported properties <nl> - - ' https : / / www . zabbix . com / documentation / 3 . 2 / manual / api / reference / host / object # host_inventory ' <nl> + - U ( https : / / www . zabbix . com / documentation / 3 . 2 / manual / api / reference / host / object # host_inventory ) <nl> version_added : ' 2 . 5 ' <nl> status : <nl> description : <nl> <nl> description : <nl> - The name of the Zabbix proxy to be used . <nl> interfaces : <nl> + type : list <nl> description : <nl> - List of interfaces to be created for the host ( see example below ) . <nl> - - ' Available keys are : I ( dns ) , I ( ip ) , I ( main ) , I ( port ) , I ( type ) , I ( useip ) , and I ( bulk ) . ' <nl> - - Please review the interface documentation for more information on the supported properties <nl> - - ' https : / / www . zabbix . com / documentation / 2 . 0 / manual / appendix / api / hostinterface / definitions # host_interface ' <nl> - - If an interface definition is incomplete , this module will attempt to fill in sensible values . <nl> - - I ( type ) can also be C ( agent ) , C ( snmp ) , C ( ipmi ) , or C ( jmx ) instead of its numerical value . <nl> + - For more information , review host interface documentation at <nl> + - U ( https : / / www . zabbix . com / documentation / 4 . 0 / manual / api / reference / hostinterface / object ) <nl> + suboptions : <nl> + type : <nl> + description : <nl> + - Interface type to add <nl> + - Numerical values are also accepted for interface type <nl> + - 1 = agent <nl> + - 2 = snmp <nl> + - 3 = ipmi <nl> + - 4 = jmx <nl> + choices : [ ' agent ' , ' snmp ' , ' ipmi ' , ' jmx ' ] <nl> + required : true <nl> + main : <nl> + type : int <nl> + description : <nl> + - Whether the interface is used as default . <nl> + - If multiple interfaces with the same type are provided , only one can be default . <nl> + - 0 ( not default ) , 1 ( default ) <nl> + default : 0 <nl> + choices : [ 0 , 1 ] <nl> + useip : <nl> + type : int <nl> + description : <nl> + - Connect to host interface with IP address instead of DNS name . <nl> + - 0 ( don ' t use ip ) , 1 ( use ip ) <nl> + default : 0 <nl> + choices : [ 0 , 1 ] <nl> + ip : <nl> + type : str <nl> + description : <nl> + - IP address used by host interface . <nl> + - Required if I ( useip = 1 ) . <nl> + default : ' ' <nl> + dns : <nl> + type : str <nl> + description : <nl> + - DNS name of the host interface . <nl> + - Required if I ( useip = 0 ) . <nl> + default : ' ' <nl> + port : <nl> + type : str <nl> + description : <nl> + - Port used by host interface . <nl> + - If not specified , default port for each type of interface is used <nl> + - 10050 if I ( type = ' agent ' ) <nl> + - 161 if I ( type = ' snmp ' ) <nl> + - 623 if I ( type = ' ipmi ' ) <nl> + - 12345 if I ( type = ' jmx ' ) <nl> + bulk : <nl> + type : int <nl> + description : <nl> + - Whether to use bulk SNMP requests . <nl> + - 0 ( don ' t use bulk requests ) , 1 ( use bulk requests ) <nl> + choices : [ 0 , 1 ] <nl> + default : 1 <nl> default : [ ] <nl> tls_connect : <nl> description : <nl> <nl> tls_psk : <nl> description : <nl> - PSK value is a hard to guess string of hexadecimal digits . <nl> - - The preshared key , at least 32 hex digits . Required if either tls_connect or tls_accept has PSK enabled . <nl> + - The preshared key , at least 32 hex digits . Required if either I ( tls_connect ) or I ( tls_accept ) has PSK enabled . <nl> - Works only with > = Zabbix 3 . 0 <nl> version_added : ' 2 . 5 ' <nl> ca_cert : <nl>\n", "msg": "zabbix_host - added documentation for interfaces sub options ( )\n"}
{"diff_id": 41099, "repo": "tornadoweb/tornado\n", "sha": "b6ea32d35d3a2b6692aa855e9679aefab4ecc2d5\n", "time": "2012-05-24T17:23:34Z\n", "diff": "mmm a / tornado / web . py <nl> ppp b / tornado / web . py <nl> def __init__ ( self , application , request , * * kwargs ) : <nl> self . ui [ \" modules \" ] = self . ui [ \" _modules \" ] <nl> self . clear ( ) <nl> # Check since connection is not available in WSGI <nl> - if hasattr ( self . request , \" connection \" ) : <nl> + if getattr ( self . request , \" connection \" , None ) : <nl> self . request . connection . stream . set_close_callback ( <nl> self . on_connection_close ) <nl> self . initialize ( * * kwargs ) <nl>\n", "msg": "Better connection check in RequestHandler\n"}
{"diff_id": 41144, "repo": "zulip/zulip\n", "sha": "4e8054b84aeaaa4463ae46286c2ddc46d1e466ce\n", "time": "2016-08-04T22:57:03Z\n", "diff": "mmm a / zerver / tests / tests . py <nl> ppp b / zerver / tests / tests . py <nl> <nl> from __future__ import absolute_import <nl> from __future__ import print_function <nl> <nl> - from typing import Any , Callable , Dict , Iterable , List , Tuple , TypeVar <nl> + from typing import Any , Callable , Dict , Iterable , List , Mapping , Tuple , TypeVar <nl> from mock import patch , MagicMock <nl> <nl> from django . http import HttpResponse <nl> def test_error_handling ( self ) : <nl> @ queue_processors . assign_queue ( ' unreliable_worker ' ) <nl> class UnreliableWorker ( queue_processors . QueueProcessingWorker ) : <nl> def consume ( self , data ) : <nl> - # type : ( str ) - > None <nl> - if data = = ' unexpected behaviour ' : <nl> + # type : ( Mapping [ str , Any ] ) - > None <nl> + if data [ \" type \" ] = = ' unexpected behaviour ' : <nl> raise Exception ( ' Worker task not performing as expected ! ' ) <nl> - processed . append ( data ) <nl> + processed . append ( data [ \" type \" ] ) <nl> <nl> def _log_problem ( self ) : <nl> # type : ( ) - > None <nl> def _log_problem ( self ) : <nl> <nl> fake_client = self . FakeClient ( ) <nl> for msg in [ ' good ' , ' fine ' , ' unexpected behaviour ' , ' back to normal ' ] : <nl> - fake_client . queue . append ( ( ' unreliable_worker ' , msg ) ) <nl> + fake_client . queue . append ( ( ' unreliable_worker ' , { ' type ' : msg } ) ) <nl> <nl> fn = os . path . join ( settings . QUEUE_ERROR_DIR , ' unreliable_worker . errors ' ) <nl> try : <nl> def _log_problem ( self ) : <nl> self . assertEqual ( processed , [ ' good ' , ' fine ' , ' back to normal ' ] ) <nl> line = open ( fn ) . readline ( ) . strip ( ) <nl> event = ujson . loads ( line . split ( ' \\ t ' ) [ 1 ] ) <nl> - self . assertEqual ( event , ' unexpected behaviour ' ) <nl> + self . assertEqual ( event [ \" type \" ] , ' unexpected behaviour ' ) <nl> <nl> def test_worker_noname ( self ) : <nl> # type : ( ) - > None <nl> def __init__ ( self ) : <nl> # type : ( ) - > None <nl> super ( TestWorker , self ) . __init__ ( ) <nl> def consume ( self , data ) : <nl> - # type : ( str ) - > None <nl> + # type : ( Mapping [ str , Any ] ) - > None <nl> pass <nl> with self . assertRaises ( queue_processors . WorkerDeclarationException ) : <nl> TestWorker ( ) <nl>\n", "msg": "Refactor queue worker tests to match actual API .\n"}
{"diff_id": 41165, "repo": "celery/celery\n", "sha": "fd9ea7b50ff22cf34425156c800cd246b45a4fad\n", "time": "2016-12-25T04:44:21Z\n", "diff": "mmm a / celery / bin / beat . py <nl> ppp b / celery / bin / beat . py <nl> <nl> <nl> . . cmdoption : : - - pidfile <nl> <nl> - Optional file used to store the process pid . <nl> + File used to store the process pid . Defaults to ` celerybeat . pid ` . <nl> <nl> The program won ' t start if this file already exists <nl> and the pid is still alive . <nl>\n", "msg": "Specify default value for pidfile option of celery beat . ( )\n"}
{"diff_id": 41186, "repo": "celery/celery\n", "sha": "a7c3c04b2d430e0e8ca35eba551097d91bab6f8c\n", "time": "2010-12-07T11:56:52Z\n", "diff": "mmm a / celery / task / base . py <nl> ppp b / celery / task / base . py <nl> <nl> TaskSet = sets . TaskSet <nl> subtask = sets . subtask <nl> <nl> - PERIODIC_DEPRECATION_TEXT = \" \" \" \\ <nl> - Periodic task classes has been deprecated and will be removed <nl> - in celery v3 . 0 . <nl> - <nl> - Please use the CELERYBEAT_SCHEDULE setting instead : <nl> - <nl> - CELERYBEAT_SCHEDULE = { <nl> - name : dict ( task = task_name , schedule = run_every , <nl> - args = ( ) , kwargs = { } , options = { } , relative = False ) <nl> - } <nl> - <nl> - \" \" \" <nl> extract_exec_options = mattrgetter ( \" queue \" , \" routing_key \" , <nl> \" exchange \" , \" immediate \" , <nl> \" mandatory \" , \" priority \" , <nl> class PeriodicTask ( Task ) : <nl> ignore_result = True <nl> type = \" periodic \" <nl> relative = False <nl> + options = None <nl> <nl> def __init__ ( self ) : <nl> app = app_or_default ( ) <nl> def __init__ ( self ) : <nl> \" Periodic tasks must have a run_every attribute \" ) <nl> self . run_every = maybe_schedule ( self . run_every , self . relative ) <nl> <nl> - # Periodic task classes is pending deprecation . <nl> - warnings . warn ( PendingDeprecationWarning ( PERIODIC_DEPRECATION_TEXT ) ) <nl> - <nl> # For backward compatibility , add the periodic task to the <nl> # configuration schedule instead . <nl> app . conf . CELERYBEAT_SCHEDULE [ self . name ] = { <nl> def __init__ ( self ) : <nl> \" schedule \" : self . run_every , <nl> \" args \" : ( ) , <nl> \" kwargs \" : { } , <nl> - \" options \" : { } , <nl> + \" options \" : self . options or { } , <nl> \" relative \" : self . relative , <nl> } <nl> <nl>\n", "msg": "PeriodicTask / periodic_task is * not * pending deprecation , also add options argument to periodic_task to set execution options ( like expires )\n"}
{"diff_id": 41194, "repo": "zulip/zulip\n", "sha": "b577bd54cd0d31d0ee913e1843870912c11677b2\n", "time": "2012-12-05T18:54:43Z\n", "diff": "mmm a / zephyr / lib / bugdown / __init__ . py <nl> ppp b / zephyr / lib / bugdown / __init__ . py <nl> def sanitize_url ( self , url ) : <nl> See the docstring on markdown . inlinepatterns . LinkPattern . sanitize_url . <nl> \" \" \" <nl> url = url . replace ( ' ' , ' % 20 ' ) <nl> - if not self . markdown . safeMode : <nl> - # Return immediately bipassing parsing . <nl> - return url <nl> <nl> try : <nl> scheme , netloc , path , params , query , fragment = url = urlparse . urlparse ( url ) <nl>\n", "msg": "bugdown : Remove code path to bypass sanitize_url\n"}
{"diff_id": 41269, "repo": "matplotlib/matplotlib\n", "sha": "04f13a4266b608a6d8bd2978e8f3b5ad636f276e\n", "time": "2014-07-18T16:23:14Z\n", "diff": "mmm a / setupext . py <nl> ppp b / setupext . py <nl> class Png ( SetupPackage ) : <nl> name = \" png \" <nl> <nl> def check ( self ) : <nl> + status , output = getstatusoutput ( \" libpng - config - - version \" ) <nl> + if status = = 0 : <nl> + version = output <nl> + else : <nl> + version = None <nl> + <nl> try : <nl> return self . _check_for_pkg_config ( <nl> ' libpng ' , ' png . h ' , <nl> - min_version = ' 1 . 2 ' ) <nl> + min_version = ' 1 . 2 ' , version = version ) <nl> except CheckFailed as e : <nl> self . __class__ . found_external = False <nl> return str ( e ) + ' Using unknown version . ' <nl> def get_extension ( self ) : <nl> ] <nl> ext = make_extension ( ' matplotlib . _png ' , sources ) <nl> pkg_config . setup_extension ( <nl> - ext , ' libpng ' , default_libraries = [ ' png ' , ' z ' ] ) <nl> + ext , ' libpng ' , default_libraries = [ ' png ' , ' z ' ] , <nl> + alt_exec = ' libpng - config - - ldflags ' ) <nl> Numpy ( ) . add_flags ( ext ) <nl> CXX ( ) . add_flags ( ext ) <nl> return ext <nl>\n", "msg": "Add ability to use libpng - config to get linker flags\n"}
{"diff_id": 41331, "repo": "ansible/ansible\n", "sha": "1ae799f3618d04affdbc08bc69f2dbfbeab75e8d\n", "time": "2016-12-08T16:23:07Z\n", "diff": "mmm a / lib / ansible / modules / web_infrastructure / supervisorctl . py <nl> ppp b / lib / ansible / modules / web_infrastructure / supervisorctl . py <nl> def take_action_on_processes ( processes , status_filter , action , expected_result ) : <nl> if module . check_mode : <nl> module . exit_json ( changed = True ) <nl> for process_name in to_take_action_on : <nl> - rc , out , err = run_supervisorctl ( action , process_name ) <nl> + rc , out , err = run_supervisorctl ( action , process_name , check_rc = True ) <nl> if ' % s : % s ' % ( process_name , expected_result ) not in out : <nl> module . fail_json ( msg = out ) <nl> <nl> module . exit_json ( changed = True , name = name , state = state , affected = to_take_action_on ) <nl> <nl> if state = = ' restarted ' : <nl> - rc , out , err = run_supervisorctl ( ' update ' ) <nl> + rc , out , err = run_supervisorctl ( ' update ' , check_rc = True ) <nl> processes = get_matched_processes ( ) <nl> take_action_on_processes ( processes , lambda s : True , ' restart ' , ' started ' ) <nl> <nl>\n", "msg": "Better error handling in supervisorctl module .\n"}
{"diff_id": 41490, "repo": "python/cpython\n", "sha": "af5bacf9bb9b9729fac3da50111441b1f0efdd44\n", "time": "2011-07-15T15:50:15Z\n", "diff": "mmm a / Lib / test / support . py <nl> ppp b / Lib / test / support . py <nl> def requires ( resource , msg = None ) : <nl> return <nl> if not is_resource_enabled ( resource ) : <nl> if msg is None : <nl> - msg = \" Use of the ` % s ' resource not enabled \" % resource <nl> + msg = \" Use of the % r resource not enabled \" % resource <nl> raise ResourceDenied ( msg ) <nl> <nl> def requires_linux_version ( * min_version ) : <nl> def check_valid_file ( fn ) : <nl> f = check_valid_file ( fn ) <nl> if f is not None : <nl> return f <nl> - raise TestFailed ( ' invalid resource \" % s \" ' % fn ) <nl> + raise TestFailed ( ' invalid resource % r ' % fn ) <nl> <nl> <nl> class WarningsRecorder ( object ) : <nl> def transient_internet ( resource_name , * , timeout = 30 . 0 , errnos = ( ) ) : <nl> ( ' WSANO_DATA ' , 11004 ) , <nl> ] <nl> <nl> - denied = ResourceDenied ( \" Resource ' % s ' is not available \" % resource_name ) <nl> + denied = ResourceDenied ( \" Resource % r is not available \" % resource_name ) <nl> captured_errnos = errnos <nl> gai_errnos = [ ] <nl> if not captured_errnos : <nl>\n", "msg": "Always use repr for regrtest resources names\n"}
{"diff_id": 41526, "repo": "zulip/zulip\n", "sha": "08d890e671ff456b7584827850724a7412dcdba6\n", "time": "2018-07-24T00:37:24Z\n", "diff": "mmm a / zerver / tests / test_events . py <nl> ppp b / zerver / tests / test_events . py <nl> def do_test_subscribe_events ( self , include_subscribers : bool ) - > None : <nl> ( ' is_announcement_only ' , check_bool ) , <nl> ( ' in_home_view ' , check_bool ) , <nl> ( ' name ' , check_string ) , <nl> + ( ' audible_notifications ' , check_bool ) , <nl> + ( ' email_notifications ' , check_bool ) , <nl> ( ' desktop_notifications ' , check_bool ) , <nl> ( ' push_notifications ' , check_bool ) , <nl> - ( ' audible_notifications ' , check_bool ) , <nl> ( ' stream_id ' , check_int ) , <nl> ( ' history_public_to_subscribers ' , check_bool ) , <nl> + ( ' pin_to_top ' , check_bool ) , <nl> + ( ' stream_weekly_traffic ' , check_none_or ( check_int ) ) , <nl> + ( ' is_old_stream ' , check_bool ) , <nl> ] <nl> if include_subscribers : <nl> subscription_fields . append ( ( ' subscribers ' , check_list ( check_int ) ) ) # type : ignore <nl> subscription_schema_checker = check_list ( <nl> - check_dict ( subscription_fields ) , # TODO : Can this be converted to check_dict_only ? <nl> + check_dict_only ( subscription_fields ) , <nl> ) <nl> stream_create_schema_checker = self . check_events_dict ( [ <nl> ( ' type ' , equals ( ' stream ' ) ) , <nl>\n", "msg": "test_events : Use check_dict_only for stream_weekly_traffic .\n"}
{"diff_id": 41615, "repo": "home-assistant/core\n", "sha": "56b38e64aeea12269b36d11849e0952377510c16\n", "time": "2015-12-17T06:53:10Z\n", "diff": "mmm a / homeassistant / components / automation / template . py <nl> ppp b / homeassistant / components / automation / template . py <nl> <nl> \" \" \" <nl> import logging <nl> <nl> - from homeassistant . const import CONF_VALUE_TEMPLATE <nl> + from homeassistant . const import CONF_VALUE_TEMPLATE , EVENT_STATE_CHANGED <nl> from homeassistant . exceptions import TemplateError <nl> - from homeassistant . helpers . event import track_state_change <nl> from homeassistant . util import template <nl> <nl> _LOGGER = logging . getLogger ( __name__ ) <nl> def trigger ( hass , config , action ) : <nl> _LOGGER . error ( \" Missing configuration key % s \" , CONF_VALUE_TEMPLATE ) <nl> return False <nl> <nl> - # Get all entity ids <nl> - all_entity_ids = hass . states . entity_ids ( ) <nl> - <nl> # Local variable to keep track of if the action has already been triggered <nl> already_triggered = False <nl> <nl> - def state_automation_listener ( entity , from_s , to_s ) : <nl> + def event_listener ( event ) : <nl> \" \" \" Listens for state changes and calls action . \" \" \" <nl> nonlocal already_triggered <nl> template_result = _check_template ( hass , value_template ) <nl> def state_automation_listener ( entity , from_s , to_s ) : <nl> elif not template_result : <nl> already_triggered = False <nl> <nl> - track_state_change ( hass , all_entity_ids , state_automation_listener ) <nl> - <nl> + hass . bus . listen ( EVENT_STATE_CHANGED , event_listener ) <nl> return True <nl> <nl> <nl>\n", "msg": "Change method of listening to state changes\n"}
{"diff_id": 41664, "repo": "ipython/ipython\n", "sha": "ee5ae0cb3d17808e98116693bad603aa936c3dd1\n", "time": "2010-07-31T19:29:05Z\n", "diff": "mmm a / IPython / core / inputsplitter . py <nl> ppp b / IPython / core / inputsplitter . py <nl> def remove_comments ( src ) : <nl> <nl> def get_input_encoding ( ) : <nl> \" \" \" Return the default standard input encoding . \" \" \" <nl> - return getattr ( sys . stdin , ' encoding ' , ' ascii ' ) <nl> + # There are strange environments for which sys . stdin . encoding is None . We <nl> + # ensure that a valid encoding is returned . <nl> + encoding = getattr ( sys . stdin , ' encoding ' , None ) <nl> + if encoding is None : <nl> + encoding = ' ascii ' <nl> + return encoding <nl> <nl> # mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - - <nl> # Classes and functions <nl>\n", "msg": "Made blockbreakers ' input encoding detection more robust to strange\n"}
{"diff_id": 41694, "repo": "pypa/pipenv\n", "sha": "f57d1753868d4fa8eb5c94a8d68eebdd13fddc9e\n", "time": "2017-05-30T19:45:13Z\n", "diff": "mmm a / pipenv / cli . py <nl> ppp b / pipenv / cli . py <nl> def do_install_dependencies ( dev = False , only = False , bare = False , requirements = Fals <nl> del v [ ' hash ' ] <nl> <nl> # Convert the deps to pip - compatible arguments . <nl> - hashed_deps = convert_deps_to_pip ( deps , r = False ) <nl> - vcs_deps_path = convert_deps_to_pip ( vcs_deps ) <nl> + deps_list = [ ( d , ignore_hashes ) for d in convert_deps_to_pip ( deps , r = False ) ] <nl> + if len ( vcs_deps ) : <nl> + deps_list . extend ( ( d , True ) for d in convert_deps_to_pip ( vcs_deps , r = False ) ) <nl> <nl> # - - requirements was passed . <nl> if requirements : <nl> - click . echo ( ' \\ n ' . join ( hashed_deps ) ) <nl> - with open ( vcs_deps_path ) as f : <nl> - click . echo ( f . read ( ) ) <nl> + click . echo ( ' \\ n ' . join ( deps_list ) ) <nl> sys . exit ( 0 ) <nl> <nl> # pip install : <nl> - for dep in progress . bar ( hashed_deps ) : <nl> + for dep , ignore_hash in progress . bar ( deps_list ) : <nl> <nl> - c = pip_install ( dep , ignore_hashes = ignore_hashes , allow_global = allow_global ) <nl> + c = pip_install ( dep , ignore_hashes = ignore_hash , allow_global = allow_global ) <nl> <nl> if c . return_code ! = 0 : <nl> click . echo ( crayons . red ( ' An error occured while installing ! ' ) ) <nl> def do_install_dependencies ( dev = False , only = False , bare = False , requirements = Fals <nl> click . echo ( crayons . blue ( format_pip_error ( c . err ) ) ) <nl> if ' PACKAGES DO NOT MATCH THE HASHES ' in c . err : <nl> click . echo ( crayons . yellow ( ' You can supply the - - ignore - hashes option to ' <nl> - ' \\ ' pipenv install \\ ' to bypass this feature . ' ) ) <nl> + ' \\ ' pipenv install \\ ' to bypass this feature . ' ) ) <nl> sys . exit ( c . return_code ) <nl> <nl> - if len ( vcs_deps ) : <nl> - with spinner ( ) : <nl> - c = pip_install ( r = vcs_deps_path , ignore_hashes = True , allow_global = allow_global ) <nl> - <nl> - if c . return_code ! = 0 : <nl> - click . echo ( crayons . red ( ' An error occured while installing ! ' ) ) <nl> - click . echo ( crayons . blue ( format_pip_error ( c . err ) ) ) <nl> - if ' PACKAGES DO NOT MATCH THE HASHES ' in c . err : <nl> - click . echo ( crayons . yellow ( ' You can supply the - - ignore - hashes option to ' <nl> - ' \\ ' pipenv install \\ ' to bypass this feature . ' ) ) <nl> - sys . exit ( c . return_code ) <nl> - <nl> - # Cleanup the temp requirements file . <nl> - if requirements : <nl> - os . remove ( vcs_deps_path ) <nl> - <nl> <nl> def do_download_dependencies ( dev = False , only = False , bare = False ) : <nl> \" \" \" \" Executes the download functionality . \" \" \" <nl>\n", "msg": "simplify installation separation for regular vs vcs dependencies\n"}
{"diff_id": 41727, "repo": "spotify/luigi\n", "sha": "41d5257cb54281afd473eb9c9d3f09d14a93b76c\n", "time": "2015-05-19T00:25:10Z\n", "diff": "mmm a / luigi / contrib / hdfs / snakebite_client . py <nl> ppp b / luigi / contrib / hdfs / snakebite_client . py <nl> def count ( self , path ) : <nl> return { ' content_size ' : content_size , ' dir_count ' : dir_count , <nl> ' file_count ' : file_count } <nl> <nl> - def copy ( self , path ) : <nl> + def copy ( self , path , destination ) : <nl> \" \" \" <nl> Raise a NotImplementedError exception . <nl> \" \" \" <nl> - return NotImplementedError ( \" SnakebiteClient in luigi doesn ' t implement copy \" ) <nl> + raise NotImplementedError ( \" SnakebiteClient in luigi doesn ' t implement copy \" ) <nl> <nl> def put ( self , local_path , destination ) : <nl> \" \" \" <nl> Raise a NotImplementedError exception . <nl> \" \" \" <nl> - return NotImplementedError ( \" Snakebite doesn ' t implement put \" ) <nl> + raise NotImplementedError ( \" Snakebite doesn ' t implement put \" ) <nl> <nl> def get ( self , path , local_destination ) : <nl> \" \" \" <nl> def touchz ( self , path ) : <nl> \" \" \" <nl> Raise a NotImplementedError exception . <nl> \" \" \" <nl> - return NotImplementedError ( \" SnakebiteClient in luigi doesn ' t implement touchz \" ) <nl> + raise NotImplementedError ( \" SnakebiteClient in luigi doesn ' t implement touchz \" ) <nl>\n", "msg": "Fixes error raising in unimplemented snakebite operations .\n"}
{"diff_id": 41814, "repo": "celery/celery\n", "sha": "1919256eb52dc7a6b4287403a5cd77f9a6b430b5\n", "time": "2015-10-26T12:26:45Z\n", "diff": "mmm a / celery / events / __init__ . py <nl> ppp b / celery / events / __init__ . py <nl> def _publish ( self , event , producer , routing_key , retry = False , <nl> raise <nl> self . _outbound_buffer . append ( ( event , routing_key , exc ) ) <nl> <nl> - def send ( self , type , retry = False , retry_policy = None , blind = False , <nl> - Event = Event , utcoffset = utcoffset , * * fields ) : <nl> + def send ( self , type , blind = False , utcoffset = utcoffset , retry = False , <nl> + retry_policy = None , Event = Event , * * fields ) : <nl> \" \" \" Send event . <nl> <nl> : param type : Event type name , with group separated by dash ( ` - ` ) . <nl> def send ( self , type , retry = False , retry_policy = None , blind = False , <nl> elif self . on_send_buffered : <nl> self . on_send_buffered ( ) <nl> else : <nl> - return self . publish ( type , fields , self . producer , retry = retry , <nl> - retry_policy = retry_policy , blind = blind , <nl> - Event = Event ) <nl> + return self . publish ( type , fields , self . producer , blind = blind , <nl> + Event = Event , retry = retry , <nl> + retry_policy = retry_policy ) <nl> <nl> def flush ( self , errors = True , groups = True ) : <nl> \" \" \" Flushes the outbound buffer . \" \" \" <nl>\n", "msg": "reorder args to be more backward compatible\n"}
{"diff_id": 41921, "repo": "scikit-learn/scikit-learn\n", "sha": "7eda3998e4f9e7962a7805c83c5900c904f5a2e0\n", "time": "2012-10-27T18:38:41Z\n", "diff": "mmm a / sklearn / cluster / spectral . py <nl> ppp b / sklearn / cluster / spectral . py <nl> def _set_diag ( laplacian , value ) : <nl> <nl> <nl> def spectral_embedding ( adjacency , n_components = 8 , mode = None , <nl> - random_state = None ) : <nl> + random_state = None , eig_tol = 0 . 0 ) : <nl> \" \" \" Project the sample on the first eigen vectors of the graph Laplacian <nl> <nl> The adjacency matrix is used to compute a normalized graph Laplacian <nl> def spectral_embedding ( adjacency , n_components = 8 , mode = None , <nl> lobpcg eigen vectors decomposition when mode = = ' amg ' . By default <nl> arpack is used . <nl> <nl> + eig_tol : float , optional , default : 0 . 0 <nl> + Stopping criterion for eigendecomposition of the Laplacian matrix <nl> + when using arpack mode . <nl> + <nl> Returns <nl> mmmmmm - <nl> embedding : array , shape : ( n_samples , n_components ) <nl> def spectral_embedding ( adjacency , n_components = 8 , mode = None , <nl> # in standard mode . <nl> try : <nl> lambdas , diffusion_map = eigsh ( - laplacian , k = n_components , <nl> - sigma = 1 . 0 , which = ' LM ' ) <nl> + sigma = 1 . 0 , which = ' LM ' , <nl> + tol = eig_tol ) <nl> embedding = diffusion_map . T [ : : - 1 ] * dd <nl> except RuntimeError : <nl> # When submatrices are exactly singular , an LU decomposition <nl> def spectral_embedding ( adjacency , n_components = 8 , mode = None , <nl> raise ValueError <nl> return embedding <nl> <nl> - <nl> + def discretization ( eigen_vec ) : <nl> + from scipy . sparse import csc_matrix <nl> + from scipy . linalg import LinAlgError <nl> + eps = 2 . 2204e - 16 <nl> + <nl> + m = eigen_vec . shape [ 0 ] <nl> + <nl> + # Normalize the eigenvectors <nl> + norm_ones = np . linalg . norm ( np . ones ( ( m , 1 ) ) ) <nl> + for i in range ( eigen_vec . shape [ 1 ] ) : <nl> + eigen_vec [ : , i ] = ( eigen_vec [ : , i ] / np . linalg . norm ( eigen_vec [ : , i ] ) ) * norm_ones <nl> + if eigen_vec [ 0 , i ] ! = 0 : <nl> + eigen_vec [ : , i ] = - 1 * eigen_vec [ : , i ] * np . sign ( eigen_vec [ 0 , i ] ) <nl> + <nl> + # Normalize the rows of the eigenvectors <nl> + n , k = eigen_vec . shape <nl> + vm = np . kron ( np . ones ( ( 1 , k ) ) , np . sqrt ( ( eigen_vec * eigen_vec ) . sum ( 1 ) [ : , np . newaxis ] ) ) <nl> + eigen_vec = eigen_vec / vm <nl> + <nl> + svd_restarts = 0 <nl> + exitLoop = 0 <nl> + <nl> + # # # if there is an exception we try to randomize and rerun SVD again <nl> + # # # do this 30 times <nl> + while ( svd_restarts < 30 ) and ( exitLoop = = 0 ) : <nl> + <nl> + # initialize algorithm with a random ordering of eigenvectors <nl> + c = np . zeros ( ( n , 1 ) ) <nl> + R = np . zeros ( ( k , k ) ) <nl> + R [ : , 0 ] = eigen_vec [ int ( np . random . rand ( 1 ) * ( n - 1 ) ) , : ] . transpose ( ) <nl> + <nl> + for j in range ( 1 , k ) : <nl> + c = c + np . abs ( eigen_vec . dot ( R [ : , j - 1 ] ) ) <nl> + R [ : , j ] = eigen_vec [ c . argmin ( ) , : ] . T <nl> + <nl> + <nl> + lastObjectiveValue = 0 <nl> + nbIterationsDiscretisation = 0 <nl> + nbIterationsDiscretisationMax = 20 <nl> + <nl> + while exitLoop = = 0 : <nl> + nbIterationsDiscretisation = nbIterationsDiscretisation + 1 <nl> + <nl> + tDiscrete = eigen_vec . dot ( R ) <nl> + <nl> + j = np . reshape ( np . asarray ( tDiscrete . argmax ( 1 ) ) , n ) <nl> + eigenvec_discrete = csc_matrix ( ( np . ones ( len ( j ) ) , ( range ( 0 , n ) , np . array ( j ) ) ) , shape = ( n , k ) ) <nl> + <nl> + tSVD = eigenvec_discrete . transpose ( ) * eigen_vec <nl> + <nl> + try : <nl> + U , S , Vh = np . linalg . svd ( tSVD ) <nl> + except LinAlgError : <nl> + print \" SVD did not converge , randomizing and trying again \" <nl> + break <nl> + <nl> + NcutValue = 2 * ( n - S . sum ( ) ) <nl> + if ( ( abs ( NcutValue - lastObjectiveValue ) < eps ) or <nl> + ( nbIterationsDiscretisation > nbIterationsDiscretisationMax ) ) : <nl> + exitLoop = 1 <nl> + else : <nl> + # otherwise calculate rotation and continue <nl> + lastObjectiveValue = NcutValue <nl> + R = np . matrix ( Vh ) . transpose ( ) * np . matrix ( U ) . transpose ( ) <nl> + <nl> + if exitLoop = = 0 : <nl> + raise ValueError ( ' SVD did not converge ' ) <nl> + <nl> + y_pred = np . dot ( eigenvec_discrete . toarray ( ) , np . diag ( np . arange ( k ) ) ) . sum ( 1 ) <nl> + <nl> + return y_pred <nl> + <nl> def spectral_clustering ( affinity , n_clusters = 8 , n_components = None , mode = None , <nl> - random_state = None , n_init = 10 , k = None ) : <nl> + random_state = None , n_init = 10 , k = None , eig_tol = 0 . 0 , <nl> + embed_solve = ' kmeans ' ) : <nl> \" \" \" Apply k - means to a projection to the normalized laplacian <nl> <nl> In practice Spectral Clustering is very useful when the structure of <nl> def spectral_clustering ( affinity , n_clusters = 8 , n_components = None , mode = None , <nl> centroid seeds . The final results will be the best output of <nl> n_init consecutive runs in terms of inertia . <nl> <nl> + eig_tol : float , optional , default : 0 . 0 <nl> + Stopping criterion for eigendecomposition of the Laplacian matrix <nl> + when using arpack mode . <nl> + <nl> + embed_solve : { ' kmeans ' , ' discrete ' } , default : ' kmeans ' <nl> + The strategy to use to solve the clustering problem in the embedding <nl> + space . <nl> + <nl> Returns <nl> mmmmmm - <nl> labels : array of integers , shape : n_samples <nl> def spectral_clustering ( affinity , n_clusters = 8 , n_components = None , mode = None , <nl> random_state = check_random_state ( random_state ) <nl> n_components = n_clusters if n_components is None else n_components <nl> maps = spectral_embedding ( affinity , n_components = n_components , <nl> - mode = mode , random_state = random_state ) <nl> - maps = maps [ 1 : ] <nl> - _ , labels , _ = k_means ( maps . T , n_clusters , random_state = random_state , <nl> - n_init = n_init ) <nl> + mode = mode , random_state = random_state , <nl> + eig_tol = eig_tol ) <nl> + <nl> + if embed_solve = = ' kmeans ' : <nl> + maps = maps [ 1 : ] <nl> + _ , labels , _ = k_means ( maps . T , n_clusters , random_state = random_state , <nl> + n_init = n_init ) <nl> + else : <nl> + labels = discretization ( maps . T ) <nl> + <nl> return labels <nl> <nl> <nl> class SpectralClustering ( BaseEstimator , ClusterMixin ) : <nl> centroid seeds . The final results will be the best output of <nl> n_init consecutive runs in terms of inertia . <nl> <nl> + eig_tol : float , optional , default : 0 . 0 <nl> + Stopping criterion for eigendecomposition of the Laplacian matrix <nl> + when using arpack mode . <nl> + <nl> + embed_solve : { ' kmeans ' , ' discrete ' } , default : ' kmeans ' <nl> + The strategy to use to solve the clustering problem in the embedding <nl> + space . <nl> <nl> Attributes <nl> mmmmmmmmm - <nl> class SpectralClustering ( BaseEstimator , ClusterMixin ) : <nl> <nl> def __init__ ( self , n_clusters = 8 , mode = None , random_state = None , n_init = 10 , <nl> gamma = 1 . , affinity = ' rbf ' , n_neighbors = 10 , k = None , <nl> - precomputed = False ) : <nl> + precomputed = False , eig_tol = 0 . 0 , embed_solve = ' kmeans ' ) : <nl> if not k is None : <nl> warnings . warn ( \" ' k ' was renamed to n_clusters \" , DeprecationWarning ) <nl> n_clusters = k <nl> def __init__ ( self , n_clusters = 8 , mode = None , random_state = None , n_init = 10 , <nl> self . gamma = gamma <nl> self . affinity = affinity <nl> self . n_neighbors = n_neighbors <nl> + self . eig_tol = eig_tol <nl> + self . embed_solve = embed_solve <nl> <nl> def fit ( self , X ) : <nl> \" \" \" Creates an affinity matrix for X using the selected affinity , <nl> def fit ( self , X ) : <nl> self . random_state = check_random_state ( self . random_state ) <nl> self . labels_ = spectral_clustering ( self . affinity_matrix_ , <nl> n_clusters = self . n_clusters , mode = self . mode , <nl> - random_state = self . random_state , n_init = self . n_init ) <nl> + random_state = self . random_state , n_init = self . n_init , <nl> + eig_tol = self . eig_tol , embed_solve = self . embed_solve ) <nl> return self <nl> <nl> @ property <nl>\n", "msg": "Discretization method for spectral clustering added along with tolerence setting to loosen eigendecomposition constraints\n"}
{"diff_id": 41952, "repo": "ansible/ansible\n", "sha": "c36597411744eac295922003d052e36f41f71f14\n", "time": "2013-03-15T18:31:27Z\n", "diff": "mmm a / lib / ansible / playbook / task . py <nl> ppp b / lib / ansible / playbook / task . py <nl> def __init__ ( self , play , ds , module_vars = None , additional_conditions = None ) : <nl> self . args = ds . get ( ' args ' , { } ) <nl> <nl> if self . sudo : <nl> - self . sudo_user = ds . get ( ' sudo_user ' , play . sudo_user ) <nl> + self . sudo_user = utils . template ( play . basedir , ds . get ( ' sudo_user ' , play . sudo_user ) , play . vars ) <nl> self . sudo_pass = ds . get ( ' sudo_pass ' , play . playbook . sudo_pass ) <nl> else : <nl> self . sudo_user = None <nl>\n", "msg": "Added variable expansion to task sudo_user parameter\n"}
{"diff_id": 41956, "repo": "plotly/dash\n", "sha": "e6547a5c93a9046f8b8b8147251845e37926e946\n", "time": "2020-04-17T20:56:58Z\n", "diff": "mmm a / tests / integration / devtools / test_callback_validation . py <nl> ppp b / tests / integration / devtools / test_callback_validation . py <nl> def multipage_app ( validation = False ) : <nl> layout_page_2 = html . Div ( <nl> [ <nl> html . H2 ( \" Page 2 \" ) , <nl> - dcc . Dropdown ( <nl> - id = \" page - 2 - dropdown \" , <nl> - options = [ { \" label \" : i , \" value \" : i } for i in [ \" LA \" , \" NYC \" , \" MTL \" ] ] , <nl> - value = \" LA \" , <nl> - ) , <nl> + dcc . Input ( id = \" page - 2 - input \" , value = \" LA \" ) , <nl> html . Div ( id = \" page - 2 - display - value \" ) , <nl> html . Br ( ) , <nl> dcc . Link ( ' Navigate to \" / \" ' , id = \" p2_index \" , href = \" / \" ) , <nl> def update_output ( n_clicks , input1 , input2 ) : <nl> <nl> # Page 2 callbacks <nl> @ app . callback ( <nl> - Output ( \" page - 2 - display - value \" , \" children \" ) , [ Input ( \" page - 2 - dropdown \" , \" value \" ) ] <nl> + Output ( \" page - 2 - display - value \" , \" children \" ) , [ Input ( \" page - 2 - input \" , \" value \" ) ] <nl> ) <nl> def display_value ( value ) : <nl> print ( \" display_value \" ) <nl> def test_dvcv014_multipage_errors ( dash_duo ) : <nl> specs = [ <nl> [ <nl> \" ID not found in layout \" , <nl> - [ ' \" page - 2 - dropdown \" ' , \" page - 2 - display - value . children \" ] , <nl> + [ ' \" page - 2 - input \" ' , \" page - 2 - display - value . children \" ] , <nl> ] , <nl> [ \" ID not found in layout \" , [ ' \" submit - button \" ' , \" output - state . children \" ] ] , <nl> [ <nl>\n", "msg": "switch validation_layout test to use input instead of dropdown\n"}
{"diff_id": 42064, "repo": "ipython/ipython\n", "sha": "5f5908d5f500cd97a3276b09ca109e4b1a21b7d4\n", "time": "2010-09-02T18:20:29Z\n", "diff": "mmm a / IPython / frontend / qt / console / console_widget . py <nl> ppp b / IPython / frontend / qt / console / console_widget . py <nl> <nl> # Standard library imports <nl> + from os . path import commonprefix <nl> import re <nl> import sys <nl> from textwrap import dedent <nl> def __init__ ( self , parent = None , * * kw ) : <nl> self . _reading = False <nl> self . _reading_callback = None <nl> self . _tab_width = 8 <nl> + self . _text_completing_pos = 0 <nl> <nl> # Set a monospaced font . <nl> self . reset_font ( ) <nl> def redo ( self ) : <nl> def reset_font ( self ) : <nl> \" \" \" Sets the font to the default fixed - width font for this platform . <nl> \" \" \" <nl> - # FIXME : font family and size should be configurable by the user . <nl> if sys . platform = = ' win32 ' : <nl> # FIXME : we should test whether Consolas is available and use it <nl> # first if it is . Consolas ships by default from Vista onwards , <nl> def _append_plain_text_keeping_prompt ( self , text ) : <nl> self . _show_prompt ( ) <nl> self . input_buffer = input_buffer <nl> <nl> + def _clear_temporary_buffer ( self ) : <nl> + \" \" \" Clears the \" temporary text \" buffer , i . e . all the text following <nl> + the prompt region . <nl> + \" \" \" <nl> + cursor = self . _get_prompt_cursor ( ) <nl> + found_block = cursor . movePosition ( QtGui . QTextCursor . NextBlock ) <nl> + if found_block : <nl> + while found_block and \\ <nl> + cursor . block ( ) . text ( ) . startsWith ( self . _continuation_prompt ) : <nl> + found_block = cursor . movePosition ( QtGui . QTextCursor . NextBlock ) <nl> + cursor . movePosition ( QtGui . QTextCursor . Left ) # Grab the newline . <nl> + cursor . movePosition ( QtGui . QTextCursor . End , <nl> + QtGui . QTextCursor . KeepAnchor ) <nl> + cursor . removeSelectedText ( ) <nl> + <nl> def _complete_with_items ( self , cursor , items ) : <nl> \" \" \" Performs completion with ' items ' at the specified cursor location . <nl> \" \" \" <nl> + if self . _text_completing_pos : <nl> + self . _clear_temporary_buffer ( ) <nl> + self . _text_completing_pos = 0 <nl> + <nl> if len ( items ) = = 1 : <nl> cursor . setPosition ( self . _control . textCursor ( ) . position ( ) , <nl> QtGui . QTextCursor . KeepAnchor ) <nl> cursor . insertText ( items [ 0 ] ) <nl> + <nl> elif len ( items ) > 1 : <nl> + current_pos = self . _control . textCursor ( ) . position ( ) <nl> + prefix = commonprefix ( items ) <nl> + if prefix : <nl> + cursor . setPosition ( current_pos , QtGui . QTextCursor . KeepAnchor ) <nl> + cursor . insertText ( prefix ) <nl> + current_pos = cursor . position ( ) <nl> + <nl> if self . gui_completion : <nl> + cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( prefix ) ) <nl> self . _completion_widget . show_items ( cursor , items ) <nl> else : <nl> + cursor . beginEditBlock ( ) <nl> + self . _append_plain_text ( ' \\ n ' ) <nl> self . _page ( self . _format_as_columns ( items ) ) <nl> + cursor . endEditBlock ( ) <nl> + <nl> + cursor . setPosition ( current_pos ) <nl> + self . _control . moveCursor ( QtGui . QTextCursor . End ) <nl> + self . _control . setTextCursor ( cursor ) <nl> + self . _text_completing_pos = current_pos <nl> <nl> def _control_key_down ( self , modifiers ) : <nl> \" \" \" Given a KeyboardModifiers flags object , return whether the Control <nl> def _create_control ( self ) : <nl> control . setAcceptRichText ( False ) <nl> control . installEventFilter ( self ) <nl> control . setContextMenuPolicy ( QtCore . Qt . CustomContextMenu ) <nl> + control . cursorPositionChanged . connect ( self . _cursor_position_changed ) <nl> control . customContextMenuRequested . connect ( self . _show_context_menu ) <nl> control . copyAvailable . connect ( self . copy_available ) <nl> control . redoAvailable . connect ( self . redo_available ) <nl> def _event_filter_console_keypress ( self , event ) : <nl> alt_down = event . modifiers ( ) & QtCore . Qt . AltModifier <nl> shift_down = event . modifiers ( ) & QtCore . Qt . ShiftModifier <nl> <nl> + # Special handling when tab completing in text mode : <nl> + if self . _text_completing_pos : <nl> + if key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return , <nl> + QtCore . Qt . Key_Escape ) : <nl> + self . _clear_temporary_buffer ( ) <nl> + self . _text_completing_pos = 0 <nl> + <nl> if event . matches ( QtGui . QKeySequence . Paste ) : <nl> # Call our paste instead of the underlying text widget ' s . <nl> self . paste ( ) <nl> def _page ( self , text ) : <nl> \" \" \" Displays text using the pager if it exceeds the height of the <nl> visible area . <nl> \" \" \" <nl> - if self . paging ! = ' none ' : <nl> + if self . paging = = ' none ' : <nl> + self . _append_plain_text ( text ) <nl> + else : <nl> line_height = QtGui . QFontMetrics ( self . font ) . height ( ) <nl> minlines = self . _control . viewport ( ) . height ( ) / line_height <nl> if re . match ( \" ( ? : [ ^ \\ n ] * \\ n ) { % i } \" % minlines , text ) : <nl> def _page ( self , text ) : <nl> self . _page_control . setFocus ( ) <nl> else : <nl> self . layout ( ) . setCurrentWidget ( self . _page_control ) <nl> - return <nl> - if self . _executing : <nl> - self . _append_plain_text ( text ) <nl> - else : <nl> - self . _append_plain_text_keeping_prompt ( text ) <nl> + else : <nl> + self . _append_plain_text ( text ) <nl> <nl> def _prompt_started ( self ) : <nl> \" \" \" Called immediately after a new prompt is displayed . <nl> def _show_prompt ( self , prompt = None , html = False , newline = True ) : <nl> self . _prompt_pos = self . _get_end_cursor ( ) . position ( ) <nl> self . _prompt_started ( ) <nl> <nl> + # mmmmmm Signal handlers mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm - <nl> + <nl> + def _cursor_position_changed ( self ) : <nl> + \" \" \" Clears the temporary buffer based on the cursor position . <nl> + \" \" \" <nl> + if self . _text_completing_pos : <nl> + document = self . _control . document ( ) <nl> + if self . _text_completing_pos < document . characterCount ( ) : <nl> + cursor = self . _control . textCursor ( ) <nl> + pos = cursor . position ( ) <nl> + text_cursor = self . _control . textCursor ( ) <nl> + text_cursor . setPosition ( self . _text_completing_pos ) <nl> + if pos < self . _text_completing_pos or \\ <nl> + cursor . blockNumber ( ) > text_cursor . blockNumber ( ) : <nl> + self . _clear_temporary_buffer ( ) <nl> + self . _text_completing_pos = 0 <nl> + else : <nl> + self . _clear_temporary_buffer ( ) <nl> + self . _text_completing_pos = 0 <nl> + <nl> <nl> class HistoryConsoleWidget ( ConsoleWidget ) : <nl> \" \" \" A ConsoleWidget that keeps a history of the commands that have been <nl>\n", "msg": "Major improvements to terminal - style tab completion . Still work to be done , but a decent start .\n"}
{"diff_id": 42341, "repo": "tornadoweb/tornado\n", "sha": "dd706f9fa64476533f3e6623e70833cbb5208951\n", "time": "2010-12-11T00:04:39Z\n", "diff": "mmm a / tornado / stack_context . py <nl> ppp b / tornado / stack_context . py <nl> def __init__ ( self , context_factory ) : <nl> ' ' ' <nl> self . context_factory = context_factory <nl> <nl> + # Note that some of this code is duplicated in ExceptionStackContext <nl> + # below . ExceptionStackContext is more common and doesn ' t need <nl> + # the full generality of this class . <nl> def __enter__ ( self ) : <nl> self . old_contexts = _state . contexts <nl> - _state . contexts = self . old_contexts + ( self . context_factory , ) <nl> + # _state . contexts is a tuple of ( class , arg ) pairs <nl> + _state . contexts = ( self . old_contexts + <nl> + ( ( StackContext , self . context_factory ) , ) ) <nl> try : <nl> self . context = self . context_factory ( ) <nl> self . context . __enter__ ( ) <nl> def __exit__ ( self , type , value , traceback ) : <nl> finally : <nl> _state . contexts = self . old_contexts <nl> <nl> - def ExceptionStackContext ( exception_handler ) : <nl> - ' ' ' Specialization of StackContext for exception handling . <nl> + class ExceptionStackContext ( object ) : <nl> + def __init__ ( self , exception_handler ) : <nl> + ' ' ' Specialization of StackContext for exception handling . <nl> <nl> - The supplied exception_handler function will be called in the <nl> - event of an uncaught exception in this context . The semantics are <nl> - similar to a try / finally clause , and intended use cases are to log <nl> - an error , close a socket , or similar cleanup actions . The <nl> - exc_info triple ( type , value , traceback ) will be passed to the <nl> - exception_handler function . <nl> + The supplied exception_handler function will be called in the <nl> + event of an uncaught exception in this context . The semantics are <nl> + similar to a try / finally clause , and intended use cases are to log <nl> + an error , close a socket , or similar cleanup actions . The <nl> + exc_info triple ( type , value , traceback ) will be passed to the <nl> + exception_handler function . <nl> <nl> - If the exception handler returns true , the exception will be <nl> - consumed and will not be propagated to other exception handlers . <nl> - ' ' ' <nl> - class Context ( object ) : <nl> - def __enter__ ( self ) : <nl> - pass <nl> - def __exit__ ( self , type , value , traceback ) : <nl> + If the exception handler returns true , the exception will be <nl> + consumed and will not be propagated to other exception handlers . <nl> + ' ' ' <nl> + self . exception_handler = exception_handler <nl> + <nl> + def __enter__ ( self ) : <nl> + self . old_contexts = _state . contexts <nl> + _state . contexts = ( self . old_contexts + <nl> + ( ( ExceptionStackContext , self . exception_handler ) , ) ) <nl> + <nl> + def __exit__ ( self , type , value , traceback ) : <nl> + try : <nl> if type is not None : <nl> - return exception_handler ( type , value , traceback ) <nl> - return StackContext ( Context ) <nl> + return self . exception_handler ( type , value , traceback ) <nl> + finally : <nl> + _state . contexts = self . old_contexts <nl> <nl> class NullContext ( object ) : <nl> ' ' ' Resets the StackContext . <nl> def wrapped ( callback , contexts , * args , * * kwargs ) : <nl> # _state . contexts will have elements not in contexts . Use <nl> # NullContext to clear the state and then recreate from contexts . <nl> if ( len ( _state . contexts ) > len ( contexts ) or <nl> - any ( a is not b <nl> + any ( a [ 1 ] is not b [ 1 ] <nl> for a , b in itertools . izip ( _state . contexts , contexts ) ) ) : <nl> # contexts have been removed or changed , so start over <nl> new_contexts = ( [ NullContext ( ) ] + <nl> - [ StackContext ( c ) for c in contexts ] ) <nl> + [ cls ( arg ) for ( cls , arg ) in contexts ] ) <nl> else : <nl> - new_contexts = [ StackContext ( c ) <nl> - for c in contexts [ len ( _state . contexts ) : ] ] <nl> + new_contexts = [ cls ( arg ) <nl> + for ( cls , arg ) in contexts [ len ( _state . contexts ) : ] ] <nl> if len ( new_contexts ) > 1 : <nl> with contextlib . nested ( * new_contexts ) : <nl> callback ( * args , * * kwargs ) <nl>\n", "msg": "Implement ExceptionStackContext independently of StackContext to improve\n"}
{"diff_id": 42367, "repo": "python/cpython\n", "sha": "82ebc27357891ec342b8602fb28189751b8d06e6\n", "time": "1999-08-03T15:32:48Z\n", "diff": "mmm a / Doc / tools / sgmlconv / docfixer . py <nl> ppp b / Doc / tools / sgmlconv / docfixer . py <nl> def cleanup_synopses ( doc , fragment ) : <nl> create_module_info ( doc , node ) <nl> <nl> <nl> - def remap_element_names ( root , name_map ) : <nl> - queue = [ ] <nl> - for child in root . childNodes : <nl> - if child . nodeType = = ELEMENT : <nl> - queue . append ( child ) <nl> - while queue : <nl> - node = queue . pop ( ) <nl> - tagName = node . tagName <nl> - if name_map . has_key ( tagName ) : <nl> - name , attrs = name_map [ tagName ] <nl> - node . _node . name = name <nl> - for attr , value in attrs . items ( ) : <nl> - node . setAttribute ( attr , value ) <nl> - for child in node . childNodes : <nl> - if child . nodeType = = ELEMENT : <nl> - queue . append ( child ) <nl> - <nl> - <nl> def fixup_table_structures ( doc , fragment ) : <nl> - # must be done after remap_element_names ( ) , or the tables won ' t be found <nl> for table in find_all_elements ( fragment , \" table \" ) : <nl> fixup_table ( doc , table ) <nl> <nl> def move_elements_by_name ( doc , source , dest , name , sep = None ) : <nl> \" section \" , \" subsection \" , \" subsubsection \" , <nl> \" paragraph \" , \" subparagraph \" , \" back - matter \" , <nl> \" howto \" , \" manual \" , <nl> + \" item \" , \" itemize \" , \" fulllineitems \" , \" enumeration \" , \" descriptionlist \" , <nl> + \" definitionlist \" , \" definition \" , <nl> ) <nl> <nl> PARA_LEVEL_ELEMENTS = ( <nl> def move_elements_by_name ( doc , source , dest , name , sep = None ) : <nl> \" funcdesc \" , \" methoddesc \" , \" excdesc \" , \" memberdesc \" , \" membderdescni \" , <nl> \" funcdescni \" , \" methoddescni \" , \" excdescni \" , <nl> \" tableii \" , \" tableiii \" , \" tableiv \" , \" localmoduletable \" , <nl> - \" sectionauthor \" , \" seealso \" , <nl> + \" sectionauthor \" , \" seealso \" , \" itemize \" , <nl> # include < para > , so we can just do it again to get subsequent paras : <nl> PARA_ELEMENT , <nl> ) <nl> <nl> PARA_LEVEL_PRECEEDERS = ( <nl> - \" index \" , \" indexii \" , \" indexiii \" , \" indexiv \" , \" setindexsubitem \" , <nl> + \" setindexsubitem \" , <nl> \" stindex \" , \" obindex \" , \" COMMENT \" , \" label \" , \" input \" , \" title \" , <nl> \" versionadded \" , \" versionchanged \" , \" declaremodule \" , \" modulesynopsis \" , <nl> - \" moduleauthor \" , \" indexterm \" , <nl> + \" moduleauthor \" , \" indexterm \" , \" leader \" , <nl> ) <nl> <nl> <nl> def fixup_refmodindexes_chunk ( container ) : <nl> ewrite ( entry . toxml ( ) + \" \\ n \" ) <nl> continue <nl> found = 0 <nl> - module_name = entry . getAttribute ( \" name \" ) <nl> + module_name = entry . getAttribute ( \" module \" ) <nl> for node in module_entries : <nl> if len ( node . childNodes ) ! = 1 : <nl> continue <nl> def convert ( ifp , ofp ) : <nl> normalize ( fragment ) <nl> fixup_paras ( doc , fragment ) <nl> fixup_sectionauthors ( doc , fragment ) <nl> - remap_element_names ( fragment , { <nl> - \" tableii \" : ( \" table \" , { \" cols \" : \" 2 \" } ) , <nl> - \" tableiii \" : ( \" table \" , { \" cols \" : \" 3 \" } ) , <nl> - \" tableiv \" : ( \" table \" , { \" cols \" : \" 4 \" } ) , <nl> - \" lineii \" : ( \" row \" , { } ) , <nl> - \" lineiii \" : ( \" row \" , { } ) , <nl> - \" lineiv \" : ( \" row \" , { } ) , <nl> - \" refmodule \" : ( \" module \" , { \" link \" : \" link \" } ) , <nl> - } ) <nl> fixup_table_structures ( doc , fragment ) <nl> fixup_rfc_references ( doc , fragment ) <nl> fixup_signatures ( doc , fragment ) <nl>\n", "msg": "remap_element_names ( ) : Only used for things that the new conversion\n"}
{"diff_id": 42445, "repo": "python/cpython\n", "sha": "c10039c0110f75ceb0065df56a0409cc1e635958\n", "time": "2001-10-18T19:34:00Z\n", "diff": "mmm a / Lib / test / test_hotshot . py <nl> ppp b / Lib / test / test_hotshot . py <nl> def get_events_wotime ( self ) : <nl> <nl> def check_events ( self , expected ) : <nl> events = self . get_events_wotime ( ) <nl> + if not __debug__ : <nl> + # Running under - O , so we don ' t get LINE events <nl> + expected = [ ev for ev in expected if ev [ 0 ] ! = LINE ] <nl> if events ! = expected : <nl> self . fail ( <nl> \" events did not match expectation ; got : \\ n % s \\ nexpected : \\ n % s \" <nl>\n", "msg": "Do not expect line number events when running under \" python - O \" .\n"}
{"diff_id": 42451, "repo": "matplotlib/matplotlib\n", "sha": "9cb8cb26866cec810c3ada8a056824ec0201989e\n", "time": "2019-05-18T14:17:35Z\n", "diff": "mmm a / lib / matplotlib / colors . py <nl> ppp b / lib / matplotlib / colors . py <nl> class ColorConverter ( object ) : <nl> <nl> <nl> def makeMappingArray ( N , data , gamma = 1 . 0 ) : <nl> - \" \" \" Create an * N * - element 1 - d lookup table <nl> - <nl> - * data * represented by a list of x , y0 , y1 mapping correspondences . <nl> - Each element in this list represents how a value between 0 and 1 <nl> - ( inclusive ) represented by x is mapped to a corresponding value <nl> - between 0 and 1 ( inclusive ) . The two values of y are to allow <nl> - for discontinuous mapping functions ( say as might be found in a <nl> - sawtooth ) where y0 represents the value of y for values of x <nl> - < = to that given , and y1 is the value to be used for x > than <nl> - that given ) . The list must start with x = 0 , end with x = 1 , and <nl> - all values of x must be in increasing order . Values between <nl> - the given mapping points are determined by simple linear interpolation . <nl> - <nl> - Alternatively , data can be a function mapping values between 0 - 1 <nl> - to 0 - 1 . <nl> - <nl> - The function returns an array \" result \" where ` ` result [ x * ( N - 1 ) ] ` ` <nl> - gives the closest value for values of x between 0 and 1 . <nl> + r \" \" \" Create an * N * - element 1 - d lookup table . <nl> + <nl> + This assumes a mapping : math : ` f : [ 0 , 1 ] \\ rightarrow [ 0 , 1 ] ` . The returned <nl> + data is an array of N values : math : ` y = f ( x ) ` where x is sampled from <nl> + [ 0 , 1 ] . <nl> + <nl> + By default ( * gamma * = 1 ) x is equidistantly sampled from [ 0 , 1 ] . The <nl> + * gamma * correction factor : math : ` \\ gamma ` distorts this equidistant <nl> + sampling by : math : ` x \\ rightarrow x ^ \\ gamma ` . <nl> + <nl> + Parameters <nl> + mmmmmmmmm - <nl> + N : int <nl> + The number of elements of the created lookup table . <nl> + This must be N > = 1 . <nl> + data : Mx3 array - like or callable <nl> + Defines the mapping : math : ` f ` . <nl> + <nl> + If a Mx3 array - like , the rows define values ( x , y0 , y1 ) . The x values <nl> + must start with x = 0 , end with x = 1 , and all x values be in increasing <nl> + order . <nl> + <nl> + A value between : math : ` x_i ` and : math : ` x_ { i + 1 } ` is mapped to the range <nl> + : math : ` y ^ 1_ { i - 1 } \\ ldots y ^ 0_i ` by linear interpolation . <nl> + <nl> + For the simple case of a y - continuous mapping , y0 and y1 are identical . <nl> + <nl> + The two values of y are to allow for discontinuous mapping functions . <nl> + E . g . a sawtooth with a period of 0 . 2 and an amplitude of 1 would be : : <nl> + <nl> + [ ( 0 , 1 , 0 ) , ( 0 . 2 , 1 , 0 ) , ( 0 . 4 , 1 , 0 ) , . . . , [ ( 1 , 1 , 0 ) ] <nl> + <nl> + In the special case of ` ` N = = 1 ` ` , by convention the returned value <nl> + is y0 for x = = 1 . <nl> + <nl> + If * data * is a callable , it must accept and return numpy arrays : : <nl> + <nl> + data ( x : ndarray ) - > ndarray <nl> + <nl> + and map values between 0 - 1 to 0 - 1 . <nl> + gamma : float <nl> + Gamma correction factor for input distribution x of the mapping . <nl> + <nl> + See also https : / / en . wikipedia . org / wiki / Gamma_correction . <nl> + <nl> + Returns <nl> + mmmmmm - <nl> + lut : array <nl> + The lookup table where ` ` lut [ x * ( N - 1 ) ] ` ` gives the closest value <nl> + for values of x between 0 and 1 . <nl> + <nl> + Notes <nl> + mmm - - <nl> + This function is internally used for ` . LinearSegmentedColormaps ` . <nl> \" \" \" <nl> <nl> if callable ( data ) : <nl>\n", "msg": "Update docstring of makeMappingArray\n"}
{"diff_id": 42453, "repo": "zulip/zulip\n", "sha": "7c59d4100646a764a7cd629ea6234adb9cfc07b0\n", "time": "2013-01-29T20:56:16Z\n", "diff": "mmm a / zephyr / tests . py <nl> ppp b / zephyr / tests . py <nl> <nl> from django . conf import settings <nl> import re <nl> import sys <nl> + import random <nl> <nl> try : <nl> settings . TEST_SUITE <nl> def test_set_invalid_property ( self ) : <nl> self . assert_json_error ( result , <nl> \" Unknown property or invalid verb for bad \" ) <nl> <nl> + class SubscriptionAPITest ( AuthedTestCase ) : <nl> + fixtures = [ ' messages . json ' ] <nl> + <nl> + def setUp ( self ) : <nl> + \" \" \" <nl> + All tests will be logged in as hamlet . Also save various useful values <nl> + as attributes that tests can access . <nl> + \" \" \" <nl> + self . test_email = \" hamlet @ humbughq . com \" <nl> + self . login ( self . test_email ) <nl> + self . user_profile = self . get_user_profile ( self . test_email ) <nl> + self . realm = self . user_profile . realm <nl> + self . streams = self . get_streams ( self . test_email ) <nl> + <nl> + def get_streams ( self , email ) : <nl> + \" \" \" <nl> + Helper function to get the stream names for a user <nl> + \" \" \" <nl> + user_profile = self . get_user_profile ( email ) <nl> + subs = Subscription . objects . filter ( <nl> + user_profile = user_profile , <nl> + active = True , <nl> + recipient__type = Recipient . STREAM ) <nl> + return [ get_display_recipient ( sub . recipient ) for sub in subs ] <nl> + <nl> + def make_random_stream_names ( self , existing_stream_names , names_to_avoid ) : <nl> + \" \" \" <nl> + Helper function to make up random stream names . It takes <nl> + existing_stream_names and randomly appends a digit to the end of each , <nl> + but avoids names that appear in the list names_to_avoid . <nl> + \" \" \" <nl> + random_streams = [ ] <nl> + for stream in existing_stream_names : <nl> + random_stream = stream + str ( random . randint ( 0 , 9 ) ) <nl> + if not random_stream in names_to_avoid : <nl> + random_streams . append ( random_stream ) <nl> + return random_streams <nl> + <nl> + def test_successful_subscriptions_list ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / list should successfully return your subscriptions . <nl> + \" \" \" <nl> + result = self . client . post ( \" / json / subscriptions / list \" , { } ) <nl> + self . assert_json_success ( result ) <nl> + json = simplejson . loads ( result . content ) <nl> + self . assertIn ( \" subscriptions \" , json ) <nl> + for stream , color in json [ \" subscriptions \" ] : <nl> + self . assertIsInstance ( stream , str ) <nl> + self . assertIsInstance ( color , str ) <nl> + # check that the stream name corresponds to an actual stream <nl> + try : <nl> + Stream . objects . get ( name__iexact = stream , realm = self . realm ) <nl> + except Stream . DoesNotExist : <nl> + self . fail ( \" stream does not exist \" ) <nl> + list_streams = [ stream for stream , color in json [ \" subscriptions \" ] ] <nl> + # also check that this matches the list of your subscriptions <nl> + self . assertItemsEqual ( list_streams , self . streams ) <nl> + <nl> + def helper_check_subs_before_and_after ( self , url , subscriptions , other_params , json_dict , email , new_subs ) : <nl> + \" \" \" <nl> + Helper function that posts to url with the subscriptions value set to <nl> + subscriptions and optional other_params specified as a dict . It checks <nl> + that the returned JSON dict contains all keys in json_dict with the <nl> + corresponding correct values . Finally , it checks that the subscriptions <nl> + after this call for the given email are equal to new_subs . <nl> + \" \" \" <nl> + data = { \" subscriptions \" : simplejson . dumps ( subscriptions ) } <nl> + data . update ( other_params ) <nl> + result = self . client . post ( url , data ) <nl> + self . assert_json_success ( result ) <nl> + json = simplejson . loads ( result . content ) <nl> + for key , val in json_dict . iteritems ( ) : <nl> + self . assertIn ( key , json ) <nl> + if isinstance ( val , list ) : <nl> + self . assertItemsEqual ( val , json [ key ] ) # we don ' t care about the order of the items <nl> + else : <nl> + self . assertEqual ( val , json [ key ] ) <nl> + new_streams = self . get_streams ( email ) <nl> + self . assertItemsEqual ( new_streams , new_subs ) <nl> + <nl> + def test_successful_subscriptions_add ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / add should successfully add streams , and <nl> + should determine which are new subscriptions vs which were already <nl> + subscribed . We randomly generate stream names to add , because it <nl> + doesn ' t matter whether the stream already exists . <nl> + \" \" \" <nl> + self . assertNotEqual ( len ( self . streams ) , 0 ) # necessary for full test coverage <nl> + add_streams = self . make_random_stream_names ( self . streams , self . streams ) <nl> + self . assertNotEqual ( len ( add_streams ) , 0 ) # necessary for full test coverage <nl> + self . helper_check_subs_before_and_after ( \" / json / subscriptions / add \" , self . streams + add_streams , { } , <nl> + { \" subscribed \" : add_streams , <nl> + \" already_subscribed \" : self . streams } , <nl> + self . test_email , self . streams + add_streams ) <nl> + <nl> + def test_subscriptions_add_too_long ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / add on a stream whose name is > 30 <nl> + characters should return a JSON error . <nl> + \" \" \" <nl> + # character limit is 30 characters <nl> + long_stream_name = \" a \" * 31 <nl> + result = self . client . post ( \" / json / subscriptions / add \" , <nl> + { \" subscriptions \" : simplejson . dumps ( [ long_stream_name ] ) } ) <nl> + self . assert_json_error ( result , <nl> + \" Stream name ( % s ) too long . \" % ( long_stream_name , ) ) <nl> + <nl> + def test_subscriptions_add_invalid_stream ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / add on a stream whose name is invalid ( as <nl> + defined by valid_stream_name in zephyr / views . py ) should return a JSON <nl> + error . <nl> + \" \" \" <nl> + # currently , the only invalid name is the empty string <nl> + invalid_stream_name = \" \" <nl> + result = self . client . post ( \" / json / subscriptions / add \" , <nl> + { \" subscriptions \" : simplejson . dumps ( [ invalid_stream_name ] ) } ) <nl> + self . assert_json_error ( result , <nl> + \" Invalid stream name ( % s ) . \" % ( invalid_stream_name , ) ) <nl> + <nl> + def test_subscriptions_add_for_principal ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / add on behalf of another principal ( for <nl> + whom you have permission to add subscriptions ) should successfully add <nl> + those subscriptions and send a message to the subscribee notifying <nl> + them . <nl> + \" \" \" <nl> + other_email = \" iago @ humbughq . com \" <nl> + other_profile = UserProfile . objects . get ( user__email = other_email ) <nl> + self . assertIsInstance ( other_profile , UserProfile ) <nl> + current_streams = self . get_streams ( other_email ) <nl> + self . assertNotEqual ( len ( current_streams ) , 0 ) # necessary for full test coverage <nl> + add_streams = self . make_random_stream_names ( current_streams , current_streams ) <nl> + self . assertNotEqual ( len ( add_streams ) , 0 ) # necessary for full test coverage <nl> + streams_to_sub = add_streams [ : 1 ] # just add one , to make the message easier to check <nl> + streams_to_sub . extend ( current_streams ) <nl> + self . helper_check_subs_before_and_after ( \" / json / subscriptions / add \" , streams_to_sub , <nl> + { \" principal \" : other_email } , <nl> + { \" subscribed \" : add_streams [ : 1 ] , <nl> + \" already_subscribed \" : current_streams } , <nl> + other_email , streams_to_sub ) <nl> + # verify that the user was sent a message informing them about the subscription <nl> + msg = Message . objects . latest ( ' id ' ) <nl> + self . assertEqual ( msg . recipient . type , msg . recipient . PERSONAL ) <nl> + self . assertEqual ( msg . sender_id , UserProfile . objects . get ( user__email = \" humbug + notifications @ humbughq . com \" ) . id ) <nl> + expected_msg = ( \" Hi there ! We thought you ' d like to know that % s just \" <nl> + \" subscribed you to stream ' % s ' \" <nl> + % ( self . user_profile . full_name , add_streams [ 0 ] ) ) <nl> + self . assertEqual ( msg . content , expected_msg ) <nl> + recipients = get_display_recipient ( msg . recipient ) <nl> + self . assertEqual ( len ( recipients ) , 1 ) <nl> + self . assertEqual ( recipients [ 0 ] [ ' email ' ] , other_email ) <nl> + <nl> + def test_subscription_add_invalid_principal ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / add on behalf of a principal that does not <nl> + exist should return a JSON error . <nl> + \" \" \" <nl> + invalid_principal = \" rosencrantz - and - guildenstern @ humbughq . com \" <nl> + # verify that invalid_principal actually doesn ' t exist <nl> + with self . assertRaises ( UserProfile . DoesNotExist ) : <nl> + UserProfile . objects . get ( user__email = invalid_principal ) <nl> + result = self . client . post ( \" / json / subscriptions / add \" , <nl> + { \" subscriptions \" : simplejson . dumps ( self . streams ) , <nl> + \" principal \" : invalid_principal } ) <nl> + self . assert_json_error ( result , \" User not authorized to execute queries on behalf of ' % s ' \" <nl> + % ( invalid_principal , ) ) <nl> + <nl> + def test_subscription_add_principal_other_realm ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / add on behalf of a principal in another <nl> + realm should return a JSON error . <nl> + \" \" \" <nl> + principal = \" starnine @ mit . edu \" <nl> + profile = UserProfile . objects . get ( user__email = principal ) <nl> + # verify that principal exists ( thus , the reason for the error is the cross - realming ) <nl> + self . assertIsInstance ( profile , UserProfile ) <nl> + result = self . client . post ( \" / json / subscriptions / add \" , <nl> + { \" subscriptions \" : simplejson . dumps ( self . streams ) , <nl> + \" principal \" : principal } ) <nl> + self . assert_json_error ( result , \" User not authorized to execute queries on behalf of ' % s ' \" <nl> + % ( principal , ) ) <nl> + <nl> + def test_successful_subscriptions_remove ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / remove should successfully remove streams , <nl> + and should determine which were removed vs which weren ' t subscribed to . <nl> + We cannot randomly generate stream names because the remove code <nl> + verifies whether streams exist . <nl> + \" \" \" <nl> + if len ( self . streams ) < 2 : <nl> + self . fail ( ) # necesssary for full test coverage <nl> + streams_to_remove = self . streams [ 1 : ] <nl> + not_subbed = [ ] <nl> + for stream in Stream . objects . all ( ) : <nl> + if not stream . name in self . streams : <nl> + not_subbed . append ( stream . name ) <nl> + random . shuffle ( not_subbed ) <nl> + self . assertNotEqual ( len ( not_subbed ) , 0 ) # necessary for full test coverage <nl> + try_to_remove = not_subbed [ : 3 ] # attempt to remove up to 3 streams not already subbed to <nl> + streams_to_remove . extend ( try_to_remove ) <nl> + self . helper_check_subs_before_and_after ( \" / json / subscriptions / remove \" , streams_to_remove , { } , <nl> + { \" removed \" : self . streams [ 1 : ] , <nl> + \" not_subscribed \" : try_to_remove } , <nl> + self . test_email , [ self . streams [ 0 ] ] ) <nl> + <nl> + def test_subscriptions_remove_fake_stream ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / remove on a stream that doesn ' t exist <nl> + should return a JSON error . <nl> + \" \" \" <nl> + all_stream_names = [ stream . name for stream in Stream . objects . filter ( realm = self . realm ) ] <nl> + random_streams = self . make_random_stream_names ( self . streams , all_stream_names ) <nl> + self . assertNotEqual ( len ( random_streams ) , 0 ) # necessary for full test coverage <nl> + streams_to_remove = random_streams [ : 1 ] # pick only one fake stream , to make checking the error message easy <nl> + result = self . client . post ( \" / json / subscriptions / remove \" , <nl> + { \" subscriptions \" : simplejson . dumps ( streams_to_remove ) } ) <nl> + self . assert_json_error ( result , \" Stream % s does not exist \" % ( random_streams [ 0 ] , ) ) <nl> + <nl> + def helper_subscriptions_exists ( self , stream , exists , subscribed ) : <nl> + \" \" \" <nl> + A helper function that calls / json / subscriptions / exists on a stream and <nl> + verifies that the returned JSON dictionary has the exists and <nl> + subscribed values passed in as parameters . ( If subscribed should not be <nl> + present , pass in None . ) <nl> + \" \" \" <nl> + result = self . client . post ( \" / json / subscriptions / exists \" , <nl> + { \" stream \" : stream } ) <nl> + self . assert_json_success ( result ) <nl> + json = simplejson . loads ( result . content ) <nl> + self . assertIn ( \" exists \" , json ) <nl> + self . assertEqual ( json [ \" exists \" ] , exists ) <nl> + if not subscribed is None : <nl> + self . assertIn ( \" subscribed \" , json ) <nl> + self . assertEqual ( json [ \" subscribed \" ] , subscribed ) <nl> + <nl> + def test_successful_subscriptions_exists_subbed ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / exist on a stream to which you are subbed <nl> + should return that it exists and that you are subbed . <nl> + \" \" \" <nl> + self . assertNotEqual ( len ( self . streams ) , 0 ) # necessary for full test coverage <nl> + self . helper_subscriptions_exists ( self . streams [ 0 ] , True , True ) <nl> + <nl> + def test_successful_subscriptions_exists_not_subbed ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / exist on a stream to which you are not <nl> + subbed should return that it exists and that you are not subbed . <nl> + \" \" \" <nl> + all_stream_names = [ stream . name for stream in Stream . objects . filter ( realm = self . realm ) ] <nl> + streams_not_subbed = list ( set ( all_stream_names ) - set ( self . streams ) ) <nl> + self . assertNotEqual ( len ( streams_not_subbed ) , 0 ) # necessary for full test coverage <nl> + self . helper_subscriptions_exists ( streams_not_subbed [ 0 ] , True , False ) <nl> + <nl> + def test_subscriptions_does_not_exist ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / exist on a stream that doesn ' t exist should <nl> + return that it doesn ' t exist . <nl> + \" \" \" <nl> + all_stream_names = [ stream . name for stream in Stream . objects . filter ( realm = self . realm ) ] <nl> + random_streams = self . make_random_stream_names ( self . streams , all_stream_names ) <nl> + self . assertNotEqual ( len ( random_streams ) , 0 ) # necessary for full test coverage <nl> + self . helper_subscriptions_exists ( random_streams [ 0 ] , False , None ) <nl> + <nl> + def test_subscriptions_exist_invalid_name ( self ) : <nl> + \" \" \" <nl> + Calling / json / subscriptions / exist on a stream whose name is invalid ( as <nl> + defined by valid_stream_name in zephyr / views . py ) should return a JSON <nl> + error . <nl> + \" \" \" <nl> + # currently , the only invalid stream name is the empty string <nl> + invalid_stream_name = \" \" <nl> + result = self . client . post ( \" / json / subscriptions / exists \" , <nl> + { \" stream \" : invalid_stream_name } ) <nl> + self . assert_json_error ( result , \" Invalid characters in stream name \" ) <nl> + <nl> class GetOldMessagesTest ( AuthedTestCase ) : <nl> fixtures = [ ' messages . json ' ] <nl> <nl>\n", "msg": "Added unit tests for several more methods :\n"}
{"diff_id": 42478, "repo": "scrapy/scrapy\n", "sha": "55edf8d3b8885541cdbf9d1c62d9a6bbf634e2a0\n", "time": "2020-08-17T08:50:52Z\n", "diff": "mmm a / scrapy / downloadermiddlewares / httpcache . py <nl> ppp b / scrapy / downloadermiddlewares / httpcache . py <nl> <nl> from email . utils import formatdate <nl> + from typing import Optional , Type , TypeVar <nl> <nl> from twisted . internet import defer <nl> from twisted . internet . error import ( <nl> <nl> from twisted . web . client import ResponseFailed <nl> <nl> from scrapy import signals <nl> + from scrapy . crawler import Crawler <nl> from scrapy . exceptions import IgnoreRequest , NotConfigured <nl> + from scrapy . http . request import Request <nl> + from scrapy . http . response import Response <nl> + from scrapy . settings import Settings <nl> + from scrapy . spiders import Spider <nl> + from scrapy . statscollectors import StatsCollector <nl> from scrapy . utils . misc import load_object <nl> <nl> <nl> + HttpCacheMiddlewareTV = TypeVar ( \" HttpCacheMiddlewareTV \" , bound = \" HttpCacheMiddleware \" ) <nl> + <nl> + <nl> class HttpCacheMiddleware : <nl> <nl> DOWNLOAD_EXCEPTIONS = ( defer . TimeoutError , TimeoutError , DNSLookupError , <nl> class HttpCacheMiddleware : <nl> ConnectionLost , TCPTimedOutError , ResponseFailed , <nl> IOError ) <nl> <nl> - def __init__ ( self , settings , stats ) : <nl> + def __init__ ( self , settings : Settings , stats : StatsCollector ) - > None : <nl> if not settings . getbool ( ' HTTPCACHE_ENABLED ' ) : <nl> raise NotConfigured <nl> self . policy = load_object ( settings [ ' HTTPCACHE_POLICY ' ] ) ( settings ) <nl> def __init__ ( self , settings , stats ) : <nl> self . stats = stats <nl> <nl> @ classmethod <nl> - def from_crawler ( cls , crawler ) : <nl> + def from_crawler ( cls : Type [ HttpCacheMiddlewareTV ] , crawler : Crawler ) - > HttpCacheMiddlewareTV : <nl> o = cls ( crawler . settings , crawler . stats ) <nl> crawler . signals . connect ( o . spider_opened , signal = signals . spider_opened ) <nl> crawler . signals . connect ( o . spider_closed , signal = signals . spider_closed ) <nl> return o <nl> <nl> - def spider_opened ( self , spider ) : <nl> + def spider_opened ( self , spider : Spider ) - > None : <nl> self . storage . open_spider ( spider ) <nl> <nl> - def spider_closed ( self , spider ) : <nl> + def spider_closed ( self , spider : Spider ) - > None : <nl> self . storage . close_spider ( spider ) <nl> <nl> - def process_request ( self , request , spider ) : <nl> + def process_request ( self , request : Request , spider : Spider ) - > Optional [ Response ] : <nl> if request . meta . get ( ' dont_cache ' , False ) : <nl> - return <nl> + return None <nl> <nl> # Skip uncacheable requests <nl> if not self . policy . should_cache_request ( request ) : <nl> request . meta [ ' _dont_cache ' ] = True # flag as uncacheable <nl> - return <nl> + return None <nl> <nl> # Look for cached response and check if expired <nl> cachedresponse = self . storage . retrieve_response ( spider , request ) <nl> def process_request ( self , request , spider ) : <nl> if self . ignore_missing : <nl> self . stats . inc_value ( ' httpcache / ignore ' , spider = spider ) <nl> raise IgnoreRequest ( \" Ignored request not in cache : % s \" % request ) <nl> - return # first time request <nl> + return None # first time request <nl> <nl> # Return cached response only if not expired <nl> cachedresponse . flags . append ( ' cached ' ) <nl> def process_request ( self , request , spider ) : <nl> # process_response hook <nl> request . meta [ ' cached_response ' ] = cachedresponse <nl> <nl> - def process_response ( self , request , response , spider ) : <nl> + return None <nl> + <nl> + def process_response ( self , request : Request , response : Response , spider : Spider ) - > Response : <nl> if request . meta . get ( ' dont_cache ' , False ) : <nl> return response <nl> <nl> def process_response ( self , request , response , spider ) : <nl> # RFC2616 requires origin server to set Date header , <nl> # https : / / www . w3 . org / Protocols / rfc2616 / rfc2616 - sec14 . html # sec14 . 18 <nl> if ' Date ' not in response . headers : <nl> - response . headers [ ' Date ' ] = formatdate ( usegmt = 1 ) <nl> + response . headers [ ' Date ' ] = formatdate ( usegmt = True ) <nl> <nl> # Do not validate first - hand responses <nl> cachedresponse = request . meta . pop ( ' cached_response ' , None ) <nl> def process_response ( self , request , response , spider ) : <nl> self . _cache_response ( spider , response , request , cachedresponse ) <nl> return response <nl> <nl> - def process_exception ( self , request , exception , spider ) : <nl> + def process_exception ( <nl> + self , request : Request , exception : Exception , spider : Spider <nl> + ) - > Optional [ Response ] : <nl> cachedresponse = request . meta . pop ( ' cached_response ' , None ) <nl> if cachedresponse is not None and isinstance ( exception , self . DOWNLOAD_EXCEPTIONS ) : <nl> self . stats . inc_value ( ' httpcache / errorrecovery ' , spider = spider ) <nl> return cachedresponse <nl> + return None <nl> <nl> - def _cache_response ( self , spider , response , request , cachedresponse ) : <nl> + def _cache_response ( <nl> + self , spider : Spider , response : Response , request : Request , cachedresponse : Optional [ Response ] <nl> + ) - > None : <nl> if self . policy . should_cache_response ( response , request ) : <nl> self . stats . inc_value ( ' httpcache / store ' , spider = spider ) <nl> self . storage . store_response ( spider , request , response ) <nl>\n", "msg": "Add typing hint to httpcache downloadermiddlewares ( )\n"}
{"diff_id": 42585, "repo": "scikit-learn/scikit-learn\n", "sha": "e58764feabb61aca5af3f9d0ab4a73ed4380d937\n", "time": "2014-08-19T14:06:00Z\n", "diff": "mmm a / sklearn / datasets / samples_generator . py <nl> ppp b / sklearn / datasets / samples_generator . py <nl> def make_multilabel_classification ( n_samples = 100 , n_features = 20 , n_classes = 5 , <nl> The number of classes of the classification problem . <nl> <nl> n_labels : int , optional ( default = 2 ) <nl> - The average number of labels per instance . Number of labels follows <nl> - a Poisson distribution that never takes the value 0 . <nl> + The average number of labels per instance . More precisely , the number <nl> + of labels per sample is drawn from a Poisson distribution with <nl> + ` ` n_labels ` ` as its expected value , but samples are bounded ( using <nl> + rejection sampling ) by ` ` n_classes ` ` , and must be nonzero if <nl> + ` ` allow_unlabeled ` ` is False . <nl> <nl> length : int , optional ( default = 50 ) <nl> - Sum of the features ( number of words if documents ) . <nl> + The sum of the features ( number of words if documents ) is drawn from <nl> + a Poisson distribution with this expected value . <nl> <nl> allow_unlabeled : bool , optional ( default = True ) <nl> If ` ` True ` ` , some instances might not belong to any class . <nl>\n", "msg": "DOC more explicit parameter descriptions in make_multilabel_classification\n"}
{"diff_id": 42703, "repo": "explosion/spaCy\n", "sha": "8f7eeb1c2d9636598da6def5495f6821c19e71a7\n", "time": "2015-03-26T15:44:45Z\n", "diff": "mmm a / bin / parser / train . py <nl> ppp b / bin / parser / train . py <nl> def train ( Language , train_loc , model_dir , n_iter = 15 , feat_set = u ' basic ' , seed = 0 , <nl> nlp = Language ( ) <nl> ent_strings = [ None ] * ( max ( nlp . entity . moves . label_ids . values ( ) ) + 1 ) <nl> for label , i in nlp . entity . moves . label_ids . items ( ) : <nl> - ent_strings [ i ] = label <nl> + if i > = 0 : <nl> + ent_strings [ i ] = label <nl> <nl> print \" Itn . \\ tUAS \\ tNER F . \\ tTag % \" <nl> for itn in range ( n_iter ) : <nl> def train ( Language , train_loc , model_dir , n_iter = 15 , feat_set = u ' basic ' , seed = 0 , <nl> nlp . tagger . model . end_training ( ) <nl> <nl> <nl> - def evaluate ( Language , dev_loc , model_dir , gold_preproc = False ) : <nl> - global loss <nl> + def evaluate ( Language , dev_loc , model_dir , gold_preproc = False , verbose = False ) : <nl> assert not gold_preproc <nl> nlp = Language ( ) <nl> gold_tuples = read_docparse_file ( dev_loc ) <nl> def evaluate ( Language , dev_loc , model_dir , gold_preproc = False ) : <nl> for raw_text , segmented_text , annot_tuples in gold_tuples : <nl> tokens = nlp ( raw_text ) <nl> gold = GoldParse ( tokens , annot_tuples ) <nl> - scorer . score ( tokens , gold , verbose = False ) <nl> + scorer . score ( tokens , gold , verbose = verbose ) <nl> return scorer <nl> <nl> <nl> - <nl> @ plac . annotations ( <nl> train_loc = ( \" Training file location \" , ) , <nl> dev_loc = ( \" Dev . file location \" , ) , <nl> def evaluate ( Language , dev_loc , model_dir , gold_preproc = False ) : <nl> def main ( train_loc , dev_loc , model_dir , n_sents = 0 ) : <nl> train ( English , train_loc , model_dir , <nl> gold_preproc = False , force_gold = False , n_sents = n_sents ) <nl> - scorer = evaluate ( English , dev_loc , model_dir , gold_preproc = False ) <nl> + scorer = evaluate ( English , dev_loc , model_dir , gold_preproc = False , verbose = False ) <nl> print ' POS ' , scorer . tags_acc <nl> print ' UAS ' , scorer . uas <nl> print ' LAS ' , scorer . las <nl>\n", "msg": "* Add verbose flag for Scorer , for debugging , and fix ent_strings bug\n"}
{"diff_id": 42759, "repo": "zulip/zulip\n", "sha": "ff34d07fa05282d0e1041354764ea9a9623e5d62\n", "time": "2018-04-02T05:29:23Z\n", "diff": "mmm a / zerver / lib / export . py <nl> ppp b / zerver / lib / export . py <nl> def fix_realm_authentication_bitfield ( data : TableData , table : TableName , field_n <nl> values_as_int = int ( values_as_bitstring , 2 ) <nl> item [ field_name ] = values_as_int <nl> <nl> + def update_model_ids ( model : Any , data : TableData , table : TableName , related_table : TableName ) - > None : <nl> + old_id_list = current_table_ids ( data , table ) <nl> + allocated_id_list = allocate_ids ( model , len ( data [ table ] ) ) <nl> + for item in range ( len ( data [ table ] ) ) : <nl> + update_id_map ( related_table , old_id_list [ item ] , allocated_id_list [ item ] ) <nl> + re_map_foreign_keys ( data [ table ] , ' id ' , related_table = related_table , id_field = True ) <nl> + <nl> def bulk_import_model ( data : TableData , model : Any , table : TableName , <nl> dump_file_id : Optional [ str ] = None ) - > None : <nl> # TODO , deprecate dump_file_id <nl> def do_import_realm ( import_dir : Path ) - > Realm : <nl> with open ( realm_data_filename ) as f : <nl> data = ujson . load ( f ) <nl> <nl> - stream_id_list = current_table_ids ( data , ' zerver_stream ' ) <nl> - allocated_stream_id_list = allocate_ids ( Stream , len ( data [ ' zerver_stream ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_stream ' ] ) ) : <nl> - update_id_map ( ' stream ' , stream_id_list [ item ] , allocated_stream_id_list [ item ] ) <nl> + update_model_ids ( Stream , data , ' zerver_stream ' , ' stream ' ) <nl> re_map_foreign_keys ( data [ ' zerver_realm ' ] , ' notifications_stream ' , related_table = \" stream \" ) <nl> <nl> fix_datetime_fields ( data , ' zerver_realm ' ) <nl> fix_realm_authentication_bitfield ( data , ' zerver_realm ' , ' authentication_methods ' ) <nl> - realm_id_list = current_table_ids ( data , ' zerver_realm ' ) <nl> - allocated_realm_id_list = allocate_ids ( Realm , len ( data [ ' zerver_realm ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_realm ' ] ) ) : <nl> - update_id_map ( ' realm ' , realm_id_list [ item ] , allocated_realm_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_realm ' ] , ' id ' , related_table = \" realm \" , id_field = True ) <nl> + update_model_ids ( Realm , data , ' zerver_realm ' , ' realm ' ) <nl> <nl> realm = Realm ( * * data [ ' zerver_realm ' ] [ 0 ] ) <nl> if realm . notifications_stream_id is not None : <nl> def do_import_realm ( import_dir : Path ) - > Realm : <nl> # Stream objects are created by Django . <nl> fix_datetime_fields ( data , ' zerver_stream ' ) <nl> re_map_foreign_keys ( data [ ' zerver_stream ' ] , ' realm ' , related_table = \" realm \" ) <nl> - re_map_foreign_keys ( data [ ' zerver_stream ' ] , ' id ' , related_table = \" stream \" , id_field = True ) <nl> bulk_import_model ( data , Stream , ' zerver_stream ' ) <nl> <nl> realm . notifications_stream_id = notifications_stream_id <nl> def do_import_realm ( import_dir : Path ) - > Realm : <nl> re_map_foreign_keys ( data [ ' zerver_realmemoji ' ] , ' author ' , related_table = \" user_profile \" ) <nl> for ( table , model , related_table ) in realm_tables : <nl> re_map_foreign_keys ( data [ table ] , ' realm ' , related_table = \" realm \" ) <nl> - old_id_list = current_table_ids ( data , table ) <nl> - allocated_id_list = allocate_ids ( model , len ( data [ table ] ) ) <nl> - for item in range ( len ( data [ table ] ) ) : <nl> - update_id_map ( related_table , old_id_list [ item ] , allocated_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ table ] , ' id ' , related_table = related_table , id_field = True ) <nl> + update_model_ids ( model , data , table , related_table ) <nl> bulk_import_model ( data , model , table ) <nl> <nl> # Remap the user IDs for notification_bot and friends to their <nl> def do_import_realm ( import_dir : Path ) - > Realm : <nl> del user_profile_dict [ ' user_permissions ' ] <nl> del user_profile_dict [ ' groups ' ] <nl> <nl> - user_id_list = current_table_ids ( data , ' zerver_userprofile ' ) <nl> - allocated_user_id_list = allocate_ids ( UserProfile , len ( data [ ' zerver_userprofile ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_userprofile ' ] ) ) : <nl> - update_id_map ( ' user_profile ' , user_id_list [ item ] , allocated_user_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_userprofile ' ] , ' id ' , related_table = \" user_profile \" , id_field = True ) <nl> + update_model_ids ( UserProfile , data , ' zerver_userprofile ' , ' user_profile ' ) <nl> <nl> user_profiles = [ UserProfile ( * * item ) for item in data [ ' zerver_userprofile ' ] ] <nl> for user_profile in user_profiles : <nl> def do_import_realm ( import_dir : Path ) - > Realm : <nl> recipient_field = True , id_field = True ) <nl> re_map_foreign_keys ( data [ ' zerver_recipient ' ] , ' type_id ' , related_table = \" user_profile \" , <nl> recipient_field = True , id_field = True ) <nl> - recipient_id_list = current_table_ids ( data , ' zerver_recipient ' ) <nl> - allocated_recipient_id_list = allocate_ids ( Recipient , len ( data [ ' zerver_recipient ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_recipient ' ] ) ) : <nl> - update_id_map ( ' recipient ' , recipient_id_list [ item ] , allocated_recipient_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_recipient ' ] , ' id ' , related_table = \" recipient \" , id_field = True ) <nl> + update_model_ids ( Recipient , data , ' zerver_recipient ' , ' recipient ' ) <nl> bulk_import_model ( data , Recipient , ' zerver_recipient ' ) <nl> <nl> re_map_foreign_keys ( data [ ' zerver_subscription ' ] , ' user_profile ' , related_table = \" user_profile \" ) <nl> re_map_foreign_keys ( data [ ' zerver_subscription ' ] , ' recipient ' , related_table = \" recipient \" ) <nl> - subscription_id_list = current_table_ids ( data , ' zerver_subscription ' ) <nl> - allocated_subscription_id_list = allocate_ids ( Subscription , len ( data [ ' zerver_subscription ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_subscription ' ] ) ) : <nl> - update_id_map ( ' subscription ' , subscription_id_list [ item ] , <nl> - allocated_subscription_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_subscription ' ] , ' id ' , related_table = \" subscription \" , id_field = True ) <nl> + update_model_ids ( Subscription , data , ' zerver_subscription ' , ' subscription ' ) <nl> bulk_import_model ( data , Subscription , ' zerver_subscription ' ) <nl> <nl> fix_datetime_fields ( data , ' zerver_userpresence ' ) <nl> re_map_foreign_keys ( data [ ' zerver_userpresence ' ] , ' user_profile ' , related_table = \" user_profile \" ) <nl> re_map_foreign_keys ( data [ ' zerver_userpresence ' ] , ' client ' , related_table = ' client ' ) <nl> - userpresence_id_list = current_table_ids ( data , ' zerver_userpresence ' ) <nl> - allocated_userpresence_id_list = allocate_ids ( UserPresence , len ( data [ ' zerver_userpresence ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_userpresence ' ] ) ) : <nl> - update_id_map ( ' user_presence ' , userpresence_id_list [ item ] , <nl> - allocated_userpresence_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_userpresence ' ] , ' id ' , related_table = \" user_presence \" , id_field = True ) <nl> + update_model_ids ( UserPresence , data , ' zerver_userpresence ' , ' user_presence ' ) <nl> bulk_import_model ( data , UserPresence , ' zerver_userpresence ' ) <nl> <nl> fix_datetime_fields ( data , ' zerver_useractivity ' ) <nl> re_map_foreign_keys ( data [ ' zerver_useractivity ' ] , ' user_profile ' , related_table = \" user_profile \" ) <nl> re_map_foreign_keys ( data [ ' zerver_useractivity ' ] , ' client ' , related_table = ' client ' ) <nl> - useractivity_id_list = current_table_ids ( data , ' zerver_useractivity ' ) <nl> - allocated_useractivity_id_list = allocate_ids ( UserActivity , len ( data [ ' zerver_useractivity ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_useractivity ' ] ) ) : <nl> - update_id_map ( ' useractivity ' , useractivity_id_list [ item ] , <nl> - allocated_useractivity_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_useractivity ' ] , ' id ' , related_table = \" useractivity \" , id_field = True ) <nl> + update_model_ids ( UserActivity , data , ' zerver_useractivity ' , ' useractivity ' ) <nl> bulk_import_model ( data , UserActivity , ' zerver_useractivity ' ) <nl> <nl> fix_datetime_fields ( data , ' zerver_useractivityinterval ' ) <nl> re_map_foreign_keys ( data [ ' zerver_useractivityinterval ' ] , ' user_profile ' , related_table = \" user_profile \" ) <nl> - useractivityinterval_id_list = current_table_ids ( data , ' zerver_useractivityinterval ' ) <nl> - allocated_useractivityinterval_id_list = allocate_ids ( UserActivityInterval , <nl> - len ( data [ ' zerver_useractivityinterval ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_useractivityinterval ' ] ) ) : <nl> - update_id_map ( ' useractivityinterval ' , useractivityinterval_id_list [ item ] , <nl> - allocated_useractivityinterval_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_useractivityinterval ' ] , ' id ' , related_table = \" useractivityinterval \" , <nl> - id_field = True ) <nl> + update_model_ids ( UserActivityInterval , data , ' zerver_useractivityinterval ' , <nl> + ' useractivityinterval ' ) <nl> bulk_import_model ( data , UserActivityInterval , ' zerver_useractivityinterval ' ) <nl> <nl> # Import uploaded files and avatars <nl> def import_message_data ( import_dir : Path ) - > None : <nl> re_map_foreign_keys ( data [ ' zerver_message ' ] , ' recipient ' , related_table = \" recipient \" ) <nl> re_map_foreign_keys ( data [ ' zerver_message ' ] , ' sending_client ' , related_table = ' client ' ) <nl> fix_datetime_fields ( data , ' zerver_message ' ) <nl> - message_id_list = current_table_ids ( data , ' zerver_message ' ) <nl> - allocated_message_id_list = allocate_ids ( Message , len ( data [ ' zerver_message ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_message ' ] ) ) : <nl> - update_id_map ( ' message ' , message_id_list [ item ] , allocated_message_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_message ' ] , ' id ' , related_table = \" message \" , id_field = True ) <nl> + update_model_ids ( Message , data , ' zerver_message ' , ' message ' ) <nl> bulk_import_model ( data , Message , ' zerver_message ' ) <nl> <nl> # Due to the structure of these message chunks , we ' re <nl> def import_message_data ( import_dir : Path ) - > None : <nl> re_map_foreign_keys ( data [ ' zerver_usermessage ' ] , ' message ' , related_table = \" message \" ) <nl> re_map_foreign_keys ( data [ ' zerver_usermessage ' ] , ' user_profile ' , related_table = \" user_profile \" ) <nl> fix_bitfield_keys ( data , ' zerver_usermessage ' , ' flags ' ) <nl> - usermessage_id_list = current_table_ids ( data , ' zerver_usermessage ' ) <nl> - allocated_usermessage_id_list = allocate_ids ( UserMessage , len ( data [ ' zerver_usermessage ' ] ) ) <nl> - for item in range ( len ( data [ ' zerver_usermessage ' ] ) ) : <nl> - update_id_map ( ' usermessage ' , usermessage_id_list [ item ] , <nl> - allocated_usermessage_id_list [ item ] ) <nl> - re_map_foreign_keys ( data [ ' zerver_usermessage ' ] , ' id ' , related_table = \" usermessage \" , <nl> - id_field = True ) <nl> + update_model_ids ( UserMessage , data , ' zerver_usermessage ' , ' usermessage ' ) <nl> bulk_import_model ( data , UserMessage , ' zerver_usermessage ' ) <nl> <nl> dump_file_id + = 1 <nl>\n", "msg": "import script : Add function to update model ids after allocation .\n"}
{"diff_id": 42815, "repo": "ansible/ansible\n", "sha": "12db261d50151ba2679d476c7406e124e20981ce\n", "time": "2016-12-08T16:32:38Z\n", "diff": "mmm a / lib / ansible / modules / extras / network / f5 / bigip_pool_member . py <nl> ppp b / lib / ansible / modules / extras / network / f5 / bigip_pool_member . py <nl> <nl> default : present <nl> choices : [ ' present ' , ' absent ' ] <nl> aliases : [ ] <nl> + session_state : <nl> + description : <nl> + - Set new session availability status for pool member <nl> + version_added : \" 1 . 9 \" <nl> + required : false <nl> + default : null <nl> + choices : [ ' enabled ' , ' disabled ' ] <nl> + aliases : [ ] <nl> + monitor_state : <nl> + description : <nl> + - Set monitor availability status for pool member <nl> + version_added : \" 1 . 9 \" <nl> + required : false <nl> + default : null <nl> + choices : [ ' enabled ' , ' disabled ' ] <nl> + aliases : [ ] <nl> pool : <nl> description : <nl> - Pool name . This pool must exist . <nl> <nl> host = \" { { ansible_default_ipv4 [ \" address \" ] } } \" <nl> port = 80 <nl> <nl> + <nl> + # The BIG - IP GUI doesn ' t map directly to the API calls for \" Pool - > <nl> + # Members - > State \" . The following states map to API monitor <nl> + # and session states . <nl> + # <nl> + # Enabled ( all traffic allowed ) : <nl> + # monitor_state = enabled , session_state = enabled <nl> + # Disabled ( only persistent or active connections allowed ) : <nl> + # monitor_state = enabled , session_state = disabled <nl> + # Forced offline ( only active connections allowed ) : <nl> + # monitor_state = disabled , session_state = disabled <nl> + # <nl> + # See https : / / devcentral . f5 . com / questions / icontrol - equivalent - call - for - b - node - down <nl> + <nl> + - name : Force pool member offline <nl> + local_action : > <nl> + bigip_pool_member <nl> + server = lb . mydomain . com <nl> + user = admin <nl> + password = mysecret <nl> + state = present <nl> + session_state = disabled <nl> + monitor_state = disabled <nl> + pool = matthite - pool <nl> + partition = matthite <nl> + host = \" { { ansible_default_ipv4 [ \" address \" ] } } \" <nl> + port = 80 <nl> + <nl> ' ' ' <nl> <nl> try : <nl> def set_ratio ( api , pool , address , port , ratio ) : <nl> members = [ { ' address ' : address , ' port ' : port } ] <nl> api . LocalLB . Pool . set_member_ratio ( pool_names = [ pool ] , members = [ members ] , ratios = [ [ ratio ] ] ) <nl> <nl> + def set_member_session_enabled_state ( api , pool , address , port , session_state ) : <nl> + members = [ { ' address ' : address , ' port ' : port } ] <nl> + session_state = [ \" STATE_ % s \" % session_state . strip ( ) . upper ( ) ] <nl> + api . LocalLB . Pool . set_member_session_enabled_state ( pool_names = [ pool ] , members = [ members ] , session_states = [ session_state ] ) <nl> + <nl> + def get_member_session_status ( api , pool , address , port ) : <nl> + members = [ { ' address ' : address , ' port ' : port } ] <nl> + result = api . LocalLB . Pool . get_member_session_status ( pool_names = [ pool ] , members = [ members ] ) [ 0 ] [ 0 ] <nl> + result = result . split ( \" SESSION_STATUS_ \" ) [ - 1 ] . lower ( ) <nl> + return result <nl> + <nl> + def set_member_monitor_state ( api , pool , address , port , monitor_state ) : <nl> + members = [ { ' address ' : address , ' port ' : port } ] <nl> + monitor_state = [ \" STATE_ % s \" % monitor_state . strip ( ) . upper ( ) ] <nl> + api . LocalLB . Pool . set_member_monitor_state ( pool_names = [ pool ] , members = [ members ] , monitor_states = [ monitor_state ] ) <nl> + <nl> + def get_member_monitor_status ( api , pool , address , port ) : <nl> + members = [ { ' address ' : address , ' port ' : port } ] <nl> + result = api . LocalLB . Pool . get_member_monitor_status ( pool_names = [ pool ] , members = [ members ] ) [ 0 ] [ 0 ] <nl> + result = result . split ( \" MONITOR_STATUS_ \" ) [ - 1 ] . lower ( ) <nl> + return result <nl> + <nl> def main ( ) : <nl> module = AnsibleModule ( <nl> argument_spec = dict ( <nl> def main ( ) : <nl> password = dict ( type = ' str ' , required = True ) , <nl> validate_certs = dict ( default = ' yes ' , type = ' bool ' ) , <nl> state = dict ( type = ' str ' , default = ' present ' , choices = [ ' present ' , ' absent ' ] ) , <nl> + session_state = dict ( type = ' str ' , choices = [ ' enabled ' , ' disabled ' ] ) , <nl> + monitor_state = dict ( type = ' str ' , choices = [ ' enabled ' , ' disabled ' ] ) , <nl> pool = dict ( type = ' str ' , required = True ) , <nl> partition = dict ( type = ' str ' , default = ' Common ' ) , <nl> host = dict ( type = ' str ' , required = True , aliases = [ ' address ' , ' name ' ] ) , <nl> def main ( ) : <nl> password = module . params [ ' password ' ] <nl> validate_certs = module . params [ ' validate_certs ' ] <nl> state = module . params [ ' state ' ] <nl> + session_state = module . params [ ' session_state ' ] <nl> + monitor_state = module . params [ ' monitor_state ' ] <nl> partition = module . params [ ' partition ' ] <nl> pool = \" / % s / % s \" % ( partition , module . params [ ' pool ' ] ) <nl> connection_limit = module . params [ ' connection_limit ' ] <nl> def main ( ) : <nl> set_rate_limit ( api , pool , address , port , rate_limit ) <nl> if ratio is not None : <nl> set_ratio ( api , pool , address , port , ratio ) <nl> + if session_state is not None : <nl> + set_member_session_enabled_state ( api , pool , address , port , session_state ) <nl> + if monitor_state is not None : <nl> + set_member_monitor_state ( api , pool , address , port , monitor_state ) <nl> result = { ' changed ' : True } <nl> else : <nl> # pool member exists - - potentially modify attributes <nl> def main ( ) : <nl> if not module . check_mode : <nl> set_ratio ( api , pool , address , port , ratio ) <nl> result = { ' changed ' : True } <nl> + if session_state is not None : <nl> + session_status = get_member_session_status ( api , pool , address , port ) <nl> + if session_state = = ' enabled ' and session_status = = ' forced_disabled ' : <nl> + if not module . check_mode : <nl> + set_member_session_enabled_state ( api , pool , address , port , session_state ) <nl> + result = { ' changed ' : True } <nl> + elif session_state = = ' disabled ' and session_status ! = ' force_disabled ' : <nl> + if not module . check_mode : <nl> + set_member_session_enabled_state ( api , pool , address , port , session_state ) <nl> + result = { ' changed ' : True } <nl> + if monitor_state is not None : <nl> + monitor_status = get_member_monitor_status ( api , pool , address , port ) <nl> + if monitor_state = = ' enabled ' and monitor_status = = ' forced_down ' : <nl> + if not module . check_mode : <nl> + set_member_monitor_state ( api , pool , address , port , monitor_state ) <nl> + result = { ' changed ' : True } <nl> + elif monitor_state = = ' disabled ' and monitor_status ! = ' forced_down ' : <nl> + if not module . check_mode : <nl> + set_member_monitor_state ( api , pool , address , port , monitor_state ) <nl> + result = { ' changed ' : True } <nl> <nl> except Exception , e : <nl> module . fail_json ( msg = \" received exception : % s \" % e ) <nl>\n", "msg": "Support for monitor and session state manipulation added to bigip_pool_member module\n"}
{"diff_id": 42845, "repo": "ytdl-org/youtube-dl\n", "sha": "509c630db8cdaff473f95805cda1ae350107e36b\n", "time": "2015-05-14T07:09:56Z\n", "diff": "mmm a / youtube_dl / extractor / canalplus . py <nl> ppp b / youtube_dl / extractor / canalplus . py <nl> class CanalplusIE ( InfoExtractor ) : <nl> } <nl> <nl> _TESTS = [ { <nl> - ' url ' : ' http : / / www . canalplus . fr / c - infos - documentaires / pid1830 - c - zapping . html ? vid = 922470 ' , <nl> - ' md5 ' : ' 3db39fb48b9685438ecf33a1078023e4 ' , <nl> + ' url ' : ' http : / / www . canalplus . fr / c - emissions / pid1830 - c - zapping . html ? vid = 1263092 ' , <nl> ' info_dict ' : { <nl> - ' id ' : ' 922470 ' , <nl> + ' id ' : ' 1263092 ' , <nl> ' ext ' : ' flv ' , <nl> - ' title ' : ' Zapping - 26 / 08 / 13 ' , <nl> - ' description ' : ' Le meilleur de toutes les cha\u00eenes , tous les jours . \\ nEmission du 26 ao\u00fbt 2013 ' , <nl> - ' upload_date ' : ' 20130826 ' , <nl> + ' title ' : ' Le Zapping - 13 / 05 / 15 ' , <nl> + ' description ' : ' md5 : 09738c0d06be4b5d06a0940edb0da73f ' , <nl> + ' upload_date ' : ' 20150513 ' , <nl> } , <nl> } , { <nl> ' url ' : ' http : / / www . piwiplus . fr / videos - piwi / pid1405 - le - labyrinthe - boing - super - ranger . html ? vid = 1108190 ' , <nl> class CanalplusIE ( InfoExtractor ) : <nl> ' skip ' : ' videos get deleted after a while ' , <nl> } , { <nl> ' url ' : ' http : / / www . itele . fr / france / video / aubervilliers - un - lycee - en - colere - 111559 ' , <nl> - ' md5 ' : ' 65aa83ad62fe107ce29e564bb8712580 ' , <nl> ' info_dict ' : { <nl> ' id ' : ' 1213714 ' , <nl> ' ext ' : ' flv ' , <nl>\n", "msg": "[ CanalplusIE ] Update tests that were no longer working\n"}
{"diff_id": 42968, "repo": "getredash/redash\n", "sha": "c850acb3b9ee7cdad4f63ba9dc2abb28af054d1a\n", "time": "2016-04-01T01:06:54Z\n", "diff": "mmm a / redash / query_runner / elasticsearch . py <nl> ppp b / redash / query_runner / elasticsearch . py <nl> def _get_mappings ( self , url ) : <nl> r = requests . get ( url , auth = self . auth ) <nl> mappings_data = r . json ( ) <nl> <nl> - logger . debug ( mappings_data ) <nl> - <nl> for index_name in mappings_data : <nl> index_mappings = mappings_data [ index_name ] <nl> for m in index_mappings . get ( \" mappings \" , { } ) : <nl> def add_column_if_needed ( mappings , column_name , friendly_name , result_columns , r <nl> \" type \" : mappings . get ( column_name , \" string \" ) } ) <nl> result_columns_index [ friendly_name ] = result_columns [ - 1 ] <nl> <nl> + def getRow ( rows , row ) : <nl> + if row = = None : <nl> + row = { } <nl> + rows . append ( row ) <nl> + return row <nl> + <nl> + def collect_value ( mappings , row , key , value , type ) : <nl> + if result_fields and key not in result_fields_index : <nl> + return <nl> + <nl> + mappings [ key ] = type <nl> + add_column_if_needed ( mappings , key , key , result_columns , result_columns_index ) <nl> + row [ key ] = value <nl> + <nl> + def collect_aggregations ( mappings , rows , parentKey , data , row , result_columns , result_columns_index ) : <nl> + <nl> + if isinstance ( data , dict ) : <nl> + <nl> + for key , value in data . iteritems ( ) : <nl> + val = collect_aggregations ( mappings , rows , parentKey if key = = ' buckets ' else key , value , row , result_columns , result_columns_index ) <nl> + if val : <nl> + row = getRow ( rows , row ) <nl> + collect_value ( mappings , row , key , val , ' long ' ) <nl> + <nl> + for dataKey in [ ' value ' , ' doc_count ' ] : <nl> + if not dataKey in data : <nl> + continue <nl> + if ' key ' in data and len ( data . keys ( ) ) = = 2 : <nl> + collect_value ( mappings , row , data [ ' key ' ] if not ' key_as_string ' in data else data [ ' key_as_string ' ] , data [ dataKey ] ) <nl> + else : <nl> + return data [ dataKey ] <nl> + <nl> + elif isinstance ( data , list ) : <nl> + <nl> + for value in data : <nl> + resultRow = getRow ( rows , row ) <nl> + collect_aggregations ( mappings , rows , parentKey , value , resultRow , result_columns , result_columns_index ) <nl> + if ' key ' in value : <nl> + collect_value ( mappings , resultRow , parentKey , value [ ' key ' ] , ' string ' ) <nl> + <nl> + return None <nl> + <nl> result_columns_index = { c [ \" name \" ] : c for c in result_columns } <nl> <nl> result_fields_index = { } <nl> def add_column_if_needed ( mappings , column_name , friendly_name , result_columns , r <nl> for r in result_fields : <nl> result_fields_index [ r ] = None <nl> <nl> - for h in raw_result [ \" hits \" ] [ \" hits \" ] : <nl> - row = { } <nl> + if ' error ' in raw_result : <nl> <nl> - for field , column in ELASTICSEARCH_BUILTIN_FIELDS_MAPPING . iteritems ( ) : <nl> - if field in h : <nl> - add_column_if_needed ( mappings , field , column , result_columns , result_columns_index ) <nl> - row [ column ] = h [ field ] <nl> + error = raw_result [ ' error ' ] <nl> + if len ( error ) > 10240 : <nl> + error = error [ : 10240 ] + ' . . . continues ' <nl> <nl> - column_name = \" _source \" if \" _source \" in h else \" fields \" <nl> - for column in h [ column_name ] : <nl> - if result_fields and column not in result_fields_index : <nl> - continue <nl> + raise Exception ( error ) <nl> <nl> - add_column_if_needed ( mappings , column , column , result_columns , result_columns_index ) <nl> + elif ' aggregations ' in raw_result : <nl> <nl> - value = h [ column_name ] [ column ] <nl> - row [ column ] = value [ 0 ] if isinstance ( value , list ) and len ( value ) = = 1 else value <nl> + if result_fields : <nl> + for field in result_fields : <nl> + add_column_if_needed ( mappings , field , field , result_columns , result_columns_index ) <nl> <nl> + for key , data in raw_result [ \" aggregations \" ] . iteritems ( ) : <nl> + collect_aggregations ( mappings , result_rows , key , data , None , result_columns , result_columns_index ) <nl> + <nl> + logger . debug ( \" result_rows \" , str ( result_rows ) ) <nl> + logger . debug ( \" result_columns \" , str ( result_columns ) ) <nl> + <nl> + elif ' hits ' in raw_result and ' hits ' in raw_result [ ' hits ' ] : <nl> + <nl> + if result_fields : <nl> + for field in result_fields : <nl> + add_column_if_needed ( mappings , field , field , result_columns , result_columns_index ) <nl> + <nl> + for h in raw_result [ \" hits \" ] [ \" hits \" ] : <nl> + row = { } <nl> + <nl> + column_name = \" _source \" if \" _source \" in h else \" fields \" <nl> + for column in h [ column_name ] : <nl> + if result_fields and column not in result_fields_index : <nl> + continue <nl> + <nl> + add_column_if_needed ( mappings , column , column , result_columns , result_columns_index ) <nl> + <nl> + value = h [ column_name ] [ column ] <nl> + row [ column ] = value [ 0 ] if isinstance ( value , list ) and len ( value ) = = 1 else value <nl> <nl> - if row and len ( row ) > 0 : <nl> result_rows . append ( row ) <nl> + else : <nl> + <nl> + raise Exception ( ' Seems you \\ ' ve hit an unsupported feature ' ) <nl> <nl> <nl> class Kibana ( BaseElasticSearch ) : <nl> def run_query ( self , query ) : <nl> query_dict = json . loads ( query ) <nl> <nl> index_name = query_dict . pop ( \" index \" , \" \" ) <nl> + result_fields = query_dict . pop ( \" result_fields \" , None ) <nl> <nl> if not self . server_url : <nl> error = \" Missing configuration key ' server ' \" <nl> def run_query ( self , query ) : <nl> <nl> mappings = self . _get_mappings ( mapping_url ) <nl> <nl> - logger . debug ( json . dumps ( mappings , indent = 4 ) ) <nl> - <nl> params = { \" source \" : json . dumps ( query_dict ) } <nl> logger . debug ( \" Using URL : % s \" , url ) <nl> logger . debug ( \" Using params : % s \" , params ) <nl> def run_query ( self , query ) : <nl> <nl> result_columns = [ ] <nl> result_rows = [ ] <nl> - self . _parse_results ( mappings , None , r . json ( ) , result_columns , result_rows ) <nl> + self . _parse_results ( mappings , result_fields , r . json ( ) , result_columns , result_rows ) <nl> <nl> json_data = json . dumps ( { <nl> \" columns \" : result_columns , <nl> def run_query ( self , query ) : <nl> <nl> register ( Kibana ) <nl> register ( ElasticSearch ) <nl> + <nl>\n", "msg": "Extend ElasticSearch query_runner to support aggregations\n"}
{"diff_id": 43332, "repo": "zulip/zulip\n", "sha": "14f6625569fbdc0af5ba462a9ee177693803c64f\n", "time": "2013-11-18T18:57:01Z\n", "diff": "mmm a / zerver / lib / event_queue . py <nl> ppp b / zerver / lib / event_queue . py <nl> def load_event_queues ( ) : <nl> def send_restart_events ( ) : <nl> event = dict ( type = ' restart ' , server_generation = settings . SERVER_GENERATION ) <nl> for client in clients . itervalues ( ) : <nl> - # All clients get restart events <nl> + if not client . accepts_event_type ( ' restart ' ) : <nl> + continue <nl> client . add_event ( event . copy ( ) ) <nl> <nl> def setup_event_queue ( ) : <nl>\n", "msg": "Don ' t send server restart events to clients that don ' t request them .\n"}
{"diff_id": 43462, "repo": "ytdl-org/youtube-dl\n", "sha": "631f73978c0ee851950ac697dfd73f9092abd3c3\n", "time": "2013-03-04T21:16:42Z\n", "diff": "mmm a / youtube_dl / FileDownloader . py <nl> ppp b / youtube_dl / FileDownloader . py <nl> def extract_info ( self , url ) : <nl> raise <nl> if not suitable_found : <nl> self . trouble ( u ' ERROR : no suitable InfoExtractor : % s ' % url ) <nl> + def extract_info_iterable ( self , urls ) : <nl> + ' ' ' <nl> + Return the videos founded for the urls <nl> + ' ' ' <nl> + results = [ ] <nl> + for url in urls : <nl> + results . extend ( self . extract_info ( url ) ) <nl> + return results <nl> <nl> def process_info ( self , info_dict ) : <nl> \" \" \" Process a single dictionary returned by an InfoExtractor . \" \" \" <nl>\n", "msg": "Add a method for extracting info from a list of urls\n"}
{"diff_id": 43558, "repo": "numpy/numpy\n", "sha": "453172fa1a10aecb9f79cffa9cf1a7233827169d\n", "time": "2009-11-30T17:35:08Z\n", "diff": "mmm a / pavement . py <nl> ppp b / pavement . py <nl> <nl> \" 2 . 6 \" : [ \" / Library / Frameworks / Python . framework / Versions / 2 . 6 / bin / python \" ] <nl> } <nl> <nl> - SSE3_CFG = { ' BLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse3 ' , ' LAPACK ' : r ' C : \\ local \\ lib \\ yop \\ sse3 ' } <nl> - SSE2_CFG = { ' BLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse2 ' , ' LAPACK ' : r ' C : \\ local \\ lib \\ yop \\ sse2 ' } <nl> + SSE3_CFG = { ' ATLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse3 ' } <nl> + SSE2_CFG = { ' ATLAS ' : r ' C : \\ local \\ lib \\ yop \\ sse2 ' } <nl> NOSSE_CFG = { ' BLAS ' : r ' C : \\ local \\ lib \\ yop \\ nosse ' , ' LAPACK ' : r ' C : \\ local \\ lib \\ yop \\ nosse ' } <nl> <nl> SITECFG = { \" sse2 \" : SSE2_CFG , \" sse3 \" : SSE3_CFG , \" nosse \" : NOSSE_CFG } <nl>\n", "msg": "REL : fix atlas detection for SSE2 / SSE3 wininst .\n"}
{"diff_id": 43580, "repo": "zulip/zulip\n", "sha": "be309bc8b1f3dd3264239f058dac7a6f5e04af66\n", "time": "2017-10-26T17:29:17Z\n", "diff": "mmm a / zerver / lib / subdomains . py <nl> ppp b / zerver / lib / subdomains . py <nl> def is_subdomain_root_or_alias ( request ) : <nl> subdomain = _extract_subdomain ( request ) <nl> return not subdomain or subdomain in settings . ROOT_SUBDOMAIN_ALIASES <nl> <nl> - def check_subdomain ( realm_subdomain , user_subdomain ) : <nl> - # type : ( Optional [ Text ] , Text ) - > bool <nl> - if realm_subdomain is not None : <nl> - if realm_subdomain ! = user_subdomain : <nl> - return False <nl> - return True <nl> - <nl> def user_matches_subdomain ( realm_subdomain , user_profile ) : <nl> # type : ( Optional [ Text ] , UserProfile ) - > bool <nl> - return check_subdomain ( realm_subdomain , user_profile . realm . subdomain ) <nl> + if realm_subdomain is None : <nl> + return True <nl> + return user_profile . realm . subdomain = = realm_subdomain <nl> <nl> def is_root_domain_available ( ) : <nl> # type : ( ) - > bool <nl>\n", "msg": "subdomains : Complete the refactor to user_matches_subdomain .\n"}
{"diff_id": 43705, "repo": "ipython/ipython\n", "sha": "a65d8801c46860d0bfb05a3aeb34c88b142c9b1b\n", "time": "2010-01-01T00:49:43Z\n", "diff": "mmm a / IPython / core / pylabtools . py <nl> ppp b / IPython / core / pylabtools . py <nl> def pylab_activate ( user_ns , gui = None , import_all = True ) : <nl> matplotlib . interactive ( True ) <nl> <nl> print \" \" \" <nl> - Welcome to pylab , a matplotlib - based Python environment . <nl> - Backend in use : % s <nl> + Welcome to pylab , a matplotlib - based Python environment [ backend : % s ] . <nl> For more information , type ' help ( pylab ) ' . \" \" \" % backend <nl> <nl> return gui <nl>\n", "msg": "Small fix to info message when pylab starts .\n"}
{"diff_id": 43853, "repo": "home-assistant/core\n", "sha": "431045f03650a772b66811717be197aa14a56f1e\n", "time": "2020-07-01T05:55:31Z\n", "diff": "mmm a / homeassistant / components / volumio / media_player . py <nl> ppp b / homeassistant / components / volumio / media_player . py <nl> async def async_media_pause ( self ) : <nl> else : <nl> await self . send_volumio_msg ( \" commands \" , params = { \" cmd \" : \" pause \" } ) <nl> <nl> + async def async_media_stop ( self ) : <nl> + \" \" \" Send media_stop command to media player . \" \" \" <nl> + await self . send_volumio_msg ( \" commands \" , params = { \" cmd \" : \" stop \" } ) <nl> + <nl> async def async_set_volume_level ( self , volume ) : <nl> \" \" \" Send volume_up command to media player . \" \" \" <nl> await self . send_volumio_msg ( <nl>\n", "msg": "Add media_stop for volumio integration ( )\n"}
{"diff_id": 43903, "repo": "pandas-dev/pandas\n", "sha": "1b53d8864af0ed936f84d0935e2cc360dc9f8de7\n", "time": "2017-03-31T06:40:58Z\n", "diff": "mmm a / pandas / compat / numpy / function . py <nl> ppp b / pandas / compat / numpy / function . py <nl> def __init__ ( self , defaults , fname = None , method = None , <nl> <nl> def __call__ ( self , args , kwargs , fname = None , <nl> max_fname_arg_count = None , method = None ) : <nl> - fname = self . fname if fname is None else fname <nl> - max_fname_arg_count = ( self . max_fname_arg_count if <nl> - max_fname_arg_count is None <nl> - else max_fname_arg_count ) <nl> - method = self . method if method is None else method <nl> - <nl> - if method = = ' args ' : <nl> - validate_args ( fname , args , max_fname_arg_count , self . defaults ) <nl> - elif method = = ' kwargs ' : <nl> - validate_kwargs ( fname , kwargs , self . defaults ) <nl> - elif method = = ' both ' : <nl> - validate_args_and_kwargs ( fname , args , kwargs , <nl> - max_fname_arg_count , <nl> - self . defaults ) <nl> - else : <nl> - raise ValueError ( \" invalid validation method \" <nl> - \" ' { method } ' \" . format ( method = method ) ) <nl> + if args or kwargs : <nl> + fname = self . fname if fname is None else fname <nl> + max_fname_arg_count = ( self . max_fname_arg_count if <nl> + max_fname_arg_count is None <nl> + else max_fname_arg_count ) <nl> + method = self . method if method is None else method <nl> + <nl> + if method = = ' args ' : <nl> + validate_args ( fname , args , max_fname_arg_count , self . defaults ) <nl> + elif method = = ' kwargs ' : <nl> + validate_kwargs ( fname , kwargs , self . defaults ) <nl> + elif method = = ' both ' : <nl> + validate_args_and_kwargs ( fname , args , kwargs , <nl> + max_fname_arg_count , <nl> + self . defaults ) <nl> + else : <nl> + raise ValueError ( \" invalid validation method \" <nl> + \" ' { method } ' \" . format ( method = method ) ) <nl> <nl> <nl> ARGMINMAX_DEFAULTS = dict ( out = None ) <nl>\n", "msg": "Only call validation functions when args / kwargs are passed ( )\n"}
{"diff_id": 44032, "repo": "python/cpython\n", "sha": "cf6bea3dc70977e424ac6b5f1d48b0c1a4dbf186\n", "time": "2000-04-10T01:15:06Z\n", "diff": "mmm a / Lib / distutils / sysconfig . py <nl> ppp b / Lib / distutils / sysconfig . py <nl> <nl> from errors import DistutilsPlatformError <nl> <nl> <nl> - prefix = os . path . normpath ( sys . prefix ) <nl> - exec_prefix = os . path . normpath ( sys . exec_prefix ) <nl> + PREFIX = os . path . normpath ( sys . prefix ) <nl> + EXEC_PREFIX = os . path . normpath ( sys . exec_prefix ) <nl> <nl> <nl> def get_python_inc ( plat_specific = 0 ) : <nl> def get_python_inc ( plat_specific = 0 ) : <nl> ( namely config . h ) . <nl> <nl> \" \" \" <nl> - the_prefix = ( plat_specific and exec_prefix or prefix ) <nl> + prefix = ( plat_specific and EXEC_PREFIX or PREFIX ) <nl> if os . name = = \" posix \" : <nl> - return os . path . join ( the_prefix , \" include \" , \" python \" + sys . version [ : 3 ] ) <nl> + return os . path . join ( prefix , \" include \" , \" python \" + sys . version [ : 3 ] ) <nl> elif os . name = = \" nt \" : <nl> - return os . path . join ( the_prefix , \" Include \" ) # include or Include ? <nl> + return os . path . join ( prefix , \" Include \" ) # include or Include ? <nl> elif os . name = = \" mac \" : <nl> - return os . path . join ( the_prefix , \" Include \" ) <nl> + return os . path . join ( prefix , \" Include \" ) <nl> else : <nl> raise DistutilsPlatformError , \\ <nl> ( \" I don ' t know where Python installs its C header files \" + <nl> def get_python_lib ( plat_specific = 0 , standard_lib = 0 ) : <nl> directory for site - specific modules . <nl> <nl> \" \" \" <nl> - the_prefix = ( plat_specific and exec_prefix or prefix ) <nl> + prefix = ( plat_specific and EXEC_PREFIX or PREFIX ) <nl> <nl> if os . name = = \" posix \" : <nl> - libpython = os . path . join ( the_prefix , <nl> + libpython = os . path . join ( prefix , <nl> \" lib \" , \" python \" + sys . version [ : 3 ] ) <nl> if standard_lib : <nl> return libpython <nl> def get_python_lib ( plat_specific = 0 , standard_lib = 0 ) : <nl> <nl> elif os . name = = \" nt \" : <nl> if standard_lib : <nl> - return os . path . join ( the_prefix , \" Lib \" ) <nl> + return os . path . join ( PREFIX , \" Lib \" ) <nl> else : <nl> - return the_prefix <nl> + return prefix <nl> <nl> elif os . name = = \" mac \" : <nl> if platform_specific : <nl> if standard_lib : <nl> - return os . path . join ( exec_prefix , \" Mac \" , \" Plugins \" ) <nl> + return os . path . join ( EXEC_PREFIX , \" Mac \" , \" Plugins \" ) <nl> else : <nl> raise DistutilsPlatformError , \\ <nl> \" OK , where DO site - specific extensions go on the Mac ? \" <nl> else : <nl> if standard_lib : <nl> - return os . path . join ( prefix , \" Lib \" ) <nl> + return os . path . join ( PREFIX , \" Lib \" ) <nl> else : <nl> raise DistutilsPlatformError , \\ <nl> \" OK , where DO site - specific modules go on the Mac ? \" <nl> def _init_nt ( ) : <nl> g [ ' INCLUDEPY ' ] = get_python_inc ( plat_specific = 0 ) <nl> <nl> g [ ' SO ' ] = ' . pyd ' <nl> - g [ ' exec_prefix ' ] = exec_prefix <nl> + g [ ' exec_prefix ' ] = EXEC_PREFIX <nl> <nl> <nl> def _init_mac ( ) : <nl> def _init_mac ( ) : <nl> # load the installed config . h ( what if not installed ? - still need to <nl> # be able to install packages which don ' t require compilation ) <nl> parse_config_h ( open ( <nl> - os . path . join ( sys . exec_prefix , \" Mac \" , \" Include \" , \" config . h \" ) ) , g ) <nl> + os . path . join ( EXEC_PREFIX , \" Mac \" , \" Include \" , \" config . h \" ) ) , g ) <nl> # set basic install directories <nl> g [ ' LIBDEST ' ] = get_python_lib ( plat_specific = 0 , standard_lib = 1 ) <nl> g [ ' BINLIBDEST ' ] = get_python_lib ( plat_specific = 1 , standard_lib = 1 ) <nl> def _init_mac ( ) : <nl> g [ ' INCLUDEPY ' ] = get_python_inc ( plat_specific = 0 ) <nl> <nl> g [ ' SO ' ] = ' . ppc . slb ' <nl> - g [ ' exec_prefix ' ] = sys . exec_prefix <nl> - print sys . prefix <nl> + g [ ' exec_prefix ' ] = EXEC_PREFIX <nl> + print sys . prefix , PREFIX <nl> <nl> # XXX are these used anywhere ? <nl> - g [ ' install_lib ' ] = os . path . join ( sys . exec_prefix , \" Lib \" ) <nl> - g [ ' install_platlib ' ] = os . path . join ( sys . exec_prefix , \" Mac \" , \" Lib \" ) <nl> + g [ ' install_lib ' ] = os . path . join ( EXEC_PREFIX , \" Lib \" ) <nl> + g [ ' install_platlib ' ] = os . path . join ( EXEC_PREFIX , \" Mac \" , \" Lib \" ) <nl> <nl> <nl> try : <nl>\n", "msg": "Better variable names here and there .\n"}
{"diff_id": 44048, "repo": "ansible/ansible\n", "sha": "069feb5d9c76a0195ec3a262c8940bbe65cf3b7e\n", "time": "2012-09-24T20:37:51Z\n", "diff": "mmm a / lib / ansible / playbook / play . py <nl> ppp b / lib / ansible / playbook / play . py <nl> def _get_vars ( self ) : <nl> raise errors . AnsibleError ( \" ' vars_prompt ' item is missing ' name : ' \" ) <nl> <nl> vname = var [ ' name ' ] <nl> - prompt = \" % s : \" % var . get ( \" prompt \" , vname ) <nl> + prompt = util . template ( None , \" % s : \" % var . get ( \" prompt \" , vname ) , self . vars ) <nl> private = var . get ( \" private \" , True ) <nl> <nl> confirm = var . get ( \" confirm \" , False ) <nl>\n", "msg": "Template the variable prompt to customize the message\n"}
{"diff_id": 44063, "repo": "ansible/ansible\n", "sha": "30ed29ed46c5c3c9cee3769516e335dd61fd2edf\n", "time": "2018-01-23T14:14:56Z\n", "diff": "mmm a / lib / ansible / plugins / filter / ipaddr . py <nl> ppp b / lib / ansible / plugins / filter / ipaddr . py <nl> def network_in_network ( value , test ) : <nl> return False <nl> <nl> <nl> + def reduce_on_network ( value , network ) : <nl> + ' ' ' <nl> + Reduces a list of addresses to only the addresses that match a given network . <nl> + <nl> + : param : value : The list of addresses to filter on . <nl> + : param : network : The network to validate against . <nl> + <nl> + : return : The reduced list of addresses . <nl> + ' ' ' <nl> + # normalize network variable into an ipaddr <nl> + n = _address_normalizer ( network ) <nl> + <nl> + # get first and last addresses as integers to compare value and test ; or cathes value when case is / 32 <nl> + n_first = ipaddr ( ipaddr ( n , ' network ' ) or ipaddr ( n , ' address ' ) , ' int ' ) <nl> + n_last = ipaddr ( ipaddr ( n , ' broadcast ' ) or ipaddr ( n , ' address ' ) , ' int ' ) <nl> + <nl> + # create an empty list to fill and return <nl> + r = [ ] <nl> + <nl> + for address in value : <nl> + # normalize address variables into an ipaddr <nl> + a = _address_normalizer ( address ) <nl> + <nl> + # get first and last addresses as integers to compare value and test ; or cathes value when case is / 32 <nl> + a_first = ipaddr ( ipaddr ( a , ' network ' ) or ipaddr ( a , ' address ' ) , ' int ' ) <nl> + a_last = ipaddr ( ipaddr ( a , ' broadcast ' ) or ipaddr ( a , ' address ' ) , ' int ' ) <nl> + <nl> + if _range_checker ( a_first , n_first , n_last ) and _range_checker ( a_last , n_first , n_last ) : <nl> + r . append ( address ) <nl> + <nl> + return r <nl> + <nl> + <nl> # Returns the SLAAC address within a network for a given HW / MAC address . <nl> # Usage : <nl> # <nl> class FilterModule ( object ) : <nl> ' next_nth_usable ' : next_nth_usable , <nl> ' network_in_network ' : network_in_network , <nl> ' network_in_usable ' : network_in_usable , <nl> + ' reduce_on_network ' : reduce_on_network , <nl> ' nthhost ' : nthhost , <nl> ' previous_nth_usable ' : previous_nth_usable , <nl> ' slaac ' : slaac , <nl>\n", "msg": "Implement the reduce_on_network method to filter a list of IP addresses on a given range . ( ) ( )\n"}
{"diff_id": 44146, "repo": "ansible/ansible\n", "sha": "12431d23cdb47d89318b54575a5342418b26ba82\n", "time": "2018-04-11T19:50:35Z\n", "diff": "mmm a / contrib / inventory / vmware_inventory . py <nl> ppp b / contrib / inventory / vmware_inventory . py <nl> def read_settings ( self ) : <nl> self . debugl ( ' lower keys is % s ' % self . lowerkeys ) <nl> self . skip_keys = list ( config . get ( ' vmware ' , ' skip_keys ' ) . split ( ' , ' ) ) <nl> self . debugl ( ' skip keys is % s ' % self . skip_keys ) <nl> - self . host_filters = list ( config . get ( ' vmware ' , ' host_filters ' ) . split ( ' , ' ) ) <nl> + temp_host_filters = list ( config . get ( ' vmware ' , ' host_filters ' ) . split ( ' } } , ' ) ) <nl> + for host_filter in temp_host_filters : <nl> + host_filter = host_filter . rstrip ( ) <nl> + if host_filter ! = \" \" : <nl> + if not host_filter . endswith ( \" } } \" ) : <nl> + host_filter + = \" } } \" <nl> + self . host_filters . append ( host_filter ) <nl> self . debugl ( ' host filters are % s ' % self . host_filters ) <nl> - self . groupby_patterns = list ( config . get ( ' vmware ' , ' groupby_patterns ' ) . split ( ' , ' ) ) <nl> - self . debugl ( ' groupby patterns are % s ' % self . groupby_patterns ) <nl> <nl> + temp_groupby_patterns = list ( config . get ( ' vmware ' , ' groupby_patterns ' ) . split ( ' } } , ' ) ) <nl> + for groupby_pattern in temp_groupby_patterns : <nl> + groupby_pattern = groupby_pattern . rstrip ( ) <nl> + if groupby_pattern ! = \" \" : <nl> + if not groupby_pattern . endswith ( \" } } \" ) : <nl> + groupby_pattern + = \" } } \" <nl> + self . groupby_patterns . append ( groupby_pattern ) <nl> + self . debugl ( ' groupby patterns are % s ' % self . groupby_patterns ) <nl> # Special feature to disable the brute force serialization of the <nl> # virtulmachine objects . The key name for these properties does not <nl> # matter because the values are just items for a larger list . <nl>\n", "msg": "VMware : Support for multiple jinja filters in vmware_inventory ( )\n"}
{"diff_id": 44195, "repo": "matplotlib/matplotlib\n", "sha": "4b6112e9a87d117839f32ffe5b5b099f44eda781\n", "time": "2016-03-29T15:53:14Z\n", "diff": "mmm a / examples / pylab_examples / quiver_demo . py <nl> ppp b / examples / pylab_examples / quiver_demo . py <nl> <nl> # 6 <nl> plt . figure ( ) <nl> M = np . zeros ( U . shape , dtype = ' bool ' ) <nl> - M [ U . shape [ 0 ] / 3 : 2 * U . shape [ 0 ] / 3 , <nl> - U . shape [ 1 ] / 3 : 2 * U . shape [ 1 ] / 3 ] = True <nl> + XMaskStart = int ( U . shape [ 0 ] / 3 ) <nl> + YMaskStart = int ( U . shape [ 1 ] / 3 ) <nl> + XMaskStop = int ( 2 * U . shape [ 0 ] / 3 ) <nl> + YMaskStop = int ( 2 * U . shape [ 1 ] / 3 ) <nl> + <nl> + M [ XMaskStart : XMaskStop , <nl> + YMaskStart : YMaskStop ] = True <nl> U = ma . masked_array ( U , mask = M ) <nl> V = ma . masked_array ( V , mask = M ) <nl> Q = plt . quiver ( U , V ) <nl>\n", "msg": "Quiver_demo cast indices to int before indexing numpy array\n"}
{"diff_id": 44282, "repo": "python/cpython\n", "sha": "6ecf76ea3617dc32ce7fcba732e2ee9fb48b1e30\n", "time": "2010-12-14T01:22:50Z\n", "diff": "mmm a / Lib / test / test_time . py <nl> ppp b / Lib / test / test_time . py <nl> def test_asctime ( self ) : <nl> def test_asctime_bounding_check ( self ) : <nl> self . _bounds_checking ( time . asctime ) <nl> <nl> + @ unittest . skipIf ( not hasattr ( time , \" tzset \" ) , <nl> + \" time module has no attribute tzset \" ) <nl> def test_tzset ( self ) : <nl> - if not hasattr ( time , \" tzset \" ) : <nl> - return # Can ' t test this ; don ' t want the test suite to fail <nl> <nl> from os import environ <nl> <nl>\n", "msg": "Use skipIf instead of a return when attribute doesn ' t exist .\n"}
{"diff_id": 44428, "repo": "zulip/zulip\n", "sha": "924c5fea2417a2cd78fdcf39cbaa8c7c92f96991\n", "time": "2013-10-21T18:37:36Z\n", "diff": "mmm a / api / zulip / __init__ . py <nl> ppp b / api / zulip / __init__ . py <nl> def generate_option_group ( parser ) : <nl> group . add_option ( ' - v ' , ' - - verbose ' , <nl> action = ' store_true ' , <nl> help = ' Provide detailed output . ' ) <nl> + group . add_option ( ' - - client ' , <nl> + action = ' store ' , <nl> + default = \" API : Python \" , <nl> + help = optparse . SUPPRESS_HELP ) <nl> <nl> return group <nl> <nl> def init_from_options ( options ) : <nl> return Client ( email = options . email , api_key = options . api_key , config_file = options . config_file , <nl> - verbose = options . verbose , site = options . site ) <nl> + verbose = options . verbose , site = options . site , client = options . client ) <nl> <nl> class Client ( object ) : <nl> def __init__ ( self , email = None , api_key = None , config_file = None , <nl>\n", "msg": "api : Add support for specifying client using zulip . init_from_options .\n"}
